<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script></script>
  <title>Scholar Parser</title>
</head>

<body>
  <div id="bar"></div>
  <div id="wrapper">
    <!-- <div class="paper">
            <h1>paper.title</h1>
            <ol>
                <li>
                    <div class="name"><a href="#">citationName</a></div>
                    <div class="author">citation.authors</div>
                    <div class="comment">
                        <ul>
                            <li>comment</li>
                            <li>comment</li>
                        </ul>
                    </div>
                </li>
                <li>
                    ....
                </li>
            </ol>
        </div> -->
  </div>
</body>
<script>
  // 侧边栏js
  var count = 0;
  var fellows = {
    "Daniel Andrews": "IEEE Fellow",
    "David August": "IEEE Fellow",
    "Ewert Bengtsson": "IEEE Fellow",
    "Ricardo Bianchini": "IEEE Fellow, ACM Fellow",
    "Kenneth Birman": "IEEE Fellow",
    "Aaron Bobick": "IEEE Fellow",
    "Azzedine Boukerche": "IEEE Fellow",
    "Rajkumar Buyya": "IEEE Fellow",
    "Christian Cachin": "IEEE Fellow, ACM Fellow",
    "Jiannong Cao": "IEEE Fellow",
    "Joseph Cavallaro": "IEEE Fellow",
    "Elizabeth Chang": "IEEE Fellow",
    "Xiuzhen Cheng": "IEEE Fellow",
    "Henrik Christensen": "IEEE Fellow",
    "Sajal Das": "IEEE Fellow",
    "Dipankar Dasgupta": "IEEE Fellow",
    "Joe Decuir": "IEEE Fellow",
    "Murthy Devarakonda": "IEEE Fellow",
    "Peter Dinda": "IEEE Fellow",
    "Rolf Drechsler": "IEEE Fellow",
    "Stephanie Forrest": "IEEE Fellow",
    "Henry Fuchs": "IEEE Fellow, ACM Fellow",
    "Pascale Fung": "IEEE Fellow",
    "Patrick Girard": "IEEE Fellow",
    "Manimaran Govindarasu": "IEEE Fellow",
    "S. Gunasekaran": "IEEE Fellow",
    "Dan Gusfield": "IEEE Fellow, ACM Fellow",
    "Dan Halperin": "IEEE Fellow, ACM Fellow",
    "Constance Heitmeyer": "IEEE Fellow",
    "Abdelsalam (Sumi) Helal": "IEEE Fellow",
    "Joerg Henkel": "IEEE Fellow, ACM Fellow",
    "Jianying Hu": "IEEE Fellow",
    "Ravishankar Iyer": "IEEE Fellow, ACM Fellow",
    "Qiang Ji": "IEEE Fellow",
    "Hong Jiang": "IEEE Fellow",
    "Anupam Joshi": "IEEE Fellow",
    "Christoforos Kozyrakis": "IEEE Fellow",
    "David Kriegman": "IEEE Fellow",
    "Deepa Kundur": "IEEE Fellow",
    "Edmund Lam": "IEEE Fellow",
    "Steven Levitan": "IEEE Fellow",
    "Keqin Li": "IEEE Fellow",
    "Baochun Li": "IEEE Fellow",
    "Ling Liu": "IEEE Fellow",
    "Cheng-Lin Liu": "IEEE Fellow",
    "Yunhao Liu": "IEEE Fellow, ACM Fellow",
    "Wenjing Lou": "IEEE Fellow",
    "Scott Mahlke": "IEEE Fellow, ACM Fellow",
    "Diana Marculescu": "IEEE Fellow, ACM Fellow",
    "Patrick McDaniel": "IEEE Fellow, ACM Fellow",
    "Hong Mei": "IEEE Fellow, ACM Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ethan Miller": "IEEE Fellow",
    "Stefan Mozar": "IEEE Fellow",
    "Alexandru Nicolau": "IEEE Fellow",
    "Radha Poovendran": "IEEE Fellow",
    "Rasheek Rifaat": "IEEE Fellow",
    "Eric Rotenberg": "IEEE Fellow",
    "Ponnuswamy Sadayappan": "IEEE Fellow",
    "Michael Shebanow": "IEEE Fellow",
    "Wonyong Sung": "IEEE Fellow",
    "James Truchard": "IEEE Fellow",
    "Mahesh Viswanathan": "IEEE Fellow",
    "Geoffrey Webb": "IEEE Fellow",
    "David Weiss": "IEEE Fellow",
    "Yuan Xie": "IEEE Fellow, ACM Fellow",
    "Bulent Yener": "IEEE Fellow",
    "Wang Yi": "IEEE Fellow, ACM Fellow",
    "Moti Yung": "IEEE Fellow, ACM Fellow",
    "Yin Zhang": "IEEE Fellow",
    "Kun Zhou": "IEEE Fellow, ACM Fellow, IEEE/ACM Fellow",
    "Yuanyuan Zhou": "IEEE Fellow, ACM Fellow",
    "David Abramso": "IEEE Fellow",
    "Kiyoharu Aizaw": "IEEE Fellow",
    "Alexandre Alves da Silv": "IEEE Fellow",
    "Lorenzo Alvis": "IEEE Fellow",
    "David Atienza\u2014EPFL": "IEEE Fellow",
    "Ronald Azum": "IEEE Fellow",
    "Michael Branick": "IEEE Fellow",
    "David Brook": "IEEE Fellow",
    "Umit Catalyure": "IEEE Fellow",
    "Tony Cha": "IEEE Fellow",
    "Shigang Che": "IEEE Fellow",
    "Shu-Ching Che": "IEEE Fellow",
    "Degang Che": "IEEE Fellow",
    "Xilin Che": "IEEE Fellow",
    "Armando Colomb": "IEEE Fellow",
    "Tracy Cam": "IEEE Fellow",
    "Lorrie Crano": "IEEE Fellow",
    "Timothy Davi": "IEEE Fellow",
    "Leila De Florian": "IEEE Fellow",
    "Robert Den": "IEEE Fellow",
    "Yixin Dia": "IEEE Fellow",
    "Schahram Dustda": "IEEE Fellow",
    "Dinei Florenci": "IEEE Fellow",
    "Michael Fran": "IEEE Fellow",
    "Fernando Gomid": "IEEE Fellow",
    "Gerhard Hanck": "IEEE Fellow",
    "Edwin Hancoc": "IEEE Fellow",
    "Scott Hauc": "IEEE Fellow",
    "Larry Hec": "IEEE Fellow",
    "Gernot Heise": "IEEE Fellow",
    "Xiaobo H": "IEEE Fellow",
    "Yu Charlie H": "IEEE Fellow",
    "Mahmut Kandemi": "IEEE Fellow",
    "Nam Sung Ki": "IEEE Fellow",
    "Mark Laubac": "IEEE Fellow",
    "Hui Le": "IEEE Fellow",
    "Charles Leiserso": "IEEE Fellow",
    "Xuemin Lin\u2014UNSW": "IEEE Fellow",
    "Stefano Lonard": "IEEE Fellow",
    "Chenyang L": "IEEE Fellow",
    "Shih-Lien L": "IEEE Fellow",
    "Joao Marques Silv": "IEEE Fellow",
    "Nenad Medvidovi": "IEEE Fellow",
    "Dimitri Metaxa": "IEEE Fellow",
    "Frank Muelle": "IEEE Fellow",
    "Alexandros Potamiano": "IEEE Fellow",
    "Calton P": "IEEE Fellow",
    "Wendi Rabiner Heinzelma": "IEEE Fellow",
    "Sreeranga Raja": "IEEE Fellow",
    "Kui Re": "IEEE Fellow",
    "Chris Rowe": "IEEE Fellow",
    "Jagannathan Sarangapan": "IEEE Fellow",
    "Karsten Schwan": "IEEE Fellow",
    "Sudipta Sengupt": "IEEE Fellow",
    "Weisong Sh": "IEEE Fellow",
    "Matteo Sonza Reord": "IEEE Fellow",
    "Diane Thiede Rove": "IEEE Fellow",
    "K. Venugopa": "IEEE Fellow",
    "Sarma Bala Vrudhul": "IEEE Fellow",
    "Cliff Wan": "IEEE Fellow",
    "Jia Wan": "IEEE Fellow",
    "Chengzhong X": "IEEE Fellow",
    "Daniel Zen": "IEEE Fellow",
    "Xi Zhan": "IEEE Fellow",
    "N. Asoka": "IEEE Fellow",
    "Todd Austi": "IEEE Fellow",
    "Alper Buyuktosunogl": "IEEE Fellow",
    "Franck Cappell": "IEEE Fellow",
    "Michael Care": "IEEE Fellow",
    "Luca Carlon": "IEEE Fellow",
    "Edward Chan": "IEEE Fellow",
    "Kiyoung Cho": "IEEE Fellow",
    "Sorin Cotofan": "IEEE Fellow",
    "Robert Cunningha": "IEEE Fellow",
    "Tamal De": "IEEE Fellow",
    "Danny Dole": "IEEE Fellow",
    "Dov Dor": "IEEE Fellow",
    "Frederick Dougli": "IEEE Fellow",
    "Falko Dressle": "IEEE Fellow",
    "Ram Duvvuru Srira": "IEEE Fellow",
    "Sandhya Dwarkada": "IEEE Fellow",
    "Pablo Esteve": "IEEE Fellow",
    "Edward Fo": "IEEE Fellow",
    "Minos Garofalaki": "IEEE Fellow",
    "Soonhoi H": "IEEE Fellow",
    "Saman Halgamug": "IEEE Fellow",
    "Hossam Hassanei": "IEEE Fellow",
    "Payam Heydar": "IEEE Fellow",
    "Hugues Hopp": "IEEE Fellow",
    "Zhenyu Huan": "IEEE Fellow",
    "Deog-Kyoon Jeon": "IEEE Fellow",
    "Hironori Kasahar": "IEEE Fellow",
    "Alvin Lebec": "IEEE Fellow",
    "Kuen-Jon Le": "IEEE Fellow",
    "Hsien-Hsin Le": "IEEE Fellow",
    "Hang L": "IEEE Fellow",
    "Phone Li": "IEEE Fellow",
    "Tieyan Li": "IEEE Fellow",
    "Gabriel Lo": "IEEE Fellow",
    "David Lowthe": "IEEE Fellow",
    "David Luebk": "IEEE Fellow",
    "Joseph Lyle": "IEEE Fellow",
    "Pitu Mirchandan": "IEEE Fellow",
    "Jean Michel Mulle": "IEEE Fellow",
    "Nuria Olive": "IEEE Fellow",
    "Rafail Ostrovsk": "IEEE Fellow",
    "Haesun Par": "IEEE Fellow",
    "Jung-Min Par": "IEEE Fellow",
    "Li-Shiuan Pe": "IEEE Fellow",
    "Lili Qi": "IEEE Fellow",
    "Ravi Ramamoorth": "IEEE Fellow",
    "William Regl": "IEEE Fellow",
    "Reza Rejai": "IEEE Fellow",
    "Gregg Rotherme": "IEEE Fellow",
    "Bernt Schiel": "IEEE Fellow",
    "Michael Schult": "IEEE Fellow",
    "Stanley Sclarof": "IEEE Fellow",
    "Behzad Shahrara": "IEEE Fellow",
    "Hamid Shari": "IEEE Fellow",
    "Cristina Silvan": "IEEE Fellow",
    "Alan Smeato": "IEEE Fellow",
    "Carol Smidt": "IEEE Fellow",
    "Anuj Srivastav": "IEEE Fellow",
    "Sabine Susstrun": "IEEE Fellow",
    "Nuno Vasconcelo": "IEEE Fellow",
    "Jeffrey Vette": "IEEE Fellow",
    "Wenping Wan": "IEEE Fellow",
    "Jianyong Wan": "IEEE Fellow",
    "Chonggang Wan": "IEEE Fellow",
    "David Whalle": "IEEE Fellow",
    "Richard Wolsk": "IEEE Fellow",
    "Ying W": "IEEE Fellow",
    "Alexandre Yakovle": "IEEE Fellow",
    "Mohammed Zak": "IEEE Fellow",
    "Ce Zh": "IEEE Fellow",
    "Anastasia Ailamaki": "IEEE Fellow, ACM Fellow",
    "Walid Are": "IEEE Fellow",
    "Kohtaro Asai": "IEEE Fellow",
    "Marco Caccam": "IEEE Fellow",
    "Linda Cam": "IEEE Fellow",
    "Yiran Che": "IEEE Fellow",
    "Yiu-Ming Cheun": "IEEE Fellow",
    "Gary Christense": "IEEE Fellow",
    "Thomas Coughli": "IEEE Fellow",
    "Ewa Deelma": "IEEE Fellow",
    "Alan Edelma": "IEEE Fellow",
    "Lieven Eeckhou": "IEEE Fellow",
    "Steven Feine": "IEEE Fellow",
    "Pascal Frossar": "IEEE Fellow",
    "Kevin F": "IEEE Fellow",
    "Thomas Furnes": "IEEE Fellow",
    "Mounir Ghogh": "IEEE Fellow",
    "Yihong Gon": "IEEE Fellow",
    "Minyi Gu": "IEEE Fellow",
    "Haibo H": "IEEE Fellow",
    "Tian He\u2014University of Minnesota": "IEEE Fellow",
    "Pan Hu": "IEEE Fellow",
    "Brad Hutching": "IEEE Fellow",
    "Somesh Jh": "IEEE Fellow",
    "Jiaya Ji": "IEEE Fellow",
    "Lynette Jone": "IEEE Fellow",
    "George Karypi": "IEEE Fellow",
    "Angelos Keromyti": "IEEE Fellow",
    "Roberta Klatzk": "IEEE Fellow",
    "Xenofon Koutsouko": "IEEE Fellow",
    "D. Richard Kuh": "IEEE Fellow",
    "Susan Lan": "IEEE Fellow",
    "Tao L": "IEEE Fellow",
    "Qun L": "IEEE Fellow",
    "Jiao Li-chen": "IEEE Fellow",
    "Zhouchen Li": "IEEE Fellow",
    "Giuseppe Lipar": "IEEE Fellow",
    "Jie Li": "IEEE Fellow",
    "Deepankar Medh": "IEEE Fellow",
    "Russell Meie": "IEEE Fellow",
    "Tommaso Melodi": "IEEE Fellow",
    "Abraham Mendelso": "IEEE Fellow",
    "Konstantina Nikit": "IEEE Fellow",
    "Hidetoshi Onoder": "IEEE Fellow",
    "Jignesh Pate": "IEEE Fellow",
    "Joseph Pawlowsk": "IEEE Fellow",
    "Konstantinos Psouni": "IEEE Fellow",
    "Hairong Q": "IEEE Fellow",
    "Bjoern Schulle": "IEEE Fellow",
    "Assaf Schuster\u2014Technion": "IEEE Fellow",
    "Andrew Senio": "IEEE Fellow",
    "Sanjit Seshi": "IEEE Fellow",
    "Jiwu Sh": "IEEE Fellow",
    "Yan Solihi": "IEEE Fellow",
    "Salvatore Stolf": "IEEE Fellow",
    "Peter Ston": "IEEE Fellow",
    "David Stor": "IEEE Fellow",
    "Rahul Sukthanka": "IEEE Fellow",
    "Mark Tehranipoo": "IEEE Fellow",
    "Juergen Teic": "IEEE Fellow",
    "Yingli Tia": "IEEE Fellow",
    "John Tsotso": "IEEE Fellow",
    "Giovanni Vigna\u2014University of California": "IEEE Fellow",
    "Haixun Wan": "IEEE Fellow",
    "Li-C Wan": "IEEE Fellow",
    "Mark Weis": "IEEE Fellow",
    "Laurie William": "IEEE Fellow",
    "Rebecca Wrigh": "IEEE Fellow",
    "Tao Xi": "IEEE Fellow",
    "Yanyong Zhan": "IEEE Fellow",
    "Lei Zhan": "IEEE Fellow",
    "Xiaofang Zho": "IEEE Fellow",
    "Edward Adelso": "IEEE Fellow",
    "Michael Backe": "IEEE Fellow",
    "Meng-fan Chan": "IEEE Fellow",
    "Deming Che": "IEEE Fellow",
    "Jong-Deok Cho": "IEEE Fellow",
    "Paul Cho": "IEEE Fellow",
    "Peter Clou": "IEEE Fellow",
    "Michael Condr": "IEEE Fellow",
    "Kerstin Dautenhah": "IEEE Fellow",
    "Xiaotie Den": "IEEE Fellow",
    "Meng Hwa E": "IEEE Fellow",
    "Joseph Evan": "IEEE Fellow",
    "Robert Fis": "IEEE Fellow",
    "Dimitrios Fotiadi": "IEEE Fellow",
    "Mark Fo": "IEEE Fellow",
    "Anne Gattike": "IEEE Fellow",
    "Simson Garfinke": "IEEE Fellow",
    "Mor Harchol-balte": "IEEE Fellow",
    "Ahmed Hassa": "IEEE Fellow",
    "Xiaodong H": "IEEE Fellow",
    "Ahmed A-g Helm": "IEEE Fellow",
    "Gang Hu": "IEEE Fellow",
    "Hans-arno Jacobse": "IEEE Fellow",
    "Lee Jaeji": "IEEE Fellow",
    "Hai Ji": "IEEE Fellow",
    "Irwin Kin": "IEEE Fellow",
    "Farinaz Koushanfa": "IEEE Fellow",
    "Hai L": "IEEE Fellow",
    "Shaoying Li": "IEEE Fellow",
    "Cristina Lope": "IEEE Fellow",
    "Tim Menzie": "IEEE Fellow",
    "Onur Mutl": "IEEE Fellow",
    "Jason Nie": "IEEE Fellow",
    "Danilo Pa": "IEEE Fellow",
    "Srinivasan Raman": "IEEE Fellow",
    "Mary Ellen Randal": "IEEE Fellow",
    "Amit Roy-chowdhur": "IEEE Fellow",
    "Dan Rubenstei": "IEEE Fellow",
    "Stuart Rubi": "IEEE Fellow",
    "Rajiv Sabherwa": "IEEE Fellow",
    "Kyuseok Shi": "IEEE Fellow",
    "Mei-ling Shy": "IEEE Fellow",
    "Ramesh Sitarama": "IEEE Fellow",
    "Dawn Son": "IEEE Fellow",
    "Chi-keung Tan": "IEEE Fellow",
    "Jian Tan": "IEEE Fellow",
    "Zhuowen T": "IEEE Fellow",
    "Paul Vanoorscho": "IEEE Fellow",
    "Xiaofeng Wan": "IEEE Fellow",
    "Liang Wan": "IEEE Fellow",
    "Simon Warfiel": "IEEE Fellow",
    "John Turner Whitte": "IEEE Fellow",
    "Zhaohui W": "IEEE Fellow",
    "Eric Xin": "IEEE Fellow",
    "Ming-hsuan Yan": "IEEE Fellow",
    "Xiaokang Yan": "IEEE Fellow",
    "Hiroto Yasuur": "IEEE Fellow",
    "Moustafa Yousse": "IEEE Fellow",
    "Yizhou Y": "IEEE Fellow",
    "Daqing Zhan": "IEEE Fellow",
    "Mengjie Zhan": "IEEE Fellow",
    "Lin Zhon": "IEEE Fellow",
    "Jingren Zho": "IEEE Fellow",
    "Lidong Zho": "IEEE Fellow",
    "Michael Zyd": "IEEE Fellow",
    "Hussein Abbas": "IEEE Fellow",
    "Hari Balakrishna": "IEEE Fellow",
    "Vaughn Bet": "IEEE Fellow",
    "Wei Che": "IEEE Fellow",
    "Yingying Che": "IEEE Fellow",
    "Baoquan Che": "IEEE Fellow",
    "Lei Che": "IEEE Fellow",
    "Sangyeun Ch": "IEEE Fellow",
    "Christopher Clifto": "IEEE Fellow",
    "Carolina Cruz-Neir": "IEEE Fellow",
    "Gautam Da": "IEEE Fellow",
    "Jack Davidso": "IEEE Fellow",
    "Shlomi Dole": "IEEE Fellow",
    "Touradj Ebrahim": "IEEE Fellow",
    "Rudolf Eigenman": "IEEE Fellow",
    "Ian Foste": "IEEE Fellow",
    "Edward Fran": "IEEE Fellow",
    "Richard Fujimot": "IEEE Fellow",
    "Johannes Gehrk": "IEEE Fellow",
    "Kristen Grauma": "IEEE Fellow",
    "Song Gu": "IEEE Fellow",
    "Mohammad Hajiaghay": "IEEE Fellow",
    "Mary Hal": "IEEE Fellow",
    "Aaron Hertzman": "IEEE Fellow",
    "Alan Hevne": "IEEE Fellow",
    "Markus Hofman": "IEEE Fellow",
    "Zhenjiang H": "IEEE Fellow",
    "Weijia Ji": "IEEE Fellow",
    "William Kaise": "IEEE Fellow",
    "Ramesh Karr": "IEEE Fellow",
    "Sven Koeni": "IEEE Fellow",
    "Yun L": "IEEE Fellow",
    "Mo L": "IEEE Fellow",
    "Richard Lippman": "IEEE Fellow",
    "Xue Li": "IEEE Fellow",
    "Michael Mitzenmache": "IEEE Fellow",
    "Mohamed Mokbel\u2014University of Minnesota": "IEEE Fellow",
    "Saeid Nahavand": "IEEE Fellow",
    "Marc A. Najor": "IEEE Fellow",
    "Partha Pand": "IEEE Fellow",
    "Panagiotis Papadimitrato": "IEEE Fellow",
    "Punam Sah": "IEEE Fellow",
    "Vivek Sarka": "IEEE Fellow",
    "Evgenia Smirn": "IEEE Fellow",
    "Joshua Smit": "IEEE Fellow",
    "Alex Snoere": "IEEE Fellow",
    "Gookwon Su": "IEEE Fellow",
    "Peter Varma": "IEEE Fellow",
    "Hongyi W": "IEEE Fellow",
    "Yang Xian": "IEEE Fellow",
    "Hui Xion": "IEEE Fellow",
    "Laurence Tianruo Yan": "IEEE Fellow",
    "Jieping Y": "IEEE Fellow",
    "Tong Zhan": "IEEE Fellow",
    "Yan Zhan": "IEEE Fellow",
    "Zhongfei Zhan": "IEEE Fellow",
    "Gail-Joon Ahn": "IEEE Fellow",
    "Kemal Akkaya": "IEEE Fellow",
    "Jason H. Anderson": "IEEE Fellow",
    "Vijayalakshmi Atluri": "IEEE Fellow",
    "Raymond G. Beausoleil": "IEEE Fellow",
    "Mark N. Billinghurst": "IEEE Fellow",
    "Carlos A. Busso": "IEEE Fellow",
    "Srdjan Capkun": "IEEE Fellow, ACM Fellow",
    "Yuan-Hao Chang": "IEEE Fellow",
    "Haibo Chen": "IEEE Fellow",
    "Hao Chen": "IEEE Fellow",
    "Guihai Chen": "IEEE Fellow",
    "Yixin Chen": "IEEE Fellow",
    "Shing C. Cheung": "IEEE Fellow",
    "Frederic Chong": "IEEE Fellow",
    "Andre M. Dehon": "IEEE Fellow",
    "Wei Ding": "IEEE Fellow",
    "Wenliang Du": "IEEE Fellow",
    "Juan E Gilbert": "IEEE Fellow, ACM Fellow",
    "Amar Gupta": "IEEE Fellow",
    "Chen He": "IEEE Fellow",
    "Derek W. Hoiem": "IEEE Fellow",
    "Bin Hu": "IEEE Fellow",
    "Shuiwang Ji": "IEEE Fellow",
    "Zhi Jin": "IEEE Fellow",
    "James B. Joshi": "IEEE Fellow",
    "Shin-Ichi Kawamura": "IEEE Fellow",
    "Carl Kesselman": "IEEE Fellow, ACM Fellow",
    "Tadayoshi Kohno": "IEEE Fellow",
    "Minglu Li": "IEEE Fellow",
    "Chen Li": "IEEE Fellow",
    "Guoliang Li": "IEEE Fellow",
    "Jia Li": "IEEE Fellow",
    "Haibin Ling": "IEEE Fellow",
    "Wei Liu": "IEEE Fellow",
    "Ce Liu": "IEEE Fellow",
    "Xiaoming Liu": "IEEE Fellow",
    "Rajit Manohar": "IEEE Fellow",
    "James B. Michael": "IEEE Fellow",
    "Tamara Munzner": "IEEE Fellow",
    "Premkumar Natarajan": "IEEE Fellow",
    "Suman Nath": "IEEE Fellow",
    "Ye Ouyang": "IEEE Fellow",
    "Srinivasan Parthasarathy": "IEEE Fellow",
    "Hanspeter Pfister": "IEEE Fellow, ACM Fellow",
    "Vir V. Phoha": "IEEE Fellow",
    "Moinuddin K. Qureshi": "IEEE Fellow",
    "Sherief M. Reda": "IEEE Fellow",
    "Miguel Raul D. Rodrigues": "IEEE Fellow",
    "Eunice E Santos": "IEEE Fellow",
    "Houbing Song": "IEEE Fellow",
    "Ankur Srivastava": "IEEE Fellow",
    "Zhendong Su": "IEEE Fellow, ACM Fellow",
    "Ke Tang": "IEEE Fellow",
    "Birgit Vogel-heuser": "IEEE Fellow",
    "Jianping Wang": "IEEE Fellow",
    "Jue Wang": "IEEE Fellow",
    "Wei Wang": "IEEE Fellow, ACM Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tilman Wolf": "IEEE Fellow",
    "Yongwei Wu": "IEEE Fellow",
    "Dongrui Wu": "IEEE Fellow",
    "Xiaokui Xiao": "IEEE Fellow",
    "Jingling Xue": "IEEE Fellow",
    "Danfeng Yao": "IEEE Fellow",
    "Shui Yu": "IEEE Fellow",
    "Gang Zhou": "IEEE Fellow",
    "Jun Zhu": "IEEE Fellow, IEEE Fellow, AE of TPAMI, AI",
    "Xingquan Zhu": "IEEE Fellow",
    "Chengqing Zong": "IEEE Fellow",
    "Tarek Abdelzaher": "IEEE Fellow, ACM Fellow",
    "Rajeev Balasubramonian": "IEEE Fellow",
    "David Basin": "IEEE Fellow, ACM Fellow",
    "Wim Bogaerts": "IEEE Fellow",
    "Achintya Bhowmik": "IEEE Fellow",
    "Kirk Cameron": "IEEE Fellow",
    "Calin Cascaval": "IEEE Fellow",
    "Bruce Croft": "IEEE Fellow, ACM Fellow",
    "Bronis De Supinski": "IEEE Fellow",
    "Sebastian Elbaum": "IEEE Fellow, ACM Fellow",
    "Natalie Enright Jerger": "IEEE Fellow",
    "Michael Ernst": "IEEE Fellow",
    "Evgeniy Gabrilovich": "IEEE Fellow, ACM Fellow",
    "James Gee": "IEEE Fellow",
    "Yolanda Gil": "IEEE Fellow, ACM Fellow",
    "Matthias Grossglauser": "IEEE Fellow",
    "Wolfgang Heidrich": "IEEE Fellow",
    "Stephen Hodges": "IEEE Fellow",
    "Ayanna Howard": "IEEE Fellow",
    "Bruce Jacob": "IEEE Fellow",
    "Daniel Jimenez": "IEEE Fellow",
    "Stefanos Kaxiras": "IEEE Fellow",
    "Kimberly Keeton": "IEEE Fellow, ACM Fellow",
    "Craig Knoblock": "IEEE Fellow, ACM Fellow",
    "Svetlana Lazebnik": "IEEE Fellow",
    "Huadong Ma": "IEEE Fellow",
    "Kyoung Mu Lee": "IEEE Fellow",
    "Wenke Lee": "IEEE Fellow, ACM Fellow",
    "Wonjun Lee": "IEEE Fellow",
    "Houqiang Li": "IEEE Fellow",
    "Ninghui Li": "IEEE Fellow, ACM Fellow",
    "Xiaofeng Liao": "IEEE Fellow",
    "Shixia Liu": "IEEE Fellow",
    "Le Lu": "IEEE Fellow",
    "Jose Martinez": "IEEE Fellow",
    "Sharad Mehrotra": "IEEE Fellow",
    "Prabhat Mishra": "IEEE Fellow",
    "Jose Moreira": "IEEE Fellow",
    "Katherine Morse": "IEEE Fellow",
    "Andreas Moshovos": "IEEE Fellow, ACM Fellow",
    "Alessandro Orso": "IEEE Fellow",
    "John Owens": "IEEE Fellow",
    "Ai-chun Pang": "IEEE Fellow",
    "Pietro Perona": "IEEE Fellow",
    "Louiqa Raschid": "IEEE Fellow, ACM Fellow",
    "Martin Reddy": "IEEE Fellow",
    "Grigore Rosu": "IEEE Fellow",
    "Dieter Schmalstieg": "IEEE Fellow",
    "Julia Schnabel": "IEEE Fellow",
    "Eve Schooler": "IEEE Fellow",
    "Alla Sheffer": "IEEE Fellow, ACM Fellow",
    "Richa Singh": "IEEE Fellow",
    "Willy Susilo": "IEEE Fellow",
    "Jie Tang": "IEEE Fellow, ACM Fellow",
    "My Thai": "IEEE Fellow",
    "Vincent Tseng": "IEEE Fellow",
    "Filip De Turck": "IEEE Fellow",
    "Jaideep Vaidya": "IEEE Fellow",
    "Wil Van Der Aalst": "IEEE Fellow, ACM Fellow",
    "Ganesh Venayagamoorthy": "IEEE Fellow",
    "Lizhe Wan": "IEEE Fellow",
    "Xiaorui Wang": "IEEE Fellow",
    "Yingxu Wang": "IEEE Fellow",
    "Jingyi Yu": "IEEE Fellow, IEEE Fellow, AE of TPAMI, TIP, CVIU",
    "Junsong Yuan": "IEEE Fellow",
    "Lintao Zhang": "IEEE Fellow",
    "Yu Zheng": "IEEE Fellow",
    "Thomas Zimmermann": "IEEE Fellow, ACM Fellow",
    "Murali Annavaram": "IEEE Fellow",
    "Grigoris Antoniou": "IEEE Fellow",
    "Sujata Banerjee": "IEEE Fellow",
    "Cullen Bash": "IEEE Fellow",
    "Boualem Benatallah": "IEEE Fellow",
    "Suparna Bhattacharya": "IEEE Fellow",
    "M Brian Blake": "IEEE Fellow",
    "Anita Carleton": "IEEE Fellow",
    "Samarjit Chakraborty": "IEEE Fellow",
    "Nitesh Chawla": "IEEE Fellow, ACM Fellow",
    "Jinjun Chen": "IEEE Fellow",
    "Minghua Chen": "IEEE Fellow",
    "Chee-yee Chong": "IEEE Fellow",
    "Walter Cleaveland": "IEEE Fellow",
    "Mauro Conti": "IEEE Fellow",
    "Tarek El-Bawab": "IEEE Fellow",
    "Sonia Fahmy": "IEEE Fellow",
    "Martin Farach-Colton": "IEEE Fellow, ACM Fellow",
    "Bonnie Ferri": "IEEE Fellow",
    "Giancarlo Fortino": "IEEE Fellow",
    "Xiaoming Fu": "IEEE Fellow",
    "Gerhard Hancke": "IEEE Fellow",
    "Torsten Hoefler": "IEEE Fellow, ACM Fellow",
    "Sun-yuan Hsieh": "IEEE Fellow",
    "Ihab Ilyas": "IEEE Fellow",
    "Yusheng Ji": "IEEE Fellow",
    "Admela Jukan": "IEEE Fellow",
    "Murat Kantarcioglu": "IEEE Fellow",
    "Latifur Khan": "IEEE Fellow",
    "Yves Letraon": "IEEE Fellow",
    "Keqiu Li": "IEEE Fellow",
    "Feifei Li": "IEEE Fellow, ACM Fellow",
    "Qing Li": "IEEE Fellow",
    "David Lo": "IEEE Fellow",
    "Robyn Lutz": "IEEE Fellow",
    "Sudip Misra": "IEEE Fellow",
    "Hiroyuki Mizuno": "IEEE Fellow",
    "Zhuoqing Morley Mao": "IEEE Fellow",
    "Masato Motomura": "IEEE Fellow",
    "Vittorio Murino": "IEEE Fellow",
    "Isaac Nassi": "IEEE Fellow",
    "Sam Noh": "IEEE Fellow",
    "Adrian Perrig": "IEEE Fellow, ACM Fellow",
    "Gopal Pingali": "IEEE Fellow",
    "Sorel Reisman": "IEEE Fellow",
    "Shiguang Shan": "IEEE Fellow",
    "Heng Tao Shen": "IEEE Fellow, ACM Fellow",
    "Timothy Sherwood": "IEEE Fellow",
    "S Sudarshan": "IEEE Fellow",
    "Rajeev Thakur": "IEEE Fellow",
    "Yonghong Tian": "IEEE Fellow",
    "Hanghang Tong": "IEEE Fellow",
    "Shambhu Upadhyaya": "IEEE Fellow",
    "Marten Van Dijk": "IEEE Fellow",
    "Mayank Vatsa": "IEEE Fellow",
    "Valeriy Vyatkin": "IEEE Fellow",
    "Dongmei Wang": "IEEE Fellow",
    "Haifeng Wang": "IEEE Fellow",
    "Jingdong Wang": "IEEE Fellow, Fellow of IAPR and IEEE, Associate Editor of IEEE TPAMI, IJCV, IEEE TMM, and IEEE TCSVT, area chair of CVPR, ICCV, ECCV, ACM MM, IJCAI, and AAAI",
    "Jun Wang": "IEEE Fellow",
    "Gregory Welch": "IEEE Fellow",
    "Andrew Wolfe": "IEEE Fellow",
    "Cathy Wu": "IEEE Fellow",
    "Xing Xie": "IEEE Fellow",
    "Li Xiong": "IEEE Fellow",
    "Ying Xu": "IEEE Fellow",
    "Ruigang Yang": "IEEE Fellow",
    "Zheng Yang": "IEEE Fellow",
    "Jiguo Yu": "IEEE Fellow",
    "Guoying Zhao": "IEEE Fellow",
    "Jacob Abraham": "IEEE Fellow, ACM Fellow",
    "Miron Abramovici": "IEEE Fellow",
    "Ramesh Agarwal": "IEEE Fellow",
    "Tilak Agerwala": "IEEE Fellow",
    "J Aggarwal": "IEEE Life Fellow",
    "Gul Agha": "IEEE Fellow, ACM Fellow",
    "Vishwani Agrawal": "IEEE Fellow",
    "Dharma Agrawal": "IEEE Fellow",
    "Prathima Agrawal": "IEEE Fellow",
    "A Agrawala": "IEEE Fellow",
    "Ishfaq Ahmad": "IEEE Fellow",
    "Alfred Aho": "IEEE Life Fellow",
    "Narendra Ahuja": "IEEE Fellow, ACM Fellow",
    "Alessandro Alberigi": "IEEE Life Fellow",
    "Robert Alden": "IEEE Life Fellow",
    "Charles Alexander": "IEEE Fellow",
    "Mubarak Ali Shah": "IEEE Fellow, ACM Fellow",
    "Frances Allen": "IEEE Fellow, ACM Fellow",
    "George Almasi": "IEEE Life Fellow",
    "Daniel Alspach": "IEEE Life Fellow",
    "Rajeev Alur": "IEEE Fellow, ACM Fellow",
    "Kitsutaro Amano": "IEEE Life Fellow",
    "Anthony Ambler": "IEEE Fellow",
    "Gene Amdahl": "IEEE Life Fellow",
    "Mostafa Ammar": "IEEE Fellow, ACM Fellow",
    "W.Cleon Anderson": "IEEE Life Fellow",
    "F Andrews": "IEEE Life Fellow",
    "Panayotis Antsaklis": "IEEE Fellow",
    "Ronald Arkin": "IEEE Fellow",
    "Gaston Arredondo": "IEEE Life Fellow",
    "Mr Arvind": "IEEE Fellow",
    "Minoru Asada": "IEEE Fellow",
    "Jaakko Astola": "IEEE Fellow",
    "Karl Johan Astrom": "IEEE Life Fellow",
    "A Avizienis": "IEEE Life Fellow",
    "J Aylor": "IEEE Fellow",
    "Hrvoje Babic": "IEEE Life Fellow",
    "Giorgio Baccarani": "IEEE Fellow",
    "M Bachynski": "IEEE Life Fellow",
    "Jean Bacon": "IEEE Fellow",
    "J Baer": "IEEE Life Fellow",
    "Radhakisan Baheti": "IEEE Fellow",
    "Paramvir Bahl": "IEEE Fellow",
    "Henry Baird": "IEEE Fellow",
    "Ruzena Bajcsy": "IEEE Life Fellow, ACM Fellow",
    "Prithviraj Banerjee": "IEEE Fellow, ACM Fellow",
    "John Baras": "IEEE Fellow",
    "Mario Barbacci": "IEEE Fellow",
    "P Bardell": "IEEE Life Fellow",
    "Victor Basili": "IEEE Fellow, ACM Fellow",
    "C Baugh": "IEEE Fellow",
    "John Bay": "IEEE Fellow",
    "Magdy Bayoumi": "IEEE Fellow",
    "W Beam": "IEEE Life Fellow",
    "Joanne Bechta Dugan": "IEEE Fellow",
    "Bernd Becker": "IEEE Fellow",
    "Miroslav Begovic": "IEEE Fellow",
    "Nicholas Begovich": "IEEE Life Fellow",
    "George Bekey": "IEEE Life Fellow",
    "Richard Belgard": "IEEE Fellow",
    "C Bell": "IEEE Life Fellow",
    "Jon Benediktsson": "IEEE Fellow",
    "Albert Benveniste": "IEEE Fellow",
    "L Beranek": "IEEE Life Fellow",
    "Reinaldo Bergamaschi": "IEEE Fellow",
    "Hal Berghel": "IEEE Fellow, ACM Fellow",
    "Arthur Bernstein": "IEEE Life Fellow",
    "Lawrence Bernstein": "IEEE Life Fellow, ACM Fellow",
    "P Bruce Berra": "IEEE Fellow",
    "Elisa Bertino": "IEEE Fellow, ACM Fellow",
    "Bir Bhanu": "IEEE Fellow",
    "Vijay Bhargava": "IEEE Fellow",
    "Bharat Bhargava": "IEEE Fellow",
    "Vijay Bhatkar": "IEEE Fellow",
    "Prabir Bhattacharya": "IEEE Fellow",
    "Bhargab Bhattacharya": "IEEE Fellow",
    "Laxmin Bhuyan": "IEEE Fellow",
    "Josef Bigun": "IEEE Fellow",
    "Donald Bitzer": "IEEE Life Fellow",
    "Andrew Blake": "IEEE Fellow",
    "E Bloch": "IEEE Life Fellow",
    "Gregor Bochmann": "IEEE Fellow",
    "Barry Boehm": "IEEE Fellow, ACM Fellow",
    "S Bokhari": "IEEE Fellow",
    "Duane Boning": "IEEE Fellow",
    "Piero Bonissone": "IEEE Fellow",
    "T Bonn": "IEEE Life Fellow",
    "Joseph Bordogna": "IEEE Life Fellow",
    "Bella Bose": "IEEE Fellow, ACM Fellow",
    "Pradip Bose": "IEEE Fellow",
    "Nikolaos Bourbakis": "IEEE Fellow",
    "K Bowles": "IEEE Life Fellow",
    "Kevin Bowyer": "IEEE Fellow",
    "Kim Boyer": "IEEE Fellow",
    "Ronald Brachman": "IEEE Fellow",
    "E Bradburd": "IEEE Life Fellow",
    "Jon Bredeson": "IEEE Life Fellow",
    "Richard Brent": "IEEE Fellow",
    "Melvin Breuer": "IEEE Life Fellow",
    "Joe Brewer": "IEEE Life Fellow",
    "Joseph Bronzino": "IEEE Life Fellow",
    "F Brooks": "IEEE Life Fellow",
    "D Brown": "IEEE Life Fellow",
    "Jehoshua Bruck": "IEEE Fellow",
    "C Burckhardt": "IEEE Life Fellow",
    "Walter Burkhard": "IEEE Fellow",
    "A Bush": "IEEE Life Fellow",
    "Michael Bushnell": "IEEE Fellow",
    "Jon Butler": "IEEE Fellow",
    "Luis Cabrera": "IEEE Fellow",
    "J. T. Cain": "IEEE Life Fellow",
    "Virginio Cantoni": "IEEE Fellow",
    "Cyrus Cantrell": "IEEE Fellow",
    "Gerard Capraro": "IEEE Fellow",
    "Larry Carin": "IEEE Fellow",
    "James Carlo": "IEEE Fellow",
    "M Carpentier": "IEEE Life Fellow",
    "Bill Carroll": "IEEE Life Fellow",
    "Doris Carver": "IEEE Fellow",
    "R Case": "IEEE Life Fellow",
    "Manuel Castro": "IEEE Fellow",
    "Francky V Catthoor": "IEEE Fellow",
    "Ralph Cavin": "IEEE Life Fellow",
    "Nicholas Cercone": "IEEE Fellow",
    "J Chadwick": "IEEE Life Fellow",
    "Krishnendu Chakrabarty": "IEEE Fellow, ACM Fellow",
    "Sreejit Chakravarty": "IEEE Fellow",
    "Donald Chamberlin": "IEEE Fellow, ACM Fellow",
    "B Chandrasekaran": "IEEE Life Fellow",
    "Shih Fu Chang": "IEEE Fellow, ACM Fellow",
    "Jin-Fu Chang": "IEEE Fellow",
    "Shi-Kuo Chang": "IEEE Fellow",
    "Carl Chang": "IEEE Fellow",
    "Hou Chaohuan": "IEEE Fellow",
    "Ramalingam Chellappa": "IEEE Fellow, ACM Fellow",
    "Yung-Chang Chen": "IEEE Fellow",
    "Wen-Tsuen Chen": "IEEE Fellow",
    "Di Chen": "IEEE Life Fellow",
    "Kwang-Ting Cheng": "IEEE Fellow",
    "Nim Cheung": "IEEE Fellow",
    "George Chiu": "IEEE Fellow",
    "Imrich Chlamtac": "IEEE Fellow, ACM Fellow",
    "Philip Chou": "IEEE Fellow",
    "Wu Chou": "IEEE Fellow",
    "W Chou": "IEEE Life Fellow",
    "Alok Choudhary": "IEEE Fellow, ACM Fellow",
    "C Chow": "IEEE Life Fellow",
    "D Christiansen": "IEEE Life Fellow",
    "Wesley Chu": "IEEE Life Fellow",
    "Pau Choo Chung": "IEEE Fellow",
    "Jen-Yao Chung": "IEEE Fellow",
    "Mehmet Civanlar": "IEEE Fellow",
    "D Clark": "IEEE Fellow",
    "Grace Clark": "IEEE Fellow",
    "Edmund Clarke": "IEEE Fellow, ACM Fellow",
    "Jean-Louis Coatrieux": "IEEE Fellow",
    "Robert Colwell": "IEEE Fellow",
    "Susan Conry": "IEEE Fellow",
    "Thomas Conte": "IEEE Fellow",
    "David Cooper": "IEEE Life Fellow",
    "John Copeland": "IEEE Life Fellow",
    "F Corbato": "IEEE Life Fellow",
    "Peter Ian Corke": "IEEE Fellow",
    "Ingemar Cox": "IEEE Fellow",
    "Jerome Cox": "IEEE Life Fellow",
    "Melba Crawford": "IEEE Fellow",
    "David Culler": "IEEE Fellow, ACM Fellow",
    "Jane Cullum": "IEEE Fellow",
    "Karl Current": "IEEE Fellow",
    "Bill Curtis": "IEEE Fellow, ACM Fellow",
    "George Cybenko": "IEEE Fellow",
    "Luigi Dadda": "IEEE Life Fellow",
    "Anton Dahbura": "IEEE Fellow",
    "William Dally": "IEEE Fellow, ACM Fellow",
    "Patricia Daniels": "IEEE Fellow",
    "Frederica Darema": "IEEE Fellow",
    "John Darringer": "IEEE Fellow",
    "Chita Das": "IEEE Fellow",
    "Sunil Das": "IEEE Life Fellow",
    "Belur Dasarathy": "IEEE Fellow",
    "Edward Davidson": "IEEE Life Fellow",
    "Anthony Davies": "IEEE Life Fellow",
    "Larry Davis": "IEEE Fellow",
    "Alan Davis": "IEEE Fellow",
    "C Davis": "IEEE Life Fellow",
    "W Kenneth Dawson": "IEEE Life Fellow",
    "Hugo De Man": "IEEE Fellow",
    "Giovanni De Micheli": "IEEE Fellow",
    "Bart De Moor": "IEEE Fellow",
    "Renato De Mori": "IEEE Fellow",
    "Mark Dean": "IEEE Fellow",
    "Maurizio Decina": "IEEE Fellow",
    "Jean-D Decotignie": "IEEE Fellow",
    "Edward Delp": "IEEE Fellow",
    "Thomas Demarco": "IEEE Fellow",
    "Serge Demidenko": "IEEE Fellow",
    "James Demmel": "IEEE Fellow, ACM Fellow",
    "Peter Denning": "IEEE Life Fellow",
    "J Dennis": "IEEE Life Fellow",
    "Narsingh Deo": "IEEE Life Fellow, ACM Fellow",
    "Srinivas Devadas": "IEEE Fellow, ACM Fellow",
    "P Dewilde": "IEEE Fellow",
    "Atam Dhawan": "IEEE Fellow",
    "Sang Hoo Dhong": "IEEE Fellow",
    "D Dietmeyer": "IEEE Life Fellow",
    "Tharam Dillon": "IEEE Fellow",
    "Stephen Director": "IEEE Fellow",
    "Francois Dolivo": "IEEE Fellow",
    "Jack Dongarra": "IEEE Fellow, ACM Fellow",
    "Arwin Dougal": "IEEE Life Fellow",
    "Mihai Draganescu": "IEEE Life Fellow",
    "K Drangeid": "IEEE Life Fellow",
    "Larry Druffel": "IEEE Life Fellow, ACM Fellow",
    "David Du": "IEEE Fellow",
    "Pradeep Dubey": "IEEE Fellow",
    "Michel Dubois": "IEEE Fellow, ACM Fellow",
    "Stephen Dukes": "IEEE Fellow",
    "James Duncan": "IEEE Fellow",
    "Edmund Durfee": "IEEE Fellow",
    "Hugh Durrant-Whyt": "IEEE Fellow",
    "Nikil Dutt": "IEEE Fellow",
    "Charles Dyer": "IEEE Fellow",
    "John Eidson": "IEEE Life Fellow",
    "H Eisner": "IEEE Life Fellow",
    "Mohamed El-Hawary": "IEEE Fellow",
    "L Ellis": "IEEE Life Fellow",
    "Joel Emer": "IEEE Fellow, ACM Fellow",
    "Sverre Eng": "IEEE Life Fellow",
    "Gerald Engel": "IEEE Fellow",
    "Irving Engelson": "IEEE Life Fellow",
    "Jose Epifaniodafranca": "IEEE Fellow",
    "Milos Ercegovac": "IEEE Fellow",
    "Rolf Ernst": "IEEE Fellow",
    "Edward Ernst": "IEEE Life Fellow",
    "Raffaele Esposito": "IEEE Life Fellow",
    "Thelma Estrin": "IEEE Life Fellow",
    "Robert Everett": "IEEE Life Fellow",
    "Yuguang Fang": "IEEE Fellow, ACM Fellow",
    "Robert Fano": "IEEE Life Fellow",
    "K Fegley": "IEEE Life Fellow",
    "Ephraim Feig": "IEEE Fellow",
    "Stuart Feldman": "IEEE Fellow, ACM Fellow",
    "Dagan Feng": "IEEE Fellow",
    "Tse-Yun Feng": "IEEE Life Fellow, ACM Fellow",
    "Domenico Ferrari": "IEEE Life Fellow, ACM Fellow",
    "Wolfgang Fichtner": "IEEE Fellow",
    "M Fischler": "IEEE Life Fellow",
    "Tor Fjeldly": "IEEE Fellow",
    "M Flynn": "IEEE Life Fellow",
    "James Foley": "IEEE Life Fellow",
    "Jose Fortes": "IEEE Fellow",
    "Thomas Fortmann": "IEEE Life Fellow",
    "H Frank": "IEEE Life Fellow",
    "Mark Franklin": "IEEE Life Fellow",
    "Paul Franzon": "IEEE Fellow",
    "Luigi Fratta": "IEEE Fellow",
    "Herbert Freeman": "IEEE Life Fellow, ACM Fellow",
    "P Freeman": "IEEE Life Fellow",
    "Ophir Frieder": "IEEE Fellow, ACM Fellow",
    "B Friedland": "IEEE Life Fellow",
    "Ivan Frisch": "IEEE Life Fellow",
    "Arrigo Frisiani": "IEEE Life Fellow",
    "F Froehlich": "IEEE Life Fellow",
    "Victor Frost": "IEEE Fellow",
    "W Kent Fuchs": "IEEE Fellow",
    "Hiromichi Fujisawa": "IEEE Fellow",
    "Hideo Fujiwara": "IEEE Fellow",
    "Keinosuke Fukunaga": "IEEE Life Fellow",
    "Daniel Gajski": "IEEE Life Fellow",
    "U Galil": "IEEE Life Fellow",
    "R Gallager": "IEEE Life Fellow",
    "Kenneth Galloway": "IEEE Life Fellow",
    "Daniel Gamota": "IEEE Fellow",
    "Karl Ganzhorn": "IEEE Life Fellow",
    "Guang Gao": "IEEE Fellow, ACM Fellow",
    "Oscar Garcia": "IEEE Life Fellow",
    "Vijay Garg": "IEEE Fellow",
    "Harvey Garner": "IEEE Life Fellow",
    "Jean-Luc Gaudiot": "IEEE Fellow",
    "Gerard Gaynor": "IEEE Life Fellow",
    "Charles Gear": "IEEE Life Fellow",
    "Randy Geiger": "IEEE Fellow",
    "Nicholas Georganas": "IEEE Fellow",
    "Mario Gerla": "IEEE Fellow, ACM Fellow",
    "Arif Ghafoor": "IEEE Fellow",
    "Carlo Ghezzi": "IEEE Fellow, ACM Fellow",
    "Joydeep Ghosh": "IEEE Fellow",
    "Sakti Ghosh": "IEEE Life Fellow",
    "Barry Gilbert": "IEEE Fellow",
    "Elmer Gilbert": "IEEE Life Fellow",
    "C Lee Giles": "IEEE Fellow",
    "Wolfgang Giloi": "IEEE Life Fellow",
    "Richard Gitlin": "IEEE Life Fellow",
    "Manfred Glesner": "IEEE Fellow",
    "Amrit Goel": "IEEE Life Fellow",
    "Kenneth Goff": "IEEE Life Fellow",
    "Maya Gokhale": "IEEE Fellow",
    "Jacob Goldberg": "IEEE Life Fellow",
    "Dmitry Goldgof": "IEEE Fellow",
    "S Jamaloddin Golestani": "IEEE Fellow",
    "Forouzan Golshani": "IEEE Fellow",
    "Cesar Gonzales": "IEEE Fellow",
    "Mario Gonzalez": "IEEE Life Fellow",
    "David Goodenough": "IEEE Fellow",
    "James Goodman": "IEEE Fellow, ACM Fellow",
    "David Goodman": "IEEE Life Fellow",
    "Inder Gopal": "IEEE Fellow",
    "B Gopinath": "IEEE Fellow",
    "Steven Gorshe": "IEEE Fellow",
    "Satoshi Goto": "IEEE Fellow",
    "Venu Govindaraju": "IEEE Fellow, ACM Fellow",
    "Ambuj Goyal": "IEEE Fellow, ACM Fellow",
    "Hans Graf": "IEEE Fellow",
    "Martin Graham": "IEEE Life Fellow",
    "James Greenleaf": "IEEE Life Fellow",
    "W Eric Grimson": "IEEE Fellow",
    "Stephen Grossberg": "IEEE Fellow",
    "Michael Gschwind": "IEEE Fellow",
    "Walter Guggenbuehl": "IEEE Life Fellow",
    "Rajesh Gupta": "IEEE Fellow, ACM Fellow",
    "Rajiv Gupta": "IEEE Fellow, ACM Fellow",
    "Dong Ha": "IEEE Fellow",
    "Susan Hackwood": "IEEE Fellow",
    "J Haddad": "IEEE Life Fellow",
    "Gregory Hager": "IEEE Fellow, ACM Fellow",
    "Kazuo Hagimoto": "IEEE Fellow",
    "Yoshiaki Hagiwara": "IEEE Fellow",
    "Brent Hailpern": "IEEE Fellow",
    "Ibrahim Hajj": "IEEE Life Fellow",
    "Lawrence Hall": "IEEE Fellow",
    "Fumio Harashima": "IEEE Life Fellow",
    "David Harel": "IEEE Fellow, ACM Fellow",
    "Robert Harrington": "IEEE Fellow",
    "Michael Harrison": "IEEE Fellow",
    "Peter Hart": "IEEE Life Fellow, ACM Fellow",
    "Reiner Hartenstein": "IEEE Life Fellow",
    "Martin Hasler": "IEEE Fellow",
    "Boudewijn Haverkort": "IEEE Fellow",
    "John Hayes": "IEEE Fellow",
    "Munro Haynes": "IEEE Life Fellow",
    "Philip Heidelberger": "IEEE Fellow, ACM Fellow",
    "George Heilmeier": "IEEE Life Fellow",
    "John Hennessy": "IEEE Fellow",
    "Thomas Henzinger": "IEEE Fellow",
    "Eric Herz": "IEEE Life Fellow",
    "William Higgins": "IEEE Fellow",
    "J Hilibrand": "IEEE Life Fellow",
    "F Hill": "IEEE Fellow",
    "Mark Hill": "IEEE Fellow",
    "Harvard Hinton": "IEEE Fellow",
    "Shigeichi Hirasawa": "IEEE Life Fellow",
    "Tin Ho": "IEEE Fellow",
    "A Hoagland": "IEEE Life Fellow",
    "L Hobbs": "IEEE Life Fellow",
    "Ronald Hoelzeman": "IEEE Life Fellow",
    "M Hoff": "IEEE Life Fellow",
    "Charles Holland": "IEEE Fellow",
    "Se June Hong": "IEEE Fellow",
    "John Hopcroft": "IEEE Life Fellow",
    "R Howe": "IEEE Life Fellow",
    "Yu Hen Hu": "IEEE Fellow",
    "Shing Huang": "IEEE Fellow",
    "T Huang": "IEEE Life Fellow",
    "Johannes Huber": "IEEE Fellow",
    "Donna Hudson": "IEEE Fellow",
    "Joseph Hughes": "IEEE Fellow",
    "Michael Huhns": "IEEE Fellow",
    "Watts Humphrey": "IEEE Life Fellow",
    "Harry Huskey": "IEEE Life Fellow",
    "Jenq-Neng Hwang": "IEEE Fellow",
    "Wei Hwang": "IEEE Fellow",
    "Wen-Mei Hwu": "IEEE Fellow, ACM Fellow",
    "Oscar Ibarra": "IEEE Fellow, ACM Fellow",
    "Tadao Ichikawa": "IEEE Life Fellow",
    "Katsuo Ikeda": "IEEE Life Fellow",
    "Katsushi Ikeuchi": "IEEE Fellow",
    "Hideki Imai": "IEEE Fellow",
    "A Ince": "IEEE Life Fellow",
    "Ronald Indeck": "IEEE Fellow",
    "K Irani": "IEEE Life Fellow",
    "Mary Irwin": "IEEE Fellow",
    "J Irwin": "IEEE Life Fellow",
    "Toru Ishida": "IEEE Fellow",
    "Rokuya Ishii": "IEEE Fellow",
    "Andre Ivanov": "IEEE Fellow",
    "J Iwersen": "IEEE Life Fellow",
    "Sitharama Iyengar": "IEEE Fellow, ACM Fellow",
    "Joseph Ja Ja": "IEEE Fellow",
    "Richard Jaeger": "IEEE Life Fellow",
    "Ravi Jain": "IEEE Fellow",
    "Anil Jain": "IEEE Fellow",
    "Raj Jain": "IEEE Fellow, ACM Fellow",
    "Ramesh Jain": "IEEE Fellow",
    "Pankaj Jalote": "IEEE Fellow",
    "Leah Jamieson": "IEEE Fellow",
    "Raymond Jarvis": "IEEE Fellow",
    "Mehdi Jazayeri": "IEEE Fellow",
    "William Jenkins": "IEEE Fellow",
    "Christian Jensen": "IEEE Fellow",
    "Niraj Jha": "IEEE Fellow, ACM Fellow",
    "Gunnar Johannsen": "IEEE Life Fellow",
    "Barry Johnson": "IEEE Fellow",
    "Sandra Johnson": "IEEE Fellow",
    "Clark Johnson": "IEEE Life Fellow",
    "Anita Jones": "IEEE Fellow",
    "E Jones": "IEEE Life Fellow",
    "A Jordan": "IEEE Life Fellow",
    "A Joshi": "IEEE Life Fellow",
    "Jing-Yang Jou": "IEEE Fellow",
    "Norman Jouppi": "IEEE Fellow, ACM Fellow",
    "William Joyner": "IEEE Fellow",
    "Graham Jullien": "IEEE Fellow",
    "J Jump": "IEEE Life Fellow",
    "Janusz Kacprzyk": "IEEE Fellow",
    "R Kahn": "IEEE Life Fellow",
    "T Kailath": "IEEE Life Fellow",
    "Moshe Kam": "IEEE Fellow",
    "Hisao Kameda": "IEEE Fellow",
    "Mohamed Kamel": "IEEE Fellow",
    "Michitaka Kameyama": "IEEE Fellow",
    "Takeo Kanade": "IEEE Fellow, ACM Fellow",
    "Laveen Kanal": "IEEE Life Fellow",
    "Kenichi Kanatani": "IEEE Fellow",
    "Abraham Kandel": "IEEE Life Fellow, ACM Fellow",
    "Dilip Kandlur": "IEEE Fellow",
    "Sung Mo Kang": "IEEE Fellow",
    "Rohit Kapur": "IEEE Fellow",
    "M Karnaugh": "IEEE Life Fellow",
    "Mark Karpovsky": "IEEE Fellow",
    "Rangachar Kasturi": "IEEE Fellow",
    "Randy Katz": "IEEE Fellow",
    "Joseph Katz": "IEEE Fellow",
    "K Katzeff": "IEEE Life Fellow",
    "Jacob Katzenelson": "IEEE Life Fellow",
    "Arie Kaufman": "IEEE Fellow",
    "Myron Kayton": "IEEE Life Fellow",
    "Zvi Kedem": "IEEE Fellow, ACM Fellow",
    "Samuel Keene": "IEEE Life Fellow",
    "Richard Kemmerer": "IEEE Fellow",
    "W Kerwin": "IEEE Life Fellow",
    "Marion Keyes": "IEEE Fellow",
    "Oussama Khatib": "IEEE Fellow",
    "Masatsuga Kidode": "IEEE Fellow",
    "K Kim": "IEEE Fellow",
    "Yongmin Kim": "IEEE Fellow",
    "C Kime": "IEEE Life Fellow",
    "W King": "IEEE Life Fellow",
    "Kozo Kinoshita": "IEEE Life Fellow",
    "Scott Kirkpatrick": "IEEE Fellow, ACM Fellow",
    "Kenichi Kitayama": "IEEE Fellow",
    "L Kleinrock": "IEEE Life Fellow",
    "Kevin Kloker": "IEEE Fellow",
    "Torleiv Klove": "IEEE Fellow",
    "Cetin Koc": "IEEE Fellow",
    "Dan Koditschek": "IEEE Fellow",
    "Peter Kogge": "IEEE Fellow",
    "F Kohli": "IEEE Life Fellow",
    "Hermann Kopetz": "IEEE Fellow",
    "Israel Koren": "IEEE Fellow",
    "Henry Korth": "IEEE Fellow",
    "S Rao Kosaraju": "IEEE Life Fellow",
    "Mitsumasa Koyanagi": "IEEE Fellow",
    "Donald Kraft": "IEEE Fellow",
    "H Kressel": "IEEE Life Fellow",
    "P Krishnaprasad": "IEEE Fellow",
    "Raghuram Krishnapuram": "IEEE Fellow",
    "Eric Kronstadt": "IEEE Fellow",
    "S Kubina": "IEEE Life Fellow",
    "Paul Kuehn": "IEEE Fellow",
    "Benjamin Kuipers": "IEEE Fellow",
    "Casimir Kulikowski": "IEEE Fellow",
    "Vipin Kumar": "IEEE Fellow, ACM Fellow",
    "Luis Kun": "IEEE Fellow",
    "Sandip Kundu": "IEEE Fellow",
    "Tosiyasu Kunii": "IEEE Life Fellow",
    "Murat Kunt": "IEEE Fellow",
    "Wolfgang Kunz": "IEEE Fellow",
    "Sy-Yen Kuo": "IEEE Fellow",
    "Chung-C Kuo": "IEEE Fellow",
    "Fadi Kurdahi": "IEEE Fellow",
    "K Kurokawa": "IEEE Life Fellow",
    "James Kurose": "IEEE Fellow, ACM Fellow",
    "Kazuo Kyuma": "IEEE Fellow",
    "Thomas La Porta": "IEEE Fellow",
    "Parag Lala": "IEEE Fellow",
    "Jaynarayan Lala": "IEEE Fellow",
    "Theodore Laliotis": "IEEE Life Fellow",
    "Simon Lam": "IEEE Fellow",
    "Phillip Laplante": "IEEE Fellow",
    "Arvid Larson": "IEEE Life Fellow",
    "Alan Laub": "IEEE Fellow",
    "Neal Laurance": "IEEE Life Fellow",
    "Stephen Lavenberg": "IEEE Fellow",
    "Duncan Lawrie": "IEEE Fellow",
    "Harold Lawson": "IEEE Fellow",
    "Edward Lazowska": "IEEE Fellow, ACM Fellow",
    "Franz Leberl": "IEEE Fellow",
    "Insup Lee": "IEEE Fellow, ACM Fellow",
    "Edward Lee": "IEEE Fellow",
    "Der Tsai Lee": "IEEE Fellow",
    "Ruby Lee": "IEEE Fellow",
    "Sukhan Lee": "IEEE Fellow",
    "Will Leland": "IEEE Fellow",
    "George Lendaris": "IEEE Life Fellow",
    "B Leon": "IEEE Life Fellow",
    "C.T. Leondes": "IEEE Life Fellow",
    "Alberto Leon-Garcia": "IEEE Fellow",
    "Victor Leung": "IEEE Fellow",
    "Wu-Hon Leung": "IEEE Fellow",
    "Stefano Levialdi": "IEEE Life Fellow",
    "Martin Levine": "IEEE Life Fellow",
    "Henry Levy": "IEEE Fellow",
    "Peter Lewis": "IEEE Life Fellow",
    "Victor Li": "IEEE Fellow",
    "Chung Li": "IEEE Fellow",
    "Ching Li": "IEEE Life Fellow",
    "Paul Liao": "IEEE Fellow",
    "Jorg Liebeherr": "IEEE Fellow",
    "Michael Lightner": "IEEE Fellow",
    "Leo Ligthart": "IEEE Fellow",
    "Fred Liguori": "IEEE Life Fellow",
    "David Lilja": "IEEE Fellow",
    "Bao-Shuh Lin": "IEEE Fellow",
    "J Lindenlaub": "IEEE Life Fellow",
    "Bruce Lindsay": "IEEE Fellow, ACM Fellow",
    "Stuart Lipoff": "IEEE Fellow",
    "G Lipovski": "IEEE Fellow",
    "John Little": "IEEE Fellow",
    "Jane Liu": "IEEE Fellow",
    "Chen-Ching Liu": "IEEE Fellow",
    "Ming Liu": "IEEE Life Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chao-Ning Liu": "IEEE Life Fellow",
    "Hans-A Loeliger": "IEEE Fellow",
    "Murray Loew": "IEEE Fellow",
    "William Loftus": "IEEE Fellow",
    "Joseph Logue": "IEEE Life Fellow",
    "David Lomet": "IEEE Life Fellow",
    "Darrell Long": "IEEE Fellow",
    "Philip Lopresti": "IEEE Life Fellow",
    "Alexander Loui": "IEEE Fellow",
    "Michael Loui": "IEEE Fellow",
    "Steven Low": "IEEE Fellow",
    "Fabrizio Luccio": "IEEE Life Fellow",
    "Robert Lucky": "IEEE Life Fellow",
    "J Luh": "IEEE Life Fellow",
    "Stephen Lundstrom": "IEEE Life Fellow",
    "Peilin Luo": "IEEE Life Fellow",
    "David Lynch": "IEEE Life Fellow",
    "Michael Rung-Tsong Lyu": "IEEE Fellow, ACM Fellow",
    "John Macdonald": "IEEE Life Fellow",
    "Benoit M Macq": "IEEE Fellow",
    "Vijay Madisetti": "IEEE Fellow",
    "Azad Madni": "IEEE Fellow",
    "John Makhoul": "IEEE Life Fellow",
    "Jitendra Malik": "IEEE Fellow, ACM Fellow, Fellow of the IEEE, ACM, and the American Academy of Arts and Sciences, and a member of the National Academy of Sciences and the National Academy of Engineering",
    "Henrique Malvar": "IEEE Fellow",
    "Ebrahim Mamdani": "IEEE Life Fellow",
    "Laurence Manning": "IEEE Life Fellow",
    "P Mantey": "IEEE Life Fellow",
    "Petros Maragos": "IEEE Fellow",
    "H Marcy": "IEEE Life Fellow",
    "M M Marek-Sadowska": "IEEE Fellow",
    "Akira Masaki": "IEEE Fellow",
    "Gerald Masson": "IEEE Fellow",
    "Takashi Matsumoto": "IEEE Fellow",
    "Yoshihiro Matsumoto": "IEEE Life Fellow",
    "Yasuo Matsuyama": "IEEE Fellow",
    "Ueli Maurer": "IEEE Fellow",
    "Roy Maxion": "IEEE Fellow",
    "Gary May": "IEEE Fellow",
    "George Mcclure": "IEEE Life Fellow",
    "Edward McCluskey": "IEEE Life Fellow, ACM Fellow",
    "Nancy Mead": "IEEE Fellow",
    "Gerard Medioni": "IEEE Fellow",
    "Raman Mehra": "IEEE Fellow",
    "James Meindl": "IEEE Life Fellow",
    "Rami Melhem": "IEEE Fellow",
    "Jaishankar Menon": "IEEE Fellow",
    "P Menon": "IEEE Life Fellow",
    "C Merriam": "IEEE Life Fellow",
    "John Metzner": "IEEE Life Fellow",
    "J Meyer": "IEEE Life Fellow",
    "Marlin Mickle": "IEEE Life Fellow",
    "Tetsuya Miki": "IEEE Fellow",
    "Raymond Miller": "IEEE Life Fellow, ACM Fellow",
    "William Miller": "IEEE Life Fellow",
    "David Mills": "IEEE Fellow",
    "Veljko Milutinovic": "IEEE Fellow",
    "Yinghua Min": "IEEE Fellow",
    "Jack Minker": "IEEE Life Fellow, ACM Fellow",
    "Frederick Mintzer": "IEEE Fellow",
    "J Modestino": "IEEE Life Fellow",
    "K Mohiuddin": "IEEE Fellow",
    "James Moore": "IEEE Fellow",
    "Nelson Morgan": "IEEE Fellow",
    "Kinji Mori": "IEEE Fellow",
    "Lloyd Morley": "IEEE Life Fellow",
    "George Moschytz": "IEEE Life Fellow",
    "Joel Moses": "IEEE Fellow, ACM Fellow",
    "Hussein Mouftah": "IEEE Fellow",
    "James Moulic": "IEEE Fellow",
    "Samiha Mourad": "IEEE Fellow",
    "Trevor Mudge": "IEEE Fellow, ACM Fellow",
    "George Mueller": "IEEE Life Fellow",
    "Amar Mukherjee": "IEEE Fellow",
    "Koso Murakami": "IEEE Fellow",
    "Hiroshi Murase": "IEEE Fellow",
    "Tadao Murata": "IEEE Life Fellow",
    "J Musa": "IEEE Life Fellow",
    "Makoto Nagao": "IEEE Fellow",
    "H Nagel": "IEEE Life Fellow",
    "George Nagy": "IEEE Life Fellow",
    "Klara Nahrstedt": "IEEE Fellow, ACM Fellow",
    "Ravi Nair": "IEEE Fellow",
    "Tsuneo Nakahara": "IEEE Life Fellow",
    "Tadao Nakamura": "IEEE Fellow",
    "Yukihiro Nakamura": "IEEE Fellow",
    "Ryohei Nakatsu": "IEEE Fellow",
    "Wataru Nakayama": "IEEE Fellow",
    "Takashi Nanya": "IEEE Fellow",
    "C Narayanaswami": "IEEE Fellow",
    "Sani Nassif": "IEEE Fellow",
    "Erich Neuhold": "IEEE Fellow",
    "Peter Neumann": "IEEE Life Fellow",
    "Yrjo Neuvo": "IEEE Fellow",
    "Ramakant Nevatia": "IEEE Life Fellow",
    "Lionel Ni": "IEEE Fellow",
    "David Nicol": "IEEE Fellow",
    "Paul Nielsen": "IEEE Fellow",
    "Jurg Nievergelt": "IEEE Life Fellow",
    "Paul Nikolich": "IEEE Fellow",
    "Stig Nilsson": "IEEE Life Fellow",
    "Shogo Nishida": "IEEE Fellow",
    "Takao Nishizeki": "IEEE Fellow, ACM Fellow",
    "Tohei Nitta": "IEEE Life Fellow",
    "Josef Nossek": "IEEE Fellow",
    "David Notkin": "IEEE Fellow",
    "Henri Nussbaumer": "IEEE Life Fellow",
    "Roy Nutter": "IEEE Fellow",
    "Mohammad Obaidat": "IEEE Fellow",
    "Lawrence O'Gorman": "IEEE Fellow",
    "Maciej Ogorzalek": "IEEE Fellow",
    "Tadahiro Ohmi": "IEEE Fellow",
    "Tatsuo Ohtsuki": "IEEE Life Fellow",
    "Erkki Oja": "IEEE Fellow",
    "O Olukotun": "IEEE Fellow",
    "Kinji Ono": "IEEE Life Fellow",
    "Morio Onoe": "IEEE Life Fellow",
    "Daniel Ostapko": "IEEE Life Fellow",
    "Joern Ostermann": "IEEE Fellow",
    "Bjorn Ottersten": "IEEE Fellow",
    "Thomas Overbye": "IEEE Fellow",
    "David Padua": "IEEE Fellow, ACM Fellow",
    "James Palmer": "IEEE Life Fellow",
    "Cherri Pancake": "IEEE Fellow",
    "Sethuraman Panchanathan": "IEEE Fellow, ACM Fellow",
    "D Panda": "IEEE Fellow",
    "Karen Panetta": "IEEE Fellow",
    "Christos Papachristou": "IEEE Life Fellow",
    "N Papanikolopoulos": "IEEE Fellow",
    "Stephen Pardee": "IEEE Life Fellow",
    "Behrooz Parhami": "IEEE Fellow",
    "Alice Parker": "IEEE Fellow",
    "Edward Parrish": "IEEE Life Fellow",
    "Rajnikant Patel": "IEEE Fellow",
    "Lalit Patnaik": "IEEE Fellow",
    "Yale Patt": "IEEE Fellow, ACM Fellow",
    "David Patterson": "IEEE Fellow, ACM Fellow",
    "Sanjoy Paul": "IEEE Fellow",
    "Raymond Paul": "IEEE Fellow",
    "T Pavlidis": "IEEE Life Fellow",
    "Witold Pedrycz": "IEEE Fellow",
    "Soo-Chang Pei": "IEEE Fellow",
    "P Penfield": "IEEE Life Fellow",
    "Ronald Perrott": "IEEE Fellow",
    "W Peterson": "IEEE Life Fellow",
    "Dragutin Petkovic": "IEEE Fellow",
    "S Petrillo": "IEEE Life Fellow",
    "Frederick Petry": "IEEE Fellow",
    "Hoang Pham": "IEEE Fellow",
    "Rosalind Picard": "IEEE Fellow",
    "Michael Picheny": "IEEE Fellow",
    "Raymond Pickholtz": "IEEE Life Fellow",
    "R Piloty": "IEEE Life Fellow",
    "Peter Pirsch": "IEEE Fellow",
    "Ioannis Pitas": "IEEE Fellow",
    "M Pitke": "IEEE Life Fellow",
    "Vincenzo Piuri": "IEEE Fellow",
    "Rejean Plamondon": "IEEE Fellow",
    "A Pohm": "IEEE Life Fellow",
    "Irith Pomeranz": "IEEE Fellow",
    "James Pomerene": "IEEE Life Fellow",
    "Jean Ponce": "IEEE Fellow, ERC advanced grant, Editor-in-Chief of the International Journal of Computer Vision",
    "S Pookaiyaudom": "IEEE Fellow",
    "Douglass Post": "IEEE Fellow",
    "Dhiraj Pradhan": "IEEE Fellow, ACM Fellow",
    "Birendra Prasada": "IEEE Life Fellow",
    "Viktor Prasanna": "IEEE Fellow, ACM Fellow",
    "R Pritchard": "IEEE Life Fellow",
    "Walter Proebster": "IEEE Life Fellow",
    "John Pullen": "IEEE Fellow",
    "C Raghavendra": "IEEE Fellow",
    "S Rajasekaran": "IEEE Fellow",
    "Rochit Rajsuman": "IEEE Fellow",
    "G V S Raju": "IEEE Life Fellow",
    "S Ramadorai": "IEEE Fellow",
    "Raghu Ramakrishnan": "IEEE Fellow, ACM Fellow",
    "C Ramamoorthy": "IEEE Life Fellow",
    "Krithi Ramamritham": "IEEE Fellow",
    "N Ranganathan": "IEEE Fellow",
    "Sanjay Ranka": "IEEE Fellow",
    "Anders Rantzer": "IEEE Fellow",
    "Nageswara Rao": "IEEE Fellow",
    "Robert Rassa": "IEEE Fellow",
    "Nalini Ratha": "IEEE Fellow",
    "R Ray": "IEEE Fellow",
    "Sudhakar Reddy": "IEEE Life Fellow",
    "G Redinbo": "IEEE Life Fellow",
    "Daniel Reed": "IEEE Fellow",
    "Jeffrey Reed": "IEEE Fellow",
    "T Regulinski": "IEEE Life Fellow",
    "Johan Reiber": "IEEE Fellow",
    "Martin Reiser": "IEEE Fellow",
    "Aristides Requicha": "IEEE Life Fellow, ACM Fellow",
    "T V Rhyne": "IEEE Life Fellow",
    "William Riddle": "IEEE Fellow",
    "Robert Rivers": "IEEE Life Fellow",
    "Laura Roa": "IEEE Fellow",
    "Yves Robert": "IEEE Fellow",
    "Thomas Robertazzi": "IEEE Fellow",
    "Arthur Robinson": "IEEE Life Fellow",
    "Mortimer Rogoff": "IEEE Life Fellow",
    "Hans Rombach": "IEEE Fellow",
    "Ronny Ronen": "IEEE Fellow",
    "Arnold Rosenberg": "IEEE Fellow, ACM Fellow",
    "David Rosenblum": "IEEE Fellow",
    "Charles Rosenthal": "IEEE Life Fellow",
    "Tamas Roska": "IEEE Fellow",
    "Christian Roux": "IEEE Fellow",
    "Izhak Rubin": "IEEE Life Fellow",
    "Enrique Ruspini": "IEEE Fellow",
    "Roy Russo": "IEEE Life Fellow",
    "Rob Rutenbar": "IEEE Fellow",
    "C Rypinski": "IEEE Life Fellow",
    "Martin Sachs": "IEEE Fellow",
    "Andrew Sage": "IEEE Life Fellow",
    "C Sah": "IEEE Life Fellow",
    "Sudhakar Sahasrabudhe": "IEEE Fellow",
    "Sartaj Sahni": "IEEE Fellow",
    "George Sai-Halasz": "IEEE Fellow",
    "Nobuji Saito": "IEEE Life Fellow",
    "Tadao Saito": "IEEE Life Fellow",
    "Karem Sakallah": "IEEE Fellow, ACM Fellow",
    "Ken Sakamura": "IEEE Fellow",
    "Hiroshi Sakou": "IEEE Fellow",
    "Jerome Saltzer": "IEEE Life Fellow",
    "Kewal Saluja": "IEEE Fellow",
    "Betty Salzberg": "IEEE Fellow",
    "Ahmed Sameh": "IEEE Fellow, ACM Fellow",
    "Hanan Samet": "IEEE Fellow, ACM Fellow",
    "William Sanders": "IEEE Fellow, ACM Fellow",
    "Arthur Sanderson": "IEEE Fellow",
    "Ravi Sandhu": "IEEE Fellow",
    "Majid Sarrafzadeh": "IEEE Fellow",
    "Tsutomu Sasao": "IEEE Fellow",
    "M Satyanarayanan": "IEEE Fellow",
    "John Savage": "IEEE Life Fellow",
    "Yvon Savaria": "IEEE Fellow",
    "Jacob Savir": "IEEE Fellow",
    "Alexander Sawchuk": "IEEE Fellow",
    "Peter Scheuermann": "IEEE Fellow",
    "Richard Schlichting": "IEEE Fellow",
    "Paul Schneck": "IEEE Fellow, ACM Fellow",
    "Norman Schneidewind": "IEEE Life Fellow",
    "J Schoeffler": "IEEE Life Fellow",
    "Mischa Schwartz": "IEEE Life Fellow",
    "Edmund Schweitzer": "IEEE Fellow",
    "Norman Scott": "IEEE Life Fellow",
    "Carl Sechen": "IEEE Fellow",
    "Zary Segall": "IEEE Fellow",
    "Terrence Sejnowski": "IEEE Fellow",
    "Siegfried Selberherr": "IEEE Fellow",
    "S Sensiper": "IEEE Life Fellow",
    "Carlo Sequin": "IEEE Life Fellow",
    "Ishwar Sethi": "IEEE Fellow",
    "M Sezan": "IEEE Fellow",
    "Lui Sha": "IEEE Fellow, ACM Fellow",
    "Zong Sha": "IEEE Life Fellow",
    "Ghavam Shahidi": "IEEE Fellow",
    "Linda Shapiro": "IEEE Fellow",
    "Gustave Shapiro": "IEEE Life Fellow",
    "Mary Shaw": "IEEE Fellow",
    "Leonard Shaw": "IEEE Life Fellow",
    "Shashi Shekhar": "IEEE Fellow",
    "John Sheppard": "IEEE Fellow",
    "Amit Sheth": "IEEE Fellow, ACM Fellow",
    "Kiyohiro Shikano": "IEEE Fellow",
    "Naohisa Shimomura": "IEEE Life Fellow",
    "Kang Shin": "IEEE Fellow, ACM Fellow",
    "Isao Shirakawa": "IEEE Life Fellow",
    "Norio Shiratori": "IEEE Fellow",
    "S Shiva": "IEEE Fellow",
    "M Shooman": "IEEE Life Fellow",
    "Bruce Shriver": "IEEE Fellow",
    "Heung-Yeung Shum": "IEEE Fellow, ACM Fellow, IEEE/ACM Fellow, International Fellow of the Royal Academy Society of Engineering in the United Kingdom, editorial boards for TPAMI and the IJCV",
    "Jyuo-Min Shyu": "IEEE Fellow",
    "Howard Siegel": "IEEE Fellow, ACM Fellow",
    "Daniel Siewiorek": "IEEE Fellow, ACM Fellow",
    "Abraham Silberschatz": "IEEE Fellow, ACM Fellow",
    "D Simmons": "IEEE Life Fellow",
    "Walter Sincoskie": "IEEE Fellow",
    "Adit Singh": "IEEE Fellow",
    "Mukesh Singhal": "IEEE Fellow",
    "Bhabani Sinha": "IEEE Fellow",
    "J Sklansky": "IEEE Life Fellow",
    "Martha Sloan": "IEEE Life Fellow, ACM Fellow",
    "John Smith": "IEEE Fellow",
    "James Smith": "IEEE Fellow",
    "Alan Smith": "IEEE Fellow, ACM Fellow",
    "T Smith": "IEEE Fellow",
    "Jonathan Smith": "IEEE Fellow",
    "M Smith": "IEEE Life Fellow",
    "K Smith": "IEEE Life Fellow",
    "Burton Smith": "IEEE Life Fellow, ACM Fellow",
    "Marc Snir": "IEEE Fellow, ACM Fellow",
    "Larry Snyder": "IEEE Fellow, ACM Fellow",
    "Gurindar Sohi": "IEEE Fellow",
    "David Soldan": "IEEE Fellow",
    "Mani Soma": "IEEE Fellow",
    "Arun Somani": "IEEE Fellow",
    "Stefano Spaccapietra": "IEEE Fellow",
    "Eugene Spafford": "IEEE Fellow",
    "L Spandorfer": "IEEE Life Fellow",
    "H Spang": "IEEE Life Fellow",
    "Alfred Spector": "IEEE Fellow",
    "John Spragins": "IEEE Life Fellow",
    "Mark S Squillante": "IEEE Fellow, ACM Fellow",
    "Sargur Srihari": "IEEE Fellow",
    "Pradip Srimani": "IEEE Fellow",
    "Mani Srivastava": "IEEE Fellow",
    "Peter Staecker": "IEEE Life Fellow",
    "John Stankovic": "IEEE Fellow",
    "Ralf Steinmetz": "IEEE Fellow, ACM Fellow",
    "Per Stenstrom": "IEEE Fellow",
    "J Stiffler": "IEEE Life Fellow",
    "Guy St-Jean": "IEEE Fellow",
    "James Stoffel": "IEEE Fellow",
    "Ivan Stojmenovic": "IEEE Fellow",
    "Ernest Stokely": "IEEE Life Fellow",
    "W Stone": "IEEE Fellow",
    "Harold Stone": "IEEE Life Fellow, ACM Fellow",
    "Athanasios Stouraitis": "IEEE Fellow",
    "D Strain": "IEEE Life Fellow",
    "Charles Stroud": "IEEE Fellow",
    "Bjarne Stroustrup": "IEEE Fellow, ACM Fellow",
    "Ryszard Struzak": "IEEE Life Fellow",
    "Tatsuya Suda": "IEEE Fellow",
    "Ching Suen": "IEEE Life Fellow",
    "Gary Sullivan": "IEEE Fellow",
    "J Suran": "IEEE Life Fellow",
    "Christer Svensson": "IEEE Fellow",
    "Earl Swartzlander": "IEEE Fellow",
    "Katia Sycara": "IEEE Fellow",
    "Richard Szeliski": "IEEE Fellow, ACM Fellow, \u7f8e\u56fd\u5de5\u7a0b\u9662\u9662\u58eb\uff0cACM Fellow\uff0cIEEE Fellow\uff0c\u66fe\u4efbIJCV\u3001TPAMI\u7b49\u9876\u520a\u7f16\u59d4",
    "Janos Sztipanovits": "IEEE Fellow",
    "Stephen Szygenda": "IEEE Life Fellow",
    "Boleslaw Szymanski": "IEEE Fellow",
    "Yoshitaka Takasaki": "IEEE Life Fellow",
    "Tieniu Tan": "IEEE Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hidehiko Tanaka": "IEEE Fellow",
    "Richard Tanaka": "IEEE Life Fellow",
    "Andrew Tanenbaum": "IEEE Fellow",
    "Yuan Yan Tang": "IEEE Fellow",
    "Steven Tanimoto": "IEEE Fellow",
    "R Tanner": "IEEE Fellow",
    "Tzyh-Jong Tarn": "IEEE Life Fellow",
    "Gabriel Taubin": "IEEE Fellow",
    "Robert Tausworthe": "IEEE Life Fellow",
    "Frederick Taylor": "IEEE Life Fellow",
    "David Tennenhouse": "IEEE Fellow",
    "Lewis Terman": "IEEE Life Fellow",
    "Demetri Terzopoulos": "IEEE Fellow, ACM Fellow",
    "Stuart Tewksbury": "IEEE Fellow",
    "H Thal": "IEEE Life Fellow",
    "R Thayer": "IEEE Life Fellow",
    "Donald Thomas": "IEEE Fellow",
    "Alexander Thomasian": "IEEE Fellow",
    "Craig Thompson": "IEEE Fellow",
    "K Thulasiraman": "IEEE Fellow",
    "M Thuraisingham": "IEEE Fellow",
    "James Tien": "IEEE Fellow",
    "Fouad Tobagi": "IEEE Fellow",
    "Iwao Toda": "IEEE Fellow",
    "Yoshihiro Tohma": "IEEE Life Fellow",
    "Shoji Tominaga": "IEEE Fellow",
    "Willis Tompkins": "IEEE Life Fellow",
    "Jun-Ichiro Toriwaki": "IEEE Fellow",
    "H Torng": "IEEE Life Fellow",
    "Josep Torrellas": "IEEE Fellow, ACM Fellow",
    "Donald Towsley": "IEEE Fellow",
    "J Tracey": "IEEE Life Fellow",
    "Harry Tredennick": "IEEE Fellow",
    "Louise Trevillyan": "IEEE Fellow",
    "Timothy Trick": "IEEE Life Fellow",
    "Satish Tripathi": "IEEE Fellow",
    "Anand Tripathi": "IEEE Fellow",
    "Leonard Tripp": "IEEE Fellow",
    "Kishor Trivedi": "IEEE Fellow",
    "Charles Trowbridge": "IEEE Fellow",
    "W Trybula": "IEEE Life Fellow",
    "Jingpha Tsai": "IEEE Fellow",
    "Hidenori Tsuji": "IEEE Life Fellow",
    "Shigeo Tsujii": "IEEE Life Fellow",
    "Jonathan Turner": "IEEE Fellow, ACM Fellow",
    "Spyridon Tzafestas": "IEEE Life Fellow",
    "Shunsuke Uemura": "IEEE Fellow",
    "S Unger": "IEEE Life Fellow",
    "Andre Vacroux": "IEEE Life Fellow",
    "Vijay Vaishnavi": "IEEE Fellow",
    "Jose Valdez C": "IEEE Life Fellow",
    "Mateo Valero": "IEEE Fellow, ACM Fellow",
    "Tibor Vamos": "IEEE Life Fellow",
    "A Van De Goor": "IEEE Fellow",
    "J Van Ness": "IEEE Life Fellow",
    "A Van Roggen": "IEEE Life Fellow",
    "Andries Vandam": "IEEE Life Fellow",
    "Joos Vandewalle": "IEEE Fellow",
    "Murali Varanasi": "IEEE Life Fellow",
    "Baba Vemuri": "IEEE Fellow",
    "A Venetsanopoulos": "IEEE Life Fellow",
    "Paulo Verissimo": "IEEE Fellow",
    "Martin Vetterli": "IEEE Fellow, ACM Fellow",
    "Mathukumal Vidyasagar": "IEEE Fellow",
    "Max Viergever": "IEEE Fellow",
    "Jeffrey Vitter": "IEEE Fellow",
    "H Voelcker": "IEEE Life Fellow",
    "R Volz": "IEEE Life Fellow",
    "Mladen Vouk": "IEEE Fellow",
    "Benjamin Wah": "IEEE Fellow",
    "Michael Waidner": "IEEE Fellow",
    "Steven Wallach": "IEEE Fellow",
    "Laung Wang": "IEEE Fellow",
    "Jhing Wang": "IEEE Fellow",
    "Roy Want": "IEEE Fellow, ACM Fellow",
    "W Ware": "IEEE Life Fellow",
    "Pramod Warty": "IEEE Fellow",
    "Anthony Wasserman": "IEEE Fellow",
    "Tadashi Watanabe": "IEEE Fellow",
    "Hitoshi Watanabe": "IEEE Life Fellow",
    "Layne Watson": "IEEE Fellow",
    "R Waxman": "IEEE Life Fellow",
    "Alfred Weaver": "IEEE Fellow",
    "Harry Wechsler": "IEEE Fellow",
    "Stuart Wecker": "IEEE Fellow, ACM Fellow",
    "Mark Wegman": "IEEE Fellow, ACM Fellow",
    "Louis Weinberg": "IEEE Life Fellow",
    "Arnold Weinberger": "IEEE Life Fellow",
    "Stephen Weinstein": "IEEE Life Fellow",
    "Uri Weiser": "IEEE Fellow, ACM Fellow",
    "Paul Wesling": "IEEE Fellow",
    "Burnell West": "IEEE Life Fellow",
    "Elaine Weyuker": "IEEE Fellow, ACM Fellow",
    "Kyu Whang": "IEEE Fellow",
    "Jacob White": "IEEE Fellow",
    "Bernard Widrow": "IEEE Life Fellow",
    "Gio Wiederhold": "IEEE Fellow, ACM Fellow",
    "Thomas Williams": "IEEE Fellow",
    "Robin Williams": "IEEE Life Fellow, ACM Fellow",
    "Jeannette Wing": "IEEE Fellow",
    "Omar Wing": "IEEE Life Fellow",
    "O Winn": "IEEE Life Fellow",
    "Arthur Winston": "IEEE Life Fellow",
    "Joel Wolf": "IEEE Fellow",
    "Wayne Wolf": "IEEE Fellow",
    "Jack Wolf": "IEEE Life Fellow",
    "Martin Wong": "IEEE Fellow, ACM Fellow",
    "C Wong": "IEEE Life Fellow",
    "W Wonham": "IEEE Life Fellow",
    "Helen Wood": "IEEE Fellow",
    "David Wood": "IEEE Fellow",
    "C Woodside": "IEEE Life Fellow",
    "Ja-Ling Wu": "IEEE Fellow",
    "Kun-Lung Wu": "IEEE Fellow",
    "Chung-Yu Wu": "IEEE Fellow",
    "Cheng Wen Wu": "IEEE Fellow",
    "Wm Wulf": "IEEE Fellow",
    "Ning Xi": "IEEE Fellow",
    "Shuzo Yajima": "IEEE Life Fellow",
    "Akihiko Yamada": "IEEE Life Fellow",
    "Fuqing Yang": "IEEE Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Kazuo Yano": "IEEE Fellow",
    "Xin Yao": "IEEE Fellow",
    "Hiroshi Yasuda": "IEEE Fellow",
    "Stephen Yau": "IEEE Life Fellow",
    "R Yavatkar": "IEEE Fellow",
    "Raymond Yeh": "IEEE Life Fellow",
    "John Yen": "IEEE Fellow",
    "Daniel Yeung": "IEEE Fellow",
    "Pen-Chung Yew": "IEEE Fellow",
    "Sung-Joo Yoo": "IEEE Fellow",
    "Hoi-Jun Yoo": "IEEE Fellow",
    "Tzay Young": "IEEE Life Fellow",
    "Marshall Yovits": "IEEE Life Fellow",
    "Philip Yu": "IEEE Fellow",
    "H Yu": "IEEE Life Fellow",
    "Lotfi Zadeh": "IEEE Life Fellow",
    "Avideh Zakhor": "IEEE Fellow",
    "Bernard Zeigler": "IEEE Fellow",
    "Alexander Zelinsky": "IEEE Fellow",
    "Marvin Zelkowitz": "IEEE Fellow",
    "Zhengyou Zhang": "IEEE Fellow, ACM Fellow",
    "Hongjiang Zhang": "IEEE Fellow",
    "Ya-Qin Zhang": "IEEE Fellow",
    "Wei Zhao": "IEEE Fellow",
    "Nan-Ning Zheng": "IEEE Fellow",
    "George Zobrist": "IEEE Life Fellow",
    "Albert Zomaya": "IEEE Fellow",
    "Yervant Zorian": "IEEE Fellow",
    "Steven Zucker": "IEEE Fellow",
    "Willy Zwaenepoel": "IEEE Fellow",
    "Maneesh Agrawala": "ACM Fellow",
    "Anima Anandkumar": "ACM Fellow",
    "David Atienza Alonso": "ACM Fellow",
    "Boaz Barak": "ACM Fellow",
    "Michel Beaudouin-Lafon": "ACM Fellow",
    "Peter Boncz": "ACM Fellow",
    "Luis H Ceze": "ACM Fellow",
    "Ranveer Chandra": "ACM Fellow",
    "Ed H. Chi": "ACM Fellow",
    "Corinna Cortes": "ACM Fellow",
    "Constantinos Daskalakis": "ACM Fellow",
    "Bronis R. de Supinski": "ACM Fellow",
    "Kalyanmoy Deb": "ACM Fellow",
    "Kevin Fu": "ACM Fellow",
    "Craig Gotsman": "ACM Fellow",
    "Ahmed E. Hassan": "ACM Fellow",
    "Sumi Helal": "ACM Fellow",
    "Manuel V Hermenegildo": "ACM Fellow",
    "Michael W Hicks": "ACM Fellow",
    "Jason Hong": "ACM Fellow",
    "Sandy Irani": "ACM Fellow",
    "Hiroshi Ishii": "ACM Fellow",
    "Alfons Kemper": "ACM Fellow",
    "Samir Khuller": "ACM Fellow",
    "Farinaz Koushanfar": "ACM Fellow",
    "Chung C Kuo": "ACM Fellow",
    "Hang Li": "ACM Fellow",
    "Jimmy Lin": "ACM Fellow",
    "Radu Marculescu": "ACM Fellow",
    "David M Mount": "ACM Fellow",
    "Gonzalo Navarro": "ACM Fellow",
    "Rafael Pass": "ACM Fellow",
    "Marc Pollefeys": "ACM Fellow, IEEE Fellow\uff0c\u9a6c\u5c14\u5956\u5f97\u4e3b\uff0c\u66fe\u4efbIJCV\u3001TPAMI\u7b49\u9876\u520a\u7f16\u59d4\uff0c\u5fae\u8f6fMR&AI\u82cf\u9ece\u4e16\u5b9e\u9a8c\u5ba4\u4e3b\u4efb",
    "Alex Pothen": "ACM Fellow",
    "Moinuddin Qureshi": "ACM Fellow",
    "Ashutosh Sabharwal": "ACM Fellow",
    "Tim Sherwood": "ACM Fellow",
    "Stefano Soatto": "ACM Fellow, ACM Fellow, IEEE Fellow,  Marr Prize, Associate Editor of IEEE TPAMI, a Member of the Editorial Board of IJCV",
    "John Stasko": "ACM Fellow",
    "Gary J. Sullivan": "ACM Fellow",
    "Jaime Teevan": "ACM Fellow",
    "Kentaro Toyama": "ACM Fellow",
    "Rene Vidal": "ACM Fellow",
    "Eric Xing": "ACM Fellow",
    "Dong Yu": "ACM Fellow",
    "Yizhou Yu": "ACM Fellow",
    "Haitao Zheng": "ACM Fellow",
    "Wenwu Zhu": "ACM Fellow",
    "Denis Zorin": "ACM Fellow",
    "Leonard M. Adleman": "ACM Fellow",
    "David A Bader": "ACM Fellow",
    "Meenakshi Balakrishnan": "ACM Fellow",
    "Mark Braverman": "ACM Fellow",
    "Linda Jean Camp": "ACM Fellow",
    "Edward Y Chang": "ACM Fellow",
    "Tanzeem Choudhury": "ACM Fellow",
    "Daniel Cohen-Or": "ACM Fellow, ACM Fellow\uff0c\u66fe\u4efbTOG\u3001TVCG\u7b49\u9876\u520a\u7f16\u59d4",
    "Gautam Das": "ACM Fellow",
    "Anind Dey": "ACM Fellow",
    "Lieven Eeckhout": "ACM Fellow",
    "Amos Fiat": "ACM Fellow",
    "Hubertus Franke": "ACM Fellow",
    "Batya Friedman": "ACM Fellow",
    "Judith Gal-Ezer": "ACM Fellow",
    "Deepak Ganesan": "ACM Fellow",
    "Anupam Gupta": "ACM Fellow",
    "Zygmunt J. Haas": "ACM Fellow",
    "Elad Hazan": "ACM Fellow",
    "Xiaobo Sharon Hu": "ACM Fellow",
    "Paola Inverardi": "ACM Fellow",
    "Zachary Ives": "ACM Fellow",
    "Sushil Jajodia": "ACM Fellow",
    "Ranjit Jhala": "ACM Fellow",
    "David R Kaeli": "ACM Fellow",
    "Jonathan Katz": "ACM Fellow",
    "Robert Kleinberg": "ACM Fellow",
    "Thomas Lengauer": "ACM Fellow",
    "Hai Li": "ACM Fellow",
    "Tie-Yan Liu": "ACM Fellow",
    "Steve Marschner": "ACM Fellow",
    "Matthew T Mason": "ACM Fellow",
    "Dale A Miller": "ACM Fellow",
    "Elchanan Mossel": "ACM Fellow",
    "Bernhard Nebel": "ACM Fellow",
    "Bjorner Nikolaj": "ACM Fellow",
    "Rafail Ostrovsky": "ACM Fellow",
    "Joel Ouaknine": "ACM Fellow",
    "David Z. Pan": "ACM Fellow",
    "Rosalind Wright Picard": "ACM Fellow",
    "Shaz Qadeer": "ACM Fellow",
    "Glenn Ricart": "ACM Fellow",
    "Tajana Rosing": "ACM Fellow",
    "Robert B Ross": "ACM Fellow",
    "Szymon Rusinkiewicz": "ACM Fellow",
    "Pierangela Samarati": "ACM Fellow",
    "Sunita Sarawagi": "ACM Fellow",
    "Bernt Schiele": "ACM Fellow",
    "Munindar P. Singh": "ACM Fellow",
    "Aravinda P Sistla": "ACM Fellow",
    "Scott Smolka": "ACM Fellow",
    "Mark Tehranipoor": "ACM Fellow",
    "Luca Trevisan": "ACM Fellow",
    "Wenping Wang": "ACM Fellow, IEEE/ACM Fellow",
    "Brent Waters": "ACM Fellow",
    "Ryen W White": "ACM Fellow",
    "Jacob Otto Wobbrock": "ACM Fellow",
    "Tao Xie": "ACM Fellow",
    "Ming-Hsuan Yang": "ACM Fellow",
    "Mohammed Zaki": "ACM Fellow",
    "Ben Y. Zhao": "ACM Fellow",
    "Lin Zhong": "ACM Fellow",
    "Shlomo Zilberstein": "ACM Fellow",
    "Daniel J Abadi": "ACM Fellow",
    "James Allan": "ACM Fellow",
    "Srinivas Aluru": "ACM Fellow",
    "Andrea Arpaci-Dusseau": "ACM Fellow",
    "Remzi Arpaci-Dusseau": "ACM Fellow",
    "Suman Banerjee": "ACM Fellow",
    "Manuel Blum": "ACM Fellow",
    "Lionel Briand": "ACM Fellow",
    "David Brooks": "ACM Fellow",
    "Ran Canetti": "ACM Fellow",
    "John Canny": "ACM Fellow",
    "Anantha Chandrakasan": "ACM Fellow",
    "Yao-Wen Chang": "ACM Fellow",
    "Moses Charikar": "ACM Fellow",
    "Yiran Chen": "ACM Fellow",
    "Graham R. Cormode": "ACM Fellow",
    "Patrick Cousot": "ACM Fellow",
    "Mathieu Desbrun": "ACM Fellow",
    "Whitfield Diffie": "ACM Fellow",
    "Bonnie J Dorr": "ACM Fellow",
    "Nicholas Duffield": "ACM Fellow",
    "Alan Edelman": "ACM Fellow",
    "Thomas Eiter": "ACM Fellow",
    "Cormac Flanagan": "ACM Fellow",
    "Jodi Forlizzi": "ACM Fellow",
    "Dieter Fox": "ACM Fellow, IEEE/AAAI,ACM Fellow, editor of the IEEE Transactions on Robots",
    "Sanjay Ghemawat": "ACM Fellow",
    "Antonio Gonzalez": "ACM Fellow",
    "Andrew D. Gordon": "ACM Fellow",
    "Steven Gribble": "ACM Fellow",
    "Susanne E Hambrusch": "ACM Fellow",
    "Martin Hellman": "ACM Fellow",
    "Nicholas Higham": "ACM Fellow",
    "C. Antony R. Hoare": "ACM Fellow",
    "Holger H. Hoos": "ACM Fellow",
    "Ihab F. Ilyas": "ACM Fellow",
    "Lizy Kurian John": "ACM Fellow",
    "Joost-Pieter Katoen": "ACM Fellow",
    "Nam Sung Kim": "ACM Fellow",
    "Sven Koenig": "ACM Fellow",
    "David Kotz": "ACM Fellow",
    "Arvind Krishnamurthy": "ACM Fellow",
    "Ravi Kumar": "ACM Fellow",
    "Brian Levine": "ACM Fellow",
    "Kevin Leyton-Brown": "ACM Fellow",
    "Xuelong LI": "ACM Fellow",
    "Steven H. Low": "ACM Fellow",
    "Chenyang Lu": "ACM Fellow",
    "Samuel Madden": "ACM Fellow",
    "David Maltz": "ACM Fellow",
    "Volker Markl": "ACM Fellow",
    "Maja Mataric": "ACM Fellow",
    "Filippo Menczer": "ACM Fellow",
    "Jose Meseguer": "ACM Fellow",
    "Meredith Ringel Morris": "ACM Fellow",
    "Nachiappan Nagappan": "ACM Fellow",
    "Radhika Nagpal": "ACM Fellow",
    "Moni Naor": "ACM Fellow",
    "Chandra Narayanaswami": "ACM Fellow",
    "Sam H. Noh": "ACM Fellow",
    "Prakash Panangaden": "ACM Fellow",
    "Manish Parashar": "ACM Fellow",
    "Keshab K. Parhi": "ACM Fellow",
    "Haesun Park": "ACM Fellow",
    "Gordon Plotkin": "ACM Fellow",
    "Michael O. Rabin": "ACM Fellow",
    "Kui Ren": "ACM Fellow",
    "Paul Resnick": "ACM Fellow",
    "Mary Beth Rosson": "ACM Fellow",
    "Steven Salzberg": "ACM Fellow",
    "Sanjit Arunkumar Seshia": "ACM Fellow",
    "Adi Shamir": "ACM Fellow",
    "Adam Smith": "ACM Fellow",
    "Olga Sorkine-Hornung": "ACM Fellow",
    "Rick Stevens": "ACM Fellow",
    "Peter Stone": "ACM Fellow",
    "Yufei Tao": "ACM Fellow",
    "Leandros Tassiulas": "ACM Fellow",
    "Kenneth Lane Thompson": "ACM Fellow",
    "Andrew Tomkins": "ACM Fellow",
    "Olga Troyanskaya": "ACM Fellow",
    "Matthew A Turk": "ACM Fellow",
    "Toby Walsh": "ACM Fellow",
    "Laurie Ann Williams": "ACM Fellow",
    "Cathy H Wu": "ACM Fellow",
    "Shuicheng Yan": "ACM Fellow, AAAI/ACM/SAEng/IEEE/IAPR Fellow",
    "Michael J Zyda": "ACM Fellow",
    "Scott J Aaronson": "ACM Fellow",
    "Saman Amarasinghe": "ACM Fellow",
    "Kavita Bala": "ACM Fellow",
    "Magdalena Balazinska": "ACM Fellow",
    "Paul Beame": "ACM Fellow",
    "Emery David Berger": "ACM Fellow",
    "Ronald F Boisvert": "ACM Fellow",
    "Bradley G Calder": "ACM Fellow",
    "Diego Calvanese": "ACM Fellow",
    "Claire Cardie": "ACM Fellow",
    "Timothy Chan": "ACM Fellow",
    "Kanianthra Mani Chandy": "ACM Fellow",
    "Xilin Chen": "ACM Fellow",
    "Elizabeth Frances Churchill": "ACM Fellow",
    "Philip R Cohen": "ACM Fellow",
    "Vincent Conitzer": "ACM Fellow",
    "Noshir Contractor": "ACM Fellow",
    "Matthew B Dwyer": "ACM Fellow",
    "Elena Ferrari": "ACM Fellow",
    "Michael J. Freedman": "ACM Fellow",
    "Deborah Frincke": "ACM Fellow",
    "Lise Getoor": "ACM Fellow",
    "Maria L Gini": "ACM Fellow",
    "Subbarao Kambhampati": "ACM Fellow",
    "Tamara G Kolda": "ACM Fellow",
    "Xiangyang Li": "ACM Fellow",
    "Songwu Lu": "ACM Fellow",
    "Wendy Elizabeth Mackay": "ACM Fellow",
    "Sheila McIlraith": "ACM Fellow",
    "Rada Mihalcea": "ACM Fellow",
    "Robin R Murphy": "ACM Fellow",
    "Marc Najork": "ACM Fellow",
    "Jason Nieh": "ACM Fellow",
    "Timothy Pinkston": "ACM Fellow",
    "Mihai Pop": "ACM Fellow",
    "Andreas Reuter": "ACM Fellow",
    "Jeffrey S Rosenschein": "ACM Fellow",
    "Srinivasan Seshan": "ACM Fellow",
    "Prashant J Shenoy": "ACM Fellow",
    "Peter W Shor": "ACM Fellow",
    "Mona Singh": "ACM Fellow",
    "Ramesh Kumar Sitaraman": "ACM Fellow",
    "Dawn Song": "ACM Fellow",
    "Salvatore J Stolfo": "ACM Fellow",
    "Dacheng Tao": "ACM Fellow, IEEE Fellow, \u6fb3\u5927\u5229\u4e9a\u79d1\u5b66\u9662\u9662\u58eb\u3001\u65b0\u5357\u5a01\u5c14\u58eb\u7687\u5bb6\u5b66\n\u9662\u9662\u58eb\u3001\u6b27\u6d32\u79d1\u5b66\u9662\u9662\u58eb, ACM Fellow, AAAS Fellow",
    "Moshe Tennenholtz": "ACM Fellow",
    "Giovanni Vigna": "ACM Fellow",
    "Nisheeth Vishnoi": "ACM Fellow",
    "Darrell Whitley": "ACM Fellow",
    "Moustafa A Youssef": "ACM Fellow",
    "Carlo A Zaniolo": "ACM Fellow",
    "Lidong Zhou": "ACM Fellow",
    "Krste Asanovic": "ACM Fellow",
    "N Asokan": "ACM Fellow",
    "Paul Barham": "ACM Fellow",
    "Peter L Bartlett": "ACM Fellow",
    "Elizabeth Belding": "ACM Fellow",
    "Rastislav Bodik": "ACM Fellow",
    "Katy Borner": "ACM Fellow",
    "Amy S Bruckman": "ACM Fellow",
    "Jan Camenisch": "ACM Fellow",
    "Adnan Darwiche": "ACM Fellow",
    "Andre M Dehon": "ACM Fellow",
    "Premkumar T Devanbu": "ACM Fellow",
    "Tamal K Dey": "ACM Fellow",
    "Sandhya Dwarkadas": "ACM Fellow",
    "Steven Feiner": "ACM Fellow",
    "Tim Finin": "ACM Fellow",
    "Thomas Funkhouser": "ACM Fellow, ACM Fellow\uff0cFellow of Alfred P. Sloan Foundation",
    "Minos Garofalakis": "ACM Fellow",
    "Mohammad T. Hajiaghayi": "ACM Fellow",
    "Tian He": "ACM Fellow",
    "Wendi Beth Heinzelman": "ACM Fellow",
    "Aaron Hertzmann": "ACM Fellow",
    "Jessica K Hodgins": "ACM Fellow",
    "John Hughes": "ACM Fellow",
    "Johan H\u00e5stad": "ACM Fellow",
    "Charles Lee Isbell": "ACM Fellow",
    "Sanjeev Khanna": "ACM Fellow",
    "Lillian Lee": "ACM Fellow",
    "Tom Leighton": "ACM Fellow",
    "Fei-Fei Li": "ACM Fellow",
    "Michael Littman": "ACM Fellow",
    "Huan Liu": "ACM Fellow",
    "Jiebo Luo": "ACM Fellow",
    "Bruce M Maggs": "ACM Fellow",
    "Bangalore S Manjunath": "ACM Fellow",
    "Vishal Misra": "ACM Fellow",
    "Frank Mueller": "ACM Fellow",
    "David Parkes": "ACM Fellow",
    "Gurudatta Parulkar": "ACM Fellow",
    "Toniann Pitassi": "ACM Fellow",
    "Lili Qiu": "ACM Fellow",
    "Matthew Roughan": "ACM Fellow",
    "Amit Sahai": "ACM Fellow",
    "Alex C. Snoeren": "ACM Fellow",
    "Gerald Tesauro": "ACM Fellow",
    "Bhavani Thuraisingham": "ACM Fellow",
    "Salil Vadhan": "ACM Fellow",
    "Ellen M Voorhees": "ACM Fellow",
    "Avi Wigderson": "ACM Fellow",
    "Alec Wolman": "ACM Fellow",
    "Lars Birkedal": "ACM Fellow",
    "Edouard Bugnion": "ACM Fellow",
    "Margaret Burnett": "ACM Fellow",
    "Edith Cohen": "ACM Fellow",
    "Dorin Comaniciu": "ACM Fellow",
    "Susan Dray": "ACM Fellow",
    "Edward Alan Fox": "ACM Fellow",
    "Richard M Fujimoto": "ACM Fellow",
    "Shafi Goldwasser": "ACM Fellow",
    "Carla Gomes": "ACM Fellow",
    "Martin Grohe": "ACM Fellow",
    "Aarti Gupta": "ACM Fellow",
    "Venkatesan Guruswami": "ACM Fellow",
    "Steven Michael Hand": "ACM Fellow",
    "Mor Harchol-Balter": "ACM Fellow",
    "Laxmikant Kale": "ACM Fellow",
    "Michael Kass": "ACM Fellow",
    "Angelos Dennis Keromytis": "ACM Fellow",
    "Edward Knightly": "ACM Fellow",
    "Li Erran Li": "ACM Fellow",
    "Gabriel H Loh": "ACM Fellow",
    "Tomas Lozano-Perez": "ACM Fellow",
    "Clifford A Lynch": "ACM Fellow",
    "Yi Ma": "ACM Fellow, associate editor TPAMI, IJCV, IEEE, SIAM; IEEE Fellow, ACM Fellow, SIAM Fellow; ",
    "Andrew K. Mccallum": "ACM Fellow",
    "Silvio Micali": "ACM Fellow",
    "Gail C Murphy": "ACM Fellow",
    "Onur Mutlu": "ACM Fellow",
    "Nuria Oliver": "ACM Fellow",
    "Balaji Prabhakar": "ACM Fellow",
    "Tal Rabin": "ACM Fellow",
    "K. K. Ramakrishnan": "ACM Fellow",
    "Ravi Ramamoorthi": "ACM Fellow, ACM Fellow\uff0cIEEE Fellow\uff0cSIGGRAPH Academy",
    "Yvonne Rogers": "ACM Fellow",
    "Yong Rui": "ACM Fellow",
    "Bernhard Schoelkopf": "ACM Fellow",
    "Steve Seitz": "ACM Fellow",
    "Michael Sipser": "ACM Fellow",
    "Anand Sivasubramaniam": "ACM Fellow",
    "Mani B. Srivastava": "ACM Fellow",
    "Alexander Vardy": "ACM Fellow",
    "Geoffrey Voelker": "ACM Fellow",
    "Qiang Yang": "ACM Fellow",
    "Chengxiang Zhai": "ACM Fellow",
    "Aidong Zhang": "ACM Fellow",
    "Noga Alon": "ACM Fellow",
    "Paul Barford": "ACM Fellow",
    "Luca Benini": "ACM Fellow",
    "Stephen Blackburn": "ACM Fellow",
    "Dan Boneh": "ACM Fellow",
    "Carla Brodley": "ACM Fellow",
    "Justine Cassell": "ACM Fellow",
    "Erik Demaine": "ACM Fellow",
    "Allison Druin": "ACM Fellow",
    "Fredo Durand": "ACM Fellow",
    "Nick Feamster": "ACM Fellow",
    "Jason Flinn": "ACM Fellow",
    "William Freeman": "ACM Fellow",
    "Robert L. Grossman": "ACM Fellow",
    "James Hendler": "ACM Fellow",
    "Monika Henzinger": "ACM Fellow",
    "Anthony Hey": "ACM Fellow",
    "Xuedong Huang": "ACM Fellow",
    "Daniel Jackson": "ACM Fellow",
    "Robert J.K. Jacob": "ACM Fellow",
    "Somesh Jha": "ACM Fellow",
    "Ravi Kannan": "ACM Fellow",
    "Anne-Marie Kermarrec": "ACM Fellow",
    "Martin Kersten": "ACM Fellow",
    "Christos Kozyrakis": "ACM Fellow",
    "Marta Kwiatkowska": "ACM Fellow",
    "James Landay": "ACM Fellow",
    "K. Rustan M. Leino": "ACM Fellow",
    "Joseph Bryan Lyles": "ACM Fellow",
    "Todd C Mowry": "ACM Fellow",
    "Sharon Oviatt": "ACM Fellow",
    "Venkata Padmanabhan": "ACM Fellow",
    "Shwetak N Patel": "ACM Fellow",
    "David Peleg": "ACM Fellow",
    "Radia Perlman": "ACM Fellow",
    "Ganesan Ramalingam": "ACM Fellow",
    "Holly E Rushmeier": "ACM Fellow",
    "Michael E Saks": "ACM Fellow",
    "Sachin S. Sapatnekar": "ACM Fellow",
    "Abigail Sellen": "ACM Fellow",
    "Sudipta Sengupta": "ACM Fellow",
    "Andre Seznec": "ACM Fellow",
    "Valerie Taylor": "ACM Fellow",
    "Carlo Tomasi": "ACM Fellow",
    "Paul Van Oorschot": "ACM Fellow",
    "Manuela Veloso": "ACM Fellow",
    "Zhi-Hua Zhou": "ACM Fellow",
    "Nancy M Amato": "ACM Fellow",
    "David M. Blei": "ACM Fellow",
    "Naehyuck Chang": "ACM Fellow",
    "Hsinchun Chen": "ACM Fellow",
    "Mary P. Czerwinski": "ACM Fellow",
    "Giuseppe De Giacomo": "ACM Fellow",
    "Paul Dourish": "ACM Fellow",
    "Cynthia Dwork": "ACM Fellow",
    "Kevin Fall": "ACM Fellow",
    "Babak Falsafi": "ACM Fellow",
    "Michael Franz": "ACM Fellow",
    "Orna Grumberg": "ACM Fellow",
    "Ramanathan Guha": "ACM Fellow",
    "Jayant R Haritsa": "ACM Fellow",
    "Julia Hirschberg": "ACM Fellow",
    "Piotr Indyk": "ACM Fellow",
    "Tei-Wei Kuo": "ACM Fellow",
    "Xavier Leroy": "ACM Fellow",
    "Chih-Jen Lin": "ACM Fellow",
    "Bing Liu": "ACM Fellow",
    "Michael George Luby": "ACM Fellow",
    "Ueli M Maurer": "ACM Fellow",
    "Victor Miller": "ACM Fellow",
    "Elizabeth D. Mynatt": "ACM Fellow",
    "Judea Pearl": "ACM Fellow",
    "Jian Pei": "ACM Fellow",
    "Frank Pfenning": "ACM Fellow",
    "Dragomir R Radev": "ACM Fellow",
    "Sriram Rajamani": "ACM Fellow",
    "Pablo Rodriguez": "ACM Fellow",
    "Shmuel Sagiv": "ACM Fellow",
    "Peter Schroeder": "ACM Fellow",
    "Assaf Schuster": "ACM Fellow",
    "Kevin Skadron": "ACM Fellow",
    "Wang-Chiew Tan": "ACM Fellow",
    "Santosh Vempala": "ACM Fellow",
    "Tandy Warnow": "ACM Fellow",
    "Michael Wooldridge": "ACM Fellow",
    "Samson Abramsky": "ACM Fellow",
    "Vikram Adve": "ACM Fellow",
    "Foto Afrati": "ACM Fellow",
    "Charles W Bachman": "ACM Fellow",
    "Allan Borodin": "ACM Fellow",
    "Alan Bundy": "ACM Fellow",
    "Lorrie Faith Cranor": "ACM Fellow",
    "Timothy Alden Davis": "ACM Fellow",
    "Inderjit Dhillon": "ACM Fellow",
    "Nikil D. Dutt": "ACM Fellow",
    "Faith Ellen": "ACM Fellow",
    "Michael D Ernst": "ACM Fellow",
    "Adam Finkelstein": "ACM Fellow",
    "Juliana Freire": "ACM Fellow",
    "Johannes Gehrke": "ACM Fellow",
    "Eric Grimson": "ACM Fellow",
    "Mark Guzdial": "ACM Fellow",
    "Gernot Heiser": "ACM Fellow",
    "Eric Horvitz": "ACM Fellow",
    "Thorsten Joachims": "ACM Fellow",
    "Michael Kearns": "ACM Fellow",
    "Valerie King": "ACM Fellow",
    "Sarit Kraus": "ACM Fellow",
    "Leslie Lamport": "ACM Fellow",
    "Sharad Malik": "ACM Fellow",
    "Yishay Mansour": "ACM Fellow",
    "Subhasish Mitra": "ACM Fellow",
    "Michael Mitzenmacher": "ACM Fellow",
    "Robert Morris": "ACM Fellow",
    "Vijaykrishnan Narayanan": "ACM Fellow",
    "Shamkant Navathe": "ACM Fellow",
    "Jignesh M Patel": "ACM Fellow",
    "Parthasarathy Ranganathan": "ACM Fellow",
    "Omer Reingold": "ACM Fellow",
    "Tom Rodden": "ACM Fellow",
    "Ronitt Rubinfeld": "ACM Fellow",
    "Daniela Rus": "ACM Fellow",
    "Alberto Luigi Sangiovanni Vincentelli": "ACM Fellow",
    "Henning Schulzrinne": "ACM Fellow",
    "Stuart Shieber": "ACM Fellow",
    "Ramakrishnan Srikant": "ACM Fellow",
    "Aravind Srinivasan": "ACM Fellow",
    "S. Sudarshan": "ACM Fellow",
    "Paul Syverson": "ACM Fellow",
    "Gene Tsudik": "ACM Fellow",
    "Stephen J Whittaker": "ACM Fellow",
    "Mark S Ackerman": "ACM Fellow",
    "Charu Chandra Aggarwal": "ACM Fellow",
    "James H Anderson": "ACM Fellow",
    "Mihir Bellare": "ACM Fellow",
    "Christine L Borgman": "ACM Fellow",
    "Stefano Ceri": "ACM Fellow",
    "Ingemar J. Cox": "ACM Fellow",
    "Carlos J P De Lucena": "ACM Fellow",
    "Rina Dechter": "ACM Fellow",
    "Chip Elliott": "ACM Fellow",
    "David Forsyth": "ACM Fellow",
    "Wen Gao": "ACM Fellow, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "David Garlan": "ACM Fellow",
    "James Gosling": "ACM Fellow",
    "Peter Haas": "ACM Fellow",
    "Marti Hearst": "ACM Fellow",
    "Matthias Jarke": "ACM Fellow",
    "Sampath Kumar Kannan": "ACM Fellow",
    "David J Kasik": "ACM Fellow",
    "Dina Katabi": "ACM Fellow",
    "Henry A Kautz": "ACM Fellow",
    "Jon Kleinberg": "ACM Fellow",
    "Panganamala Kumar": "ACM Fellow",
    "Douglas Lea": "ACM Fellow",
    "Yoelle Maarek": "ACM Fellow",
    "Christopher Manning": "ACM Fellow",
    "Madhav Marathe": "ACM Fellow",
    "John Mellor-Crummey": "ACM Fellow",
    "Greg Morrisett": "ACM Fellow",
    "Andrew Myers": "ACM Fellow",
    "Dana Nau": "ACM Fellow",
    "Satish Rao": "ACM Fellow",
    "S E Robertson": "ACM Fellow",
    "Timothy Roscoe": "ACM Fellow",
    "Timoleon Sellis": "ACM Fellow",
    "Dennis E Shasha": "ACM Fellow",
    "Nir N Shavit": "ACM Fellow",
    "Kyuseok Shim": "ACM Fellow",
    "Padhraic Smyth": "ACM Fellow",
    "Milind Tambe": "ACM Fellow",
    "Val Tannen": "ACM Fellow",
    "David Williamson": "ACM Fellow",
    "Limsoon Wong": "ACM Fellow",
    "Ellen Zegura": "ACM Fellow",
    "David Zuckerman": "ACM Fellow",
    "Gustavo Alonso": "ACM Fellow",
    "Lars Arge": "ACM Fellow",
    "Pierre Baldi": "ACM Fellow",
    "Hans Boehm": "ACM Fellow",
    "Craig Boutilier": "ACM Fellow",
    "Tracy Camp": "ACM Fellow",
    "Rick Cattell": "ACM Fellow",
    "Larry S Davis": "ACM Fellow",
    "Ahmed Elmagarmid": "ACM Fellow",
    "Wenfei Fan": "ACM Fellow",
    "Lixin Gao": "ACM Fellow",
    "Simson L Garfinkel": "ACM Fellow",
    "Garth A Gibson": "ACM Fellow",
    "Saul Greenberg": "ACM Fellow",
    "Markus Gross": "ACM Fellow",
    "David Paul Grove": "ACM Fellow",
    "Jonathan Grudin": "ACM Fellow",
    "Rachid Guerraoui": "ACM Fellow",
    "Manish Gupta": "ACM Fellow",
    "John Hershberger": "ACM Fellow",
    "Andrew Kahng": "ACM Fellow",
    "Anna Karlin": "ACM Fellow",
    "Srinivasan Keshav": "ACM Fellow",
    "Gregor Kiczales": "ACM Fellow",
    "Masaru Kitsuregawa": "ACM Fellow",
    "Leonid Libkin": "ACM Fellow",
    "Tova Milo": "ACM Fellow",
    "Joseph O'Rourke": "ACM Fellow",
    "Benjamin Pierce": "ACM Fellow",
    "Keshav K Pingali": "ACM Fellow",
    "Andrew M Pitts": "ACM Fellow",
    "Rajeev  Ramnarain Rastogi": "ACM Fellow",
    "Raj Reddy": "ACM Fellow",
    "Keith Ross": "ACM Fellow",
    "Robert Schreiber": "ACM Fellow",
    "Steven Scott": "ACM Fellow",
    "Bart Selman": "ACM Fellow",
    "Ron Shamir": "ACM Fellow",
    "Yoav Shoham": "ACM Fellow",
    "Joseph Sifakis": "ACM Fellow",
    "Alistair Sinclair": "ACM Fellow",
    "Clifford Stein": "ACM Fellow",
    "Ion Stoica": "ACM Fellow",
    "Roberto Tamassia": "ACM Fellow",
    "Walter F. Tichy": "ACM Fellow",
    "Patrick Valduriez": "ACM Fellow",
    "Leslie G Valiant": "ACM Fellow",
    "Katherine Yelick": "ACM Fellow",
    "Ramin Zabih": "ACM Fellow",
    "Xiaodong Zhang": "ACM Fellow",
    "Serge Abiteboul": "ACM Fellow",
    "Divyakant Agrawal": "ACM Fellow",
    "Ronald M Baecker": "ACM Fellow",
    "Thomas Ball": "ACM Fellow",
    "Guy Blelloch": "ACM Fellow",
    "Carl Ebeling": "ACM Fellow",
    "David Eppstein": "ACM Fellow",
    "Geoffrey Fox": "ACM Fellow",
    "George Furnas": "ACM Fellow",
    "David K Gifford": "ACM Fellow",
    "Ramesh Govindan": "ACM Fellow",
    "Baining Guo": "ACM Fellow, IEEE/ACM Fellow, Canadian Academy of  engineering, Editor of Computer Graphics",
    "David Heckerman": "ACM Fellow",
    "Gerard J. Holzmann": "ACM Fellow",
    "Hugues Hoppe": "ACM Fellow",
    "Christian S. Jensen": "ACM Fellow",
    "Howard J Karloff": "ACM Fellow",
    "Stephen Keckler": "ACM Fellow",
    "Peter B. Key": "ACM Fellow",
    "Robert E Kraut": "ACM Fellow",
    "Susan Landau": "ACM Fellow",
    "Ming C Lin": "ACM Fellow",
    "Peter Magnusson": "ACM Fellow",
    "Dahlia Malkhi": "ACM Fellow",
    "Keith Marzullo": "ACM Fellow",
    "SATOSHI MATSUOKA": "ACM Fellow",
    "Nelson Max": "ACM Fellow",
    "Joseph Mitchell": "ACM Fellow",
    "Shubu Mukherjee": "ACM Fellow",
    "Beng Chin Ooi": "ACM Fellow",
    "Zehra Ozsoyoglu": "ACM Fellow",
    "Janos Pach": "ACM Fellow",
    "Linda Petzold": "ACM Fellow",
    "Martha Pollack": "ACM Fellow",
    "Dan Roth": "ACM Fellow",
    "John Sanguinetti": "ACM Fellow",
    "Margo Seltzer": "ACM Fellow",
    "Amit Singhal": "ACM Fellow",
    "Diane L Souvaine": "ACM Fellow",
    "Divesh Srivastava": "ACM Fellow",
    "Dan Suciu": "ACM Fellow",
    "Dean Tullsen": "ACM Fellow",
    "Amin Vahdat": "ACM Fellow",
    "David Wetherall": "ACM Fellow",
    "Frank Kenneth Zadeck": "ACM Fellow",
    "David A Abramson": "ACM Fellow",
    "Sarita Adve": "ACM Fellow",
    "Lorenzo Alvisi": "ACM Fellow",
    "Luiz Andre Barroso": "ACM Fellow",
    "Douglas C Burger": "ACM Fellow",
    "Jennifer Chayes": "ACM Fellow",
    "Peter Chen": "ACM Fellow",
    "Anne Condon": "ACM Fellow",
    "Mark Crovella": "ACM Fellow",
    "Ron Cytron": "ACM Fellow",
    "Michael D Dahlin": "ACM Fellow",
    "Amr El Abbadi": "ACM Fellow",
    "Carla S. Ellis": "ACM Fellow",
    "Christos Faloutsos": "ACM Fellow",
    "Kathleen S Fisher": "ACM Fellow",
    "Wendy Hall": "ACM Fellow",
    "Jean-Pierre Hubaux": "ACM Fellow",
    "Michael  I. Jordan": "ACM Fellow",
    "Lydia Kavraki": "ACM Fellow",
    "Sara Kiesler": "ACM Fellow",
    "Philip N Klein": "ACM Fellow",
    "Donald Kossmann": "ACM Fellow",
    "John Launchbury": "ACM Fellow",
    "Richard F Lyon": "ACM Fellow",
    "Raymond Mooney": "ACM Fellow",
    "S. Muthukrishnan": "ACM Fellow",
    "Fernando Pereira": "ACM Fellow",
    "Pavel Pevzner": "ACM Fellow",
    "Dieter Rombach": "ACM Fellow",
    "David S. Rosenblum": "ACM Fellow",
    "Stefan Savage": "ACM Fellow",
    "Robert B Schnabel": "ACM Fellow",
    "Daniel A Spielman": "ACM Fellow",
    "Subhash Suri": "ACM Fellow",
    "Frank Wm Tompa": "ACM Fellow",
    "Stephen Trimberger": "ACM Fellow",
    "David M Ungar": "ACM Fellow",
    "Andreas Zeller": "ACM Fellow",
    "Shumin Zhai": "ACM Fellow",
    "Hagit Attiya": "ACM Fellow",
    "David F Bacon": "ACM Fellow",
    "Ricardo A Baeza-Yates": "ACM Fellow",
    "Chandrajit L. Bajaj": "ACM Fellow",
    "Vijay P Bhatkar": "ACM Fellow",
    "Jose A Blakeley": "ACM Fellow",
    "Gaetano Borriello": "ACM Fellow",
    "Nell B. Dale": "ACM Fellow",
    "Bruce Davie": "ACM Fellow",
    "Jeffrey A Dean": "ACM Fellow",
    "Thomas L Dean": "ACM Fellow",
    "Bruce R. Donald": "ACM Fellow",
    "Thomas D Erickson": "ACM Fellow",
    "Paulo Esteves-Verissimo": "ACM Fellow",
    "Gerhard Fischer": "ACM Fellow",
    "Ian T Foster": "ACM Fellow",
    "Andrew V Goldberg": "ACM Fellow",
    "Michael T Goodrich": "ACM Fellow",
    "Joseph M Hellerstein": "ACM Fellow",
    "Laurie J Hendren": "ACM Fellow",
    "Urs Hoelzle": "ACM Fellow",
    "Farnam Jahanian": "ACM Fellow",
    "Erich L Kaltofen": "ACM Fellow",
    "David Karger": "ACM Fellow",
    "Arie E Kaufman": "ACM Fellow",
    "Hans-Peter Kriegel": "ACM Fellow",
    "Maurizio Lenzerini": "ACM Fellow",
    "John Chi-Shing Lui": "ACM Fellow",
    "Dinesh Manocha": "ACM Fellow",
    "Margaret Martonosi": "ACM Fellow",
    "Yossi Matias": "ACM Fellow",
    "RJ Miller": "ACM Fellow",
    "John T Riedl": "ACM Fellow",
    "Martin Rinard": "ACM Fellow",
    "Patricia G. Selinger": "ACM Fellow",
    "Rudrapatna K Shyamasundar": "ACM Fellow",
    "Shang-Hua Teng": "ACM Fellow",
    "Chandramohan A Thekkath": "ACM Fellow",
    "Robbert Van Renesse": "ACM Fellow",
    "Baba C Vemuri": "ACM Fellow",
    "Kyu-Young Whang": "ACM Fellow",
    "Yorick Wilks": "ACM Fellow",
    "Terry Winograd": "ACM Fellow",
    "Martin Abadi": "ACM Fellow",
    "Gregory D. Abowd": "ACM Fellow",
    "Alex Aiken": "ACM Fellow",
    "Sanjeev Arora": "ACM Fellow",
    "Hari Balakrishnan": "ACM Fellow",
    "William A.S. Buxton": "ACM Fellow",
    "Kenneth Clarkson": "ACM Fellow",
    "Jason Cong": "ACM Fellow",
    "Perry Cook": "ACM Fellow",
    "Stephen A Cook": "ACM Fellow",
    "Jack Davidson": "ACM Fellow",
    "Umeshwar Dayal": "ACM Fellow",
    "Xiaotie Deng": "ACM Fellow",
    "J.J. Garcia-Luna-Aceves": "ACM Fellow",
    "Michel X Goemans": "ACM Fellow",
    "Pat Hanrahan": "ACM Fellow",
    "Charles H House": "ACM Fellow",
    "Watts S Humphrey": "ACM Fellow",
    "Alan Kay": "ACM Fellow",
    "Joseph A Konstan": "ACM Fellow",
    "Roy Levin": "ACM Fellow",
    "Paul G Lowney": "ACM Fellow",
    "Kathryn S McKinley": "ACM Fellow",
    "Bertrand Meyer": "ACM Fellow",
    "John C. Mitchell": "ACM Fellow",
    "Ian Munro": "ACM Fellow",
    "Judith S Olson": "ACM Fellow",
    "Lawrence C Paulson": "ACM Fellow",
    "Hamid Pirahesh": "ACM Fellow",
    "Brian Randell": "ACM Fellow",
    "Michael Reiter": "ACM Fellow",
    "Jennifer Rexford": "ACM Fellow",
    "Jonathan Rose": "ACM Fellow",
    "Mendel Rosenblum": "ACM Fellow",
    "Rob A Rutenbar": "ACM Fellow",
    "Tuomas Sandholm": "ACM Fellow",
    "Vivek Sarkar": "ACM Fellow",
    "Per O Stenstrom": "ACM Fellow",
    "Madhu Sudan": "ACM Fellow",
    "Douglas B Terry": "ACM Fellow",
    "Anant Agarwal": "ACM Fellow",
    "Utpal Banerjee": "ACM Fellow",
    "Catriel Beeri": "ACM Fellow",
    "Avrim Blum": "ACM Fellow",
    "Eric A. Brewer": "ACM Fellow",
    "Andrei Broder": "ACM Fellow",
    "Michael F. Cohen": "ACM Fellow",
    "Larry Constantine": "ACM Fellow",
    "Danny Dolev": "ACM Fellow",
    "Rodney Downey": "ACM Fellow",
    "Edward A Feigenbaum": "ACM Fellow",
    "Edward Felten": "ACM Fellow",
    "Lance Fortnow": "ACM Fellow",
    "Georg Gottlob": "ACM Fellow",
    "Richard Hull": "ACM Fellow",
    "Daniel Huttenlocher": "ACM Fellow",
    "Tao Jiang": "ACM Fellow",
    "John C Klensin": "ACM Fellow",
    "Monica Lam": "ACM Fellow",
    "Marc Levoy": "ACM Fellow",
    "Bud Mishra": "ACM Fellow",
    "Eliot Moss": "ACM Fellow",
    "Rajeev Motwani": "ACM Fellow",
    "Martin Odersky": "ACM Fellow",
    "Gary M Olson": "ACM Fellow",
    "Randy Pausch": "ACM Fellow",
    "Amir Pnueli": "ACM Fellow",
    "Eric S Roberts": "ACM Fellow",
    "Donald E Thomas": "ACM Fellow",
    "Philip Wadler": "ACM Fellow",
    "Mitchell Wand": "ACM Fellow",
    "HongJiang Zhang": "ACM Fellow",
    "Eric Allender": "ACM Fellow",
    "Arvind Arvind": "ACM Fellow",
    "Mikhail Atallah": "ACM Fellow",
    "Ming-Syan Chen": "ACM Fellow",
    "Susan T Dumais": "ACM Fellow",
    "Usama M Fayyad": "ACM Fellow",
    "Matthias Felleisen": "ACM Fellow",
    "Kenneth Forbus": "ACM Fellow",
    "Phillip B Gibbons": "ACM Fellow",
    "Lee Giles": "ACM Fellow",
    "Albert G Greenberg": "ACM Fellow",
    "William D Gropp": "ACM Fellow",
    "Roch Guerin": "ACM Fellow",
    "John Guttag": "ACM Fellow",
    "Laura M Haas": "ACM Fellow",
    "Alon Yitzchak Halevy": "ACM Fellow",
    "Anthony C Hearn": "ACM Fellow",
    "Thomas A Henzinger": "ACM Fellow",
    "John E. Laird": "ACM Fellow",
    "James Larus": "ACM Fellow",
    "Charles E Leiserson": "ACM Fellow",
    "Ming Li": "ACM Fellow",
    "Nick McKeown": "ACM Fellow",
    "J Strother Moore": "ACM Fellow",
    "Alan Newell": "ACM Fellow",
    "Peter Norvig": "ACM Fellow",
    "Dianne Prost OLeary": "ACM Fellow",
    "Dan R Olsen": "ACM Fellow",
    "Oyekunle Olukotun": "ACM Fellow",
    "M. Tamer Ozsu": "ACM Fellow",
    "Vern Paxson": "ACM Fellow",
    "Michael Scott": "ACM Fellow",
    "Alfred Z Spector": "ACM Fellow",
    "Victor Vianu": "ACM Fellow",
    "Marianne Winslett": "ACM Fellow",
    "Alexander L Wolf": "ACM Fellow",
    "Bryant W York": "ACM Fellow",
    "Stanley Zdonik": "ACM Fellow",
    "Lixia Zhang": "ACM Fellow",
    "Thomas Anderson": "ACM Fellow",
    "Dines Bjorner": "ACM Fellow",
    "Stephen Bourne": "ACM Fellow",
    "Rodney A Brooks": "ACM Fellow",
    "Surajit Chaudhuri": "ACM Fellow",
    "Keith D Cooper": "ACM Fellow",
    "David Dill": "ACM Fellow",
    "Christophe Diot": "ACM Fellow",
    "Michael J Franklin": "ACM Fellow",
    "Robert Harper": "ACM Fellow",
    "Maurice Herlihy": "ACM Fellow",
    "Phokion Kolaitis": "ACM Fellow",
    "T V Lakshman": "ACM Fellow",
    "Brad A Myers": "ACM Fellow",
    "David M Nicol": "ACM Fellow",
    "Krishna Palem": "ACM Fellow",
    "Thomas Reps": "ACM Fellow",
    "Mikkel Thorup": "ACM Fellow",
    "Eli Upfal": "ACM Fellow",
    "Umesh Vazirani": "ACM Fellow",
    "Vijay V Vazirani": "ACM Fellow",
    "Gerhard Weikum": "ACM Fellow",
    "Daniel Weld": "ACM Fellow",
    "Michael Wellman": "ACM Fellow",
    "Jennifer Widom": "ACM Fellow",
    "Walter Willinger": "ACM Fellow",
    "David A Wood": "ACM Fellow",
    "Hui Zhang": "ACM Fellow",
    "Janis A Bubenko": "ACM Fellow",
    "Luca Cardelli": "ACM Fellow",
    "Andrew A Chien": "ACM Fellow",
    "George E Collins": "ACM Fellow",
    "Allan Gottlieb": "ACM Fellow",
    "Vicki Hanson": "ACM Fellow",
    "Mark D. Hill": "ACM Fellow",
    "Yannis Ioannidis": "ACM Fellow",
    "Frans Kaashoek": "ACM Fellow",
    "Per-Ake Larson": "ACM Fellow",
    "Peter Lee": "ACM Fellow",
    "Paul Mockapetris": "ACM Fellow",
    "Simon L Peyton-Jones": "ACM Fellow",
    "Richard Schantz": "ACM Fellow",
    "Michael D Schroeder": "ACM Fellow",
    "Stamatis Vassiliadis": "ACM Fellow",
    "Benjamin W. Wah": "ACM Fellow",
    "David S Wise": "ACM Fellow",
    "Rakesh Agrawal": "ACM Fellow",
    "Victor Bahl": "ACM Fellow",
    "Bonnie Berger": "ACM Fellow",
    "John Carroll": "ACM Fellow",
    "Richard Demillo": "ACM Fellow",
    "Barbara J Grosz": "ACM Fellow",
    "Brent T Hailpern": "ACM Fellow",
    "Jiawei Han": "ACM Fellow",
    "Mary Harrold": "ACM Fellow",
    "Mark A Horowitz": "ACM Fellow",
    "Paul Hudak": "ACM Fellow",
    "H V Jagadish": "ACM Fellow",
    "Anil K Jain": "ACM Fellow",
    "Ramesh C Jain": "ACM Fellow",
    "Dexter Kozen": "ACM Fellow",
    "Yi-Bing Lin": "ACM Fellow",
    "Kathleen McKeown": "ACM Fellow",
    "Thomas Moran": "ACM Fellow",
    "Eugene Myers": "ACM Fellow",
    "Craig Partridge": "ACM Fellow",
    "Daniel A Reed": "ACM Fellow",
    "Stuart Russell": "ACM Fellow",
    "Scott J Shenker": "ACM Fellow",
    "Gurindar S Sohi": "ACM Fellow",
    "C J Van Rijsbergen": "ACM Fellow",
    "Pankaj Agarwal": "ACM Fellow",
    "Vishwani D Agrawal": "ACM Fellow",
    "Ozalp Babaoglu": "ACM Fellow",
    "Jon Crowcroft": "ACM Fellow",
    "Thomas G Dietterich": "ACM Fellow",
    "Susan Eggers": "ACM Fellow",
    "Harold N Gabow": "ACM Fellow",
    "Adolfo Guzman": "ACM Fellow",
    "Joseph Halpern": "ACM Fellow",
    "Neil Immerman": "ACM Fellow",
    "Sidney Karin": "ACM Fellow",
    "Wendy A Kellogg": "ACM Fellow",
    "David B Lomet": "ACM Fellow",
    "Gary L Miller": "ACM Fellow",
    "C. Mohan": "ACM Fellow",
    "Jeffrey F Naughton": "ACM Fellow",
    "Bantwal R Rau": "ACM Fellow",
    "David H Salesin": "ACM Fellow",
    "Mahadev Satyanarayanan": "ACM Fellow",
    "George Varghese": "ACM Fellow",
    "John Wilkes": "ACM Fellow",
    "Robert Aiken": "ACM Fellow",
    "Tetsuo Asano": "ACM Fellow",
    "Philip A Bernstein": "ACM Fellow",
    "Joel S Birnbaum": "ACM Fellow",
    "Alan H Borning": "ACM Fellow",
    "Yuri Breitbart": "ACM Fellow",
    "Jin-Yi Cai": "ACM Fellow",
    "David D Clark": "ACM Fellow",
    "Susan B Davidson": "ACM Fellow",
    "Johan DeKleer": "ACM Fellow",
    "Giovanni DeMicheli": "ACM Fellow",
    "David J. Farber": "ACM Fellow",
    "Joan Feigenbaum": "ACM Fellow",
    "Sally J Floyd": "ACM Fellow",
    "Erol Gelenbe": "ACM Fellow",
    "John P Hayes": "ACM Fellow",
    "Joseph F Jaja": "ACM Fellow",
    "Robert E Kahn": "ACM Fellow",
    "Sung Mo Kang ": "ACM Fellow",
    "Richard B Kieburtz": "ACM Fellow",
    "Robert A Kowalski": "ACM Fellow",
    "Jeffrey Kramer": "ACM Fellow",
    "Ruby B Lee": "ACM Fellow",
    "Witold Litwin": "ACM Fellow",
    "Barton P Miller": "ACM Fellow",
    "Jeffrey C Mogul": "ACM Fellow",
    "Donald A. Norman": "ACM Fellow",
    "Cherri M Pancake": "ACM Fellow",
    "Christos Papadimitriou": "ACM Fellow",
    "Donn B Parker": "ACM Fellow",
    "Janak H Patel": "ACM Fellow",
    "Ira Pohl": "ACM Fellow",
    "John Mark Pullen": "ACM Fellow",
    "Prabhakar Raghavan": "ACM Fellow",
    "Krithivasan Ramamritham": "ACM Fellow",
    "John C Reynolds": "ACM Fellow",
    "George Robertson": "ACM Fellow",
    "Nick Roussopoulos": "ACM Fellow",
    "Krishan K. Sabnani": "ACM Fellow",
    "Ravinderpal S Sandhu": "ACM Fellow",
    "Hans-Joerg Schek": "ACM Fellow",
    "Richard D Schlichting": "ACM Fellow",
    "David Bernard Shmoys": "ACM Fellow",
    "Marilyn Claire Wolf": "ACM Fellow",
    "Ouri Wolfson": "ACM Fellow",
    "Pamela Zave": "ACM Fellow",
    "Francine Berman": "ACM Fellow",
    "Laxminarayan Bhuyan Bhuyan": "ACM Fellow",
    "Alan W Biermann": "ACM Fellow",
    "Shahid H Bokhari": "ACM Fellow",
    "Randal E Bryant": "ACM Fellow",
    "Peter Buneman": "ACM Fellow",
    "Stuart K. Card": "ACM Fellow",
    "Michael J Carey": "ACM Fellow",
    "Douglas E Comer": "ACM Fellow",
    "Karen Duncan": "ACM Fellow",
    "Deborah Estrin": "ACM Fellow",
    "Ronald Fagin": "ACM Fellow",
    "Peter A Freeman": "ACM Fellow",
    "Wesley Kent Fuchs": "ACM Fellow",
    "Donald J Haderle": "ACM Fellow",
    "Michael Heath": "ACM Fellow",
    "Leonard Kleinrock": "ACM Fellow",
    "Henry F Korth": "ACM Fellow",
    "Axel Van Lamsweerde": "ACM Fellow",
    "Raymond A Lorie": "ACM Fellow",
    "Donald W Loveland": "ACM Fellow",
    "Albert R Meyer": "ACM Fellow",
    "James H Morris": "ACM Fellow",
    "Larry L Peterson": "ACM Fellow",
    "Moshe Y Vardi": "ACM Fellow",
    "David S Warren": "ACM Fellow",
    "Reinhard Wilhelm": "ACM Fellow",
    "Willy E Zwaenepoel": "ACM Fellow",
    "Marc Auslander": "ACM Fellow",
    "Ken Birman": "ACM Fellow",
    "Ronald J. Brachman": "ACM Fellow",
    "Robert T Braden": "ACM Fellow",
    "Rob Cook": "ACM Fellow",
    "Joseph S DeBlasi": "ACM Fellow",
    "Richard J Fateman": "ACM Fellow",
    "James D Foley": "ACM Fellow",
    "John D Gannon": "ACM Fellow",
    "Charles M Geschke": "ACM Fellow",
    "Robert L Glass": "ACM Fellow",
    "Ronald L. Graham": "ACM Fellow",
    "Leonidas Guibas": "ACM Fellow, ACM Fellow, IEEE Fellow, National Academy of Engineering Member, National Academy of Arts and Sciences Member",
    "Toshihide Ibaraki": "ACM Fellow",
    "Philip M Lewis": "ACM Fellow",
    "David MacQueen": "ACM Fellow",
    "C. Dianne Martin": "ACM Fellow",
    "Larry M Masinter": "ACM Fellow",
    "Kurt Mehlhorn": "ACM Fellow",
    "David L Mills": "ACM Fellow",
    "Pamela Samuelson": "ACM Fellow",
    "Richard Snodgrass": "ACM Fellow",
    "Mary Lou Soffa": "ACM Fellow",
    "Chung Jen Tan": "ACM Fellow",
    "Koji Torii": "ACM Fellow",
    "David L Waltz": "ACM Fellow",
    "John Warnock": "ACM Fellow",
    "Akinori Yonezawa": "ACM Fellow",
    "Dharma P Agrawal": "ACM Fellow",
    "Gregory R Andrews": "ACM Fellow",
    "Andrew W Appel": "ACM Fellow",
    "James C Browne": "ACM Fellow",
    "Robert S Cartwright": "ACM Fellow",
    "Peter P Chen": "ACM Fellow",
    "Lori Clarke": "ACM Fellow",
    "Richard J Cole": "ACM Fellow",
    "Clarence A Ellis": "ACM Fellow",
    "Richard Gabriel": "ACM Fellow",
    "Gopal Krishna Gupta": "ACM Fellow",
    "James Jay Horning": "ACM Fellow",
    "Neil Jones": "ACM Fellow",
    "Aravind K Joshi": "ACM Fellow",
    "Stephen T Kent": "ACM Fellow",
    "Simon S Lam": "ACM Fellow",
    "Kai Li": "ACM Fellow",
    "David Maier": "ACM Fellow",
    "David S Notkin": "ACM Fellow",
    "Susan H Nycum": "ACM Fellow",
    "Leon J Osterweil": "ACM Fellow",
    "Venkat Rangan": "ACM Fellow",
    "John T Richards": "ACM Fellow",
    "Lawrence A Rowe": "ACM Fellow",
    "Barbara Gershon Ryder": "ACM Fellow",
    "Alan Selman": "ACM Fellow",
    "Carlo H Sequin": "ACM Fellow",
    "Eugene H. Spafford": "ACM Fellow",
    "Eva Tardos": "ACM Fellow",
    "Richard N Taylor": "ACM Fellow",
    "Albert J Turner": "ACM Fellow",
    "Emmerich Welzl": "ACM Fellow",
    "Jeannette M. Wing": "ACM Fellow",
    "Mihalis Yannakakis": "ACM Fellow",
    "Stuart Zweben": "ACM Fellow",
    "Ian F Akyildiz": "ACM Fellow",
    "Jean-Loup E Baer": "ACM Fellow",
    "Roger R Bate": "ACM Fellow",
    "J D Couger": "ACM Fellow",
    "Gordon B Davis": "ACM Fellow",
    "David P Dobkin": "ACM Fellow",
    "Hector Garcia-Molina": "ACM Fellow",
    "Irene Greif": "ACM Fellow",
    "Yuri Gurevich": "ACM Fellow",
    "John L Hennessy": "ACM Fellow",
    "Richard A. Kemmerer": "ACM Fellow",
    "H W Lawson": "ACM Fellow",
    "Der-Tsai Lee": "ACM Fellow",
    "Richard Lipton": "ACM Fellow",
    "Nancy Lynch": "ACM Fellow",
    "Daniel A. Menasce": "ACM Fellow",
    "Ron Perrott": "ACM Fellow",
    "Nicholas Pippenger": "ACM Fellow",
    "Vaughan Ronald Pratt": "ACM Fellow",
    "John H Reif": "ACM Fellow",
    "Raymond Reiter": "ACM Fellow",
    "Robert Sedgewick": "ACM Fellow",
    "Kenneth C Sevcik": "ACM Fellow",
    "Micha Sharir": "ACM Fellow",
    "Alan C Shaw": "ACM Fellow",
    "Ben Shneiderman": "ACM Fellow",
    "Kenneth Steiglitz": "ACM Fellow",
    "Donald F Towsley": "ACM Fellow",
    "Peter Widmayer": "ACM Fellow",
    "Robert Wilensky": "ACM Fellow",
    "Philip S Yu": "ACM Fellow",
    "Paolo Zanella": "ACM Fellow",
    "William Richards Adrion": "ACM Fellow",
    "Alfred V Aho": "ACM Fellow",
    "Kurt B Akeley": "ACM Fellow",
    "Gregor V Bochmann": "ACM Fellow",
    "Anita Borg": "ACM Fellow",
    "B. Chandrasekaran": "ACM Fellow",
    "Bernard Chazelle": "ACM Fellow",
    "George Dodd": "ACM Fellow",
    "Jose L. Encarnacao": "ACM Fellow",
    "Jeanne Ferrante": "ACM Fellow",
    "Michael J Fischer": "ACM Fellow",
    "Dennis Frailey": "ACM Fellow",
    "Robert M Graham": "ACM Fellow",
    "Michael A Harrison": "ACM Fellow",
    "Mary Jane Irwin": "ACM Fellow",
    "Jeffrey Jaffe": "ACM Fellow",
    "Anita K Jones": "ACM Fellow",
    "Randy H. Katz": "ACM Fellow",
    "Maria Klawe": "ACM Fellow",
    "Lawrence H Landweber": "ACM Fellow",
    "Michael E Lesk": "ACM Fellow",
    "Henry M Levy": "ACM Fellow",
    "Barbara Liskov": "ACM Fellow",
    "Richard R Muntz": "ACM Fellow",
    "Richard E Nance": "ACM Fellow",
    "Bryan Preas": "ACM Fellow",
    "TRN Rao": "ACM Fellow",
    "Edward M Reingold": "ACM Fellow",
    "John Rice": "ACM Fellow",
    "Sartaj K Sahni": "ACM Fellow",
    "John E Savage": "ACM Fellow",
    "Ravi Sethi": "ACM Fellow",
    "Mary M Shaw": "ACM Fellow",
    "John A Stankovic": "ACM Fellow",
    "Larry Stockmeyer": "ACM Fellow",
    "Andrew S Tanenbaum": "ACM Fellow",
    "Mary K Vernon": "ACM Fellow",
    "Uzi Vishkin": "ACM Fellow",
    "Jeffrey S. Vitter": "ACM Fellow",
    "Anthony I Wasserman": "ACM Fellow",
    "Fred W Weingarten": "ACM Fellow",
    "Ian Witten": "ACM Fellow",
    "Marshall C Yovits": "ACM Fellow",
    "Paul W Abrahams": "ACM Fellow",
    "R L Ashenhurst": "ACM Fellow",
    "Alan H Barr": "ACM Fellow",
    "Grady Booch": "ACM Fellow",
    "David H Brandin": "ACM Fellow",
    "Richard P Brent": "ACM Fellow",
    "Loren C Carpenter": "ACM Fellow",
    "Edwin Catmull": "ACM Fellow",
    "Robert Constable": "ACM Fellow",
    "Dorothy E Denning": "ACM Fellow",
    "David DeWitt": "ACM Fellow",
    "Erwin Engeler": "ACM Fellow",
    "Zvi Galil": "ACM Fellow",
    "Michael R Garey": "ACM Fellow",
    "Myron Ginsberg": "ACM Fellow",
    "John B Goodenough": "ACM Fellow",
    "Donald Greenberg": "ACM Fellow",
    "Herbert R J Grosch": "ACM Fellow",
    "Bertram Herzog": "ACM Fellow",
    "Harold J Highland": "ACM Fellow",
    "Lance Hoffman": "ACM Fellow",
    "David S Johnson": "ACM Fellow",
    "Cliff B Jones": "ACM Fellow",
    "Kenneth W Kennedy": "ACM Fellow",
    "Won Kim": "ACM Fellow",
    "Sambasiva Kosaraju": "ACM Fellow",
    "Richard E. Ladner": "ACM Fellow",
    "S Lakshmivarahan": "ACM Fellow",
    "Nancy Leveson": "ACM Fellow",
    "Jayadev Misra": "ACM Fellow",
    "J Nievergelt": "ACM Fellow",
    "Anthony Oettinger": "ACM Fellow",
    "Franco P Preparata": "ACM Fellow",
    "Roy F Rada": "ACM Fellow",
    "Daniel J Rosenkrantz": "ACM Fellow",
    "Gerard Salton": "ACM Fellow",
    "Fred B Schneider": "ACM Fellow",
    "Norihisa Suzuki": "ACM Fellow",
    "Jeffrey D Ullman": "ACM Fellow",
    "Chris S Wallace": "ACM Fellow",
    "Peter Wegner": "ACM Fellow",
    "John R. White": "ACM Fellow",
    "J Turner Whitted": "ACM Fellow",
    "Chak-Kuen Wong": "ACM Fellow",
    "Andrew C Yao": "ACM Fellow",
    "Paul Young": "ACM Fellow",
    "James M Adams": "ACM Fellow",
    "Franz L Alt": "ACM Fellow",
    "William F. Atchison": "ACM Fellow",
    "Richard H Austing": "ACM Fellow",
    "Kenneth E Batcher": "ACM Fellow",
    "C Gordon Bell": "ACM Fellow",
    "Michael W Blasgen": "ACM Fellow",
    "Daniel Bobrow": "ACM Fellow",
    "David R. Boggs": "ACM Fellow",
    "Lorraine Borman": "ACM Fellow",
    "Charles L Bradshaw": "ACM Fellow",
    "Daniel S Bricklin": "ACM Fellow",
    "Frederick Brooks": "ACM Fellow",
    "Douglas K Brotz": "ACM Fellow",
    "Richard R. Burton": "ACM Fellow",
    "Richard G Canning": "ACM Fellow",
    "Walter Carlson": "ACM Fellow",
    "Vinton Cerf": "ACM Fellow",
    "Edgar F Codd": "ACM Fellow",
    "Ed Coffman": "ACM Fellow",
    "Fernando J Corbato": "ACM Fellow",
    "Harvey G Cragon": "ACM Fellow",
    "Thomas A D'Auria": "ACM Fellow",
    "Thomas DeFanti": "ACM Fellow",
    "Peter J Denning": "ACM Fellow",
    "Jack Dennis": "ACM Fellow",
    "L Peter Deutsch": "ACM Fellow",
    "Edsger W Dijkstra": "ACM Fellow",
    "Stephen Dunwell": "ACM Fellow",
    "J Presper Eckert": "ACM Fellow",
    "Peter Elias": "ACM Fellow",
    "Gerald L Engel": "ACM Fellow",
    "John H Esbin": "ACM Fellow",
    "Bob O Evans": "ACM Fellow",
    "Aaron Finerman": "ACM Fellow",
    "Robert W. Floyd": "ACM Fellow",
    "Michael J Flynn": "ACM Fellow",
    "Robert M Frankston": "ACM Fellow",
    "Frank Friedman": "ACM Fellow",
    "Bernard A Galler": "ACM Fellow",
    "Charles W Gear": "ACM Fellow",
    "Adele Goldberg": "ACM Fellow",
    "Calvin C. Gotlieb": "ACM Fellow",
    "Susan L Graham": "ACM Fellow",
    "Jim Gray": "ACM Fellow",
    "Cordell Green": "ACM Fellow",
    "David Joseph Gries": "ACM Fellow",
    "Carl Hammer": "ACM Fellow",
    "Richard W Hamming": "ACM Fellow",
    "Fred H Harris": "ACM Fellow",
    "Juris Hartmanis": "ACM Fellow",
    "William Daniel Hillis": "ACM Fellow",
    "John E Hopcroft": "ACM Fellow",
    "Tom Hull": "ACM Fellow",
    "J N Hume": "ACM Fellow",
    "Harry D Huskey": "ACM Fellow",
    "William Kahan": "ACM Fellow",
    "Ronald M Kaplan": "ACM Fellow",
    "Richard Karp": "ACM Fellow",
    "Donald E Knuth": "ACM Fellow",
    "David J Kuck": "ACM Fellow",
    "Thomas E Kurtz": "ACM Fellow",
    "Ray Kurzweil": "ACM Fellow",
    "Butler W Lampson": "ACM Fellow",
    "Stephen S Lavenberg": "ACM Fellow",
    "Joshua Lederberg": "ACM Fellow",
    "John A Lee": "ACM Fellow",
    "Meir Lehman": "ACM Fellow",
    "Joyce Currie Little": "ACM Fellow",
    "C.L. Liu": "ACM Fellow",
    "M Stuart Lynn": "ACM Fellow",
    "Herbert Maisel": "ACM Fellow",
    "Zohar Manna": "ACM Fellow",
    "John McCarthy": "ACM Fellow",
    "Daniel D McCracken": "ACM Fellow",
    "Paul R McJones": "ACM Fellow",
    "A J Milner": "ACM Fellow",
    "Roger M Needham": "ACM Fellow",
    "Peter G Neumann": "ACM Fellow",
    "Monroe Newborn": "ACM Fellow",
    "John Ousterhout": "ACM Fellow",
    "Susan S Owicki": "ACM Fellow",
    "David Lorge Parnas": "ACM Fellow",
    "William B Poucher": "ACM Fellow",
    "Anthony Ralston": "ACM Fellow",
    "Ronald L Rivest": "ACM Fellow",
    "Azriel Rosenfeld": "ACM Fellow",
    "Jeff Rulifson": "ACM Fellow",
    "Jean E Sammet": "ACM Fellow",
    "Dana S Scott": "ACM Fellow",
    "Herbert A Simon": "ACM Fellow",
    "Barbara B Simons": "ACM Fellow",
    "Donald R Slutz": "ACM Fellow",
    "Richard E Stearns": "ACM Fellow",
    "Thomas B Steel": "ACM Fellow",
    "Guy L Steele": "ACM Fellow",
    "Michael Stonebraker": "ACM Fellow",
    "William Strecker": "ACM Fellow",
    "Patrick Suppes": "ACM Fellow",
    "Gerald Sussman": "ACM Fellow",
    "Ivan Sutherland": "ACM Fellow",
    "Edward A Taft": "ACM Fellow",
    "Robert E Tarjan": "ACM Fellow",
    "Robert W Taylor": "ACM Fellow",
    "Charles P Thacker": "ACM Fellow",
    "Irv Traiger": "ACM Fellow",
    "Joseph Traub": "ACM Fellow",
    "Allen Tucker": "ACM Fellow",
    "Andries van Dam": "ACM Fellow",
    "Willis H Ware": "ACM Fellow",
    "Ben Wegbreit": "ACM Fellow",
    "Eric A Weiss": "ACM Fellow",
    "David John Wheeler": "ACM Fellow",
    "Maurice V. Wilkes": "ACM Fellow",
    "Shmuel Winograd": "ACM Fellow",
    "Niklaus E Wirth": "ACM Fellow",
    "Seymour J Wolfson": "ACM Fellow",
    "William A Wulf": "ACM Fellow",
    "L A Zadeh": "ACM Fellow",
    "Luc Van Gool": "Marr\u5956\u5f97\u4e3b\uff0cETH\u6559\u6388\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u5b9e\u9a8c\u5ba4\u4e3b\u4efb",
    "Alan Yuille": "IEEE Fellow",
    "Richard Hartley": "IEEE Fellow",
    "Philip H. S. Torr": "FREng Fellow, FRS Fellow, Royal Academy of Engineering Research Chair in Computer Vision and Machine Learning",
    "Song-Chun Zhu": "IEEE Fellow",
    "Andreas Geiger": "ELLIS fellow",
    "Trevor Darrell": "associate editor for the Artificial Intelligence Journal and the IEEE Transactions on Pattern Analysis and Machine Intelligence.",
    "Xiaoou Tang": "IEEE Fellow, Associate Editor of IEEE TPAMI and editor-in-chief of IJCV, founder of SenseTime",
    "Kostas Daniilidis": "IEEE Fellow, Associate Editor of IEEE TPAMI, Ruth Yalom Stone Professor of Computer Vision at the University of Pennsylvania",
    "Max Welling": "CIFAR Fellow, ELLIS Fellow, Associate editor in chief of IEEE TPAMI (2011-2015), advisory board of the Neurips foundation",
    "Christian Theobalt": "Fellow of EUROGRAPHICS, Associate editor of IEEE TPAMI and ACM ToG, Director of Visual Computing and AI Department at MPI for Informatics",
    "Steven M. Seitz": "Alfred P. Sloan Fellowship, Twice awarded the David Marr Prize, NSF Career Award, ONR Young Investigator Award",
    "William T. Freeman": "ACM Fellow, IEEE Fellow, AAAI Fellow, National Academy of Engineering Member",
    "Joshua B. Tenenbaum": "MacArthur Fellow, Fellow of Society of Experimental Psychologists, Fellow of Cognitive Science Society, associate editor of the journal Cognitive Science",
    "Silvio Savarese": "Executive Vice President and Chief Scientist of salesforce, Faculty co-director of Standford Vision and Learning Lab, NSF Career Award",
    "Li Fei-Fei": "IEEE Fellow, National Academy of Engineering Member, National Academy of Medicine Member, American Academy of Arts and Sciences Member, J.K. Aggarwal Prize of IAPR, Co-Director of the Stanford Vision and Learning Lab",
    "Thomas Brox": "ELLIS fellow, Associate Editor of IJCV, IEEE TPAMI",
    "Davide Scaramuzza": "Associate Editor for the IEEE Transactions on Robotics, IEEE Robotics and Automation Society Early Career Award",
    "Alexei A. Efros": "Journal Editorial Board of IJCV, PAMI Thomas S. Huang Memorial Prize",
    "Phillip Isola": "Google Faculty Research Award, PAMI Young Researcher Award",
    "Tae-Kyun Kim": "Associate Editor of Pattern Recognition Journal, Image and Vision Computing Journal, and IET Computer Vision",
    "Nassir Navab": "IEEE Fellow",
    "Michael J. Black": "foreign member of the Royal Swedish Academy of Sciences",
    "David A. Forsyth": "IEEE Fellow, ACM Fellow, editor of PAMI",
    "Andrew Zisserman": "Marr\u5956\u5f97\u4e3b\u3001\u82f1\u56fd\u7687\u5bb6\u5b66\u4f1a\u4f1a\u58eb\u3001BMVA Distinguished Fellowship",
    "Stephen Lin": "Editorial Board of IJCV",
    "Geoffrey Hinton": " fellow of the Royal Society, the Royal Society of Canada, and the Association for the Advancement of Artificial Intelligence, foreign member of the American Academy of Arts and Sciences and the National Academy of Engineering, and a former president of the Cognitive Science Society.",
    "Eric P. Xing": "Board member of The international machine learning society, associate editor JASA, AOAS, JMLR, MLJ and PAMI",
    "Niloy J. Mitra": "associate editor of CGF, CAGD, TOG",
    "Raquel Urtasun": "Editorial Board of IJCV",
    "Daniel Cremers": "member of the Bavarian Academy of Sciences and Humanitites. associate for ICCV, ECCV, CVPR, ACCV, IROS",
    "Ping Tan": "Director of Alibaba DAMO academy",
    "Baoquan Chen": "IEEE Fellow\uff0c\u66fe\u4efbTOG\u3001 TVCG\u7b49\u9876\u520a\u7f16\u59d4\uff0cVIS Academy",
    "Paul Debevec": "\u827e\u7f8e\u5956\u7ec8\u8eab\u6210\u5c31\u5956\u5f97\u4e3b",
    "Xin Tong": "\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u9996\u5e2d\u7814\u7a76\u5458",
    "Andrew Davison": "Fellow of the Royal Academy of Engineering",
    "Geoffrey E. Hinton": "\u56fe\u7075\u5956\uff0cFellow of the Royal Society, \u7f8e\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Gordon Wetzstein": "Associate editor of TOG",
    "Ali Farhadi": "Sloan Research Fellowships",
    "Qionghai Dai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "D.A. Forsyth": "IEEE/ACM Fellow, editor in chief, the TPAMI",
    "Antonio Torralba": "AAAI Fellow, associate editor of IJCV, program chair for CVPR",
    "Dietor Fox": "IEEE/ACM/AAAI Fellow, editor of the IEEE Transactions on Robotics, program co-chair of the 2008 AAAI Conference on Artificial Intelligence, and program chair of the 2013 Robotics: Science and Systems conference.",
    "Charlie C.L. Wang": "\u7f8e\u56fd\u673a\u68b0\u5de5\u7a0b\u5b66\u4f1aFellow",
    "Gang Hua": "IEEE & IAPR Fellow",
    "Jiri Matas": "Associate Editor of TPAMI",
    "Pascal Fua": "IEEE Fellow",
    "Erik Blasch": "IEEE Fellow, AIAA Associate Fellow",
    "Long Quan": "IEEE Fellow",
    "Andrea Vedaldi": "Associate Editor of TPAMI",
    "Bing Zeng": "IEEE Fellow",
    "Roberto Cipolla": "\u82f1\u56fd\u7687\u5bb6\u5b66\u4f1a\u9662\u58eb\uff0c\u82f1\u56fd\u7687\u5bb6\u5de5\u7a0b\u5b66\u9662\u9662\u58eb",
    "Weimin Bao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guilin Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guoliang Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hanfu Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junliang Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xingdan Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junhao Chu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tiejun Cui": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ruwei Dai": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chibiao Ding": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guangren Duan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yunmei Dong": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jiancheng Fang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dengguo Feng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Fuxi Gan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qihuang Gong": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ying Gu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiaohong Guan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guangcan Guo": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lei Guo": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yue Hao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jifeng He": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chaohuan Hou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xun Hou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jinpeng Huai": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lin Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Minqiang Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ru Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wei Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shuisheng Jian": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Fengyi Jiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jie Jiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yaqiu Jin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dingbo Kuang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiaolin Lei": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qihu Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shushen Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wei Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiang Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yanda Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhi Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Huimin Lin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guozhi Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shenggang Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Songhao Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yichun Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongtan Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianhua Lu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ruqian Lu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jian Lv": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junfa Mao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Kunchi Peng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Depei Qian": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hong Qiao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guogang Qin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xubang Shen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jian Song": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huaimin Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jiaqi Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jinlong Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianyu Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lijun Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qiming Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yangyuan Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongliang Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yuzhu Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yue Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhanguo Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhijiang Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhaohui Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dexin Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hongxin Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Peiheng Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yirong Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianbai Xia": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Libin Xiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zongben Xu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ningsheng Xu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongqi Xue": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Deren Yang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xuejun Yang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianquan Yao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qizhi Yao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hao Yin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dengyun Yu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Bo Zhang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jingzhong Zhang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianhua Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wanhua Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yaozong Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Youliao Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhiming Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Bingkun Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chaochen Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xingming Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhixin Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Luhua Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ninghua Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhongliang Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tianyou Chai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Chun Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhijie Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zuoning Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hao Dai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhonghan Deng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Wenhua Ding": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Baoyan Duan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bangkui Fan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Binxing Fang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Aiguo Fei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weihua Gui": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "You He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bitao Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Changjun Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huilin Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhiyin Kong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yushi Lan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Detian Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Deyi Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiangke Liao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yongjian Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zejin Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Teng Long": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jun Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xicheng Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiangang Luo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yi Luo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yueguang Lv": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yunhe Pan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Donglin Su": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiaguang Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ninghui Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiubin Tan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Endong Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shafei Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yaonan Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yiyin Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hanming Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiangxing Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianping Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianqi Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Manqing Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weiren Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yangsheng Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiaoniu Yang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Fuqiang Yao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Quan Yu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shaohua Yu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Baodong Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangjun Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hongke Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ping Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yaoxue Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Qinping Zhao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Nanning Zheng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weimin Zheng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "De Ben": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hegao Cai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiren Cai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jing Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lianghui Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Dianyuan Fan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiaxiong Fang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xisheng Feng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Gao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huixing Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xianyi Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhiben Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guirong Guo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Dequan He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xingui He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Qiheng Hu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Peikang Huang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Wenhan Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guofan Jin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yilian Jin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bohu Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Deren Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guojie Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lemin Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Tongbao Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Youping Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yongnian Lin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shanghe Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yunjie Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianxun Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yuanliang Ma": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Erke Mao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangnan Ni": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junhua Pan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Changxiang Shen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junhong Su": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Youxian Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yu Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Chengwei Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Renxiang Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Tianran Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zicai Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yu Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhengyao Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ziqing Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Cheng Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hequan Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shouer\u00b7Silamu Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Juyan Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zuyan Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shizhong Yang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junen Yao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Minghan Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shangfu Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shenghua Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangyi Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lvqian Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Minggao Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xixiang Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhonghua Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shan Zhong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Liwei Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shouhuan Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhongyi Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Gaofeng Zhu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Songlin Zhuang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb"
  }
  var papers = [
    {
      "title": "superplane: 3d plane detection and description from a single image",
      "id": 40,
      "valid_pdf_number": "5/6",
      "matched_pdf_number": "4/5",
      "matched_rate": 0.8,
      "citations": {
        "Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction": {
          "authors": [
            "D Chen",
            "H Li",
            "W Ye",
            "Y Wang",
            "W Xie"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10747190/",
          "ref_texts": "[64] Weicai Ye, Hai Li, Tianxiang Zhang, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. SuperPlane: 3D plane detection and description from a single image. In 2021 IEEE Virtual Reality and 3D User Interfaces (VR), pages 207\u2013215. IEEE, 2021.",
          "ref_ids": [
            "64"
          ],
          "1": "I NTRODUCTION N OVEL view synthesis and geometry reconstruction are challenging and crucial tasks in computer vision, widely used in AR/VR [13], [64], 3D content generation [10], [18], [49], [54], and autonomous driving."
        },
        "Deflowslam: Self-supervised scene motion decomposition for dynamic dense slam": {
          "authors": [
            "W Ye",
            "X Yu",
            "X Lan",
            "Y Ming",
            "J Li",
            "H Bao"
          ],
          "url": "https://zju3dv.github.io/deflowslam/doc/DeFlowSLAM.pdf",
          "ref_texts": "[1] W. Ye, H. Li, T. Zhang, X. Zhou, H. Bao, and G. Zhang, \u201cSuperPlane: 3D Plane Detection and Description from a Single Image,\u201d in IEEE Virtual Reality and 3D User Interfaces , 2021.",
          "ref_ids": [
            "1"
          ],
          "1": "In AR applications, SLAM is often leveraged to provide accurate localization for agents, which facilitates users to place virtual objects [1], while the dense reconstruction is urgently needed to better interact with the surrounding environments."
        },
        "GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting and Meshing": {
          "authors": [
            "J Zhang",
            "Y Zheng",
            "Z Li",
            "Q Dai",
            "X Yuan"
          ],
          "url": "https://arxiv.org/abs/2412.05908",
          "ref_texts": "[4] W. Ye, H. Li, T. Zhang, X. Zhou, H. Bao, and G. Zhang, \u201cSuperplane: 3d plane detection and description from a single image,\u201d in 2021 IEEE Virtual Reality and 3D User Interfaces (VR). IEEE, 2021, pp. 207\u2013215.",
          "ref_ids": [
            "4"
          ],
          "1": "8, AUGUST 2015 2 3D content with broad applications in meta-universe[3], [4], autonomous driving [5], robotics manipulation [6], etc."
        },
        "Hybrid tracker with pixel and instance for video panoptic segmentation": {
          "authors": [
            "W Ye",
            "X Lan",
            "G Su",
            "H Bao",
            "Z Cui",
            "G Zhang"
          ],
          "url": "https://arxiv.org/abs/2203.01217",
          "ref_texts": "[8] W. Ye, H. Li, T. Zhang, X. Zhou, H. Bao, and G. Zhang, \u201cSuperPlane: 3D Plane Detection and Description from a Single Image,\u201d in 2021 IEEE Virtual Reality and 3D User Interfaces (VR), 2021, pp. 207\u2013215.",
          "ref_ids": [
            "8"
          ],
          "1": "To address the issue of training convergence that usually exists for the instance-based tracker [7], [8], we present a differentiable matching layer to obtain the correspondence between different instances and then train the network with the cross-entropy loss.",
          "2": "Existing methods use triplet loss [8] or contrast loss [4] to pull the same instance as close as possible in feature space and push different instances as far as possible."
        }
      }
    },
    {
      "title": "disp r-cnn: stereo 3d object detection via shape prior guided instance disparity estimation",
      "id": 10,
      "valid_pdf_number": "67/100",
      "matched_pdf_number": "51/67",
      "matched_rate": 0.7611940298507462,
      "citations": {
        "3D object detection for autonomous driving: A comprehensive survey": {
          "authors": [
            "J Mao",
            "S Shi",
            "X Wang",
            "H Li"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-023-01790-1",
          "ref_texts": "266. Sun J., Cao Y ., Chen Q. A., Mao Z. M. (2020) Towards robust {LiDAR-based} perception in autonomous driving: General blackbox adversarial sensor attack and countermeasures. In: USENIX Security 267. Sun J., Chen L., Xie Y ., Zhang S., Jiang Q., Zhou X., Bao H. (2020) Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In: CVPR",
          "ref_ids": [
            "266"
          ],
          "1": "[266] study the general vulnerability of current LiDAR-based 3D object detection models and identify the ignored occlusion patterns in LiDAR point clouds that make vehicles vulnerable to spoofing attacks."
        },
        "Robustness-aware 3d object detection in autonomous driving: A review and outlook": {
          "authors": [
            "Z Song",
            "L Liu",
            "F Jia",
            "Y Luo",
            "C Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10637966/",
          "ref_texts": "[213] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. 10 548\u201310 557.",
          "ref_ids": [
            "213"
          ],
          "1": "Disp R-CNN [CVPR2020] [213], TL-Net [CVPR2019] [214], ZoomNet [AAAI2020] [215], IDA-3D [CVPR2020] [216], YOLOStereo3D [ICRA2021] [14], SIDE [W ACV2022][217], VPFNet [TMM2022] [218], FCNet [Entropy2022] [219], MC-Stereo [arXiv2023] [220], PCW-Net [ECCV2022] [221], ICVP [ICIP2023] [222], MoCha-Stereo [arXiv2024] [223], UCFNet [TPAMI2023] [224], IGEV-Stereo [CVPR2023] [225], NMRF-Stereo [arXiv2024] [226].",
          "2": "97 Disp R-CNN [213] CVPR2020 58.",
          "3": "This paradigm has been widely adopted by subsequent works [14], [194], [213]\u2013[217].",
          "4": "3: (a) The AP3D comparison of monocular-based methods [10], [13], [23], [28], [29], [178], [182], [183], [185], [370] and stereo-based methods [14], [17], [213], [215], [230], [233], [235], [236], [285], [371] on KITTI test dataset."
        },
        "Monodtr: Monocular 3d object detection with depth-aware transformer": {
          "authors": [
            "Chih Huang",
            "Han Wu",
            "Ting Su",
            "Winston H. Hsu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Huang_MonoDTR_Monocular_3D_Object_Detection_With_Depth-Aware_Transformer_CVPR_2022_paper.html",
          "ref_texts": "[44] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. InCVPR, 2020.",
          "ref_ids": [
            "44"
          ],
          "1": "Previous methods have achieved superior performance based on the accurate depth information from multiple sensors, such as LiDAR signal [16, 22, 39, 40] or stereo matching [6, 23, 44, 52]."
        },
        "Milestones in autonomous driving and intelligent vehicles\u2014Part II: Perception and planning": {
          "authors": [
            "L Chen",
            "S Teng",
            "B Li",
            "X Na",
            "Y Li",
            "Z Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10156892/",
          "ref_texts": "[63] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 548\u201310 557.",
          "ref_ids": [
            "63"
          ],
          "1": "Disp-RCNN [63], OC-Stereo [61] add segmentation modules paired images from stereo cameras to induce accurate association.",
          "2": "00 Disp-RCNN[63] Stereo 43."
        },
        "R-MSFM: Recurrent multi-scale feature modulation for monocular depth estimating": {
          "authors": [
            "Zhongkai Zhou",
            "Xinnan Fan",
            "Pengfei Shi",
            "Yuanxue Xin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhou_R-MSFM_Recurrent_Multi-Scale_Feature_Modulation_for_Monocular_Depth_Estimating_ICCV_2021_paper.html",
          "ref_texts": "[36] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10548\u201310557, 2020.",
          "ref_ids": [
            "36"
          ],
          "1": "Depth estimation as a low-level task is crucial to complete higher-level tasks, including 3-D reconstruction[23], autonomous driving[6], 3-D target detection[36], underwater image restoration[43], and many more."
        },
        "3d object detection from images for autonomous driving: a survey": {
          "authors": [
            "X Ma",
            "W Ouyang",
            "A Simonelli"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10373157/",
          "ref_texts": "[50] J. Sun, L. Chen, Y. Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in CVPR, 2020.",
          "ref_ids": [
            "50"
          ],
          "1": "With the Region Proposal Network (RPN) [35], the detectors can generate 2D proposals using features from the last shared convolutional layer instead of external algorithms, which saves most of the computational cost, and lots of image-based 3D detectors [41], [42], [43], [44], [45], [20], [46], [47], [48], [49], [40], [50], [51], [52], [53] adopted this design.",
          "2": "Except for the improvement of depth estimation [55], [56], [94] and stereo matching [50], [88], [93], there are some other methods that improve the quality of depth maps.",
          "3": "Compared with the 2D bounding box, the methods in [59], [51], [52], [50] adopt instance mask, which is a better filter but requires additional data with ground-truth masks (Section 6 discusses how to use auxiliary data to generate the mask annotations) Besides, the methods in [96], [97] propose to address this problem in the depth estimation phase.",
          "4": "19 Disp R-CNN[50] CVPR\u201920 \u2713 58."
        },
        "Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector": {
          "authors": [
            "Xiaoyang Guo",
            "Shaoshuai Shi",
            "Xiaogang Wang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Guo_LIGA-Stereo_Learning_LiDAR_Geometry_Aware_Representations_for_Stereo-Based_3D_Detector_ICCV_2021_paper.html",
          "ref_texts": "[49] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, 2020.",
          "ref_ids": [
            "49"
          ],
          "2": "Disp R-CNN [49] and ZoomNet [65] incorporated extra instance segmentation mask and part location map to improve detection quality.",
          "3": "06 Disp-RCNN [49] 64.",
          "4": "56 Disp-RCNN [49] 68.",
          "5": "93 Disp-RCNN [49] 37."
        },
        "Wasserstein distances for stereo disparity estimation": {
          "authors": [
            "D Garg",
            "Y Wang",
            "B Hariharan"
          ],
          "url": "https://proceedings.neurips.cc/paper/2020/hash/fe7ecc4de28b2c83c016b5c6c2acd826-Abstract.html",
          "ref_texts": "[36] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, 2020. 8",
          "ref_ids": [
            "36"
          ],
          "1": "3 DISP R-CNN [36] 74."
        },
        "Devnet: Self-supervised monocular depth learning via density volume construction": {
          "authors": [
            "K Zhou",
            "L Hong",
            "C Chen",
            "H Xu",
            "C Ye",
            "Q Hu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19842-7_8",
          "ref_texts": "38. Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp R-CNN: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, 2020. 1",
          "ref_ids": [
            "38"
          ],
          "1": "Devnet: Self-supervised monocular depth learning via density volume construction"
        },
        "Concrete 3D printing: Process parameters for process control, monitoring and diagnosis in automation and construction": {
          "authors": [
            "Tan Kai",
            "Noel Quah",
            "Yi Wei",
            "Daniel Tay",
            "Jian Hui",
            "Ming Jen",
            "Teck Neng",
            "King Ho",
            "Holden Li"
          ],
          "url": "https://www.mdpi.com/2227-7390/11/6/1499",
          "ref_texts": "184. Sun, J.; Chen, L.; Xie, Y.; Zhang, S.; Jiang, Q.; Zhou, X.; Bao, H. Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020; pp. 10545\u201310554. Available online: https://arxiv.org/abs/2004.03572v1",
          "ref_ids": [
            "184"
          ],
          "1": "Concrete 3D printing: Process parameters for process control, monitoring and diagnosis in automation and construction",
          "2": "Concrete 3D printing: Process parameters for process control, monitoring and diagnosis in automation and construction"
        },
        "Stereo CenterNet-based 3D object detection for autonomous driving": {
          "authors": [
            "Yuguang Shi",
            "Yu Guo",
            "Zhenqiang Mi",
            "Xinjie Li"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231221017264",
          "ref_texts": "[20] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10548\u201310557, 2020. 1, 3",
          "ref_ids": [
            "20"
          ],
          "1": "Stereo CenterNet-based 3D object detection for autonomous driving",
          "2": "Stereo CenterNet-based 3D object detection for autonomous driving"
        },
        "Dsgn++: Exploiting visual-spatial relation for stereo-based 3d detectors": {
          "authors": [
            "Y Chen",
            "S Huang",
            "S Liu",
            "B Yu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9852285/",
          "ref_texts": "[77] J. Sun, L. Chen, Y. Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in CVPR, 2020, pp. 10 548\u201310 557.",
          "ref_ids": [
            "77"
          ],
          "1": "Dsgn++: Exploiting visual-spatial relation for stereo-based 3d detectors",
          "2": "Dsgn++: Exploiting visual-spatial relation for stereo-based 3d detectors"
        },
        "PLUMENet: Efficient 3D object detection from stereo images": {
          "authors": [
            "Y Wang",
            "B Yang",
            "R Hu",
            "M Liang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9635875/",
          "ref_texts": "[41] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in CVPR, 2020. 2, 5, 6",
          "ref_ids": [
            "41"
          ],
          "1": "OCStereo [40], CG-stereo [16] and disp-RCNN [41] add auxiliary 2D bounding box regression or instance segmentation tasks to help distinguish the background from foreground pixels.",
          "2": "0 350 Disp-RCNN [41] 73.",
          "3": "3 400 Disp-RCNN [41] 90."
        },
        "Dort: Modeling dynamic objects in recurrent for multi-camera 3d object detection and tracking": {
          "authors": [
            "Q Lian",
            "T Wang",
            "D Lin",
            "J Pang"
          ],
          "url": "https://arxiv.org/abs/2303.16628",
          "ref_texts": "[37] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, 2020. 3",
          "ref_ids": [
            "37"
          ],
          "1": "One line of methods develop neural-network-based cost volumes [52, 53, 36, 40, 41, 37] to construct cross-frame visual cues for 3D perception."
        },
        "Dimension embeddings for monocular 3d object detection": {
          "authors": [
            "Yunpeng Zhang",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Guan Huang",
            "Dalong Du",
            "Jie Zhou",
            "Jiwen Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Dimension_Embeddings_for_Monocular_3D_Object_Detection_CVPR_2022_paper.html",
          "ref_texts": "[45] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, 2020. 1",
          "ref_ids": [
            "45"
          ],
          "1": "Most existing methods rely on point clouds from LiDAR devices [18, 40, 41, 51] or binocular images from stereo cameras [7, 20, 37, 45] for accurate 3D object detec*Corresponding author."
        },
        "BAAM: Monocular 3D pose and shape reconstruction with bi-contextual attention module and attention-guided modeling": {
          "authors": [
            "Jun Lee",
            "Hanul Kim",
            "Min Choi",
            "Gyun Jeong",
            "Yeong Jun"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.html",
          "ref_texts": "[44] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, pages 10548\u201310557, 2020. 1",
          "ref_ids": [
            "44"
          ],
          "1": "To acquire precise 3D information, some prior arts have relied on specific devices such as LiDAR [3,10,42] and stereo vision [26,44]."
        },
        "Visual tracking of deepwater animals using machine learning-controlled robotic underwater vehicles": {
          "authors": [
            "Kakani Katija",
            "Paul L. D",
            "Joost Daniels",
            "Alexandra Lapides",
            "Kevin Barnard",
            "Mike Risi",
            "Ben Y. Ranaan",
            "Benjamin G. Woodward",
            "Jonathan Takahashi"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Katija_Visual_Tracking_of_Deepwater_Animals_Using_Machine_Learning-Controlled_Robotic_Underwater_WACV_2021_paper.html",
          "ref_texts": "[42] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao. Disp R-CNN: Stereo 3D object detection via shape prior guided instance disparity estimation. In 2020 IEEE/CVF Conference on Computer V ision and P attern Recognition (CVPR), pages 10545\u201310554. IEEE, jun 2020.",
          "ref_ids": [
            "42"
          ],
          "1": "Leveraging object-centric model output, stereo matching algorithms can integrate correspondence matching into the network to outperform pixel-level matching [33], compute object-specific disparity [42], or estimate volumetric bounding boxes from monocular imagery [30]."
        },
        "Exploring intermediate representation for monocular vehicle pose estimation": {
          "authors": [
            "Shichao Li",
            "Zengqiang Yan",
            "Hongyang Li",
            "Ting Cheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Li_Exploring_intermediate_representation_for_monocular_vehicle_pose_estimation_CVPR_2021_paper.html",
          "ref_texts": "[55] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 10548\u201310557, 2020.",
          "ref_ids": [
            "55"
          ],
          "1": "T o pursue high accuracy , recent studies have exploited active sensors to obtain point clouds [63, 69, 47] or the use of stereo cameras [32, 55, 46].",
          "2": "74 Disp R-CNN [55] CVPR\u2019 20 RGB + LiDAR Stereo 93.",
          "3": "Without using LiDAR data [26] or temporal information [4], our system out-performs previous monocular RGB-based methods and even stereo methods [11, 55].",
          "4": "Ego-Net even outperforms DSGN [11] and Disp RCNN [55] that use stereo cameras."
        },
        "Keypoint3D: Keypoint-based and Anchor-Free 3D object detection for Autonomous driving with Monocular Vision": {
          "authors": [
            "Zhen Li",
            "Yuliang Gao",
            "Qingqing Hong",
            "Yuren Du",
            "Seiichi Serikawa",
            "Lifeng Zhang"
          ],
          "url": "https://www.mdpi.com/2072-4292/15/5/1210",
          "ref_texts": "15. Sun, J.; Chen, L.; Xie, Y.; Zhang, S.; Jiang, Q.; Zhou, X.; Bao, H. Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 14\u201319 June 2020; pp. 10545\u201310554. [CrossRef]",
          "ref_ids": [
            "15"
          ],
          "1": "[15] proposed Disp R-CNN to build a novel instance disparity estimation network (iDispNet) to estimate depth values only for objects of interest."
        },
        "ESGN: Efficient stereo geometry network for fast 3D object detection": {
          "authors": [
            "A Gao",
            "Y Pang",
            "J Nie",
            "Z Shao",
            "J Cao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9869894/",
          "ref_texts": "[31] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp rcnn: Stereo 3d object detection via shape prior guided instance disparity estimation. Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3, 7",
          "ref_ids": [
            "31"
          ],
          "1": "OC-Stereo [20] and Disp RCNN [31] only consider the foreground regions of point cloud and achieve a better performance.",
          "2": "63 Disp R-CNN [31] 387ms 45.",
          "3": "44 Disp R-CNN [31] 387ms 64."
        },
        "Towards fair and comprehensive comparisons for image-based 3D object detection": {
          "authors": [
            "Xinzhu Ma",
            "Yongtao Wang",
            "Yinmin Zhang",
            "Zhiyi Xia",
            "Yuan Meng",
            "Zhihui Wang",
            "Haojie Li",
            "Wanli Ouyang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ma_Towards_Fair_and_Comprehensive_Comparisons_for_Image-Based_3D_Object_Detection_ICCV_2023_paper.html",
          "ref_texts": "[74] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. InCVPR, 2020.2",
          "ref_ids": [
            "74"
          ],
          "1": "The followers in this group are involved in the following aspects: improving the quality of pseudo-LiDAR [89, 20], focusing on foreground objects [82, 74, 88], end-to-ending training [59], feature representation [47, 53], geometric constraints [85] or confidence refinement [69]."
        },
        "Semi-supervised stereo-based 3d object detection via cross-view consensus": {
          "authors": [
            "Wenhao Wu",
            "Hau San",
            "Si Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wu_Semi-Supervised_Stereo-Based_3D_Object_Detection_via_Cross-View_Consensus_CVPR_2023_paper.html",
          "ref_texts": "[35] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10548\u201310557, 2020. 2",
          "ref_ids": [
            "35"
          ],
          "1": "[35] implemented disparity estimation at the instance-level supervised by disparity maps generated from a shape prior model."
        },
        "OA-BEV: Bringing object awareness to bird's-eye-view representation for multi-camera 3D object detection": {
          "authors": [
            "X Chu",
            "J Deng",
            "Y Zhao",
            "J Ji",
            "Y Zhang",
            "H Li"
          ],
          "url": "https://arxiv.org/abs/2301.05711",
          "ref_texts": "[39] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp R-CNN: stereo 3d object detection via shape prior guided instance disparity estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3",
          "ref_ids": [
            "39"
          ],
          "1": "Disp R-CNN [39] learns depth information and semantic cues simultaneously and predicts instance-level disparity to generate instance point clouds."
        },
        "Transformer-based stereo-aware 3d object detection from binocular images": {
          "authors": [
            "H Sun",
            "Y Pang",
            "J Cao",
            "J Xie",
            "X Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10700602/",
          "ref_texts": "[26] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp R-CNN: Stereo 3D object detection via shape prior guided instance disparity estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition. Seattle, W A, USA: IEEE, Jun. 2020, pp. 10 545\u2013",
          "ref_ids": [
            "26"
          ],
          "2": "Disp R-CNN [26] generates instance-wise disparities [41] and utilizes external 3D models to densify such information as done in ZoomNet.",
          "5": "RTS3D [31], YOLOStereo3D, and Disp R-CNN [26] also use the feature pyramid for both unary features and stereo features; but do not use a method similar to the SPFPN to fuse multi-scale stereo features."
        },
        "A virtual construction vehicles and workers dataset with three-dimensional annotations": {
          "authors": [
            "G Yuexiong"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0952197624001222",
          "ref_texts": "[22] J. Sun, L. Chen, Y. Xie, S. Zhang, Q. Jiang, X. Zhou, H. Bao, Disp R -CNN: stereo 3d object detection via shape prior guided instance disparity estimation, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020), pp. 10545 \u201310554. https://doi.org/10.1109/CVPR42600.2020.01056. ",
          "ref_ids": [
            "22"
          ],
          "1": "Currently, some advanced monocular 3D object detection models include SMOKE [17], FCOS3D [18], PGD [19], MonoFlex [20], ImVoxelNet [21], while binocular/stereo models are Disp R-CNN [22], DSGN [23], YoLoStereo3D [24], DSGN++ [25], etc."
        },
        "A survey for 3d object detection algorithms from images": {
          "authors": [
            "HL Lee",
            "Y Kim",
            "BG Kim"
          ],
          "url": "https://koreascience.kr/article/JAKO202229261382087.page",
          "ref_texts": "[8] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \"Disp r-cnn: Stereo 3D object detection via shape prior guided instance disparity estimation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10548-10557. ",
          "ref_ids": [
            "8"
          ],
          "2": "The Second is Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation (Disp R-CNN) [8]."
        },
        "MS23D: A 3D object detection method using multi-scale semantic feature points to construct 3D feature layer": {
          "authors": [
            "Y Shao",
            "A Tan",
            "B Wang",
            "T Yan",
            "Z Sun",
            "Y Zhang",
            "J Liu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0893608024005471",
          "ref_texts": "[7] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, H. Bao, Disp rcnn: Stereo 3d object detection via shape prior guided instance disparity estimation, in: Proceedings of the IEEE /CVF conference on computer vision and pattern recognition, 2020, pp. 10548\u201310557.",
          "ref_ids": [
            "7"
          ],
          "1": "Introduction With the continuous development of environmental perception technology [1, 2, 3, 4, 5, 6] and 3D sensors [7, 8, 9, 10, 11, 12], the application of LiDAR in autonomous driving is gradually maturing."
        },
        "Directtracker: 3d multi-object tracking using direct image alignment and photometric bundle adjustment": {
          "authors": [
            "M Gladkova",
            "N Korobov",
            "N Demmel"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981260/",
          "ref_texts": "[4] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 10548\u201310557, 2020.",
          "ref_ids": [
            "4"
          ],
          "1": "A significant amount of work has been devoted to development of accurate 3D detectors [1], [2], [3], [4] and design of robust 3D trackers [5], [6], [7] that are capable of assigning consistent ids to 3D detections corresponding to the same physical objects."
        },
        "Towards model generalization for monocular 3d object detection": {
          "authors": [
            "Z Li",
            "Z Chen",
            "A Li",
            "L Fang",
            "Q Jiang",
            "X Liu"
          ],
          "url": "https://arxiv.org/abs/2205.11664",
          "ref_texts": "[30] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In CVPR, pages 10548\u201310557, 2020.",
          "ref_ids": [
            "30"
          ],
          "1": "Previous methods have achieved engaging performance based on the accurate spatial information from multiple sensors, such as LiDAR-scanned point clouds [47, 18, 29] or stereo images [6, 20, 30]."
        },
        "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras": {
          "authors": [
            "H Cho",
            "J Kang",
            "Y Kim",
            "KJ Yoon"
          ],
          "url": "https://arxiv.org/abs/2502.19630",
          "ref_texts": "[92] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10548\u201310557, 2020. 2",
          "ref_ids": [
            "92"
          ],
          "1": "Given that cameras offer significant cost advantages over LiDAR sensors, many researchers have focused on developing methods that leverage camera-based systems for 3D object detection using image-only inputs [7, 10, 24, 28, 31, 36, 44, 52, 58\u201360, 63, 70, 92, 98, 100, 118]."
        },
        "Prototype-aware heterogeneous task for point cloud completion": {
          "authors": [
            "J Tang",
            "J Xu",
            "J Gong",
            "H Song",
            "Y Xie",
            "L Ma"
          ],
          "url": "https://arxiv.org/abs/2209.01733",
          "ref_texts": "[34] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 10 548\u201310 557. 4",
          "ref_ids": [
            "34"
          ],
          "1": "Soft-perceptual Priors Learning Prior is widely used in 3D tasks, especially in reconstruction problems [6], [33], [34].",
          "2": "To learn the prior in a data-driven manner, the common choice is the Variational AutoEncoder (V AE) or its derivation [33], [34], which encodes all input data into a latent space which represents the shape information."
        },
        "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization": {
          "authors": [
            "J Xu",
            "Y Zhang",
            "Z Cai",
            "D Huang"
          ],
          "url": "https://arxiv.org/abs/2503.03430",
          "ref_texts": "[35] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10548\u201310557, 2020. 2",
          "ref_ids": [
            "35"
          ],
          "1": "Image-based methods can be further classified into monocular [7, 17, 38], stereo [9, 19, 35], and multi-view [14, 48, 49] approaches based on the number of onboard cameras."
        },
        "MonoLite3D: Lightweight 3D Object Properties Estimation": {
          "authors": [
            "A El-Dawy",
            "A El-Zawawi",
            "M El-Habrouk"
          ],
          "url": "https://arxiv.org/abs/2503.02201",
          "ref_texts": "[55] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in CVPR, 2020.",
          "ref_ids": [
            "55"
          ],
          "2": "Notably[55], MonoLite3D achieves these commendable orientation scores within an impressive execution time of just 0."
        },
        "Effect of Fog Particle Size Distribution on 3D Object Detection Under Adverse Weather Conditions": {
          "authors": [
            "A Shinde",
            "G Sharma",
            "M Pattanaik",
            "SN Singh"
          ],
          "url": "https://arxiv.org/abs/2408.01085",
          "ref_texts": "[14] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp R-CNN: Stereo 3D object detection via shape prior guided instance disparity estimation,\u201d in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 13-19, 2020, pp. 10 548\u201310 557.",
          "ref_ids": [
            "14"
          ],
          "1": "Commonly in algorithms such as pseudo-LiDAR++ [13], disparity-based region-based convolutional neural network (Disp R-CNN) [14], and deep stereo geometry network (DSGN) [15], stereo cameras are utilized for 3D object detection."
        },
        "Real-time Stereo-based 3D Object Detection for Streaming Perception": {
          "authors": [
            "C Li",
            "Z Gu",
            "G Chen",
            "L Huang",
            "W Zhang"
          ],
          "url": "https://arxiv.org/abs/2410.12394",
          "ref_texts": "[49] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10548\u201310557, 2020.",
          "ref_ids": [
            "49"
          ],
          "1": "These methods [31, 49, 56, 44, 42] typically begin by feeding stereo image pairs into a 2D detector or an instance segmentation task to generate prior information before performing 3D object detection."
        },
        "Star-Convolution for Image-Based 3D Object Detection": {
          "authors": [
            "Yuxuan L",
            "Zhenhua Xu",
            "Ming Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9811612/",
          "ref_texts": "[13] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, and and Hujun Bao Xiaowei Zhou. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
          "ref_ids": [
            "13"
          ],
          "1": "DispRCNN [13], ZoomNet [14], and OC Stereo [15] apply instance segmentation on both images to construct a local point cloud for each proposed instance to improve the accuracy of stereo matching on foreground pixels."
        },
        "PV-SSD: A Multi-Modal Point Cloud Feature Fusion Method for Projection Features and Variable Receptive Field Voxel Features": {
          "authors": [
            "Y Shao",
            "A Tan",
            "Z Sun",
            "E Zheng",
            "T Yan"
          ],
          "url": "https://arxiv.org/abs/2308.06791",
          "ref_texts": "[1] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. 10 548\u201310 557.",
          "ref_ids": [
            "1"
          ],
          "1": "I NTRODUCTION T HE actual application of three-dimensional sensors [1]\u2013 [4], such as LiDAR, in autonomous driving scenarios has greatly enhanced the performance of autonomous driving environment perception systems."
        },
        "DH-Fusion: Depth-Aware Hybrid Feature Fusion for Multimodal 3D Object Detection": {
          "authors": [
            "M Ji",
            "J Yang",
            "S Zhang"
          ],
          "url": "https://openreview.net/forum?id=gBOQ0ACqoO",
          "ref_texts": "[48] Sun, J., Chen, L., Xie, Y ., Zhang, S., Jiang, Q., Zhou, X., Bao, H.: Disp r-cnn: Stereo 3d object427 detection via shape prior guided instance disparity estimation. In: CVPR (2020)428",
          "ref_ids": [
            "48"
          ],
          "1": "Depending on the form of inputs,86 they can be divided into monocular [2, 24, 32, 41, 47, 55], stereo [6, 25, 30, 48, 70], and multi-view87 [19, 27, 56, 62] 3D object detectors."
        },
        "Accurate and Real-time Pseudo Lidar Detection: Is Stereo Neural Network Really Necessary?": {
          "authors": [
            "H Meng",
            "C Li",
            "G Chen",
            "A Knoll"
          ],
          "url": "https://arxiv.org/abs/2206.13858",
          "ref_texts": "[20] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao, \u201cDisp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 10 548\u201310 557, 2020.",
          "ref_ids": [
            "20"
          ],
          "1": "Disp-RCNN[20] and ZoomNet[21] utilize extra instance segmentation mask to obtain the object of interest with decent improvements.",
          "2": "63 Disp RCNN[20] S+I+C 425 ms 90.",
          "3": "9 Disp RCNN[20] 425 ms 74."
        },
        "Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection": {
          "authors": [
            "A Gao",
            "J Cao",
            "Y Pang"
          ],
          "url": "https://arxiv.org/abs/2106.10013",
          "ref_texts": "[40] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation. Proc. IEEE Conference on Computer Vision and Pattern Recognition , 2020.",
          "ref_ids": [
            "40"
          ],
          "2": "OC-Stereo [30] and Disp RCNN [40] only consider point cloud coming from the foreground regions.",
          "4": "63 Disp R-CNN [40] Depth+Instance Mask+CAD 425ms 90.",
          "5": "60 Disp R-CNN [40] Depth+Instance Mask+CAD 425ms 90."
        },
        "\u81ea\u52d5\u904b\u8ee2\u306e\u305f\u3081\u306e 3 \u6b21\u5143\u7269\u4f53\u691c\u51fa\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u95a2\u3059\u308b\u7814\u7a76": {
          "authors": [
            "\u674e\u93ae"
          ],
          "url": "https://kyutech.repo.nii.ac.jp/record/2000292/files/kou_k_581.pdf",
          "ref_texts": "[18] J. Sun, L. Chen, Y. Xie, S. Zhang, Q. Jiang, X. Zhou, H. Bao, \u201cDisp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation\u201d, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.10545-10554 (2020)",
          "ref_ids": [
            "18"
          ],
          "1": "The detecting performance of different types of fusion frameworks are evaluated on KITTI [18], Waymo [63], and Nuscenes [?] data sets.",
          "2": "[18] proposed a new framework called Disp R-CNN for 3D object detection using stereo images.",
          "5": "[18] proposed Disp R-CNN to build a novel instance disparity estimation network (iDispNet) to estimate depth values only for objects of interest."
        }
      }
    },
    {
      "title": "representing long volumetric video with temporal gaussian hierarchy",
      "id": 49,
      "valid_pdf_number": "3/3",
      "matched_pdf_number": "3/3",
      "matched_rate": 1.0,
      "citations": {
        "EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D Gaussian Representation": {
          "authors": [
            "C Zhang",
            "Y Zhou",
            "S Wang",
            "W Li",
            "D Wang",
            "Y Xu"
          ],
          "url": "https://arxiv.org/abs/2503.05162",
          "ref_texts": "[54] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. ACM Transactions on Graphics, 43(6), 2024. 3",
          "ref_ids": [
            "54"
          ],
          "1": "[5, 29, 51, 54] on the other hand, lift 3D Gaussians to 4D Gaussians primitives and facilitates a more versatile approach to handle complex scenes."
        },
        "4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives": {
          "authors": [
            "Z Yang",
            "Z Pan",
            "X Zhu",
            "L Zhang",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2412.20720",
          "ref_texts": "[47] Z. Xu, Y . Xu, Z. Yu, S. Peng, J. Sun, H. Bao, and X. Zhou, \u201cRepresenting long volumetric video with temporal gaussian hierarchy,\u201d in ACM Transactions on Graphics , 2024.",
          "ref_ids": [
            "47"
          ],
          "1": "d) Dynamic 3D Gaussians: There have been recent efforts [12], [41]\u2013[47] in extending 3DGS for dynamic scenes."
        },
        "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video": {
          "authors": [
            "Y Hong",
            "Y Wu",
            "Z Shen",
            "C Guo",
            "Y Jiang",
            "Y Zhang"
          ],
          "url": "https://arxiv.org/abs/2502.08297",
          "ref_texts": "[72] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. ACM Transactions on Graphics (TOG), 43(6):1\u201318, 2024. 3",
          "ref_ids": [
            "72"
          ],
          "1": "Recently, 3DGS [35] ensures both high quality and fast rendering, while dynamic variants [15, 24, 31, 38, 42, 59, 69, 72, 73] enable complex 4D scene reconstruction for advanced human modeling."
        }
      }
    },
    {
      "title": "hierarchical generation of human-object interactions with diffusion probabilistic models",
      "id": 25,
      "valid_pdf_number": "29/30",
      "matched_pdf_number": "23/29",
      "matched_rate": 0.7931034482758621,
      "citations": {
        "Emdm: Efficient motion diffusion model for fast and high-quality motion generation": {
          "authors": [
            "W Zhou",
            "Z Dou",
            "Z Cao",
            "Z Liao",
            "J Wang",
            "W Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72627-9_2",
          "ref_texts": "58. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of humanobject interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15061\u201315073",
          "ref_ids": [
            "58"
          ],
          "1": "The generated motion can condition on abundant, multi-modal inputs such as action labels [16, 21, 36, 55, 89], textual description [1, 4, 6, 15, 19, 20, 31, 33, 57, 78,80,97,98,101,106], incomplete pose sequences [18,23,78,84], control signals[13,27,40,43,54,58,67,72,73,81,82,88,99],musicoraudio[3,34,37,39,50,108], and so on."
        },
        "Tlcontrol: Trajectory and language control for human motion synthesis": {
          "authors": [
            "W Wan",
            "Z Dou",
            "T Komura",
            "W Wang"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-72913-3_3.pdf",
          "ref_texts": "40. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of humanobject interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15061\u201315073 (2023)",
          "ref_ids": [
            "40"
          ],
          "1": "Instead, in this paper, we propose to learn a better representation of part-based motion priors, which have demonstrated impressive results for computer character animation [22,40,52]."
        },
        "Generating human motion in 3D scenes from text descriptions": {
          "authors": [
            "Zhi Cen",
            "Huaijin Pi",
            "Sida Peng",
            "Zehong Shen",
            "Minghui Yang",
            "Shuai Zhu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Cen_Generating_Human_Motion_in_3D_Scenes_from_Text_Descriptions_CVPR_2024_paper.html",
          "ref_texts": "[57] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, 2023.",
          "ref_ids": [
            "57"
          ],
          "1": "Then we employ diffusion models [57, 75] to synthesize human motions given object-centric representation and texts.",
          "2": "To improve the performance, [57] employs a hierarchical framework that generates goal poses, milestones, and motion sequentially.",
          "3": "Given the constructed object-centric representations, we follow [57] to employ a transformer decoder architecture [79], which enables arbitrary length motions.",
          "4": "Implementation details Following [38, 57, 80, 81], we use separate diffusion models for the trajectory generation and the motion generation."
        },
        "HOI-M^ 3: Capture Multiple Humans and Objects Interaction within Contextual Environment": {
          "authors": [
            "Juze Zhang",
            "Jingyan Zhang",
            "Zining Song",
            "Zhanhe Shi",
            "Chengfeng Zhao",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_HOI-M3_Capture_Multiple_Humans_and_Objects_Interaction_within_Contextual_Environment_CVPR_2024_paper.html",
          "ref_texts": "[38] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15061\u201315073, 2023. 2",
          "ref_ids": [
            "38"
          ],
          "1": "29, 41, 64, 69] to recently emerging motion generation (MoGen) [3, 7, 9, 11, 12, 21, 23, 24, 30, 37, 38, 46, 49, 59, 66, 68, 70, 71]."
        },
        "Smoodi: Stylized motion diffusion model": {
          "authors": [
            "L Zhong",
            "Y Xie",
            "V Jampani",
            "D Sun",
            "H Jiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73232-4_23",
          "ref_texts": "33. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of humanobject interactions with diffusion probabilistic models. In: ICCV (2023)",
          "ref_ids": [
            "33"
          ],
          "1": "Inspired by the impressive performance of diffusion models in image generation, a lot of works [6,8,13,18,22,23,25,29,32,33,39,41,46, 48,53,56,60,62,64] utilize diffusion models to generate human motion."
        },
        "Synthesizing physically plausible human motions in 3d scenes": {
          "authors": [
            "L Pan",
            "J Wang",
            "B Huang",
            "J Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550906/",
          "ref_texts": "[28] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15061\u201315073, 2023. 1",
          "ref_ids": [
            "28"
          ],
          "1": "While previous kinematics-based works [7, 12, 14, 23, 28, 30, 37, 38, 47, 49] have achieved long-term human motion generation in 3D indoor scenes, their models are challenging to avoid inherently physical artifacts like penetration, floating, and foot sliding."
        },
        "Physhoi: Physics-based imitation of dynamic human-object interaction": {
          "authors": [
            "Y Wang",
            "J Lin",
            "A Zeng",
            "Z Luo",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2312.04393",
          "ref_texts": "[75] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15061\u201315073, 2023. 2, 3",
          "ref_ids": [
            "75"
          ],
          "1": "Compared to kinematics-based methods [75, 89, 105], physics-based methods are especially advantageous for generating human-object interaction since the physics simulator can explicitly constrain the dynamics of humanoids and objects.",
          "2": "Based on recent development of 3D HOI datasets [4, 18, 30, 40, 42, 87, 92, 117, 121], there emerges a branch of research focuses on generating human motion that interacts with objects [24, 44, 47, 48, 75, 88, 93, 95, 104, 110]."
        },
        "Thor: Text to human-object interaction diffusion via relation intervention": {
          "authors": [
            "Q Wu",
            "Y Shi",
            "X Huang",
            "J Yu",
            "L Xu",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2403.11208",
          "ref_texts": "45. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of humanobject interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15061\u201315073 (2023)",
          "ref_ids": [
            "45"
          ],
          "1": "Prior works on generating human-object interactions are either limited with static objects [5,19,27,45,79], or only generate the human motion of upper body [14,54,61].",
          "2": "In contrast, other works attempts to generate natural human motions in 3D indoor scenes [22,80] or interactions with seating furniture [10,25,27,45,78]."
        },
        "Stratified avatar generation from sparse observations": {
          "authors": [
            "Han Feng",
            "Wenchao Ma",
            "Quankai Gao",
            "Xianwei Zheng",
            "Nan Xue",
            "Huijuan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Feng_Stratified_Avatar_Generation_from_Sparse_Observations_CVPR_2024_paper.html",
          "ref_texts": "[31] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of humanobject interactions with diffusion probabilistic models. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 15061\u201315073, 2023. 2",
          "ref_ids": [
            "31"
          ],
          "2": "[31] develops a hierarchical generation pipeline for human-object interaction by generating initial keyframes in the motion sequence and then interpolating between them."
        },
        "Crowdmogen: Zero-shot text-driven collective motion generation": {
          "authors": [
            "X Guo",
            "M Zhang",
            "H Xie",
            "C Gu",
            "Z Liu"
          ],
          "url": "https://arxiv.org/abs/2407.06188",
          "ref_texts": "[44] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15061\u201315073, 2023.",
          "ref_ids": [
            "44"
          ],
          "1": "A range of external controls, such as action categories [15, 41, 72, 25], music [27, 74, 52, 39, 63], text [42, 56, 57, 24, 3, 34, 71, 50, 35, 47, 68, 21, 19, 64, 43, 30, 69], scenes [20, 29, 33, 60], objects [10, 26, 55, 17, 31, 44, 40, 51], and trajectories [23, 46], further 2 allows for stylized and customized motions."
        },
        "Two-person interaction augmentation with skeleton priors": {
          "authors": [
            "Baiyi Li",
            "Edmond S. L",
            "Hubert P. H",
            "He Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/html/Li_Two-Person_Interaction_Augmentation_with_Skeleton_Priors_CVPRW_2024_paper.html",
          "ref_texts": "[40] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15061\u201315073, 2023. 2",
          "ref_ids": [
            "40"
          ],
          "1": "Recently, there is a surge of deep learning methods on interactions, including human-object interaction [23, 40, 64], motion generation as reaction [6, 9, 63], from texts [29, 48] and by reinforcement learning [62, 72]."
        },
        "Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation": {
          "authors": [
            "Z Zhao",
            "M Qi",
            "H Ma"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73397-0_26",
          "ref_texts": "23. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of humanobject interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15061\u201315073 (2023)",
          "ref_ids": [
            "23"
          ],
          "1": "Furthermore, VQ-VAE [35] has demonstrated its superiority in multiple areas, such as image generation and 3D synthesis [20,23].",
          "2": "[23], who segmented the human body into five parts and encoded them into multiple codebooks for generating human motions, we extended this concept to encode the hand into multiple components."
        },
        "Ins-hoi: Instance aware human-object interactions recovery": {
          "authors": [
            "J Zhang",
            "Y Zhang",
            "H Zhang",
            "X Zhou",
            "B Zhou"
          ],
          "url": "https://arxiv.org/abs/2312.09641",
          "ref_texts": "[51] H. Pi, S. Peng, M. Yang, X. Zhou, and H. Bao, \u201cHierarchical generation of human-object interactions with diffusion probabilistic models,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023, pp. 15 061\u201315 073.",
          "ref_ids": [
            "51"
          ],
          "1": "Thanks to their efforts in data collection, numerous studies on human-object interaction, such as analyzing contact [40]\u2013[43], spatial arrangement [33], [44]\u2013[46] and generation [47]\u2013[51] have significantly advanced this field."
        },
        "SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control": {
          "authors": [
            "X Zhang",
            "S Starke",
            "V Guzov",
            "Z Zhang"
          ],
          "url": "https://arxiv.org/abs/2412.15664",
          "ref_texts": "[57] Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of human-object interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2023) 3",
          "ref_ids": [
            "57"
          ],
          "1": "Research on humanobject interaction [3, 33, 81, 83] spans a wide range, from interactions with large, static objects like chairs and beds [22, 30, 36, 54, 57, 63, 80, 86, 87], to dynamic engagements with moving objects.",
          "2": "In: CVPR Workshop on Human Motion Generation (2024) 2"
        },
        "Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation": {
          "authors": [
            "H Pi",
            "R Guo",
            "Z Shen",
            "Q Shuai",
            "Z Hu",
            "Z Wang"
          ],
          "url": "https://arxiv.org/abs/2412.13111",
          "ref_texts": "[55] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, 2023.",
          "ref_ids": [
            "55"
          ],
          "1": "Additionally, [10, 55, 78, 82] generate motion with scene information."
        },
        "FORCE: Physics-aware Human-object Interaction": {
          "authors": [
            "X Zhang",
            "BL Bhatnagar",
            "S Starke",
            "I Petrov"
          ],
          "url": "https://arxiv.org/abs/2403.11237",
          "ref_texts": "[90] Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of human-object interactions with diffusion probabilistic models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2023) 3",
          "ref_ids": [
            "90"
          ],
          "1": "However, most existing work focuses on human interactions with static scenes [20, 34, 41, 54, 57, 77, 108, 109, 111], while some studies have attempted to enhance motion generation quality by concentrating on static object interactions, such as sitting and lying on furniture [33, 53, 81, 90, 127, 136, 137, 143].",
          "2": "In: CVPR Workshop on Human Motion Generation (2024) 3"
        },
        "Human Grasp Generation for Rigid and Deformable Objects with Decomposed VQ-VAE": {
          "authors": [
            "M Qi",
            "Z Zhao",
            "H Ma"
          ],
          "url": "https://arxiv.org/abs/2501.05483",
          "ref_texts": "[30] H. Pi, S. Peng, M. Yang, X. Zhou, and H. Bao, \u201cHierarchical generation of human-object interactions with diffusion probabilistic models,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15 061\u201315 073.",
          "ref_ids": [
            "30"
          ],
          "1": "Recently, VQ-VAE [20] has shown its effectiveness across various domains, such as image generation and 3D synthesis [29], [30].",
          "2": "[30], which segmented the human body into five parts and encoded them into multiple codebooks to generate human motion, we extend this concept by encoding the hand into multiple components."
        },
        "Kinematics-based 3D Human-Object Interaction Reconstruction from Single View": {
          "authors": [
            "Y Chen",
            "C Wang"
          ],
          "url": "https://arxiv.org/abs/2407.14043",
          "ref_texts": "[31] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. 2023. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 15061\u201315073.",
          "ref_ids": [
            "31"
          ],
          "1": "While kinematics-based methods have been explored in humanobject interaction synthesis, based on recent progress in autoregressive model [8] and diffusion models [31], [20]."
        },
        "Pushing the Boundaries of Text to Motion with Arbitrary Text: A New Task": {
          "authors": [
            "R Wang",
            "C Ma",
            "G Li",
            "H Xu",
            "Y Li",
            "Z Wang"
          ],
          "url": "https://arxiv.org/abs/2404.14745",
          "ref_texts": "[31] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 3",
          "ref_ids": [
            "31"
          ],
          "1": "For instance, [15, 21, 31, 43, 45] integrated environmental elements into human motion generation, while [5, 7, 23, 37, 42, 44] utilized textual guidance to facilitate the generation of interactive motions for pairs or groups."
        },
        "CST: Character State Transformer for Object-Conditioned Human Motion Prediction": {
          "authors": [
            "Wei Tseng",
            "Rei Kawakami",
            "Satoshi Ikehata",
            "Ikuro Sato"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2025W/CV4Small/html/Tseng_CST_Character_State_Transformer_for_Object-Conditioned_Human_Motion_Prediction_WACVW_2025_paper.html",
          "ref_texts": "[23] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15061\u201315073, October 2023. 1",
          "ref_ids": [
            "23"
          ],
          "1": "Human motion generation can generally be categorized into two approaches: sequence-based methods [23] and frame-by-frame generation using auto-regressive models."
        },
        "100 Scenes": {
          "authors": [
            "HOI Scene-Aware"
          ],
          "url": "https://www.tengyu.ai/assets/pdf/CVPR24_TRUMANS.pdf",
          "ref_texts": "[37] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In International Conference on Computer Vision (ICCV), 2023. 3",
          "ref_ids": [
            "37"
          ],
          "1": "0 \u2713 SMPL-X \u2713 100 \u2713 \u2713 \u2713 \u2713 \u2713 tion [37, 52]."
        }
      }
    },
    {
      "title": "pvnet: pixel-wise voting network for 6dof pose estimation",
      "id": 2,
      "valid_pdf_number": "630/832",
      "matched_pdf_number": "515/630",
      "matched_rate": 0.8174603174603174,
      "citations": {
        "End-to-end probabilistic geometry-guided regression for 6dof object pose estimation": {
          "authors": [
            "T P\u00f6llabauer",
            "J Li",
            "V Knauthe",
            "S Berkei"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10896093/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q.-X. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 4556\u20134565, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID: 57189382",
          "ref_ids": [
            "10",
            "Online"
          ],
          "1": "Among these are SingleShotPose [9], PVNet [10], HybridPose [11], CDPN [12], EPOS [13], SurfEmb [14], ZebraPose [15], GDR-Net [16], and its improved version GDRNPP [17]."
        },
        "Novel Object 6D Pose Estimation with a Single Reference View": {
          "authors": [
            "J Liu",
            "W Sun",
            "K Zeng",
            "J Zheng",
            "H Yang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2503.05578",
          "ref_texts": "[58] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "58"
          ],
          "1": "Currently, instance-level methods [10, 11, 26, 58, 67, 68, 71, 78] have attained high precision but are limited to objects encountered during training."
        },
        "GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation": {
          "authors": [
            "W Li",
            "H Xu",
            "J Huang",
            "H Jung",
            "PKT Yu",
            "N Navab"
          ],
          "url": "https://arxiv.org/abs/2502.04293",
          "ref_texts": "[48] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "48"
          ],
          "1": "Early visual feature extractors relied on CNN backbones [28, 32, 48, 58, 72, 76] to predict or refine object poses from single RGB images."
        },
        "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation": {
          "authors": [
            "J Liu",
            "W Sun",
            "H Yang",
            "P Deng",
            "C Liu",
            "N Sebe"
          ],
          "url": "https://arxiv.org/abs/2502.02525",
          "ref_texts": "[6] S. Peng, X. Zhou, Y. Liu, H. Lin, Q. Huang, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof object pose estimation,\u201d inIEEE. Trans. Pattern. Anal. Mach. Intell., vol. 44, pp. 3212\u20133223, 2022.",
          "ref_ids": [
            "6"
          ],
          "1": "Instance-level methods [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] are restricted to specific objects the model has been trained on, which greatly limits their practical applicability."
        },
        "Diffusion Suction Grasping with Large-Scale Parcel Dataset": {
          "authors": [
            "DT Huang",
            "X He",
            "D Hua",
            "D Yu",
            "ET Lin"
          ],
          "url": "https://arxiv.org/abs/2502.07238",
          "ref_texts": "[8] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "8"
          ],
          "1": "Many object 6D pose estimation models [6], [7], [8], [9] project the pre-defined suction configuration onto the scene.",
          "2": "[6] estimated the 6D pose of the object [7], [8], [9] and projected the pre-defined suction configuration onto the objects in the scene."
        },
        "Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks": {
          "authors": [
            "Y Jin",
            "Y Zhang",
            "Z Xu",
            "W Zhang",
            "J Xu"
          ],
          "url": "https://arxiv.org/abs/2502.03877",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "5"
          ],
          "1": "[5] addressed some of these issues with PVNet, a pixel-wise voting network that improved robustness in cluttered environments."
        },
        "A survey of 6dof object pose estimation methods for different application scenarios": {
          "authors": [
            "Jian Guan",
            "Yingming Hao",
            "Qingxiao Wu",
            "Sicong Li",
            "Yingjian Fang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/4/1076",
          "ref_texts": "46. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6Dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "46"
          ],
          "1": "The second stage refers to the voting-based PVNet [46] method for template matching.",
          "2": "To address the challenges posed by severe occlusion, PVNet [46] builds upon the symmetry handling approach introduced in [60].",
          "3": "The proposed pose estimation framework in PVNet [46] regresses pixel-level vectors that point to the keypoints.",
          "4": "PVNet [46] effectively solves the problem of occlusion and has laid the foundation for much of the subsequent work in the field.",
          "6": "Hybridpose [68] proposes a network architecture based on PVNet [46], leveraging a prediction network with three intermediate representations using ResNet [69].",
          "9": "A typical example is PVNet [46], which utilizes the principles of PointNet [89].",
          "10": "There are already some methods that improve accuracy through refinement, the usage of PoseCNN [20] results in DeepIM [33] and the combination of Repose [74] and PVNet [46] are successful examples."
        },
        "6d-diff: A keypoint diffusion framework for 6d object pose estimation": {
          "authors": [
            "Li Xu",
            "Haoxuan Qu",
            "Yujun Cai",
            "Jun Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_6D-Diff_A_Keypoint_Diffusion_Framework_for_6D_Object_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u2013",
          "ref_ids": [
            "44"
          ],
          "7": "Compared to this type of direct methods, correspondence-based methods[5, 19, 43, 44, 46, 53, 56] often demonstrate better performance, which estimate 6D object poses via learning 2D-3D correspondences between the observed image and the object 3D model.",
          "9": "Later, PVNet [44] achieved better performance by estimating 2D keypoints for sampled points on the surface of the object 3D model via pixel-wise voting."
        },
        "Object pose estimation via the aggregation of diffusion features": {
          "authors": [
            "Tianfu Wang",
            "Guosheng Hu",
            "Hongguang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_Object_Pose_Estimation_via_the_Aggregation_of_Diffusion_Features_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "PVNet [28] regresses pixel-wise unit vectors pointing to 2D projections of a set of 3D keypoints."
        },
        "RDPN6D: Residual-based dense point-wise network for 6Dof object pose estimation based on RGB-D images": {
          "authors": [
            "Wei Hong",
            "Yang Hung",
            "Song Chen"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/DLGC/html/Hong_RDPN6D_Residual-based_Dense_Point-wise_Network_for_6Dof_Object_Pose_Estimation_CVPRW_2024_paper.html",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "31"
          ],
          "1": "These methods typically provide pixel-level correspondences [11, 19, 25, 41] or predict 2D image locations for predefined 3D keypoints [4, 31], resulting in more robust results.",
          "2": "PVN3D [17] extends PVNet [31] to predict the keypoints in the 3D space because errors that may appear small in the projection can significantly impact the real world.",
          "4": "Our approach follows the protocol established in [11, 31, 41\u201343], which employs the standard 15%/85% training/testing split.",
          "5": "For Linemod and Occlusion LineMOD datasets, we follow [23, 31] to report ADD(-S) 0."
        },
        "Secondpose: Se (3)-consistent dual-stream feature fusion for category-level pose estimation": {
          "authors": [
            "Yamei Chen",
            "Yan Di",
            "Guangyao Zhai",
            "Fabian Manhardt",
            "Chenyangguang Zhang",
            "Ruida Zhang",
            "Federico Tombari",
            "Nassir Navab",
            "Benjamin Busam"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_SecondPose_SE3-Consistent_Dual-Stream_Feature_Fusion_for_Category-Level_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 2",
          "ref_ids": [
            "33"
          ],
          "1": "Recent methods can be mainly categorized into three types: direct pose regression [20, 48], methods that establish 2D-3D correspondences through keypoint detection or pixel-wise 3D coordinate estimation [33, 43, 50], and approaches that learn pose-sensitive embeddings for subsequent pose retrieval [37].",
          "2": "While most keypoints based approaches rely on the P nP algorithm [33, 36, 50] to solve for pose, some methods instead employ neural networks to learn the optimization step [43]."
        },
        "Advancing 6-DoF instrument pose estimation in variable X-ray imaging geometries": {
          "authors": [
            "CGA Viviers",
            "L Filatova",
            "M Termeer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10478293/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "3": "We compare the proposed approach against seven competitive object pose estimation methods on the LINEMOD dataset: YOLO6D [7], PoseCNN [22], PVNet [23], Gen6D [15], EfficientPose [51], RNNPose [21] and EPro-PnPv2 [8]."
        },
        "Hoisdf: Constraining 3d hand-object pose estimation with global signed distance fields": {
          "authors": [
            "H Qi",
            "C Zhao",
            "M Salzmann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10657272/",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "41"
          ],
          "1": "Many excellent 3D hand [32, 45, 51, 58] and object [8, 25, 41] pose estimation algorithms have been developed."
        },
        "Ar overlay: Training image pose estimation on curved surface in a synthetic way": {
          "authors": [
            "S Huang",
            "Y Song",
            "Y Kang",
            "C Yu"
          ],
          "url": "https://arxiv.org/abs/2409.14577",
          "ref_texts": "[34] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\u201d Dec. 2018, arXiv:1812.11788 [cs].",
          "ref_ids": [
            "34",
            "cs"
          ],
          "1": "Hybrid methods like DenseFusion [33] and PVNet [34] integrate RGB data with depth information, improving accuracy and robustness in complex scenes [35\u201337]."
        },
        "Hipose: Hierarchical binary surface encoding and correspondence pruning for rgb-d 6dof object pose estimation": {
          "authors": [
            "Yongliang Lin",
            "Yongzhi Su",
            "Praveen Nathan",
            "Sandeep Inuganti",
            "Yan Di",
            "Martin Sundermeyer",
            "Fabian Manhardt",
            "Didier Stricker",
            "Jason Rambach",
            "Yu Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lin_HiPose_Hierarchical_Binary_Surface_Encoding_and_Correspondence_Pruning_for_RGB-D_CVPR_2024_paper.html",
          "ref_texts": "[42] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "42"
          ],
          "1": "Dense correspondence-based methods have been shown to outperform the keypoint-based methods [36, 40, 42, 46, 71] and holistic approaches [8, 23, 49, 63] nowadays, as also demonstrated in the BOP challenge results [54]."
        },
        "Transpose: 6d object pose estimation with geometry-aware transformer": {
          "authors": [
            "X Lin",
            "D Wang",
            "G Zhou",
            "C Liu",
            "Q Chen"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231224004235",
          "ref_texts": "[32] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H., 2019. Pvnet: Pixelwise voting network for 6dof pose estimation, in: Proceedings of the IEEE/CVFConferenceonComputerVisionandPatternRecognition, pp. 4561\u20134570.",
          "ref_ids": [
            "32"
          ],
          "1": "Recently, some researchers have applied deep neural networks to estimate 6D object pose from a single RGB image[32,31,49,23]andachievedpromisingresults.",
          "2": "PVNet[32] can learn a vector field representation directed to the 2D keypoints.",
          "4": "Wefollowpriorworks[32,31] and directly utilize the pre-trained model on the LineMod dataset for testing."
        },
        "Acr-pose: Adversarial canonical representation reconstruction network for category level 6d object pose estimation": {
          "authors": [
            "Z Fan",
            "Z Song",
            "Z Wang",
            "J Xu",
            "K Wu",
            "H Liu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3652583.3658050",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "29"
          ],
          "1": "Over the last decade, numerous 6D object pose estimation works [13, 21, 29, 34, 37, 47, 51] have emerged and been deployed in industrial products, demonstrating the usefulness of this line of research."
        },
        "Gs-pose: Category-level object pose estimation via geometric and semantic correspondence": {
          "authors": [
            "P Wang",
            "T Ikeda",
            "R Lee",
            "K Nishiwaki"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73383-3_7",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "23"
          ],
          "1": "With the advancement of deep learning methods, various learning-based pose estimation approaches have proven effective for instance-level pose estimation [3, 9\u201314, 19, 20, 22, 23, 29\u201331, 35, 39].",
          "2": "Category-Level Object Pose Estimation In the past few years, instance-level object pose estimation based on Deep Neural Networks (DNNs) has made great progress in computer vision and robotics fields [3, 9\u201314, 19, 20, 22, 23, 29\u201331, 35, 39]."
        },
        "Closure: Fast quantification of pose uncertainty sets": {
          "authors": [
            "Y Gao",
            "Y Tang",
            "H Qi",
            "H Yang"
          ],
          "url": "https://arxiv.org/abs/2403.09990",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "39"
          ],
          "1": "The first paradigm is to start by detecting salient keypoints in the sensor data \u2013often done using deep neural networks [17, 55, 39, 38]\u2013 and then leverage the maximum likelihood estimation (MLE) framework to estimate the \u2217 equal contribution."
        },
        "GBOT: Graph-based 3d object tracking for augmented reality-assisted assembly guidance": {
          "authors": [
            "S Li",
            "H Schieber",
            "N Corell",
            "B Egger"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10494181/",
          "ref_texts": "[36] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "36"
          ],
          "1": "CV] 12 Feb 2024 timation [36, 54, 58, 59] or 6D object tracking [38, 51] is essential.",
          "2": "Most deep-learning methods combine object detection or semantic segmentation with a consecutive pose estimation [22, 36, 58].",
          "3": "Furthermore, instance-level 6D object pose estimation can be distinguished into one-stage [3, 18, 64] and twostage [22,36] approaches.",
          "4": "Moreover, two-stage approaches [36,58] apply an off-the-shelf object detector and subsequently estimate the 6D pose.",
          "5": "1 Keypoints Selection To define surface keypoints on each object, we apply Farthest Point Sampling [3, 36] which initializes a keypoint set on the object surfaces and adds overall N points.",
          "6": "Generally, more keypoints slightly improve the accuracy but also increase the computational cost [36]."
        },
        "LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation": {
          "authors": [
            "R Zhang",
            "Z Huang",
            "G Wang",
            "C Zhang",
            "Y Di"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_27",
          "ref_texts": "39. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "39"
          ],
          "1": "For RGB-based methods, some directly regress object poses from input image [20,23,26,51], while others establish 2D-3D correspondences through keypoint detection or pixel-wise 3D coordinate estimation and then employ PnP algorithms to solve the pose [3,17,38,39,41,43,53]."
        },
        "A robust CoS-PVNet pose estimation network in complex scenarios": {
          "authors": [
            "Jiu Yong",
            "Xiaomei Lei",
            "Jianwu Dang",
            "Yangping Wang"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/11/2089",
          "ref_texts": "21. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6D of pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "Inspired by the U-Net network, the PVNet [21] uses residual blocks and bilinear interpolation to reconstruct a lightweight U-Net as a feature extraction network with an inference speed of 40 ms.",
          "2": "On the LineMod dataset and Occlusion LineMod dataset, CoS-PVNet is compared quantitatively with BB8 [30], YOLO-6D [12], and PVNet [21] in 2D projection metrics.",
          "3": "On the LineMod dataset and Occlusion LineMod dataset, CoS-PVNet is compared quantitatively with BB8 [30], YOLO-6D [12], and PVNet [21] in 2D projection metrics.",
          "4": "Comparative Experiment of CoS-PVNet Algorithm ADD (-S) (1) LineMod Dataset ADD (-S) Comparative Experiment Experiments are conducted on the LineMod dataset, comparing CoS-PVNet with algorithms such as YOLO-6D [12], PoseCNN [17], DenseFusion [31], Dual Stream [32], and PVNet [21].",
          "5": "(2) Comparison Experiment of the Occupation LineMod Dataset ADD (-S) Experiments are conducted on the Occlusion LineMod dataset to compare CoSPVNet with HybridPose [33], SSPE [34], RePOSE [35], SegDriven [36], PoseCNN [17] and PVNet [21]."
        },
        "Uncertainty-aware 3d object-level mapping with deep shape priors": {
          "authors": [
            "Z Liao",
            "J Yang",
            "J Qian",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611206/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "23"
          ],
          "1": "To this end, in many recent CAD-based approaches, the uncertainties are incorporated when estimating object poses [23], [24], [8], [7]."
        },
        "Towards Co-Evaluation of Cameras HDR and Algorithms for Industrial-Grade 6DoF Pose Estimation": {
          "authors": [
            "Agastya Kalra",
            "Guy Stoppi",
            "Dmitrii Marin",
            "Vage Taamazyan",
            "Aarrushi Shandilya",
            "Rishav Agarwal",
            "Anton Boykov",
            "Tze Hao",
            "Michael Stark"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Kalra_Towards_Co-Evaluation_of_Cameras_HDR_and_Algorithms_for_Industrial-Grade_6DoF_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "53"
          ],
          "1": "This mechanism depends on the camera setup and consists either of some form of PnP (for multi-view images, candidates include [18, 23, 40, 41, 59]) or ICP (for structured light sensors, candidates include [23, 30, 53, 60, 72])."
        },
        "Asdf: Assembly state detection utilizing late fusion by integrating 6d pose estimation": {
          "authors": [
            "H Schieber",
            "S Li",
            "N Corell",
            "P Beckerle"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10765472/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570. IEEE/CVF, 2019. 3",
          "ref_ids": [
            "27"
          ],
          "1": "The constant progress in 6D pose estimation is promising for single objects [45, 40, 27, 26, 10, 50], the combination of 6D pose estimation and assembly state detection is even more promising for AR guidance approaches [21, 25]."
        },
        "FocalPose++: Focal Length and Object Pose Estimation via Render and Compare": {
          "authors": [
            "M C\u00edfka",
            "G Ponimatkin",
            "Y Labb\u00e9"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10706831/",
          "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "Previous approaches for this task mainly rely on establishing local 2D-3D correspondences between an image and a 3D model using either hand-crafted [1], [5], [6], [7], [8], [9] or CNN features [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], followed by robust camera pose estimation using PnP [22].",
          "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs) [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21]."
        },
        "Research on Six-Degree-of-Freedom Refueling Robotic Arm Positioning and Docking Based on RGB-D Visual Guidance": {
          "authors": [
            "Mingbo Yang",
            "Jiapeng Liu"
          ],
          "url": "https://www.mdpi.com/2076-3417/14/11/4904",
          "ref_texts": "25. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp.",
          "ref_ids": [
            "25"
          ],
          "1": "These include direct estimation methods like PoseNet [19], SSD-6D [20], Deep-6DPose [21], PoseCNN [22], and keypoint methods such as BB8 [23], YOLO-6D [24], PVNet [25], etc."
        },
        "Bridging Domain Gap for Flight-Ready Spaceborne Vision": {
          "authors": [
            "TH Park",
            "S D'Amico"
          ],
          "url": "https://arxiv.org/abs/2409.11661",
          "ref_texts": "[99] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4556\u20134565. https://doi.org/10.1109/ CVPR.2019.00469.",
          "ref_ids": [
            "99"
          ],
          "1": "These uncertainties can be leveraged to better perform state estimation via navigation filter [16] or even incorporated into the uncertainty-aware P\ud835\udc5bP algorithm [38, 99]."
        },
        "Robust Category-Level 3D Pose Estimation from Diffusion-Enhanced Synthetic Data": {
          "authors": [
            "Jiahao Yang",
            "Wufei Ma",
            "Angtian Wang",
            "Xiaoding Yuan",
            "Alan Yuille",
            "Adam Kortylewski"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.html",
          "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1",
          "ref_ids": [
            "25"
          ],
          "1": "Pose estimation has been studied in depth on the instance level [14, 17, 19, 25, 38], and on the category-level for very specific object classes like cars [11] and faces [26]."
        },
        "An analysis of precision: occlusion and perspective geometry's role in 6D pose estimation": {
          "authors": [
            "Jeffrey Choate"
          ],
          "url": "https://link.springer.com/article/10.1007/s00521-023-09094-8",
          "ref_texts": "34. Peng S, Liu Y, Huang Q, Zhou X, Bao H (2019) Pvnet: pixelwise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 4561\u20134570",
          "ref_ids": [
            "34"
          ],
          "1": "[4, 7, 32, 34, 35, 47, 55, 56] are very similar to these approach with slight variations, all basically doing direct pose regression from a CNN."
        },
        "NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models": {
          "authors": [
            "F Milano",
            "JJ Chung",
            "H Blum"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801399/",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in CVPR, 2019. 2, 4",
          "ref_ids": [
            "29"
          ],
          "1": "For instance, keypoint-based methods [27]\u2013[29] predict the 2D location of pre-defined salient points, and use the object model to estimate the object pose based on 2D-3D correspondences.",
          "3": "Both with RGB-only and with RGB-D inputs, NeuSurfEmb achieves comparable performance to several CAD-model-based baselines [29], [46] and is outperformed by a small margin by others [2], [11], [12]."
        },
        "FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization": {
          "authors": [
            "N Ma",
            "M Wang",
            "Y Han",
            "YJ Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610549/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "24"
          ],
          "1": "Since the purpose is to capture the maximal geometric feature of the entire point cloud with the minimal number of points, we adopt the approach outlined in PVNET [24], utilizing the Farthest Point Sampling (FPS) algorithm to select n key points."
        },
        "FAFA: Frequency-Aware Flow-Aided Self-supervision for Underwater Object Pose Estimation": {
          "authors": [
            "J Tang",
            "G Wang",
            "Z Chen",
            "S Li",
            "X Li",
            "X Ji"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73021-4_21",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019) FAFA for Self-Supervised Underwater Object Pose Estimation 17",
          "ref_ids": [
            "36"
          ],
          "1": "The first category involves indirect methods [17,25,36,38,45], which identify target keypoints and then employ a PnP solver to calculate the pose."
        },
        "Mrc-net: 6-dof pose estimation with multiscale residual correlation": {
          "authors": [
            "Yuelong Li",
            "Yafei Mao",
            "Raja Bala",
            "Sunil Hadap"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_MRC-Net_6-DoF_Pose_Estimation_with_MultiScale_Residual_Correlation_CVPR_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 6, 7",
          "ref_ids": [
            "40"
          ],
          "2": "We benchmark our method against state-of-the-art techniques spanning a variety of recent approaches: EPOS [17], CDPNv2 [29], DPODv2 [44], PVNet [40], CosyPose [26], SurfEmb [13], SC6D [2], SCFlow [11], CIR [33], PFA [22], SO-Pose [7], NCF [23], CRT-6D [3], GDR-Net [52], ZebraPose [47], DProST [39], RePose [24], SegDriven [19], SingleStage [20], and CheckerPose [31]."
        },
        "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios": {
          "authors": [
            "DT Huang",
            "ET Lin",
            "L Chen",
            "LF Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10802595/",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "4"
          ],
          "3": "Therefore, PVNet [4] and PVN3D [5] use the farthest point sampling algorithm to select keypoint to reduce position errors."
        },
        "Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models": {
          "authors": [
            "H Ding",
            "L Seenivasan",
            "H Shu",
            "G Byrd",
            "H Zhang"
          ],
          "url": "https://arxiv.org/abs/2409.13107",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "26"
          ],
          "1": "While advancements in deep learning algorithms for computer vision, such as instance segmentation [21], [22], [23], [24], [25] and pose estimation [26], [27], [28], [29], [30], [31], offer an alternate vision-based, marker-less approach to extract the digital twin-based scene representation, these methods lack generalizability and fail when the observed scenario differs from the training data [32], [33], [34]."
        },
        "Embedded 3d reconstruction of dynamic objects in real time for maritime situational awareness pictures": {
          "authors": [
            "Felix Sattler"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-023-02802-4",
          "ref_texts": "35. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: pixelwise voting network for 6dof pose estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4556\u20134565. IEEE Computer Society, Los Alamitos, CA, USA",
          "ref_ids": [
            "35"
          ],
          "1": "Recent work shown by [35]a l s o demonstrates that the tasks of object detection, instance segmentation and pose estimation can be jointly estimated using a single neural network."
        },
        "Effects of Markers in Training Datasets on the Accuracy of 6D Pose Estimation": {
          "authors": [
            "Janis Rosskamp",
            "Rene Weller",
            "Gabriel Zachmann"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.html",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "The YCB-V dataset [34], comprising approximately 130,000 frames, is a popular choice for benchmarking [11, 17, 21, 31] and will thus be used in this paper."
        },
        "Learning better keypoints for multi-object 6dof pose estimation": {
          "authors": [
            "Yangzheng Wu",
            "Michael Greenspan"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "40"
          ],
          "1": "The second approach are keypoint-based methods [21, 22, 40, 52, 53], which are the main alternative to direct regression methods.",
          "2": "Despite the success of keypoint methods, the selection of the pre-defined object-centric keypoint locations has been overwhelmingly based on just two approaches, the first of which is Farthest Point Sampling (FPS) [22, 40].",
          "3": "FPS samples points on an object surface based on their relative proximities, and was originally developed for progressive image sampling [15] and subsequently repurposed for keypoint selection [40].",
          "4": "Both approaches are also heuristic, with their main objective being to produce keypoints that are geometrically dispersed, and which fall on [22, 40], or close to [52, 53], the objects\u2019 surfaces.",
          "5": "A graph network is trained to optimize a disperse set of keypoints with similarly distributed votes for keypoint voting 6DoF PE methods [22,40,52,53].",
          "6": "Motivation Existing keypoint methods [22,40,52,53] use regression networks to estimate a quantity that geometrically relates each image pixel (and/or point) to each keypoint.",
          "7": "A variety of such quantities have been explored in the literature, including the offset [22], direction vector [40], and radial distance [52, 53] between points.",
          "9": "Keypoint-based 6DoF PE Methods[22, 40, 52, 53] exhibit relatively good accuracy compared to viewpoint-based methods [32, 38, 49] or direct regression [28, 36, 54] methods.",
          "10": "Keypoint voting-based methods [21,22,40,52,53], however, can better accommodate noise.",
          "11": "Implementation Details The proposed KeyGNet is trained to generate a set of optimized keypoints, which we tested on a variety of stateof-the-art PE methods [22, 40, 53].",
          "12": "We test the optimized keypoints on three keypoint-based 6DoF PE voting methods, RCVPose [53], PVNet [40] and PVN3D [22].",
          "13": "radial [53], vector [40], and offset [22] respectively.",
          "14": "The majority of previous keypoint-based methods [22, 40, 52, 53] were designed to address the less challenging SISO case by only processing a single object at a time, which allows the training of unique network parameters for each distinct object.",
          "15": "Some [22, 40, 50] argue that more keypoints provide redundancy to the least square fitting algorithm ultimately used in the final transformation estimation, whereas others [52, 53] use as few as three keypoints to ease the estimation task of the backbone network.",
          "16": "PVNet [40] and PVN3D [22] exhibited a slight overall improvement in ADD(s) as the number of keypoints increased, which saturated when there are more than eight keypoints."
        },
        "Category Level Object Pose Estimation via Global High-Order Pooling": {
          "authors": [
            "Changhong Jiang",
            "Xiaoqiao Mu",
            "Bingbing Zhang",
            "Mujun Xie",
            "Chao Liang"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/9/1720",
          "ref_texts": "6. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201317 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "6"
          ],
          "1": "Currently, there are two types of 6D object pose estimation methods: instance-level pose estimation [2,6] and category-level pose estimation [7\u201311]."
        },
        "Rhaml: Rendezvous-based hierarchical architecture for mutual localization": {
          "authors": [
            "G Chen",
            "K Song",
            "X Xu",
            "W Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10540183/",
          "ref_texts": "[25] S. Peng, X. Zhou, Y . Liu, H. Lin, Q. Huang, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof object pose estimation,\u201d IEEE Trans. Pattern Anal. Machine Intell. , vol. 44, no. 6, pp. 3212\u20133223, 2022.",
          "ref_ids": [
            "25"
          ],
          "1": "In [25], keypoint localization is achieved through a vector-field representation.",
          "2": "As baseline methods, FrontNet [13], DOPE [14], and PVNet [25] are all retrained with our dataset."
        },
        "Pseudo-keypoint RKHS learning for self-supervised 6DoF pose estimation": {
          "authors": [
            "Y Wu",
            "M Greenspan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73027-6_3",
          "ref_texts": "61. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019) 3, 4, 6, 9",
          "ref_ids": [
            "61"
          ],
          "3": "Unlikefeature-matchingmethods,keypoint-basedmethods are typically more accurate due to redundancies encountered through voting schemes [32,61,83] and by generating confidence hypotheses of keypoints [31,87]."
        },
        "KVN: Keypoints voting network with differentiable RANSAC for stereo pose estimation": {
          "authors": [
            "I Donadi",
            "A Pretto"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10440415/",
          "ref_texts": "[3] S. Peng, X. Zhou, Y . Liu, H. Lin, Q. Huang, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof object pose estimation,\u201dIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp. 3212\u2013",
          "ref_ids": [
            "3"
          ],
          "4": "Inspired by [18], [19], we integrated PVNet [3], an established monocular pose estimation network, with a differentiable RANSAC layer that enables the network to directly infer 2D-3D keypoint correspondences.",
          "5": "Contributions Our contributions are the following: i) A novel 6D stereo object pose estimation pipeline that extends PVNet [3] with a differentiable RANSAC layer, turning it into a keypoint prediction network without losing the robustness granted by its pixel-wise voting approach; ii) a novel sub-differentiable hypotheses\u2019 scoring function along with an ablation study that supports our choice over previous proposals; iii) a challenging, real-world and fully annotated stereo object pose estimation dataset (TTD: Transparent Tableware Dataset 1); iv) an extensive performance evaluation on the challenging task of transparent object pose estimation, showing that our method achieves state-of-the-art performance on both the public TOD dataset [12] and TTD; v) An open-source implementation of KVN2.",
          "6": "PVNet [3] improved this method by predicting a unit vector pointing to each keypoint projection for each pixel in the image corresponding to the target object, and in [4] this approach is improved by accounting for the distance between pixels and keypoints.",
          "7": "PVNet: Pixel-Wise Voting Network Our work builds upon PVNet [3], a keypoint-based monocular pose estimation network."
        },
        "Keymatchnet: Zero-shot pose estimation in 3d point clouds by generalized keypoint matching": {
          "authors": [
            "F Hagelskj\u00e6r",
            "RL Haugaard"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10711403/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "PVNet [21] compute keypoint locations by first segmentation the object, and then computing the relative position of keypoint for all pixels belonging to the object."
        },
        "Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices": {
          "authors": [
            "X Yang",
            "Z Yu",
            "AG Banerjee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10711289/",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "9"
          ],
          "1": "Deep learning approaches, including direct pose regression [7], [8], keypoint detection [9], and hybrid variants, have improved accuracy and efficiency, especially when addressing the synthetic-to-real domain gap [10], [11].",
          "2": "Correspondence methods predict the 2D projections of the coordinates or keypoints to match the 3D models [9], [25], [26], avoiding regression difficulties by decomposing the problem.",
          "3": "Voting schemes improve occlusion robustness by allowing the visible portions to contribute to keypoint localization [9].",
          "4": "We synthesize an additional 3,000 images per LINEMOD object using the PVNet [9] rendering approach to expand the training data."
        },
        "Extending 6d object pose estimators for stereo vision": {
          "authors": [
            "T P\u00f6llabauer",
            "J Emrich",
            "V Knauthe",
            "A Kuijper"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-8705-0_8",
          "ref_texts": "22. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "22"
          ],
          "2": "Furthermore, the dense PV-Net [22] can be applied to stereo scenarios by predicting keypoints for each image and subsequently triangulating the object using keypoints from both images."
        },
        "Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion Error for Improved Object Pose Estimation": {
          "authors": [
            "A Li",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10810662/",
          "ref_texts": "[8] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, (2019)",
          "ref_ids": [
            "8"
          ],
          "2": "Single-view Keypoint and Heatmap Estimation The keypoint detection network is based off PVNet, a pixel-wise voting network for 6D pose estimation of a known object with a CAD model[8]."
        },
        "Multi-Modal Pose Representations for 6-DOF Object Tracking": {
          "authors": [
            "M Majcher",
            "B Kwolek"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02181-5",
          "ref_texts": "15. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: \u201cPVNet: PixelWise V oting Network for 6DoF Pose Estimation.\u201d in IEEE Conf. CVPR, pp. 4556\u20134565. (2019)",
          "ref_ids": [
            "15"
          ],
          "1": "Instead of estimating the object center, a Pixel-wise Voting Network (PVNet) [15] votes for several features of interest.",
          "2": "In order to deal with annotations required for supervised training of deep models, most of recent methods rely on training such networks on synthetic objects rendered from 3D models [15, 60].",
          "3": "For instance, 10000 images for each object have been rendered to train PVNet models [15]."
        },
        "RayEmb: Arbitrary Landmark Detection in X-Ray Images Using Ray Embedding Subspace": {
          "authors": [
            "Pragyan Shrestha",
            "Chun Xie",
            "Yuichi Yoshii",
            "Itaru Kitahara"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2024/html/Shrestha_RayEmb_Arbitrary_Landmark_Detection_in_X-Ray_Images_Using_Ray_Embedding_ACCV_2024_paper.html",
          "ref_texts": "30. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "30"
          ],
          "1": "To address the challenge of truncated objects where keypoints may lie outside the image, PVNet [30] regresses vectors that point towards the keypoints, determining the final location through a voting mechanism."
        },
        "Flying Projectile Attitude Determination from Ground-Based Monocular Imagery with a Priori Knowledge": {
          "authors": [
            "Huamei Chen",
            "Zhigang Zhu",
            "Hao Tang",
            "Erik Blasch",
            "Khanh D. Pham",
            "Genshe Chen"
          ],
          "url": "https://www.mdpi.com/2078-2489/15/4/201",
          "ref_texts": "34. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019.",
          "ref_ids": [
            "34"
          ],
          "1": "To additionally handle the case of truncated objects, a pixel-wise voting network (PVNet) [34] was proposed to learn a vector-field representation for robust 2D keypoint localization."
        },
        "AnnotateXR: An Extended Reality Workflow for Automating Data Annotation to Support Computer Vision Applications": {
          "authors": [
            "Subramanian Chidambaram",
            "Rahul Jain",
            "Sai Swarup",
            "Asim Unmesh",
            "Karthik Ramani"
          ],
          "url": "https://asmedigitalcollection.asme.org/computingengineering/article/24/12/121001/1202055",
          "ref_texts": "[27] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., 2019,\u201cPvnet: Pixel-Wise Voting Network for 6DOF Pose Estimation,\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, June 16\u201320, pp. 4561\u20134570.",
          "ref_ids": [
            "27"
          ],
          "1": "pdf by Purdue University at West Lafayette user on 04 November 2024",
          "2": "[27] Peng, S."
        },
        "VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object Pose Estimation": {
          "authors": [
            "R Lian",
            "Y Lin",
            "LJ Latecki",
            "H Ling"
          ],
          "url": "https://arxiv.org/abs/2403.14559",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: pixelwise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "To increase the robustness under various imaging conditions, most existing methods [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] first generate correspondences between 2D image pixels and 3D object points, and then regress the pose via any available Perspective-n-Point (PnP) solver [19], [20], [21].",
          "2": "The second kind of methods [9], [10], [13], [23], [18] localize predefined 3D keypoints in the input image to obtain 3D-2D correspondences, which more efficiently encode the object geometry information and facilitate the pose estimation process."
        },
        "CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation": {
          "authors": [
            "J Zhao",
            "W Quan",
            "BJ Matuszewski"
          ],
          "url": "https://arxiv.org/abs/2410.09010",
          "ref_texts": "[32] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):3212\u20133223, 2020.",
          "ref_ids": [
            "32"
          ],
          "3": "The method effectively addresses various challenging scenarios, including texture-less objects, occlusion, truncation [32], and clutter.",
          "4": "The model correspondences can be in the form of pixel-wise dense mapping [12, 18, 26, 30, 37, 38, 49], or a selection of sparse keypoints [32, 33, 43]."
        },
        "Weakly Supervised Pose Estimation of Surgical Instrument from a Single Endoscopic Image": {
          "authors": [
            "Lihua Hu",
            "Shida Feng",
            "Bo Wang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/11/3355",
          "ref_texts": "26. Peng, S.; Liu, Y.; Huang, Q.; Bao, H. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "26"
          ],
          "1": "Pixelwise Voting Network (PVNet) [26] locates feature points through a voting mechanism, demonstrating strong occlusion robustness."
        },
        "Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation": {
          "authors": [
            "J Park",
            "J Kim",
            "NI Cho"
          ],
          "url": "https://arxiv.org/abs/2401.16284",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "Typically, these approaches identify sparse keypoints [30, 35, 42], bounding box corners [10, 31], dense 2D-3D correspondence [11, 28], or UV maps [46], which are then used in PnP-RANSAC [18] for pose estimation."
        },
        "GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos": {
          "authors": [
            "Z Chen",
            "F Lu",
            "G Yu",
            "B Li",
            "S Qu",
            "Y Huang",
            "C Fu"
          ],
          "url": "https://arxiv.org/abs/2412.02267",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xibin Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "Early 6DoF object pose estimation or tracking approaches assume access to 3D models [28, 42] or category templates [13, 33] and rely on feature matching algorithms *Corresponding author: guangchen@tongji."
        },
        "A Learnable Viewpoint Evolution Method for Accurate Pose Estimation of Complex Assembled Product": {
          "authors": [
            "Delong Zhao",
            "Feifei Kong",
            "Fuzhou Du"
          ],
          "url": "https://www.mdpi.com/2076-3417/14/11/4405",
          "ref_texts": "30. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6dof pose estimation. IEEE Trans. Pattern Anal. Mach. Intell.2022, 44, 3212\u20133223. [CrossRef] [PubMed]",
          "ref_ids": [
            "30"
          ],
          "2": "DL-based methods employed in hierarchical strategy include PoseNet2 [29], single-shot pose [47], and state-of-the-art estimator PVNet [30] and MegaPose [34]."
        },
        "Six-Degree-of-Freedom Pose Estimation Method for Multi-Source Feature Points Based on Fully Convolutional Neural Network": {
          "authors": [
            "J Wang",
            "P Wu",
            "X Zhang",
            "R Xu",
            "T Wang"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02154-8",
          "ref_texts": "33. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixelwise voting network for 6dof pose estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4556\u20134565 (2019).https://doi.org/10.1109/CVPR.2019.00469",
          "ref_ids": [
            "33"
          ],
          "1": "8: ADD \u2212 S = 1 N N\u2211 i =1 min j \u2208[1,n] \u2225(R \u00b7 pi + t ) \u2212 (\u02c6R \u00b7 pi + \u02c6t )\u2225 (8) 123 131 Page 8 of 12 Journal of Intelligent & Robotic Systems (2024) 110:131 Table 2 ADD(-S) Performance Om Linemod Dataset Feature select method Suppression method Other method B-box FPS BOTH NMS Ours Overall BB8 [32] PVNet [33] GDR-Net [34] ape 82.",
          "2": "In terms of attitude solving, we Table 3 ADD(-S) Performance On Occlusion Linemod Dataset method B-box FPS BOTH PVNet [33] ape 24."
        },
        "Multi-View Metal Parts Pose Estimation Based on a Single Camera": {
          "authors": [
            "Chen Chen",
            "Xin Jiang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/11/3408",
          "ref_texts": "11. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "11"
          ],
          "3": "Hybridpose [24] extends the PVNet [11], which not only estimates keypoints but also predicts edge vectors and symmetry correspondences.",
          "4": "m = avg x\u2208M \u0000 Rgtx + Tgt \u0001 \u2212 (Rest x + Test ) (4) Figure 8 compares the qualitative results of metal parts pose estimations between our proposed method and PVNet [11]."
        },
        "Particle-based 6D Object Pose Estimation from Point Clouds using Diffusion Models": {
          "authors": [
            "C M\u00f6ller",
            "N Funk",
            "J Peters"
          ],
          "url": "https://arxiv.org/abs/2412.00835",
          "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019. 1, 2, 7",
          "ref_ids": [
            "24"
          ],
          "1": "Additionally, while many existing methods for 6D object pose estimation primarily operate in the image domain [24, 29, 39] and only incorporate depth information in the final refinement step [22], in this work, we explore working directly in the 3D point cloud domain.",
          "2": "Exemplarily, the Pixel-wise V oting Network (PVNet) [24] regresses unit vectors pointing to keypoints, HybridPose [29] leverages a multi-folded intermediate presentation consisting of keypoints, edge vectors between keypoints, and symmetry correspondences, and YOLO-6D+ [18] predicts an object\u2019s 3D bounding box and uses the its 2D projections as PnP input."
        },
        "SEMPose: A Single End-to-end Network for Multi-object Pose Estimation": {
          "authors": [
            "X Liu",
            "H Wang",
            "S Xue",
            "D Zhao"
          ],
          "url": "https://arxiv.org/abs/2411.14002",
          "ref_texts": "[8] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d inProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "8"
          ],
          "4": "For example, PVNet [8] predicts pixel-level vectors pointing to key points, then uses these vectors to determine the key points\u2019 positions through a RANSAC-basedvotingmechanism."
        },
        "Memory Efficient Deep Learning-Based Grasping Point Detection of Nontrivial Objects for Robotic Bin Picking": {
          "authors": [
            "P Dolezel",
            "D Stursa",
            "D Kopecky"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02153-9",
          "ref_texts": "29. Peng, S., Zhou, X., Liu, Y ., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Trans. Pattern Anal. Mach. Intell. (2020).https://doi.org/10.1109/ TPAMI.2020.3047388",
          "ref_ids": [
            "29"
          ],
          "1": "This approach can be further improved using pixel-wise predictions that provide the position of occluded key points [17, 29]."
        },
        "RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects": {
          "authors": [
            "JN Li",
            "T Chong",
            "Z Zhou",
            "H Yoshida",
            "K Yatani"
          ],
          "url": "https://arxiv.org/abs/2407.08081",
          "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4561\u2013",
          "ref_ids": [
            "38"
          ],
          "1": "More recently, data-driven deep learning methods [38, 47] demonstrated accurate predictions of the 6D pose of pre-defined sets of object included in carefully crafted datasets [5, 26]."
        },
        "EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models": {
          "authors": [
            "Z Hong",
            "K Zheng",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801359/",
          "ref_texts": "[42] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp. 4561\u2013",
          "ref_ids": [
            "42"
          ],
          "1": "EasyHeC trained a PVNet [42] on synthetic data to initialize the camera pose at the robot arm\u2019s zero joint pose."
        },
        "HPPS: A Hierarchical Progressive Perception System for Luggage Trolley Detection and Localization at Airports": {
          "authors": [
            "Z Sun",
            "Z Zhang",
            "J Zhao",
            "H Ye",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2405.05514",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] uses keypoints and a voting mechanism to estimate poses, handling partial occlusions and various viewing angles effectively."
        },
        "GMFlow: Global Motion-Guided Recurrent Flow for 6D Object Pose Estimation": {
          "authors": [
            "X Liu",
            "S Xue",
            "D Zhao",
            "S Ma",
            "M Jiang"
          ],
          "url": "https://arxiv.org/abs/2411.17174",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "19"
          ],
          "1": "Two prominent indirect approaches emerge, including keypoint-based methods [19], [20] and dense correspondence-based methods [21], [22].",
          "3": "Comparison with State of the Art We compare our proposed method with current state-of-theart techniques, including PoseCNN [3], PVNet [19], SO-Pose [28], DeepIM [6], GDR-Net [17], RePose [26], RNNPose [25], PFA [8], and SCFlow [10]."
        },
        "A Lightweight 6D Pose Estimation Network Based on Improved Atrous Spatial Pyramid Pooling": {
          "authors": [
            "Fupan Wang",
            "Xiaohang Tang",
            "Yadong Wu",
            "Yinfan Wang",
            "Huarong Chen",
            "Guijuan Wang",
            "Jing Liao"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/7/1321",
          "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "13"
          ],
          "1": "This was improved upon in PVNet, where sparse key points are discarded as reference points, pixel-to-key point vectors are introduced through a semantic segmentation network, and the estimation accuracy is enhanced in scenarios with occlusion and symmetric objects [13].",
          "2": "This was improved upon in PVNet, where sparse key points are discarded as reference points, pixel-to-key point vectors are introduced through a semantic segmentation network, and the estimation accuracy is enhanced in scenarios with occlusion and symmetric objects [13]."
        },
        "SurgeoNet: Realtime 3D Pose Estimation of Articulated Surgical Instruments from Stereo Images using a Synthetically-trained Network": {
          "authors": [
            "AT Aboukhadra",
            "N Robertini",
            "J Malik",
            "A Elhayek"
          ],
          "url": "https://arxiv.org/abs/2410.01293",
          "ref_texts": "14. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4561\u20134570 (2019) 3, 10, 11",
          "ref_ids": [
            "14"
          ],
          "1": "In their experiments, they compare the performance of PVNet [14] and HandObjectNet [7]."
        },
        "Synthetic Dataset Generation and Learning From Demonstration Applied to Industrial Manipulation": {
          "authors": [
            "A Barekatain",
            "HR Nohooji",
            "H Voos"
          ],
          "url": "https://arxiv.org/abs/2404.00447",
          "ref_texts": "[3] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u2013",
          "ref_ids": [
            "3"
          ],
          "1": "The output dataset has 500 images, which is subsequently used to train a state-of-the-art pose estimation method, namely PVNet [3]."
        },
        "FastPoseCNN: Real-Time Monocular Category-Level Pose and Size Estimation Framework": {
          "authors": [
            "E Davalos",
            "M Aminian"
          ],
          "url": "https://arxiv.org/abs/2406.11063",
          "ref_texts": "[38] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, page arXiv:1812.11788, 12 2020.",
          "ref_ids": [
            "38"
          ],
          "1": "These methods use CNNs for feature extraction and segmentation to perform keypoint predictions through regions [34], heatmaps [37, 18], or pixel-wise predictions [33, 38].",
          "2": "Many approaches use hough voting and unit-vectors within keypoint-based methods to determine the keypoints corresponding to objects\u2019 projected 3D bounding box edges or 3D centroid point [15, 38, 33].",
          "3": "Our approach takes inspiration from RGB methods [38, 33, 15] by using the smaller ResNet-FPN framework and regressing both intermediate representations and direct attributes.",
          "4": "For the hough voting scheme, we adapted the CUDA-accelerated implementation proposed by PVNet [38] for our problem as it provides a fast and accurate method to process multiple centroids in a batched fashion.",
          "5": "Our model adapted PVNet\u2019s CUDA-accelerated hough voting scheme [38]."
        },
        "Sim-to-Real Dataset of Industrial Metal Objects": {
          "authors": [
            "Peter De",
            "Steven Moonen",
            "Nick Michiels"
          ],
          "url": "https://www.mdpi.com/2075-1702/12/2/99",
          "ref_texts": "35. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
          "ref_ids": [
            "35"
          ],
          "1": "The authors first trained PVNet [35], a popular 6D object pose estimation method, on our dataset."
        },
        "Accurate Robot Arm Attitude Estimation Based on Multi-View Images and Super-Resolution Keypoint Detection Networks": {
          "authors": [
            "Ling Zhou",
            "Ruilin Wang",
            "Liyan Zhang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/1/305",
          "ref_texts": "10. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4556\u20134565.",
          "ref_ids": [
            "10"
          ],
          "1": "[10] proposed PVNet (Pixel-wise Voting Network), which can obtain superior keypoint detection results when the target object is partially blocked."
        },
        "Improving 2D-3D Dense Correspondences with Diffusion Models for 6D Object Pose Estimation": {
          "authors": [
            "P H\u00f6nig",
            "S Thalhammer",
            "M Vincze"
          ],
          "url": "https://arxiv.org/abs/2402.06436",
          "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "Earlier works create these 2D-3D correspondences by utilizing (sparse) keypoints [18, 17, 10]."
        },
        "PCKRF: Point Cloud Completion and Keypoint Refinement With Fusion Data for 6D Pose Estimation": {
          "authors": [
            "Y Han",
            "IH Zhan",
            "L Zeng",
            "YP Wang",
            "R Yi"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10504632/",
          "ref_texts": "[11] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "11"
          ],
          "1": "[11]\u2013[13] utilize DNNs to detect the keypoints of each object and subsequently compute the 6D pose parameters using Perspective-n-Point (PnP) for 2D keypoints or Least Squares methods for 3D keypoints.",
          "2": "PVNet [11] predicted a unit vector to each keypoint for each pixel, then voted the 2D location for each keypoint and calculated the final pose using the PnP algorithm."
        },
        "Vision-Based 6D Pose Estimation and Tracking: From Known to Novel Objects": {
          "authors": [
            "L Tian"
          ],
          "url": "https://qmro.qmul.ac.uk/xmlui/handle/123456789/99018",
          "ref_texts": "[101] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In IEEE conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "101"
          ],
          "1": "2D voting in PVNet [101], which takes RGB image as input and predicts vectors pointing to keypoints for each pixel, and localises 2D keypoints in a RANSAC-based voting scheme.",
          "2": "Instance-level methods focus on estimating the 6D pose of specific objects, which can be achieved through direct pose regression from images [156, 72, 29, 10, 115] constructing 2D-3D correspondences and solving poses with PnP algorithm [101, 162, 66, 100, 83] or finding 3D-3D matching and recovering poses with Least-squares algorithm [54, 53, 82, 70, 154].",
          "3": "In the 2D case, PVNet [101] votes 2D feature points and then finds the corresponding 2D-3D correspondences to obtain the 6D object pose.",
          "4": "It is widely used by recent deep learning-based methods [101, 54, 53].",
          "5": "The FPS algorithm is used to select points that are farthest from each other in the 3D space, which is widely used in point cloud related methods [105, 103, 101, 29].",
          "6": "Following other keypoint-based 6D object pose estimation methods [101, 54, 137], I train my model to find 8 pairs of matched 3D keypoints from two frames for changed 6D pose estimation."
        },
        "FruitBin: a tunable large-scale dataset for advancing 6D pose estimation in fruit bin-picking automation": {
          "authors": [
            "G Duret",
            "M Ali",
            "N Cazin",
            "D Mazurak"
          ],
          "url": "https://hal.science/hal-04683842/",
          "ref_texts": "30. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "30"
          ],
          "1": "We evaluated the performance of three foundational 6D pose estimation models: PVNet [30], DenseFusion [39], and GDRNPP [23,40].",
          "2": "The first method, known as PVNet [30], employs an RGB image and 3D model information of objects as input to predict the 6D pose."
        },
        "Active Perception for Estimating 6D Poses of Textureless Shiny Objects": {
          "authors": [
            "J Yang"
          ],
          "url": "https://search.proquest.com/openview/a1c3d29634ebff5d42101124599298a2/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation\u201d. In: IEEE/CVF Conference on Computer Vision and Pattern recognition (CVPR). 2019.",
          "ref_ids": [
            "27"
          ],
          "1": "As the most representative keypoint-based approach, PVNet [27] introduces a pixel-wise voting network to find object keypoints\u2019 pixel locations in the image and estimate the object pose using an uncertainty-driven PnP solver.",
          "2": "Our network architecture is based on PVNet [27]."
        },
        "Mapping Real-World Objects into Virtual Reality to Facilitate Interaction using 6DoF Pose Estimation": {
          "authors": [
            "S Pelser"
          ],
          "url": "https://scholar.sun.ac.za/bitstreams/ed797c6c-035f-4b97-a177-9bd22d3234db/download",
          "ref_texts": "[55] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4556\u20134565.",
          "ref_ids": [
            "55"
          ],
          "1": "Many DL models have been designed and tested on the LineMOD dataset used for 6DoF pose estimation, introducing CNN-based approaches that outperformed traditional methods by a significant margin, with PoseCNN [56] and PVNet [55] being some of the most well-known models."
        },
        "Six Degrees of Freedom Pose Estimation of the Rocket": {
          "authors": [
            "Zhuoyue Sun"
          ],
          "url": "https://search.proquest.com/openview/ff97e60fc01ba6e38f994dd4f5fdc1c3/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[6] SidaPeng,YuanLiu,QixingHuang,XiaoweiZhou,andHujunBao.Pvnet: Pixel-wisevoting networkfor6dofposeestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pages4561\u20134570,2019.",
          "ref_ids": [
            "6"
          ],
          "2": "PVNet: Pixel-Wise Voting Network PVNet [6] uses a CNN to detect two-dimensional keypoints of objects, which are then used with the PnP algorithm [12] to find the corresponding 3D keypoints."
        },
        "Image-Based Pose Estimation of Sub-Centimeter Industrial Parts for Automated Assembly": {
          "authors": [
            "H Abdul-Rashid",
            "Y Dai",
            "H Dinkel",
            "MQ Ta",
            "T Chen"
          ],
          "url": "https://yangfei4.github.io/clean-pvnet/assets/CoRL_2024.pdf",
          "ref_texts": "[33] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE/CVF Int. Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019.",
          "ref_ids": [
            "33"
          ],
          "1": "This paper uses the PVNet method, a supervised deep learning method to estimate poses from RGB images [33]."
        },
        "6D object position estimation from 2D images: a literature review": {
          "authors": [
            "Giorgia Marullo"
          ],
          "url": "https://link.springer.com/article/10.1007/s11042-022-14213-z",
          "ref_texts": "43. Peng S, Liu Y, Huang Q, Zhou X, Bao H (2019) PVNet: pixel-wise voting network for 6DoF pose estimation. 2019 IEEECVF Conf. Comput. Vis. Pattern Recognit. CVPR, pp 4556 \u20134565. https://doi.org/10.",
          "ref_ids": [
            "43"
          ],
          "1": "In [41, 43], the authors used CNNs to extract the features, and a shape fitting algorithm for determining the final position.",
          "2": "[43] introduced a CNN, called Pixel-wise Voting Network (PVNet), to predict the 2D-3D correspondences by regression of pixel-wise vectors to keypoints.",
          "4": "Feature-based methods (Table 1) could be the appropriate solution if the target object has a recognizable shape, but the keypoints must be accurately chosen, and a refinement step is often required [4, 41, 43, 65, 68\u201371].",
          "5": "Even though the refinement step of two-stage methods is time-consuming, some of them can run in real-time [43, 65, 71] or near real-time [41].",
          "6": "These systems have been evaluated on LINEMOD dataset [43, 65, 68\u201371], PASCAL3D + dataset [18, 41], KITTI dataset [3] or custom datasets [4]."
        },
        "Visibility aware human-object interaction tracking from single rgb camera": {
          "authors": [
            "Xianghui Xie",
            "Bharat Lal",
            "Gerard Pons"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xie_Visibility_Aware_Human-Object_Interaction_Tracking_From_Single_RGB_Camera_CVPR_2023_paper.html",
          "ref_texts": "[52] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4556\u20134565, Long Beach, CA, USA, June 2019. IEEE. 2",
          "ref_ids": [
            "52"
          ],
          "1": "On the other hand, deep learning method has also significantly improved object 6D pose estimation from single RGB images [22,25,33,45,50,52,72].",
          "2": "For object pose estimation, pixel-wise voting [52] and self-occlusion [22] are explored for more robust prediction under occlusions."
        },
        "Deep fusion transformer network with weighted vector-wise keypoints voting for robust 6d object pose estimation": {
          "authors": [
            "Jun Zhou",
            "Kai Chen",
            "Linlin Xu",
            "Qi Dou",
            "Jing Qin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Deep_Fusion_Transformer_Network_with_Weighted_Vector-Wise_Keypoints_Voting_for_ICCV_2023_paper.html",
          "ref_texts": "[47] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 2, 5, 6, 7",
          "ref_ids": [
            "47"
          ],
          "1": "For better robustness to highly occluded scenes, furthermore, the pixel/point-wise voting methods [20, 47, 49, 62] are proposed to vote for the keypoints position.",
          "2": "In this way, instead of regressing point-wise offsets to the predefined keypoints directly, we propose to predict the unit vector that represents the direction from the point pi to a 3D keypoint kj of the object, like [47] in 2D.",
          "3": "We use synthesis images in the training phase following [19, 20, 47] and follow previous works [47, 61] to split the training and testing set.",
          "4": "1d) as in [22, 47].",
          "5": "RGB RGB-D PoseCNN DeepIM [38, 61] PVNet [47] CDPN [39] DPOD [64] PointFusion [63] DenseFusion [58] G2L-Net [13] PVN3D [20] FFB6D [19] Ours ape 77.",
          "6": "Method PoseCNN [61] Pix2Pose [46] PVNet [47] Hu et al."
        },
        "Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation": {
          "authors": [
            "Heng Yang",
            "Marco Pavone"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.html",
          "ref_texts": "[72] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation.IEEE Trans. Pattern Anal. Machine Intell., 2022. 1, 2, 8",
          "ref_ids": [
            "72"
          ],
          "2": ", PVNet [72]) and show that the average pose achieves better or similar accuracy.",
          "3": "Sparse methods define a handful of keypoints and predict locations of the keypoints via direct regression [74, 89], probabilistic heatmap [67, 71], or voting [72].",
          "4": "Baselines (results adapted from [72]) Conformalized heatmap Tekin PoseCNN Oberweger PVNet gt-ball gt-ellipse frcnn-ball frcnn-ellipse objects [89][95][67][72] \u270f=0."
        },
        "Se (3) diffusion model-based point cloud registration for robust 6d object pose estimation": {
          "authors": [
            "H Jiang",
            "M Salzmann",
            "Z Dang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/43069caa6776eac8bca4bfd74d4a476d-Abstract-Conference.html",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "41"
          ],
          "1": "While substantial efforts have been dedicated to developing methods for 6D pose estimation based on RGB or RGB-D data [26, 42, 41, 52, 38, 63, 49], the advancements of 3D sensors (such as Kinect and LiDAR) and 3D registration techniques has promoted the emergence of point cloud registration-based pose estimation as a promising direction.",
          "2": "PVNet[41] establishes a pixel-wise voting network for keypoint estimation using predicted pixel-wise voting vectors."
        },
        "Tta-cope: Test-time adaptation for category-level object pose estimation": {
          "authors": [
            "Taeyeop Lee",
            "Jonathan Tremblay",
            "Valts Blukis",
            "Bowen Wen",
            "Uk Lee",
            "Inkyu Shin",
            "Stan Birchfield",
            "In So",
            "Jin Yoon"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_TTA-COPE_Test-Time_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "28"
          ],
          "1": "Advanced methods that focus on diverse variations of object 6D pose estimation have been introduced, such as known 3D objects (instancelevel) [28, 38], category-level [18, 36, 43], few-shot [52], and zero-shot pose estimation [13, 47]."
        },
        "Nerf-pose: A first-reconstruct-then-regress approach for weakly-supervised 6d object pose estimation": {
          "authors": [
            "Fu Li",
            "Shishir Reddy",
            "Hao Yu",
            "Ivan Shugurov",
            "Benjamin Busam",
            "Shaowu Yang",
            "Slobodan Ilic"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Li_NeRF-Pose_A_First-Reconstruct-Then-Regress_Approach_for_Weakly-Supervised_6D_Object_Pose_Estimation_ICCVW_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "36"
          ],
          "1": "Most of the recent approaches [23, 29, 30, 36, 37, 47, 49, 59, 63, 25, 53, 7] require 6D pose labels as supervision signals.",
          "2": "As widely used in the recent methods [23, 29, 30, 36, 37, 47, 49, 59, 63, 25, 53, 7], segmentation mask can be obtained either manually or automatically with the off-the-shelf object segmentation approaches [24, 13, 6], few-shot segmentation[56, 34], depth driven segmentation or background substaction[11], while relative camera poses can be obtained with Structure from Motion (SfM) [42], Simultaneous Localization And Mapping (SLAM) [1], Inertial Visual Odometry [28], or simply a marker board [22]."
        },
        "Shape-constraint recurrent flow for 6d object pose estimation": {
          "authors": [
            "Yang Hai",
            "Rui Song",
            "Jiaojiao Li",
            "Yinlin Hu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition, 2019. 2, 5, 6",
          "ref_ids": [
            "36"
          ],
          "1": "Related Work Object pose estimation , has shown significant improvement [36, 47, 50] after the utilization of deep learning techniques [13, 51].",
          "2": "Most recent methods create the correspondence either by predicting 2D points of some predefined 3D points [18, 21, 36, 37] or predicting the corresponding 3D point for every 2D pixel location within a segmentation mask [3, 10, 29, 42, 48, 53].",
          "4": "Comparison to the State of the Art We compare our method with most state-of-the-art methods, including PoseCNN [50], PVNet [36], SO-Pose [10], DeepIM [28], RePose [23], RNNPose [52], and PFA [16]."
        },
        "Texpose: Neural texture learning for self-supervised 6d object pose estimation": {
          "authors": [
            "Hanzhi Chen",
            "Fabian Manhardt",
            "Nassir Navab",
            "Benjamin Busam"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_TexPose_Neural_Texture_Learning_for_Self-Supervised_6D_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.1, 2",
          "ref_ids": [
            "39"
          ],
          "1": "Noteworthy, accuracy and runtime have both recently made a huge leap forward thanks to deep learning [19, 23, 31, 39, 40]."
        },
        "Learning symmetry-aware geometry correspondences for 6d object pose estimation": {
          "authors": [
            "Heng Zhao",
            "Shenxing Wei",
            "Dahu Shi",
            "Wenming Tan",
            "Zheyang Li",
            "Ye Ren",
            "Xing Wei",
            "Yi Yang",
            "Shiliang Pu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "43"
          ],
          "1": "Some existing works [43, 15] employ CNNs to detect a set of keypoints predefined on the 3D object model.",
          "2": "For instance, PVNet [43] selects K 3D keypoints from the object surface."
        },
        "Crt-6d: Fast 6d object pose estimation with cascaded refinement transformers": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "34"
          ],
          "4": "This shortfall was noticed by PVNet [34], which suggests the use of the surface region to find suitable keypoints.",
          "6": "1 8 1 8 1 1 8 8 Method PVNet [34] GDR [47] GDR [47] SO-Pose [9] ZebraPose [40] RePose [20] DeepIM [28] CRT-6D Ape 15.",
          "7": "Repose [20] proposed a faster refinement method at 18ms with 5 iterations however they require a good initialization (they use PVNet [34] which itself takes over 25ms) and it only support a single object per model."
        },
        "Nerf-loc: Visual localization with conditional neural radiance field": {
          "authors": [
            "J Liu",
            "Q Nie",
            "Y Liu",
            "C Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161420/",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "PVNet[21] 25."
        },
        "Center-based decoupled point-cloud registration for 6D object pose estimation": {
          "authors": [
            "Haobo Jiang",
            "Zheng Dang",
            "Shuo Gu",
            "Jin Xie",
            "Mathieu Salzmann",
            "Jian Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[51] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1",
          "ref_ids": [
            "51"
          ],
          "1": "While great progress has been made when exploiting RGB or RGB-D data as input [34, 54, 51, 59, 49, 69, 58], the advances in 3D sensors and deep point-cloud learning architectures have led to the development of increasingly accurate point cloud registration algorithms [61, 67, 32, 19, 14]."
        },
        "Query6dof: Learning sparse queries as implicit shape prior for category-level 6dof pose estimation": {
          "authors": [
            "Ruiqi Wang",
            "Xinggang Wang",
            "Te Li",
            "Rong Yang",
            "Minhong Wan",
            "Wenyu Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Query6DoF_Learning_Sparse_Queries_as_Implicit_Shape_Prior_for_Category-Level_ICCV_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "Instance-Level 6D Object Pose Estimation Based on the input data format, existing methods can be divided into two categories: RGB-based [15, 28, 25, 22, 37, 23] and RGB-D-based [14, 32, 10, 13] approaches."
        },
        "Easyhec: Accurate and automatic hand-eye calibration via differentiable rendering and space exploration": {
          "authors": [
            "L Chen",
            "Y Qin",
            "X Zhou",
            "H Su"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10251600/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "In our work, we adopt the PVNet [25] to perform the pose initialization.",
          "2": "We rendered 10,000 images for the PointRend [20] and another 10,000 images for the PVNet [25] training to obtain the observed segmentation mask and initial camera pose for our method.",
          "3": "Specifically, we use PVNet [25] to estimate the object poses in the camera coordinate system and then use the handeye calibration results to transform the object poses from the camera coordinate system to the base coordinate system as the input states to the CoTPC network."
        },
        "ContourPose: Monocular 6-D pose estimation method for reflective textureless metal parts": {
          "authors": [
            "Z He",
            "Q Li",
            "X Zhao",
            "J Wang",
            "H Shen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10189174/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4556\u20134565.",
          "ref_ids": [
            "10"
          ],
          "2": "PVNet[10] regresses pixelwise vectors point at the keypoints and uses these vectors to vote for the location of the keypoints.",
          "3": "HybridPose[35] extends the approach of PVNet [10] by utilizing a hybrid intermediate representation to express different geometric information in the input image, including keypoints, edge vectors, and symmetry correspondences.",
          "5": "Many methods such as PVNet[10] and PSGMN [37] will output both the keypoints information and the mask using only one decoder.",
          "6": "Common methods use the farthest point sampling (FPS) [40] algorithm to select several points at the farthest Euclidean distance on the model, such as PVNet[10].",
          "9": "Our proposed method trains a specific network to estimate a single object similar to PVNet[10],C D P N[12], and PSGMN [37]."
        },
        "SMOC-Net: leveraging camera pose for self-supervised monocular object pose estimation": {
          "authors": [
            "Tao Tan",
            "Qiulei Dong"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[22] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "22"
          ],
          "4": "[22] proposed a pixel level voting network (PVNet) by using the direction vector field to predict keypoints, which achieved good performance under severe truncation and occlusion.",
          "6": "Here, we evaluate the proposed SMOC-Net on the LineMOD dataset in comparison to some state-of-the-art methods, including three fullysupervised methods (DPOD [41], PVNet [22], CDPN [17]), three self-supervised methods that are trained with only synthetic data (AAE [31], MHP [20], DPOD [41]), one selfsupervised method that is trained with both synthetic data and un-annotated real images (DSC-PoseNet [39]), and two self-supervised methods that are trained with synthetic data + un-annotated real images + depth images (Self6D [34], Self6D++ [33])."
        },
        "Posematcher: One-shot 6d object pose estimation by deep feature matching": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Castro_PoseMatcher_One-Shot_6D_Object_Pose_Estimation_by_Deep_Feature_Matching_ICCVW_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2, 6, 7, 8",
          "ref_ids": [
            "30"
          ],
          "1": "PVNet [30] found that choosing keypoints that lie within the object\u2019s silhouette would yield better results.",
          "3": "It is interesting to observe that we also outperform PVNet[30], an instance-level pose estimator."
        },
        "Generative category-level shape and pose estimation with semantic primitives": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://proceedings.mlr.press/v205/li23d.html",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "9"
          ],
          "3": "For example, PVNet [9] predicts the 3D keypoints on the RGB image by a voting scheme."
        },
        "Toward 3d face reconstruction in perspective projection: Estimating 6dof face pose from monocular image": {
          "authors": [
            "Y Kao",
            "B Pan",
            "M Xu",
            "J Lyu",
            "X Zhu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10127617/",
          "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "15"
          ],
          "1": "Besides, 6DoF pose estimation in other objects [15]\u2013[17] always assumes a pre-defined 3D shape but we only have a face image as input.",
          "2": "For 6DoF face pose estimation, we use a two-stage pipeline [15]\u2013[17], that first chooses m2D pixels V \u2208R2\u00d7m in 2D image and learns their corresponding 3D points in reconstructed 3D face shape and 6D face pose parameters can be calculated by a PnP [18] algorithm."
        },
        "Revisiting fully convolutional geometric features for object 6d pose estimation": {
          "authors": [
            "Jaime Corsetti",
            "Davide Boscaini",
            "Fabio Poiesi"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Corsetti_Revisiting_Fully_Convolutional_Geometric_Features_for_Object_6D_Pose_Estimation_ICCVW_2023_paper.html",
          "ref_texts": "[31] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "31"
          ],
          "1": "PVN3D [13] extends PVNet [31] by incorporating 3D point cloud information."
        },
        "Checkerpose: Progressive dense keypoint localization for object pose estimation with graph neural network": {
          "authors": [
            "Ruyi Lian",
            "Haibin Ling"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lian_CheckerPose_Progressive_Dense_Keypoint_Localization_for_Object_Pose_Estimation_with_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2, 4, 5, 7",
          "ref_ids": [
            "43"
          ],
          "1": "Instead of direct estimation, correspondence guided methods [42, 48, 59, 38, 18, 43, 17, 19, 72, 40, 33, 66, 8, 56] follow a two-stage framework: they first predict a set of correspondences between 3D object frame coordinates and 2D image plane coordinates, and then recover the pose from the 3D-2D correspondences with a PnP algorithm [27, 25, 9, 64, 4].",
          "2": "Keypoint-localization based methods [42, 48, 59, 38, 18, 43, 17, 19] estimate the 2D coordinates for a sparse set of predefined 3D keypoints, while dense methods [72, 40, 33, 66, 8, 56] predict the 3D object frame coordinate of each 2D image pixel.",
          "3": ", heatmaps [42, 38] and vector-fields [43, 18]), our representation needs only 2d + 1binary bits for each keypoint, thus greatly reduces the memory usage for dense keypoint localization.",
          "4": ", voting for the vector-field representations [43]."
        },
        "Cad2render: A modular toolkit for gpu-accelerated photorealistic synthetic data generation for the manufacturing industry": {
          "authors": [
            "Steven Moonen",
            "Bram Vanherle",
            "Taoufik Bourgana",
            "Abdellatif Bey",
            "Nick Michiels"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/html/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.html",
          "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "The identification of the objects and subsequent position and pose estimation was done with two state of the art networks: YoloV4 [2] for object detection and PVNET [17] for pose estimation."
        },
        "Digital twin tracking dataset (dttd): A new rgb+ depth 3d dataset for longer-range object tracking applications": {
          "authors": [
            "Weiyu Feng",
            "Seth Z. Zhao",
            "Chuanyu Pan",
            "Adam Chang",
            "Yichen Chen",
            "Zekun Wang",
            "Allen Y. Yang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.html",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3",
          "ref_ids": [
            "23"
          ],
          "1": "6 DoF Object Pose Estimation Most data-driven methods for object pose estimation take RGB [18, 23, 29, 30] or RGB-D images [10, 11, 15, 22, 27] as input."
        },
        "Stereopose: Category-level 6d transparent object pose estimation from stereo images via back-view nocs": {
          "authors": [
            "K Chen",
            "S James",
            "C Sui",
            "YH Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160780/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "10"
          ],
          "1": ", instance-level voting field [10] and category-level NOCS map [1]) used on nontransparent objects only describe dense correspondences for the front-view1 of the object."
        },
        "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching": {
          "authors": [
            "Y Sun",
            "X Wang",
            "Y Zhang",
            "J Zhang",
            "C Jiang"
          ],
          "url": "https://arxiv.org/abs/2312.09031",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019) 1",
          "ref_ids": [
            "28"
          ],
          "1": "Common pose estimation methods often rely on detailed geometric models related to the target object [18,28,36,39]."
        },
        "Generalizable pose estimation using implicit scene representations": {
          "authors": [
            "V Saxena",
            "KR Malekshan",
            "L Tran"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161162/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "13"
          ],
          "3": "Keypoint-based approaches ([13], [14]) utilized deep neural networks to detect 2D keypoints of an object and computed 6D pose parameters with Perspective-n-Point (PnP) algorithms, improving pose estimates by a large margin."
        },
        "6d pose estimation for textureless objects on rgb frames using multi-view optimization": {
          "authors": [
            "J Yang",
            "W Xue",
            "S Ghavidel"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160529/",
          "ref_texts": "[26] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "com boost the object pose estimation performance using only RGB images [23], [24], [25], [26], [27].",
          "2": "In comparison, some recent works leverage CNNs to first predict 2D object keypoints [36], [37], [26] or dense 2D-3D correspondences [38], [39], [27], [40], and then compute the pose through 2D-3D correspondences with a PnP algorithm [41].",
          "3": "Our network architecture is based on PVNet [26].",
          "4": "For more details of the object center localization prediction, we refer the reader to [26]."
        },
        "Perceiving unseen 3d objects by poking the objects": {
          "authors": [
            "L Chen",
            "Y Song",
            "H Bao",
            "X Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160338/",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "4"
          ],
          "2": "Here, we use the PVNet [4] to demonstrate how to learn an object pose estimator based on the reconstructed object model.",
          "3": "3, given the reconstructed object model, we use the analytic method Graspit! [16] to compute Tgo and PVNet [4] to estimate Toc."
        },
        "Depth-based 6dof object pose estimation using swin transformer": {
          "authors": [
            "Z Li",
            "I Stamos"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10342215/",
          "ref_texts": "[10] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DOF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "10"
          ],
          "2": "INPUTS RGB RGB-D Depth-Only METHODSPVNet [10]Pix2Pose [42]RNNPose [43]PVN3D [15]DenseFusion [13]KPD [44]CloudAAE [16]CATRE [19]OVE6D [17](w Mask R-CNN)OVE6D [17](w GT Masks) OURS(w/o GT Mask)OURS(w GT Masks) ape 43.",
          "3": "INPUTS RGB RGB-D Depth-Only METHODSPVNet [10]Pix2Pose [42]Keypoint [45]Point-to-Keypoint [46]FFB6D [14]KPD [44] CloudAAE [16]OVE6D [17](w Mask R-CNN)OVE6D [17](w GT Masks) OURS(w/o GT Mask) OURS(w GT Masks) ape 15."
        },
        "Linear-covariance loss for end-to-end learning of 6d pose estimation": {
          "authors": [
            "Fulin Liu",
            "Yinlin Hu",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Linear-Covariance_Loss_for_End-to-End_Learning_of_6D_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1, 2",
          "ref_ids": [
            "41"
          ],
          "3": "[23] aggregate the 2D keypoint predictions from all pixels belonging to the given target; Similarly, PVNet [41] regresses the vector-field pointing from each object pixel to the 2D locations."
        },
        "YOLOPose V2: Understanding and improving transformer-based 6D pose estimation": {
          "authors": [
            "AS Periyasamy",
            "A Amini",
            "V Tsaturyan"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S092188902300129X",
          "ref_texts": "[40] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "40"
          ],
          "2": "[40] for RGB images to 3D point clouds by learning point-wise 3D keypoint offset and using a deep Hough voting network.",
          "3": "[40] instead used the Farthest Point Sampling (FPS) algorithm to sample eight keypoints on the surface of the object meshes, which are also spread out on the object to help the P nP algorithm find a more stable solution."
        },
        "Multi-view keypoints for reliable 6d object pose estimation": {
          "authors": [
            "A Li",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160354/",
          "ref_texts": "[7] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019)",
          "ref_ids": [
            "7"
          ],
          "3": "Single-view Keypoint and Heatmap Estimation The keypoint detection network is based off PVNet, a pixel-wise voting network for 6D pose estimation [7]."
        },
        "DR-pose: A two-stage deformation-and-registration pipeline for category-level 6D object pose estimation": {
          "authors": [
            "L Zhou",
            "Z Liu",
            "R Gan",
            "H Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10341552/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "7"
          ],
          "2": "The second approach uses 2D-3D [7][16][17][18][19] or 3D3D [20] correspondences correspondences to solve a PnP [21][22] problem and obtain the 6D pose."
        },
        "SD-pose: structural discrepancy aware category-level 6D object pose estimation": {
          "authors": [
            "Guowei Li",
            "Dongchen Zhu",
            "Guanghui Zhang",
            "Wenjun Shi",
            "Tianyu Zhang",
            "Xiaolin Zhang",
            "Jiamao Li"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "27"
          ],
          "1": "So far, instancelevel 6D pose estimation works [19, 29, 22, 27, 38, 17, 16] have made considerable progress.",
          "3": "Indirect voting [27, 17] first selects key point positions through RANSAC [10] voting and then calculates the 6D pose of the object according to the correspondence between key points."
        },
        "PViT-6D: Overclocking vision transformers for 6D pose estimation with confidence-level prediction and pose tokens": {
          "authors": [
            "S Stapf",
            "T Bauernfeind",
            "M Riboldi"
          ],
          "url": "https://arxiv.org/abs/2311.17504",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CoRR, abs/1812.11788, 2018. 1, 2, 8",
          "ref_ids": [
            "33"
          ],
          "1": "This has resulted in significant improvements in accuracy, as seen in models like PVNet [33], GDRNet [45], and the latest ZebraPose [38].",
          "2": "PVNet [33] introduced a twostage method that initially estimates the 2D keypoints of objects in the image.",
          "3": "This trend and the beforementioned results 7 Model PVNet [33] GDRNet [45] SO-Pose [6] CRT-6D[5] ZebraPose [38] Ours(-s) Ours(-b) Ours(-l) N 8 8 1 1 8 1 1 1 Params(M) 28.",
          "4": "Method N ADD(-S) AUC of ADD(-S) PVNet[33] 21 73."
        },
        "Deeprm: Deep recurrent matching for 6d pose refinement": {
          "authors": [
            "Alexander Avery",
            "Andreas Savakis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/html/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.html",
          "ref_texts": "[22] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNET: Pixel-wise voting network for 6dof pose estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019June:4556\u20134565, dec 2019. 2, 5, 6",
          "ref_ids": [
            "22"
          ],
          "1": "To further address the problem of occlusion, PVNet [22] introduced a pixel-wise voting network using RANSAC, resulting in an estimator that is capable of detecting keypoints, even when they are occluded.",
          "5": "Initial predictions are obtained from PVNet [22], where DeepRM outperforms all existing methods except for ZebraPose [27] and CRT-6D [4]."
        },
        "Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction": {
          "authors": [
            "Y Yang",
            "J Wu",
            "Y Wang",
            "G Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10265177/",
          "ref_texts": "[9] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "9"
          ],
          "2": "PVNet [9], employs farthest point sampling to vote for key points on the target object, predicting the direction vector pointing from each pixel to the projection point using a RANSAC voting strategy to locate the projection point."
        },
        "MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_31",
          "ref_texts": "23. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)",
          "ref_ids": [
            "23"
          ],
          "1": "Different from BB8, PVNet [23] selects the pre-defined 3D keypoints from the surface of the 3D object CAD model and then localizes the 2D pixel coordinates of these 3D keypoints in RGB images based on the pixel-wise voting schema."
        },
        "Transpose: A transformer-based 6d object pose estimation network with depth refinement": {
          "authors": [
            "M Abdulsalam",
            "N Aouf"
          ],
          "url": "https://arxiv.org/abs/2307.05561",
          "ref_texts": "[49] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "49"
          ],
          "1": "Some other methods have considered using models encompassing classical algorithm such as PnP algorithm to increase the accuracy of estimation [44], [49], [50]."
        },
        "Rigidity preserving image transformations and equivariance in perspective": {
          "authors": [
            "L Brynte",
            "G B\u00f6kman",
            "A Flinth",
            "F Kahl"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_5",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conf. Computer Vision and Pattern Recognition (2019)",
          "ref_ids": [
            "44"
          ],
          "2": "As expected, there is some discrepancy between the baseline trained by us and the original reported results, in particular Rigidity Preserving Image Transformations and Equivariance in Perspective 13 EP:Baseline EP:PY EP:RHaug PVNet[44] CDPN[38] BPnP[11] RNNPose[55] DFPN-6D[12] 95."
        },
        "NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios": {
          "authors": [
            "ET Lin",
            "WJ Lv",
            "DT Huang",
            "L Zeng"
          ],
          "url": "https://arxiv.org/abs/2311.09269",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "To address the non-linear problem in the rotation space, some works [10], [21] propose to predict corresponding keypoints and recover the 6D pose by least-squares fitting."
        },
        "Robust and Efficient Edge-guided Pose Estimation with Resolution-conditioned NeRF.": {
          "authors": [
            "L Claessens",
            "F Manhardt",
            "R Martin-Brualla"
          ],
          "url": "https://papers.bmvc2023.org/0543.pdf",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "30"
          ],
          "1": "Correspondence-based methods establish 2D3D correspondences [29, 30, 31, 32, 44], prior to leveraging a variant of the RANSAC&PnP paradigm to solve for pose."
        },
        "AttentionPose: Attention-driven end-to-end model for precise 6D pose estimation": {
          "authors": [
            "MA Rasheed",
            "RN Farhan",
            "WM Jasim"
          ],
          "url": "https://www.degruyter.com/document/doi/10.1515/jisys-2023-0153/html",
          "ref_texts": "[13] Peng S, Zhou X, Liu Y, Lin H, Huang Q, Bao H. PVNet: Pixel-wise voting network for 6DoF object pose estimation. IEEE Trans Pattern Anal Mach Intell. 2022;44(6):3212\u201323.",
          "ref_ids": [
            "13"
          ],
          "3": "PVnet [13] is a neural network developed to estimate the posture of 6D objects.",
          "4": "Table 2:Comparison with some state-of-the-art methods Object The proposed method PVNet [13] Hybrid pose [32] YOLO6D [33] DPOD [34] DPOD +[34] Efficient Pose [35] Duck 99."
        },
        "A Benchmark for Cycling Close Pass Near Miss Event Detection from Video Streams": {
          "authors": [
            "M Li",
            "T Rathnayake",
            "B Beck",
            "L Meng",
            "Z Chen"
          ],
          "url": "https://arxiv.org/abs/2304.11868",
          "ref_texts": "[40] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570",
          "ref_ids": [
            "40"
          ],
          "1": "On the other hand, end-to-end approaches [54], [37], [40] aim to directly return the 3D information and pose parameters of the camera [26], avoiding the nonlinear space for rotation regression and improving efficiency."
        },
        "An open-source recipe for building simulated robot manipulation benchmarks": {
          "authors": [
            "J Gu",
            "L Chen",
            "Z Jia",
            "F Xiang",
            "H Su"
          ],
          "url": "https://cseweb.ucsd.edu/~jigu/pdf/icra23-compare.pdf",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "In the real-world setup, we first estimate the 6-DoF object poses in the camera space by PVNet [25] and then transform the object poses to the robot base using the relative pose between the camera and the base of the robot arm obtained by hand-eye-calibration."
        },
        "Detection and Pose Estimation of Flat, Texture-Less Industry Objects on HoloLens Using Synthetic Training": {
          "authors": [
            "T P\u00f6llabauer",
            "F R\u00fccker",
            "A Franek"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_37",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019)",
          "ref_ids": [
            "32"
          ],
          "2": "PVNet [32] introduces a voting scheme, requiring pixels to vote for keypoint locations, making the predictions more robust to occlusion and truncation."
        },
        "Deep learning on point clouds with applications in vehicle self-localization": {
          "authors": [
            "N Engel"
          ],
          "url": "https://oparu.uni-ulm.de/xmlui/bitstream/123456789/48402/3/diss_nico_engel.pdf",
          "ref_texts": "[PLH+19] Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
          "ref_ids": [
            "PLH\\+19"
          ],
          "1": "[PLH+19] Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation."
        },
        "Affordance-grounded Robot Perception and Manipulation in Adversarial, Translucent, and Cluttered Environments": {
          "authors": [
            "Xiaotong Chen"
          ],
          "url": "https://deepblue.lib.umich.edu/handle/2027.42/177867",
          "ref_texts": "[147] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "147"
          ],
          "1": "Then there comes pure endto-end deep-learning pipelines from PoseCNN [192], BB8 [151], and SSD-6D [92] started the trend, followed by a large set of methods using RGB images [182, 14, 105, 147, 100, 70], or RGB-depth images [187, 73, 74] as input.",
          "2": "StereObj1M [111] benchmarked KeyPose and another RGB-based object pose estimator, PVNet [147], on more challenging objects and scenes, where both methods achieved lower accuracy with respect to the ADD-S AUC metric (introduced in [192]) with both monocular and stereo input."
        },
        "Learning Embodied AI Agents with Task Decomposition": {
          "authors": [
            "Z Jia"
          ],
          "url": "https://search.proquest.com/openview/54c5288fb3d998ed6b1314b68a1eb510/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[118] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "118"
          ],
          "1": "With an off-the-shelf pose estimation framework such as PVNet [118], we can achieve reasonable performance using the state-based CoTPC policy learned purely from simulated data."
        },
        "Vision-guided object pose estimation for robotic pushing in real-time": {
          "authors": [
            "PM van der Burg"
          ],
          "url": "https://repository.tudelft.nl/record/uuid:8155dc98-21ec-4603-8be0-cd13eec6f6ad",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, June 2019.",
          "ref_ids": [
            "33"
          ],
          "1": "Additionally, pose refinement methods rely on initial pose estimates obtained from models such as PoseCNN [48] or PVNet [33]."
        },
        "6D Pose Estimation of Weakly Textured Object Driven by Decoupling Analysis and Algorithm Fusion Strategy": {
          "authors": [
            "\u6c5f\u82cf\u79d1\u6280\u5927\u5b66\uff0c \u90d1\u5929\u5b87"
          ],
          "url": "https://www.researchsquare.com/article/rs-3105669/latest",
          "ref_texts": "32. Peng S, Liu Y , Huang Q, Zhou X, Bao H (2019) Pvnet: Pixel-wise voting netwo rk for 6dof pose estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561-4570 ",
          "ref_ids": [
            "32"
          ],
          "1": "developed the PVNet [32] network, which combines 3D shape information and 2D projection information for pose estimation."
        },
        "Image-based Object Pose Estimation for Robotic Manipulation: A Cost-effective Approach in Virtual Environment": {
          "authors": [
            "\u9ec4\u6d69\u6668"
          ],
          "url": "https://gunma-u.repo.nii.ac.jp/record/10029/files/T201D604.pdf",
          "ref_texts": "[10] Peng, S., Liu, Y ., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570). ",
          "ref_ids": [
            "10"
          ],
          "1": "In recent years, deep learning methods based on 2D image [2, 9, 10, 11] or 3D point cloud [12, 13] are proposed to solve the pose estimation problem, and achieved a good result benefiting from the powerful feature extraction ability of neural network.",
          "2": "Keypoint-based methods [10, 11]: Compared with directly predicting pose-related parameters, building 2D-3D correspondences for object pose detection is more accurate."
        },
        "DAPO: Self-Supervised Domain Adaptation for 6DoF Pose Estimation": {
          "authors": [
            "J Jin",
            "E Jeong",
            "J Cho",
            "JH Park",
            "YG Kim"
          ],
          "url": "https://sslneurips23.github.io/paper_pdfs/paper_21.pdf",
          "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "12"
          ],
          "1": "For instance, PVNet [12] utilizes a regression-based approach to predict pixel-wise unit vectors pointing to key points."
        },
        "Rilevamento di oggetti 3D da immagini 2D: metodi e applicazioni": {
          "authors": [
            "Zlatko Kovachev"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/57086",
          "ref_texts": "[13] S. Peng, Y. Liu, Q. Huang, X. Zhou e H. Bao, \u00abPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u00bb in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
          "ref_ids": [
            "13"
          ],
          "4": "[13], \u00e8 un ottimo esempio di metodo Feature-Based per la stima della posa 6D, concentrandosi specificamente su scene che presentano grandi occlusioni (g) 22 e oggetti troncati (h) (Figura 3."
        },
        "Utilizzo di reti GAN per la creazione di immagini di addestramento per l'object pose estimation": {
          "authors": [
            "R DEL BEN"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/54924",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "2"
          ],
          "1": "Finally, the best GAN models have been combined with Pixel-wise Voting Network (PVNet [2]) to compare their performance against the PVNet model trained using the conventional image Superimposing Method (PVNet-SM)."
        },
        "Deep learning on monocular object pose detection and tracking: A comprehensive overview": {
          "authors": [
            "Z Fan",
            "Y Zhu",
            "Y He",
            "Q Sun",
            "H Liu",
            "J He"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3524496",
          "ref_texts": "[118] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4561\u20134570.",
          "ref_ids": [
            "118"
          ],
          "1": "To solve this problem, PVNet [118] adopts the strategy of voting-based keypoint localization.",
          "6": "Beyond leveraging image-level consistency for self-supervised learning, inspired by recent keypoint-based methods [118, 137], DSC-PoseNet [168] develops a weakly supervised and a self-supervised learning-based pose estimation framework that enforces dual-scale keypoint consistency without using pose annotations."
        },
        "Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation": {
          "authors": [
            "Hansheng Chen",
            "Pichao Wang",
            "Fan Wang",
            "Wei Tian",
            "Lu Xiong",
            "Hao Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 4, 7",
          "ref_ids": [
            "31"
          ],
          "3": "Comparison to the State of the Art As shown in Table 2, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE [20], which adds extra overhead to the PnP-based initial estimator PVNet [31]."
        },
        "Onepose: One-shot object pose estimation without cad models": {
          "authors": [
            "Jiaming Sun",
            "Zihao Wang",
            "Siyu Zhang",
            "Xingyi He",
            "Hongcheng Zhao",
            "Guofeng Zhang",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "26"
          ],
          "4": "Our method is compared with PVNet [26] on selected objects from the OnePose dataset with the 5cm-5deg metric.",
          "5": "The proposed method is compared with PVNet [26] with 5cm-5deg on selected objects from our OnePose dataset and the results are as presented in Tab."
        },
        "Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation": {
          "authors": [
            "Yongzhi Su",
            "Mahdi Saleh",
            "Torben Fetzer",
            "Jason Rambach",
            "Nassir Navab",
            "Benjamin Busam",
            "Didier Stricker",
            "Federico Tombari"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.html",
          "ref_texts": "[49] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "49"
          ],
          "2": "BB8 [52] firstly defines the 3D object bounding box corners as the keypoints and PVNet [49] reaches high recall rate in LM [27] dataset by predicting the keypoints with a dense pixel-wise voting for sampled keypoints on the object."
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
          "authors": [
            "X He",
            "J Sun",
            "Y Wang",
            "D Huang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/e43f900f571de6c96a70d5724a0fb565-Abstract-Conference.html",
          "ref_texts": "[39] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: pixel-wise voting network for 6dof object pose estimation. T-PAMI, 2020. 1, 2, 3, 8, 9",
          "ref_ids": [
            "39"
          ],
          "2": "The experiments show that our method outperforms all existing one-shot pose estimation methods [48, 33] by a large margin and even achieves comparable results with instance-level methods [39, 29] which are trained for each object instance with a CAD model.",
          "4": "Our method is compared with PVNet[39] on objects with CAD models in the OnePose-LowTexture dataset using the ADD(S)-0.",
          "6": "For the comparison with PVNet [39], we follow its original training setting, which first samples 8 keypoints on the object surface and then trains a network using 5000 synthetic images for each object.",
          "7": "On the OnePose-LowTexture dataset, the proposed method is compared with PVNet [39] on the subset objects with scanned models.",
          "8": "4 Results on LINEMOD We compare the proposed method with OnePose [48] and Gen6D [33] which are under the One-shot setting, and Instance-level methods PVNet [39] and CDPN [29] on ADD(S)-0.",
          "9": "Our method has lower or comparable performance with instance-level methods [39, 29], which are trained to fit each object instance, and thus perform well naturally, at the expense of the tedious training for each object."
        },
        "Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images": {
          "authors": [
            "Y Liu",
            "Y Wen",
            "S Peng",
            "C Lin",
            "X Long",
            "T Komura"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_18",
          "ref_texts": "42. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019) Gen6D Pose Estimator 17",
          "ref_ids": [
            "42"
          ],
          "2": "Experiments show that without training on these objects, our method still outperforms instance-specific estimator PVNet [42] on the GenMOP dataset and another model-free MOPED [41] dataset.",
          "4": "3 Results on GenMOP For comparison, we choose the generalizable image-matching based ObjDesc [69] and two instance-specific estimators PVNet [42] and RLLG [6] as baseline methods.",
          "5": "For instance-specific estimators PVNet [42] and RLLG [6], we have to train different models for different objects separately.",
          "13": "5 Results on MOPED [41] On the MOPED dataset, we compare Gen6D with Latent-Fusion [41] and PVNet [42].",
          "15": "For training PVNet [42], we apply the same strategy as used on the GenMOP dataset.",
          "16": "On the LINEMOD [23] dataset, we use the training set of previous instance-specific estimators [59,42] as reference images and the other images are selected as query images.",
          "17": "Note that reference images are used in inference of Gen6D but not in training the Gen6D estimator while instance-specific estimators like PVNet [42] actually use these reference images to train their models.",
          "18": "Note PVNet [42] is trained on the specific test object with both synthetic and real images while our Gen6D is not trained on the test object."
        },
        "Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings": {
          "authors": [
            "Rasmus Laurvig",
            "Anders Glent"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Haugaard_SurfEmb_Dense_and_Continuous_Correspondence_Distributions_for_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "26"
          ],
          "4": "PVNet [26] regresses vector fields toward the 2D projections of a set of fixed 3D key points and handles symmetries like BB8."
        },
        "Fs6d: Few-shot 6d pose estimation of novel objects": {
          "authors": [
            "Yisheng He",
            "Yao Wang",
            "Haoqiang Fan",
            "Jian Sun",
            "Qifeng Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.html",
          "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 2, 6",
          "ref_ids": [
            "38"
          ],
          "1": "Learning-based approaches includes direct pose regression [53,58], dense correspondence exploration [29] and recent keypoint-based approaches [15, 16, 38], which improve the performance by large margins."
        },
        "Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions": {
          "authors": [
            "Van Nguyen",
            "Yinlin Hu",
            "Yang Xiao",
            "Mathieu Salzmann",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Nguyen_Templates_for_3D_Object_Pose_Estimation_Revisited_Generalization_to_New_CVPR_2022_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 2",
          "ref_ids": [
            "27"
          ],
          "1": "In particular, the robustness to partial occlusions has greatly increased [27, 16, 23], and the need for large amounts of real annotated training images has been relaxed thanks to domain transfer [1], domain randomization [35, 18, 30], and self-supervised learning [32] techniques that leverage synthetic images for training.",
          "2": "Some also show remarkable robustness to partial occlusions of the objects [23, 27, 16]."
        },
        "6-DoF pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark": {
          "authors": [
            "S Tyree",
            "J Tremblay",
            "T To",
            "J Cheng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981838/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019. 1",
          "ref_ids": [
            "6"
          ],
          "1": "Recent progress on this 6-DoF (\u201cdegrees of freedom\u201d) pose estimation problem has been significant [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]."
        },
        "Osop: A multi-stage one shot object pose estimation framework": {
          "authors": [
            "Ivan Shugurov",
            "Fu Li",
            "Benjamin Busam",
            "Slobodan Ilic"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Shugurov_OSOP_A_Multi-Stage_One_Shot_Object_Pose_Estimation_Framework_CVPR_2022_paper.html",
          "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "37"
          ],
          "1": "According to the BOP challenge [14], which combines publicly available 6 DoF pose estimation datasets and offers standardized evaluation and comparison procedures, the field is dominated by deep learning methods [2, 12, 16, 19, 21, 22, 22, 24\u201327, 36, 37, 47, 49\u201352, 59]."
        },
        "Rnnpose: Recurrent 6-dof object pose refinement with robust correspondence field estimation and pose optimization": {
          "authors": [
            "Yan Xu",
            "Yee Lin",
            "Guofeng Zhang",
            "Xiaogang Wang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "34"
          ],
          "10": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Sar-net: Shape alignment and recovery network for category-level 6d object pose and size estimation": {
          "authors": [
            "Haitao Lin",
            "Zichang Liu",
            "Chilam Cheang",
            "Yanwei Fu",
            "Guodong Guo",
            "Xiangyang Xue"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Lin_SAR-Net_Shape_Alignment_and_Recovery_Network_for_Category-Level_6D_Object_CVPR_2022_paper.html",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 3, 4, 5, 6, 7",
          "ref_ids": [
            "39"
          ],
          "7": "Compared with RGB(-D) methods [15, 39] or depth-only method [12, 13], our SAR-Net achieves comparable results in terms of ADD(-S) metric as in Tab."
        },
        "Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation": {
          "authors": [
            "Dingding Cai",
            "Janne Heikkila",
            "Esa Rahtu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cai_OVE6D_Object_Viewpoint_Encoding_for_Depth-Based_6D_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 1, 2, 6, 7, 8",
          "ref_ids": [
            "36"
          ],
          "1": "In recent works, the object pose estimation problem is commonly approached by either establishing local correspondences between the object 3D model and the observed data [16, 17, 36], or via direct regression [6, 39].",
          "2": "Related Work Pose estimation from RGB data Most RGB-based object 6D pose estimation methods [1,20,33,35,36,38,44,57] attempt to establish sparse or dense 2D-3D correspondences between the 2D coordinates in the RGB image and the 3D coordinates on the object 3D model surface.",
          "5": "2 PVNet [36] RGBD \u2713 79."
        },
        "Tracking objects as pixel-wise distributions": {
          "authors": [
            "Z Zhao",
            "Z Wu",
            "Y Zhuang",
            "B Li",
            "J Jia"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_5",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "44"
          ],
          "2": "Dense fusion [57] and pixel-wise voting network [44,18] are proposed to overcome occlusions in the object pose estimation [19]."
        },
        "Uni6d: A unified cnn framework without projection breakdown for 6d pose estimation": {
          "authors": [
            "Xiaoke Jiang",
            "Donghai Li",
            "Hao Chen",
            "Ye Zheng",
            "Rui Zhao",
            "Liwei Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Jiang_Uni6D_A_Unified_CNN_Framework_Without_Projection_Breakdown_for_6D_CVPR_2022_paper.html",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "We follow previous work [30,53] to split the training and testing sets, and we also obtain synthesis images for the training set as the same with [6, 53].",
          "2": "For LineMOD dataset, we follow [16,30] to report the accuracy of distance less than 10% of the objects\u2019 diameter (ADD-0."
        },
        "Catre: Iterative point clouds alignment for category-level object pose refinement": {
          "authors": [
            "X Liu",
            "G Wang",
            "Y Li",
            "X Ji"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_29",
          "ref_texts": "38. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "38"
          ],
          "1": "The vast majority of previous works [30,64,45,62,53,54,31,52,38] study with instance-level object pose estimation, which can be decomposed by two procedures: initial pose estimation and pose refinement."
        },
        "Perspective flow aggregation for data-limited 6d object pose estimation": {
          "authors": [
            "Y Hu",
            "P Fua",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_6",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition (2019)",
          "ref_ids": [
            "32"
          ],
          "1": "When ample amounts of annotated real images are available, deep learning-based methods now deliver excellent results [7,32,31,46,38].",
          "2": "2 Related Work 6D pose estimation is currently dominated by neural network-based methods [11,32,38,37,14,2].",
          "3": "1 Data-Limited Pose Initialization Most pose refinement methods [25,51,23] assume that rough pose estimates are provided by another approach trained on a combination of real and synthetic data [49], often augmented in some manner [32,10,14].",
          "6": "1 Comparison with the State of the Art We now compare our method to the state-of-the-art ones, PoseCNN [49], SegDriven [11], PVNet [32], GDR-Net [47], DeepIM [25], and CosyPose [23], where DeepIM and CosyPose are two refinement methods based on an iterative strategy."
        },
        "Single-stage keypoint-based category-level object pose estimation from an RGB image": {
          "authors": [
            "Y Lin",
            "J Tremblay",
            "S Tyree",
            "PA Vela"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9812299/",
          "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "28"
          ],
          "1": "Other works have explored different ways to better represent objects, including dense coordinate maps [1], keypoints [28], and symmetry correspondences [29]."
        },
        "DGECN: A depth-guided edge convolutional network for end-to-end 6D pose estimation": {
          "authors": [
            "Tuo Cao",
            "Fei Luo",
            "Yanping Fu",
            "Wenxiao Zhang",
            "Shengjie Zheng",
            "Chunxia Xiao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cao_DGECN_A_Depth-Guided_Edge_Convolutional_Network_for_End-to-End_6D_Pose_CVPR_2022_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "It is widely used in the three-dimensional registration of AR [1, 28, 45], robotic vision [27, 31] and 3D reconstruction [9, 10].",
          "2": "Current object pose estimation methods can be divided into two types: 1) the object poses are estimated using a single RGB image [17, 27, 28, 31, 45] or 2) an RGB image accompanying a depth image [14,39,41].",
          "3": "PVNet [28] and Seg-Driven [17] conducted segmentation coupled with voting for each correspondence to make the estimation more robust.",
          "4": "Afterwards, like GDR-Net [42] and PVNet [28], we locate each object in the image with the method of FCN [24].",
          "5": "To deal with multiple objects segmentation, previous works [17, 28, 41, 45] use existing detection or semantic segmentation algorithms.",
          "6": "The 3D keypoints are selected from the 3D object model as in [14, 28].",
          "7": "We follow [28] and adopt the farthest point sampling (FPS) algorithm to select keypoints on object surface.",
          "8": "4 PVNet [28] DG-PnP(Ours) 23.",
          "9": "Our DGECN is comparable to [7, 21, 42] and outperforms [16, 28].",
          "10": "9 PVNet [28] % 47."
        },
        "Learning to detect scene landmarks for camera localization": {
          "authors": [
            "Tien Do",
            "Ondrej Miksik",
            "Joseph De",
            "Hyun Soo",
            "Sudipta N. Sinha"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.html",
          "ref_texts": "[52] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019. 3",
          "ref_ids": [
            "52"
          ],
          "1": "These were initially proposed to find the 6DoF pose of small objects using random forests [34], random ferns [49], and nowadays, CNNs [48,51, 52, 55]."
        },
        "Refine-net: Normal refinement neural network for noisy point clouds": {
          "authors": [
            "H Zhou",
            "H Chen",
            "Y Zhang",
            "M Wei",
            "H Xie"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9693131/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "2"
          ],
          "1": "As standard outputs of these 3D sensors, point clouds have been flexibly used in various applications, ranging from 6-degree virtual reality [1], [2], robotics [3] to autonomous driving [4], [5]."
        },
        "Ssp-pose: Symmetry-aware shape prior deformation for direct category-level object pose estimation": {
          "authors": [
            "R Zhang",
            "Y Di",
            "F Manhardt"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981506/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "As for RGBonly methods, end-to-end methods [18], [19], [20], [21], [22], [23], [6] regress the pose parameters directly, while two-stage methods [24], [25], [26], [27], [28] first establish 2D-3D correspondences by predicting the 3D coordinate for each pixel or detecting pre-defined keypoints, and then utilize PnP/RANSAC algorithm to solve the pose from intermediate results."
        },
        "6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning": {
          "authors": [
            "L Zou",
            "Z Huang",
            "N Gu",
            "G Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9933183/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "7"
          ],
          "1": "[7] presented the prediction of a unit vector for each pixel pointing toward the keypoints."
        },
        "Semantic segmentation of outdoor panoramic images": {
          "authors": [
            "Semih Orhan"
          ],
          "url": "https://link.springer.com/article/10.1007/s11760-021-02003-3",
          "ref_texts": "19. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019) Network for 6dof Pose Estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "19"
          ],
          "1": "Many computer vision applications benefit from it, such as pedestrian detection [6,16], autonomous vehicles [22,26], pose estimation [19,27] and remote sensing [13,24]."
        },
        "Es6d: A computation efficient and symmetry-aware 6d pose regression framework": {
          "authors": [
            "Ningkai Mo",
            "Wanshui Gan",
            "Naoto Yokoya",
            "Shifeng Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Mo_ES6D_A_Computation_Efficient_and_Symmetry-Aware_6D_Pose_Regression_Framework_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "26"
          ],
          "1": "In recent years, methods based on the deep neural network (DNN) have gradually emerged [17, 22, 25, 26, 40]."
        },
        "Vote from the center: 6 dof pose estimation in rgb-d images by radial keypoint voting": {
          "authors": [
            "Y Wu",
            "M Zand",
            "A Etemad",
            "M Greenspan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20080-9_20",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "36"
          ],
          "3": "As an alternate to directly regressing keypoint coordinates, methods which vote for keypoints have been shown to be highly effective [36,49,18,37], especially when objects are partially occluded.",
          "4": "While recent voting methods have shown great promise and leading performance, they require the regression of either a 2-channel (for 2D voting) [36] or 3-channel (for 3D voting ) [14] activation map where voting quantities are accumulated in order to vote for keypoints.",
          "5": "Notably, RCVPose requires only 3 keypoints per object, which is fewer than existing methods that use 4 or more keypoints [36,14,37].",
          "9": "Specifically, our method is inspired by PVNet [36], and is most closely related to the recently proposed PVN3D of He et al.",
          "12": "This is analogous to the approach of [14], and is efficient compared to previous pure RGB approaches [36] which employ an iterative PnP method.",
          "16": "5 Keypoint Dispersion Impact on Transformation Estimation: It was suggested in [36] that 6 DoF pose estimation accuracy is improved by selecting keypoints that lie on the object surface, rather than the bounding box corners which lie just beyond the object surface."
        },
        "YOLOPose: Transformer-based multi-object 6D pose estimation using keypoint regression": {
          "authors": [
            "A Amini",
            "A Selvam Periyasamy",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-22216-0_27",
          "ref_texts": "[21] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "21"
          ],
          "3": "Method PoseCNN [30]PVNet [21]GDR-Net [29]T6D-Direct [1]YOLOPose (Ours)DeepIM [15] P."
        },
        "Sim-to-real 6d object pose estimation via iterative self-training for robotic bin picking": {
          "authors": [
            "K Chen",
            "R Cao",
            "S James",
            "Y Li",
            "YH Liu",
            "P Abbeel"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19842-7_31",
          "ref_texts": "37. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. TPAMI (2020) 1, 3",
          "ref_ids": [
            "37"
          ],
          "1": "Recently, learning-based models [22,37,44,51] that arXiv:2204.",
          "2": "1 6D Object Pose Estimation for Bin-Picking Though recent works [10,26,37,40] show superior performance on household object datasets (e."
        },
        "Focal length and object pose estimation via render and compare": {
          "authors": [
            "Georgy Ponimatkin",
            "Yann Labbe",
            "Bryan Russell",
            "Mathieu Aubry",
            "Josef Sivic"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ponimatkin_Focal_Length_and_Object_Pose_Estimation_via_Render_and_Compare_CVPR_2022_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. InCVPR, pages 4561\u20134570, 2019.2",
          "ref_ids": [
            "34"
          ],
          "1": "Previous approaches for this task primarily rely on establishing local 2D-3D correspondences between an image 3825 and a 3D model using either hand-crafted [2, 3, 7, 8, 17, 27] or CNN features [12,19,20,31,32,34,35,38,41,42,47,48], followed by robust camera pose estimation using PnP [23].",
          "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs) [19,20,31,32,34,35,38,41,42,47,48]."
        },
        "Polarimetric pose prediction": {
          "authors": [
            "D Gao",
            "Y Li",
            "P Ruhkamp",
            "I Skobleva",
            "M Wysocki"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_43",
          "ref_texts": "41. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "41"
          ],
          "1": "Some methods in this field use sparse correspondences [43,41,47,25], while others establish dense 2D-3D pairs [57,40,36,22]."
        },
        "Object level depth reconstruction for category level 6d object pose estimation from monocular rgb image": {
          "authors": [
            "Z Fan",
            "Z Song",
            "J Xu",
            "Z Wang",
            "K Wu",
            "H Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_13",
          "ref_texts": "19. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "19"
          ],
          "1": "To improve keypoint detection performance, PVNet [19] formulates a voting scheme, which is more robust towards occlusion and truncation."
        },
        "Sc6d: Symmetry-agnostic and correspondence-free 6d object pose estimation": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044457/",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "32"
          ],
          "1": "These methods [34, 32, 40, 33, 45, 13, 14] are also called keypoint-based approaches which detect the predefined keypoints (known 3D coordinates on the object 3D model) from the input data.",
          "2": "proposed an occlusion-robust approach PVNet [32], which predicts the pixel-wise voting vectors to localize the keypoints defined based on the object 3D model instead of on the 3D bounding box."
        },
        "Learning-based point cloud registration for 6d object pose estimation in the real world": {
          "authors": [
            "Z Dang",
            "L Wang",
            "Y Guo",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_2",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570. Long Beach, California (2019) 1",
          "ref_ids": [
            "44"
          ],
          "1": "In this context, great progress has been made by learning-based methods operating on RGB(D) images [32,48,44,58,42,74,62,36,61,57]."
        },
        "Translating a visual lego manual to a machine-executable plan": {
          "authors": [
            "R Wang",
            "Y Zhang",
            "J Mao",
            "CY Cheng",
            "J Wu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19836-6_38",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "32"
          ],
          "1": "Other works adopt a two-stage approach where 2D keypoints of objects are first extracted and then poses are inferred from them [33,31,32]."
        },
        "Trans6D: Transformer-based 6D object pose estimation and refinement": {
          "authors": [
            "Z Zhang",
            "W Chen",
            "L Zheng",
            "A Leonardis"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25085-9_7",
          "ref_texts": "31. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: Pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1\u20131 (2020). https://doi.org/10.1109/TPAMI.2020.3047388",
          "ref_ids": [
            "31"
          ],
          "1": "Recent works in this field can be roughly divided into two categories: i) approaches that use a CNN to regress the 6D poses directly [41,39,38,37,8,34,5] and ii) indirectly [31,30].",
          "2": "The second category overcomes this limitation by either utilizing CNNs to detect the 2D keypoints of the objects [35,31] or estimating the dense 2D-3D correspondence maps between the input image and the available 3D models [18,43].",
          "3": "For example, Pixel2Pose [30] used an auto-encoder architecture to estimate the 3D coordinates per pixel to build dense correspondences, while PVNet [31], PVN3D [15], and PointPoseNet [7] adopted a voting net to select 2D keypoint or 3D keypoint respectively to build dense correspondences.",
          "4": "Compared approaches: PVNet [31], DPOD [43], DeepIm [25] PVNet +DPOD +DeepIm +Trans6D+ 85.",
          "5": "Baseline approaches: BB8 [32], Pix2Pose [30], DPOD [43], PVNet [31], CDPN [26], Hybrid [35], GDRN [40].",
          "6": "Baseline approaches: PoseCNN [41], Pix2Pose [30], DPOD [43], PVNet [31], Single-Stage [19], HybridPose [35], GDRN [40].",
          "7": "[36], PVNet [31], Singel-Stage [20], GDR-Net [40] Methods PoseCNNDeepIMPVNetSingle-StageGDR-NetAmeni et al.",
          "8": "Table 5 compares our method with other state-of-the-art methods [31,40] on Occlusion LINEMOD dataset in terms of ADD metric."
        },
        "Pixel2mesh++: 3d mesh generation and refinement from multi-view images": {
          "authors": [
            "C Wen",
            "Y Zhang",
            "C Cao",
            "Z Li",
            "X Xue"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9763061/",
          "ref_texts": "[68] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "68"
          ],
          "1": "[68] regard camera pose estimation as a key point regression problem, and use voting strategy to densely estimate the key point offset.",
          "2": "We use four types of standard metrics to evaluate our camera pose estimation method: 2D reprojection error d2D, mean distance d3D [4], 2D reprojection accuracy Acc2D and average 3D distance of model points (ADD) accuracy metric ADD3D [68].",
          "3": "Different from the threshold selected for general 6 DOF estimation tasks [68], we use more strict criteria."
        },
        "Mv6d: Multi-view 6d pose estimation on rgb-d frames using a deep point-wise voting network": {
          "authors": [
            "F Duffhauss",
            "T Demmler"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9982268/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "Traditional pose estimation methods mostly rely on single RGB(-D) images [8], [13], [14], [7], [6] or point cloud data [15], [16], [17].",
          "2": "Pose Estimation on Single RGB Images Traditional pose estimation methods [23], [24], [25], [26], [27], [5], [28], [13] extract local features from the given RGB image and match them to the corresponding features in its 3D model.",
          "3": "However, often the generalization of direct methods is an issue due to the non-linearity of the rotation space [13].",
          "4": "[13] S."
        },
        "PIZZA: A powerful image-only zero-shot zero-CAD approach to 6 DoF tracking": {
          "authors": [
            "Y Du",
            "Y Xiao",
            "M Ramamonjisoa"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044470/",
          "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2",
          "ref_ids": [
            "37"
          ],
          "1": "These methods can be roughly divided into three types of approaches: direct regression of the object pose [23, 56], template-based matching [19, 43] that encodes images in latent spaces and compares them against a dictionary of predefined viewpoints, and keypoint prediction for predicting the 6D pose [39, 48, 37, 22, 2] or dense 2D-3D correspondences [28, 51, 58, 34] as in earlier works [46, 7]."
        },
        "Learning 6-dof object poses to grasp category-level objects by language instructions": {
          "authors": [
            "C Cheang",
            "H Lin",
            "Y Fu",
            "X Xue"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9811367/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570. 2",
          "ref_ids": [
            "24"
          ],
          "1": "The former ones [2], [12], [24], [33], [35] rely on pre-scanned CAD models of objects, which is impractical in real-world scenarios.",
          "2": "1, 2 [24] S."
        },
        "Occlusion-robust object pose estimation with holistic representation": {
          "authors": [
            "Bo Chen",
            "Jun Chin",
            "Marius Klimavicius"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2022/html/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.html",
          "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 5, 7, 8",
          "ref_ids": [
            "44"
          ],
          "3": "PVNet [44] predicts the object mask and, for each pixel within the mask, unit vectors that points to the landmarks.",
          "4": "For LINEMOD, we follow the convention of previous works [46, 57, 44, 64] by using 15% of the images of each object as training set and the remaining 85% as testing set.",
          "5": "For the YCB-Video dataset we also report the AUC metric proposed in [62] and adopted in [41, 44].",
          "6": "Implementation details For each object model we apply the farthest point sampling (FPS) algorithm [44] on the 3D point cloud and select 11 landmarks.",
          "7": "We also define a measure of incoherence ci =\u2225(xi \u2212x\u2217 i )\u2212m\u22252 (4) 2934 ADD(-S) Without refinement With refinement PVNet Pix2Pose DPOD CDPN GDR Ours SSD-6D DPOD+ HybridPose DeepIM [44] [42] [64] [35] [59] [27] [64] [51] [33] ape 43.",
          "8": "ADD(-S) Without refinement With refinement HM PVNet Hu Pix2Pose DPOD Hu2 GDR Ours DPOD+ HybridPose [41] [44] [23] [42] [64] [22] [59] [64] [51] ape 15.",
          "9": "Data efficiency The LINEMOD dataset has about 1200 images for each object, which results in approximately 180 images (15%) 2935 ADD(-S) AUC of ADD(-S) Without refinement Without refinement With refinement HM Hu Hu2 GDR Ours HM PVNet GDR Ours DeepIM CosyPose [41] [23] [22] [59] [41] [44] [59] [34] [30] master chef can 31.",
          "10": "For example, PVNet [44] renders 20000 images for each object and the same strategy is adopted in [52]."
        },
        "A visual navigation perspective for category-level object pose estimation": {
          "authors": [
            "J Guo",
            "F Zhong",
            "R Xiong",
            "Y Liu",
            "Y Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_8",
          "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019) 3",
          "ref_ids": [
            "40"
          ],
          "1": "2 Related Work Object Pose Estimation: Extensive studies have been conducted for object pose estimation of known instances [10, 12, 19, 22, 26\u201328, 35, 39, 40, 53]."
        },
        "Dcl-net: Deep correspondence learning network for 6d pose estimation": {
          "authors": [
            "H Li",
            "J Lin",
            "K Jia"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_22",
          "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019) 1, 3, 14 DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation 17",
          "ref_ids": [
            "31"
          ],
          "1": "Many of the data-driven methods [3,14,20,23,28,31,33,34,38,41] thus achieve the estimation by learning point correspondence between camera and object coordinate systems.",
          "2": "2 Related Work 6D Pose Estimation from RGB Data This body of works can be broadly categorized into three types: i) holistic methods [11,15,18] for directly estimating object poses; ii) keypoint-based methods [28,33,34], which establish 2D-3D correspondence via 2D keypoint detection, followed by a PnP/RANSAC algorithm to solve the poses; iii) dense correspondence methods [3, 20, 23, 31], which make dense pixel-wise predictions and vote for the final results."
        },
        "Fusing local similarities for retrieval-based 3d orientation estimation of unseen objects": {
          "authors": [
            "C Zhao",
            "Y Hu",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_7",
          "ref_texts": "24. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "24"
          ],
          "1": "Motivated by the tremendous success of deep learning, much effort [36,24,32] has been dedicated to developing deep networks able to recognize the objects depicted in the input image and estimate their 3D orientation.",
          "2": "PVNet [24] estimates the 2D projections of 3D points using a voting network."
        },
        "Keypoint cascade voting for point cloud based 6DoF pose estimation": {
          "authors": [
            "Y Wu",
            "A Javaheri",
            "M Zand"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044458/",
          "ref_texts": "[42] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "42"
          ],
          "1": "A number of recent leading ML approaches [42, 20, 66] have been proposed based on keypoint voting, in which the 3D scene coordinates of specific keypoints defined within an object\u2019s reference frame, are voted on and accumulated independently for each image pixel.",
          "2": "Recent works such as PVNet [42], PVN3D [20], and RCVPose [66] that exhibit leading SOTA performance, have merged voting-based methods, which are well established in the classical literature [61, 54], with recent ML-based keypoint estimation approaches.",
          "5": "Experiments We evaluate the performance of our proposed RCVPose3D, and compare it with the best performing 6DoF PE methods on the two challenging 6DoF datasets that are commonly used in related SOTA work [42, 20].",
          "7": "3 PVNet [42] \u0017 73.",
          "8": "Initially, the Smooth L1 Loss Ls is used to train both the segmentation and regression components, as in PVNet [42]."
        },
        "Photo-realistic neural domain randomization": {
          "authors": [
            "S Zakharov",
            "R Ambru\u0219",
            "V Guizilini",
            "W Kehl"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19806-9_18",
          "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "48"
          ],
          "1": "Correspondence-basedmethods [69,37,27,23,46,48] tend to show superior generalization performance in terms of adapting to different pose distributions."
        },
        "Unseen object 6D pose estimation: a benchmark and baselines": {
          "authors": [
            "M Gou",
            "H Pan",
            "HS Fang",
            "Z Liu",
            "C Lu",
            "P Tan"
          ],
          "url": "https://arxiv.org/abs/2206.11808",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "41"
          ],
          "1": "Recently, deep learning methods based on 2D image [31, 41, 44, 56] or 3D point cloud [21, 22, 54] are proposed to tackle this problem and yield better performances, benefiting from the powerful feature extraction ability of neural network."
        },
        "Spatial feature mapping for 6dof object pose estimation": {
          "authors": [
            "Jianhan Mei",
            "Xudong Jiang",
            "Henghui Ding"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0031320322003168",
          "ref_texts": "[25] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wise voting network for 6dof pose estimation, in: IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp.",
          "ref_ids": [
            "25"
          ],
          "3": "[6, 25] learn the keypoints of 2D to 3D matching through the instance segmentation framework to enhance the description of the object pose, which brings the system performance to a new level.",
          "7": "So, the 24 proposed method still achieves comparable results with the state-of-the-art three methods [57, 25, 27]."
        },
        "A 6D pose estimation for robotic bin-picking using point-pair features with curvature (Cur-PPF)": {
          "authors": [
            "Xining Cui",
            "Menghui Yu",
            "Linqigao Wu",
            "Shiqian Wu"
          ],
          "url": "https://www.mdpi.com/1424-8220/22/5/1805",
          "ref_texts": "16. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNET: Pixel-Wise Voting Network for 6dof Pose Estimation. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Los Angeles, CA, USA, 15\u201321 June 2019. Sensors 2022, 22, 1805 20 of 20",
          "ref_ids": [
            "16"
          ],
          "1": "[16] proposed to use Pixel-wise Voting Network (PVNet) to return unit vectors to key points, then used RANdom SAmple Consensus (RANSAC) to vote for key points, and finally used PnP algorithm to derive accurate poses."
        },
        "HMD-EgoPose: Head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance": {
          "authors": [
            "M Doughty",
            "NR Ghugre"
          ],
          "url": "https://link.springer.com/article/10.1007/s11548-022-02688-y",
          "ref_texts": "[18] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "18"
          ],
          "1": "have presented several strategies for surgical drill and hand pose estimation from monocular RGB data for the synthetic drill dataset based off of the PVNet [18] and HandObjectNet [26] frameworks."
        },
        "Sequential voting with relational box fields for active object detection": {
          "authors": [
            "Qichen Fu",
            "Xingyu Liu",
            "Kris Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Fu_Sequential_Voting_With_Relational_Box_Fields_for_Active_Object_Detection_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "26"
          ],
          "2": "[26, 36] use pixel-wise predictions with Hough voting to localize keypoint for pose estimation."
        },
        "Video based object 6D pose estimation using transformers": {
          "authors": [
            "A Beedu",
            "H Alamri",
            "I Essa"
          ],
          "url": "https://arxiv.org/abs/2210.13540",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "40"
          ],
          "3": "They have enabled the emergence of novel network designs such as PoseCNN [56], DPOD [59], PVNet [40], and others [8, 52, 16, 10]."
        },
        "Template-based category-agnostic instance detection for robotic manipulation": {
          "authors": [
            "Z Hu",
            "R Tan",
            "Y Zhou",
            "J Woon"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9935113/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "[21] introduced a pixel-wise voting network to enhance the representation of the key points to optimize the matching performance."
        },
        "A dynamic keypoint selection network for 6dof pose estimation": {
          "authors": [
            "H Sun",
            "T Wang",
            "E Yu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885622000014",
          "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570). ",
          "ref_ids": [
            "25"
          ],
          "1": "Recently, the explosive growth of deep learning techniques motivates several works to tackle this problem by convolution neural networks (CNNs) on RGB images [19, 25, 40] and reveal promising improvements."
        },
        "6d robotic assembly based on rgb-only object pose estimation": {
          "authors": [
            "B Fu",
            "SK Leong",
            "X Lian",
            "X Ji"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9982262/",
          "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "17"
          ],
          "1": "To enhance the robustness, SegDriven [16] and PVNet [17] employ segmentation paired with voting for each correspondence."
        },
        "Sim2real instance-level style transfer for 6d pose estimation": {
          "authors": [
            "T Ikeda",
            "S Tanishige",
            "A Amma"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981878/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "The network architecture and loss function are the same as the single image translation [25].",
          "2": "Training We selected PVNet [25] as a PoseNet for our experiments."
        },
        "Parapose: Parameter and domain randomization optimization for pose estimation using synthetic data": {
          "authors": [
            "F Hagelskj\u00e6r",
            "AG Buch"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981511/",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u2013",
          "ref_ids": [
            "27"
          ],
          "2": "In PVNet [27] 10000 images are rendered and cut and pasted onto images from the SUN397 [37] dataset.",
          "3": "PVN3D [16] use the training data and domain randomization from PVNet [27], but expand with 3D pointclouds."
        },
        "Dfbvs: Deep feature-based visual servo": {
          "authors": [
            "N Adrian",
            "VT Do",
            "QC Pham"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9926560/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "[24], [25] are some of the best performing methods which score highly on the LINEMOD dataset [26].",
          "2": "Here, we use accuracy in the image space as the accuracy metric as inspired from pose estimation field [24], [25]."
        },
        "Canonical voting: Towards robust oriented bounding box detection in 3d scenes": {
          "authors": [
            "Yang You",
            "Zelin Ye",
            "Yujing Lou",
            "Chengkun Li",
            "Lu Li",
            "Lizhuang Ma",
            "Weiming Wang",
            "Cewu Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/You_Canonical_Voting_Towards_Robust_Oriented_Bounding_Box_Detection_in_3D_CVPR_2022_paper.html",
          "ref_texts": "[14] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "PVNet [14] regresses pixel-wise unit vectors pointing to the predefined keypoints and solves a Perspective-n-Point (PnP) problem for pose estimation in RGB images."
        },
        "Dprost: Dynamic projective spatial transformer network for 6d pose estimation": {
          "authors": [
            "J Park",
            "NI Cho"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_21",
          "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "31"
          ],
          "1": "Therefore, researchers have proposed methods for applying deep learning to the object pose estimation problem with great performance [2,8,14,15,19,21,22,26,28,31,32,37,40,41].",
          "2": "On the other hand, image-space representation based methods [5, 22, 28, 29, 31, 41] may learn the perspective effect implicitly by the projected image.",
          "3": "For example, [15, 32] used the corners of a 3D bounding box, [31,37] detected projected 3D keypoints of an object, and [28] used 2D-3D coordinates to train the network.",
          "4": "Then, based on the geodesic distance of the rotation matrix, we use the farthest point sampling (FPS) algorithm in [31] to select the other reference images."
        },
        "Object-based visual camera pose estimation from ellipsoidal model and 3d-aware ellipse prediction": {
          "authors": [
            "M Zins",
            "G Simon",
            "MO Berger"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-022-01585-w",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 4561\u20134570. Computer Vision Foundation / IEEE (2019)",
          "ref_ids": [
            "32"
          ],
          "1": "Many methods exist for estimating the pose of an object with respect to the camera frame [4, 17, 23, 30, 32, 36, 48, 50, 57], however, they usually require a detailed textured model of the object and sufficient training images.",
          "2": "Many works exist on this subjects [17, 23, 32, 36, 48, 50, 57]."
        },
        "Welsa: Learning to predict 6d pose from weakly labeled data using shape alignment": {
          "authors": [
            "SR Vutukur",
            "I Shugurov",
            "B Busam",
            "A Hutter"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20074-8_37",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "28"
          ],
          "1": "Alternatively, [33,28,16] estimate a predefined set of sparse keypoints instead of dense correspondences, which has proven to be more robust to occlusions."
        },
        "Reflective texture-less object registration using multiple edge features for augmented reality assembly": {
          "authors": [
            "Z He",
            "J Zhao",
            "X Zhao",
            "W Feng",
            "Q Wang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00170-022-10333-w",
          "ref_texts": "23. S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\" 2018.",
          "ref_ids": [
            "23"
          ],
          "1": "PVNet [23] predicted the direction from each pixel to each key point, so the spatial probability distribution of two-dimensional key points can be obtained just like RANSAC."
        },
        "Casapose: Class-adaptive and semantic-aware multi-object pose estimation": {
          "authors": [
            "N Gard",
            "A Hilsmann",
            "P Eisert"
          ],
          "url": "https://arxiv.org/abs/2210.05318",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR, 2019.",
          "ref_ids": [
            "30"
          ],
          "6": "The first estimates a segmentation mask that guides the second in estimating vectors pointing to 2D projections of predefined 3D object keypoints [30].",
          "7": "We use this property to avoid the non-differentiable RANSAC estimation, commonly used to get 2D points from vector fields [30].",
          "12": "It applies DKR on the largest connected component of each object class and clearly outperforms RANSAC voting (PVRANSAC) [30] used with the same trained model."
        },
        "3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects": {
          "authors": [
            "M Wolnitza",
            "O Kaya",
            "T Kulvicius",
            "F W\u00f6rg\u00f6tter"
          ],
          "url": "https://arxiv.org/abs/2203.01051",
          "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "2 Pose estimation based on deep learning In the last years, many approaches for pose estimation based on deep learning have been proposed [22, 24, 25, 30, 32] and evaluated using data sets of the BOP challenge [15, 16]."
        },
        "Shape Enhanced Keypoints Learning with Geometric Prior for 6D Object Pose Tracking": {
          "authors": [
            "Mateusz Majcher",
            "Bogdan Kwolek"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.html",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR, pages 4556\u20134565, 2019. 2, 3",
          "ref_ids": [
            "20"
          ],
          "1": "To mitigate this effect, many top performing two-stage approaches are either based on pixel-vise voting [20] or on generating an ensemble of predictions from each image pixel or patch [17], and then aggregating them to improve final predictions.",
          "2": "Relevant Work Top-performing methods on existing benchmarks, rather than directly regressing the object pose, are based on twostage approaches [12, 16, 17, 20, 21, 24, 25, 31, 33], which first predict landmarks of the object (intermediate features) with established 2D-3D correspondences, and then utilize a PnP like algorithm to determine the pose.",
          "3": "To better cope with occluded objects, [20] proposed a neural network for pixel-wise voting for the 2D keypoints location.",
          "4": "While [21, 25] utilize bounding box corners as keypoints, more recent approaches [20] use designated surface keypoints."
        },
        "Category-agnostic 6d pose estimation with conditional neural processes": {
          "authors": [
            "Y Li",
            "N Gao",
            "H Ziesche",
            "G Neumann"
          ],
          "url": "https://arxiv.org/abs/2206.07162",
          "ref_texts": "[45] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1, 2, 6",
          "ref_ids": [
            "45"
          ],
          "2": "V oting-based approaches [26, 27, 45] generate voting candidates from feature representations, after which the RANSAC algorithm [18] or a clustering mechanism such as MeanShift [9] is applied for selecting the best candidates."
        },
        "Towards two-view 6D object pose estimation: A comparative study on fusion strategy": {
          "authors": [
            "J Wu",
            "L Liu",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981630/",
          "ref_texts": "[18] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "To use more reliable correspondence, PVNet [18] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints then vote with confidence.",
          "10": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces, bizarre viewing angle, and some normal situations.",
          "11": "Also, our method beats both [18] and RGBD-based method [6] in occluded occasions.",
          "13": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces (e), bizarre viewing angle (c), and some normal situations (a)(b)(d).",
          "14": "Also, our methods beats [18] and RGBD-based method [6] in occluded occasions (f)(g)."
        },
        "Adversarial samples for deep monocular 6d object pose estimation": {
          "authors": [
            "J Zhang",
            "W Li",
            "S Liang",
            "H Wang",
            "J Zhu"
          ],
          "url": "https://arxiv.org/abs/2203.00302",
          "ref_texts": "33. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "33"
          ],
          "1": "In particular, recent deep learning based methods show that even using only a single RGB image without any additional depth data, 6D object pose can be predicted with high accuracy on large-scale public benchmarks [33,44,25].",
          "4": "PVNet [33] is built upon the idea of voting-based key-point localization.",
          "7": "Three SOTA models are selected as the target model, which are the GDRNet [44], PVNet [33], CDPN [25].",
          "9": "4 Transferability Results In this section, we perform U6DA to three mainstream RGB based deep 6D pose estimation networks, which are the direct methods GDRNet [44], the key-point based methods PVNet [33], the dense coordinate based methods CDPN [25]."
        },
        "Review on 6d object pose estimation with the focus on indoor scene understanding": {
          "authors": [
            "N Nejatishahidin",
            "P Fayyazsanavi"
          ],
          "url": "https://arxiv.org/abs/2212.01920",
          "ref_texts": "[58] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019.",
          "ref_ids": [
            "58"
          ],
          "2": "To address the occlusion problem for keypoint detection, PVNet [58] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
          "3": "3DPVNet [44], inspired from V oteNet [12] and pvnet [58], employed deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation.",
          "4": "The promise of votingbased techniques [58, 44] are being robust to these challenges."
        },
        "CenDerNet: Center and Curvature Representations for Render-and-Compare 6D Pose Estimation": {
          "authors": [
            "P De Roovere",
            "R Daems",
            "J Croenen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25085-9_6",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "20"
          ],
          "3": "On DIMO, we show our method significantly outperforms PVNet [20], a strong single-view baseline.",
          "4": "PVNet [20] is based on estimating 2D keypoints followed by perspective n-point optimization."
        },
        "Augmented Reality Pilot Assistance System for Helicopter Shipboard Operations": {
          "authors": [
            "TO Mehling"
          ],
          "url": "https://mediatum.ub.tum.de/1655458",
          "ref_texts": "[114] Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X. (2018). PVNet: Pixel -wise Voting Network for 6DoF Pose Estimation. Retrieved from https://arxiv.org/pdf/1812.11788 ",
          "ref_ids": [
            "114"
          ],
          "1": "Therefore, well known approaches for real-time 6D pose estimation are analyzed within this work: Pose CNN [157], PV Net [114], Dense Fusion [148] and Single Shot 6D Pose [136]."
        },
        "Category-level 6d object pose estimation with flexible vector-based rotation representation": {
          "authors": [
            "W Chen",
            "X Jia",
            "Z Zhang",
            "HJ Chang",
            "L Shen"
          ],
          "url": "https://arxiv.org/abs/2212.04632",
          "ref_texts": "[23] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86."
        },
        "Weakly Supervised Learning of Keypoints for 6D Object Pose Estimation": {
          "authors": [
            "M Tian",
            "GH Lee"
          ],
          "url": "https://arxiv.org/abs/2203.03498",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2, 4, 6, 7, 8",
          "ref_ids": [
            "26"
          ],
          "2": "PVNet [26] and its extensions [10, 32] predict the 2D projections of a sparse point set selected on the surface of the object model.",
          "7": "methods Relative Pose Synthetic Data RGB with Pose Annotations OK-POSE [42] Ours AAE [34] DPOD [41] NOL [25] PVNet [26] CDPN [20] DPOD [41] ape 35.",
          "8": "PVNet [26], CDPN [20], and DPOD [41]) trained on real images annotated with full 6D object poses.",
          "9": "Methods PVNet Pix2Pose HybridPose RLLG Ours[26] [24] [32] [5] ape 15.",
          "10": "3 summaries the comparison with PVNet [26], Pix2Pose [24], HybridPose [32], and RLLG [5] on the OCCLUSION dataset in terms of ADD(-S) metric."
        },
        "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices": {
          "authors": [
            "YC Lau",
            "KW Tseng",
            "IJ Hsieh",
            "HC Tseng"
          ],
          "url": "https://arxiv.org/abs/2210.12476",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "29"
          ],
          "2": "For example, PVNet [29] may be a good choice for general use."
        },
        "Direct pose estimation from RGB images using 3D objects": {
          "authors": [
            "MA Dede",
            "Y Gen\u00e7"
          ],
          "url": "https://dergipark.org.tr/en/pub/pajes/issue/69632/1110337",
          "ref_texts": "[15] Peng S, Liu, Y, Huang Q, Zhou X, Bao H. \u201cPVNet: Pixel-Wise voting network for 6DoF pose estimation\u201d. In the IEEE Conference on Computer Vision and Pattern Recognition ",
          "ref_ids": [
            "15"
          ],
          "1": "Recent deep learning methods SSD -6D [9] and BB8 [15] is built upon well -known object detector SSD [16] [28] and YOLO [17].",
          "2": "Using these 2D -3D correspondences 6DoF pose can be recovered [12],[14],[15],[32],[33].",
          "3": "Contemporary pose estimation approaches heavily employ deep learning methods either by directly estimating the pose or use it as part of a multi -stage pipeline [8],[15],[24].",
          "4": "Another approach is to turn this problem into a classification problem [15] by discretizing the poses.",
          "5": "Peng et al [15] uses the Farthest Point Sampling algorithm (FPS) to select key-points on the target object."
        },
        "Pose-aware object recognition on a mobile platform via learned geometric representations": {
          "authors": [
            "Csaba Beleznai",
            "Philipp Ausserlechner",
            "Andreas Kriegler",
            "Wolfgang Pointner"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9828370/",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "19"
          ],
          "1": "Linemod [17], to end-to-end learned pose estimation schemes [18], [19], [20].",
          "2": "Most common approaches formulate the learning task [18], [21], [19] as corner point regression, followed by a PnP step [22]."
        },
        "6d object pose estimation in cluttered scenes from RGB images": {
          "authors": [
            "XL Yang",
            "XH Jia",
            "Y Liang",
            "LB Fan"
          ],
          "url": "https://link.springer.com/article/10.1007/s11390-021-1311-2",
          "ref_texts": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "46"
          ],
          "1": "We compare our proposed approach with the following recent advanced methods: PoseCNN [11], Seg-Driven[14], Tekin [28], SilhoNet [34], Pix2Pose [35], BB8[44], Heatmaps [45], PVnet [46], CDPN [47], and our previous work (OCP) [17]."
        },
        "One-Shot General Object Localization": {
          "authors": [
            "Y You",
            "Z Miao",
            "K Xiong",
            "W Wang",
            "C Lu"
          ],
          "url": "https://arxiv.org/abs/2211.13392",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "15"
          ],
          "1": "During inference, inspired by PVNet [15], we randomly sample point pairs and cast a center vote for each pair.",
          "2": "However, the variance of corner votes is much larger than center, which is also observed in previous work [15]."
        },
        "Shape-coded aruco: Fiducial marker for bridging 2d and 3d modalities": {
          "authors": [
            "Lilika Makabe",
            "Hiroaki Santo",
            "Fumio Okura",
            "Yasuyuki Matsushita"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2022/html/Makabe_Shape-Coded_ArUco_Fiducial_Marker_for_Bridging_2D_and_3D_Modalities_WACV_2022_paper.html",
          "ref_texts": "[27] Sida Peng, Xiaowei Zhou, Y uan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Object Pose Estimation.IEEE Transactions on P attern Analysis and Machine Intelligence (P AMI) , 2020. early access.",
          "ref_ids": [
            "27"
          ],
          "1": "Some recent methods use deep neural networks to infer the6 degrees-of-freedom (DoF) object poses from a textured 3D shape and image observations [13, 39, 41, 27, 9]."
        },
        "An intelligent robotic vision system with environment perception": {
          "authors": [
            "Y Jin"
          ],
          "url": "https://etheses.whiterose.ac.uk/31259/",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the 131 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "23"
          ],
          "3": "[23], Pixel-wise Voting Network (PVNet), which detect vectors between pixel and keypoints rather than directly regressing keypoints.",
          "4": "[9] proposed PVN3D, that is, an extension of PV-Net [23] in the 3D domain shown in the Figure 2."
        },
        "Simultaneous object detection and pose estimation under domain shift": {
          "authors": [
            "Stefan Thalhammer"
          ],
          "url": "https://repositum.tuwien.at/handle/20.500.12708/120374",
          "ref_texts": "[22] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570 (cit. on pp. 2, 6, 14, 18, 19, 31, 38, 40).",
          "ref_ids": [
            "22"
          ],
          "13": "Notably, the authors of [22] show that providing a large set of keypoint hypotheses also leads to good pose estimation results.",
          "14": "The best performing deep learning approaches for monocular 6D object pose estimation employ encoder-decoder architectures [22], [39], [40], [49], [53]."
        },
        "DenseTransformer: Direct 6D OPE using self-attention on dense representations": {
          "authors": [
            "N Desai"
          ],
          "url": "https://fse.studenttheses.ub.rug.nl/id/eprint/28988",
          "ref_texts": "[33] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "33"
          ],
          "3": "[39] proposed an end-to-end framework called the PPR-Net for 3D data where the technique is similar to the point-wise voting used in PVNet [33]."
        },
        "Perception Systems for Robust Autonomous Navigation in Natural Environments": {
          "authors": [
            "A Trabelsi"
          ],
          "url": "https://search.proquest.com/openview/70d1f296e7b8ad5411f271816f28bc54/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[32] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "32"
          ],
          "3": "[32] introduced PVNet, which stands for 'Pixel-wise Voting Network'.",
          "6": "For instance, the end-to-end extension of PVNet ([32] + [40]) outperforms its two-stage version ([32]) by 6.",
          "7": "First, the 2D-projection error, analogously to [32], measures the average distance between the 2D projections in the image space of the 3D model points, transformed using the ground-truth pose and the predicted pose."
        },
        "QUANTIZED NEURAL NETWORKS FOR 6D POSE ESTIMATION": {
          "authors": [
            "Z Zhao"
          ],
          "url": "https://robertflame.github.io/Homepage/assets/docs/theses/Master%20Thesis.pdf",
          "ref_texts": "[18] S. Peng, Y . Liu, Q. Huang, X. Zhou and H. Bao, \u2018Pvnet: Pixel-wise voting network for 6dof pose estimation,\u2019 in CVPR, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "PVNet [18] regressed keypoint-pointing vectors for each pixel and voted for keypoint locations using these vectors."
        },
        "PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation\u2013Supplementary Material\u2013": {
          "authors": [
            "T Jantos",
            "MA Hamdad",
            "W Granig",
            "S Weiss"
          ],
          "url": "https://proceedings.mlr.press/v205/jantos23a/jantos23a-supp.pdf",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "PnP IR Method PoseCNN [2] PoET PoETgt Pix2Pose [19] PVNet [20] GDR-Net [16]DeepIM [11] PE 1 1 1 N N N 1 Data real + syn pbr pbr real real + syn real + pbr real + syn Ape 9."
        },
        "RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization": {
          "authors": [
            "YXKYL Guofeng",
            "ZXWH Li"
          ],
          "url": "https://decayale.github.io/publication/rnnpose/paper.pdf",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "34"
          ],
          "1": "These methods may estimate the object\u2019s bounding box corners [35, 45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
          "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
          "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
          "8": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [59], DeepIM [23] and RePOSE [20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
          "11": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "A Smart Workcell for Automatic Pick and Sorting for Logistics": {
          "authors": [
            "N Castaman",
            "A Gottardi",
            "E Menegatti"
          ],
          "url": "https://www.research.unipd.it/handle/11577/3471171",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, Pvnet: \u201cPixelwise voting network for 6DOF pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570",
          "ref_ids": [
            "4"
          ],
          "1": "In general, this problem relies on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [2], with an increasing trend toward data-driven approaches based on deep models [3], [4]."
        },
        "Supplementary Material: Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images": {
          "authors": [
            "Y Liu",
            "Y Wen",
            "S Peng",
            "C Lin",
            "X Long",
            "T Komura"
          ],
          "url": "https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920297-supp.pdf",
          "ref_texts": "5. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "5"
          ],
          "2": "Note that reference images are used in inference of Gen6D but not in training the Gen6D estimator while instance-specific estimators like PVNet [5] actually use these reference images to train their models.",
          "3": "Note PVNet [5] is trained on the specific test object with both synthetic and real images while our Gen6D is not trained on the test object.",
          "4": "1d PVNet [5] Ours PVNet [5] Ours Eggbox 99."
        },
        "Supplementary Material for OnePose: One-Shot Object Pose Estimation without CAD Models": {
          "authors": [
            "J Sun",
            "Z Wang",
            "S Zhang",
            "X He",
            "H Zhao",
            "G Zhang"
          ],
          "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/OnePose/onepose_supp.pdf",
          "ref_texts": "[8] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3",
          "ref_ids": [
            "8"
          ],
          "1": "Implementation Details of the Evaluation of PVNet For the experiments of evaluating PVNet [8], we directly use the original implementation and training configurations provided by the authors at [1]."
        },
        "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation": {
          "authors": [
            "Gu Wang",
            "Fabian Manhardt",
            "Federico Tombari",
            "Xiangyang Ji"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.html",
          "ref_texts": "[40] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. InIEEE Conference on Computer V ision and P attern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "40"
          ],
          "4": "16617 Method w/o Refinement w/ Refinement PoseCNN [60] PVNet [40] Single-Stage [19] HybridPose [47] GDR-Net (Ours) DPOD [62] DeepIM [27] P .",
          "5": "0 PVNet [40] N 73."
        },
        "Ffb6d: A full flow bidirectional fusion network for 6d pose estimation": {
          "authors": [
            "Yisheng He",
            "Haibin Huang",
            "Haoqiang Fan",
            "Qifeng Chen",
            "Jian Sun"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/He_FFB6D_A_Full_Flow_Bidirectional_Fusion_Network_for_6D_Pose_CVPR_2021_paper.html",
          "ref_texts": "[46] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "46"
          ],
          "1": "Recently , the dramatic growth of deep learning techniques motivates several works to tackle this problem using convolution neural networks (CNNs) on RGB images [68, 46, 70, 34].",
          "2": "Instead, 2Dkeypoint-based [53, 52, 42, 29, 43, 46, 72, 38] detect 2D keypoints of objects to build the 2D-3D correspondence for pose estimation.",
          "3": "Keypoint selection Previous works [46, 17] select keypoints from the target object surface using the Farthest Point Sampling (FPS) algorithm.",
          "4": "In this way , the selected keypoints spread on the object surface and stabilize the following pose estimation procedure [46, 17].",
          "5": "W e split the training and testing set following previous works [68, 46] and generate synthesis images for training following [46, 17].",
          "6": "1d) as in [20, 46].",
          "7": "Qualitative results are reported in 3008 RGB RGB-D PoseCNN DeepIM [68, 33] PVNet[46] CDPN[34] DPOD[70] PointFusion[69] DenseFusion[65] G2LNet[7] PVN3D[17] Our FFB6D MEAN 88.",
          "8": "[26] Pix2Pose [45] PVNet [46] ADD-0."
        },
        "DexYCB: A benchmark for capturing hand grasping of objects": {
          "authors": [
            "Wei Chao",
            "Wei Yang",
            "Yu Xiang",
            "Pavlo Molchanov",
            "Ankur Handa",
            "Jonathan Tremblay",
            "Yashraj S. Narang",
            "Karl Van",
            "Umar Iqbal",
            "Stan Birchfield",
            "Jan Kautz",
            "Dieter Fox"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "27"
          ],
          "1": "State-of-the-art approaches for both 3D object pose [37, 20, 34, 27, 40, 26, 19] and 3D hand pose estimation [49, 23, 18, 2, 9, 14, 31] rely on deep learning and thus require large datasets with labeled hand or object poses for training."
        },
        "Semi-supervised 3d hand-object poses estimation with interactions in time": {
          "authors": [
            "Shaowei Liu",
            "Hanwen Jiang",
            "Jiarui Xu",
            "Sifei Liu",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Liu_Semi-Supervised_3D_Hand-Object_Poses_Estimation_With_Interactions_in_Time_CVPR_2021_paper.html",
          "ref_texts": "[43] Sida Peng, Y uan Liu, Qi-Xing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CVPR, pages 4556\u20134565, 2019.",
          "ref_ids": [
            "43"
          ],
          "1": "There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [28, 67] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [45, 60, 43, 24]."
        },
        "Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism": {
          "authors": [
            "Wei Chen",
            "Xi Jia",
            "Hyung Jin",
            "Jinming Duan",
            "Linlin Shen",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chen_FS-Net_Fast_Shape-Based_Network_for_Category-Level_6D_Object_Pose_Estimation_CVPR_2021_paper.html",
          "ref_texts": "[23] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "23"
          ],
          "1": "Correspondences-based methods trained their model to establish 2D-3D correspondences [28, 29, 23] or 3D-3D correspondences [6, 5].",
          "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86."
        },
        "Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation": {
          "authors": [
            "Kai Chen",
            "Qi Dou"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.1, 2",
          "ref_ids": [
            "20"
          ],
          "1": "Different from conventional instance-level [12, 20, 30, 35] object pose estimation, which gives instance CAD models and predicts poses for the instances that have been seen during training, category-level task requires capturing the general properties while accounting for the large variation of differPrior Point Cloud Camera Instance I Camera Instance II w/o Prior adaptation w/ Prior adaptation Figure 1.",
          "2": "Methods [20, 25, 2, 13, 17, 16] mainly focus on learning a robust embedding that is conditioned on the object pose.",
          "3": "The second group of methods [20, 12, 25, 13] assume the object 3D CAD model is available."
        },
        "So-pose: Exploiting self-occlusion for direct 6d pose estimation": {
          "authors": [
            "Yan Di",
            "Fabian Manhardt",
            "Gu Wang",
            "Xiangyang Ji",
            "Nassir Navab",
            "Federico Tombari"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Di_SO-Pose_Exploiting_Self-Occlusion_for_Direct_6D_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019. 2, 6, 7",
          "ref_ids": [
            "28"
          ],
          "1": "[28] demonstrate that keypoints away from the object surface induce larger errors and, therefore, instead sample several keypoints on the object model based on farthest point sampling.",
          "2": "HybridPose [36] follows and develops [28] by introducing hybrid representations."
        },
        "Geometry-based distance decomposition for monocular 3d object detection": {
          "authors": [
            "Xuepeng Shi",
            "Qi Ye",
            "Xiaozhi Chen",
            "Chuangrong Chen",
            "Zhixiang Chen",
            "Kyun Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.html",
          "ref_texts": "[35] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "35"
          ],
          "2": "These works [35, 17, 23, 44] recover the pose or distance by several factors, such as 2D keypoints, 2D bounding boxes, and object physical size, which achieves interpretable and robust pose or distance estimation.",
          "3": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress 2D keypoints of objects, then optimize the estimation of the 6D pose by solving a Perspective-n-Point (PnP) problem."
        },
        "Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency": {
          "authors": [
            "Jiehong Lin",
            "Zewei Wei",
            "Zhihao Li",
            "Songcen Xu",
            "Kui Jia",
            "Yuanqing Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "20"
          ],
          "2": "More recent solutions build on the power of deep networks and can directly estimate object poses from RGB images alone [16, 32, 26, 20] or RGB-D ones [17, 29]."
        },
        "Category-level 6d object pose estimation via cascaded relation and recurrent reconstruction networks": {
          "authors": [
            "J Wang",
            "K Chen",
            "Q Dou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636212/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp. 4561\u2013",
          "ref_ids": [
            "2"
          ],
          "2": "Instead of explicitly detecting and matching object keypoints, some methods [8] take corner points of 3D object bounding boxes as keypoints, or implicitly represent keypoints by a dense voting field [2]."
        },
        "Wide-depth-range 6d object pose estimation in space": {
          "authors": [
            "Yinlin Hu",
            "Sebastien Speierer",
            "Wenzel Jakob",
            "Pascal Fua",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Hu_Wide-Depth-Range_6D_Object_Pose_Estimation_in_Space_CVPR_2021_paper.html",
          "ref_texts": "[31] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InConference on Computer V ision and P attern Recognition, 2019.",
          "ref_ids": [
            "31"
          ],
          "1": "This network is usually trained to predict the image location of the 3D object bounding box corners, either in a single global fashion [18, 33, 37, 42], or by aggregating multiple local predictions to improve robustness to occlusions [29, 16, 11, 31, 43, 23].",
          "2": "We compare our results with those of PVNet [31], SimplePnP [10] and Hybrid [36]."
        },
        "Repose: Fast 6d object pose refinement via deep texture rendering": {
          "authors": [
            "Shun Iwase",
            "Xingyu Liu",
            "Rawal Khirodkar",
            "Rio Yokota",
            "Kris M. Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Iwase_RePOSE_Fast_6D_Object_Pose_Refinement_via_Deep_Texture_Rendering_ICCV_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Xiaowei Liu, and Hujun Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In CVPR, 2019. 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "1": "Related Work Two-stage pose estimation methods Recently, Oberweger [26], PVNet [27], DPOD [40], and HybridPose [33] have shown excellent performance on 6D object pose estimation using a two-stage pipeline to estimate a pose: (i) estimating a 2D representation (e.",
          "2": "Instead of regarding the corners as keypoints, PVNet [27] places the keypoints on the object surface via the farthest point sampling algorithm.",
          "3": "RePOSE adopts PVNet [27] as the initial pose estimator using the official implementation.",
          "4": "RePOSE then refines the initial pose estimate Pini = \u2126(I) where \u2126 is any pose estimation method like PVNet [27] and PoseCNN [39] in real time using differentiable Levenberg\u2013Marquardt (LM) optimization [24].",
          "5": "The pre-trained weights of PVNet [27] or PoseCNN [39] are used for the encoder and only the decoder is trained while training RePOSE.",
          "6": "We used pretrained PVNet [27] on the LineMOD and Occlusion LineMOD datasets, and PoseCNN [39] on the YCB-Video [39] dataset as the initial pose estimator \u2126.",
          "7": "Following [27], we also add 500 synthetic and fused images for LineMOD and 20K synthetic images for YCBVideo to avoid overfitting during training.",
          "8": "Metric PoseCNN [39] DeepIM [21] PVNet [27] CosyPose [19] RePOSE RePOSE w/ track AUC, ADD(-S) 61.",
          "9": "1 Refinement FPS 22 6 26 13 181 111 80 125 90 71 #Iterations 1 4 1 2 1 3 5 1 3 5 Table 2: Comparison of RePOSE on Linemod dataset with recent methods including PVNet [27], DPOD [40], HybridPose [33], and EfficientPose [9] using the ADD(-S) score.",
          "10": "As shown in Tables 2 and 3, RePOSE achieves Table 3: Comparison of RePOSE on Occlusion LineMOD dataset with recent methods including PVNet [27], DPOD [40], and HybridPose [33] using the ADD(-S) score.",
          "11": "In comparison to PVNet [27], RePOSE successfully refines the initial pose estimate in all the objects, achieving an improvement of 9.",
          "12": "The key difference is mainly on ape and duck where our initial pose estimator PVNet [27] performs poorly.",
          "13": "Object PVNet [27]RGB CNN w/ FW DPODOurs w/ FW Ours Ape 43.",
          "14": "Object PVNet [27]RGB CNN w/ FW DPODOur w/ FW Ours Ape 15.",
          "15": "Ablation Study All ablations for RePOSE are conducted on the LineMOD and Occlusion LineMOD datasets using PVNet [27] as an initial pose estimator.",
          "16": "2% absolute improvement from PVNet [27] on the LineMOD dataset [15].",
          "17": "In this experiment, we use the same initial pose estimator [27]."
        },
        "A vector-based representation to enhance head pose estimation": {
          "authors": [
            "Zongcheng Chu",
            "Dongfang Liu",
            "Yingjie Chen",
            "Zhiwen Cao"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Chu_A_Vector-Based_Representation_to_Enhance_Head_Pose_Estimation_WACV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "The approaches can be divided into two categories: [20, 33, 27] first estimate the object mask to determine its location in the image, then build the correspondence between the image pixels and the available 3D models."
        },
        "Occlusion-aware self-supervised monocular 6D object pose estimation": {
          "authors": [
            "G Wang",
            "F Manhardt",
            "X Liu",
            "X Ji"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9655492/",
          "ref_texts": "[14] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "14"
          ],
          "2": "Similarly, SegDriven [47] and PVNet [14] also regress 2D projections of associated sparse 3D keypoints, however, both employ segmentation paired with voting to improve reliability.",
          "5": "On the other hand, as for training with real pose labels, we outperform all other recently published methods including PVNet [14] and CDPN [50] reporting a mean average recall of 91.",
          "6": "6 PVNet [14] 13.",
          "7": "In TABLE 7 we compare our method against state of the art [14], [32], [33], [38] on YCBVideo w.",
          "8": "3 PVNet [14] 13."
        },
        "Dpodv2: Dense correspondence-based 6 dof pose estimation": {
          "authors": [
            "I Shugurov",
            "S Zakharov",
            "S Ilic"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9565319/",
          "ref_texts": "[34] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "34"
          ],
          "1": "Some of the most recent representatives include iPose [6], PVNet [34], DPOD [35], Pix2Pose [36], CDPN [37], and SDFlabel [38].",
          "2": "PVNet [34] takes a different approach and designs a network which for every pixel in the image regresses an offset to the predefined keypoints located on the object itself.",
          "3": "Train Data Real Method Pix2Pose [36] DPOD [35] PVNet [34] CDPN [37] HybridPose [42] Ours BB8 [30] PoseCNN [76] DPOD [35] Ours Ours Refinement DL [30] DeepIM [15] DL [35] 2 calib.",
          "4": "387 PVNet [34] ICP 0.",
          "5": "274 PVNet [34] 0."
        },
        "Survey on localization systems and algorithms for unmanned systems": {
          "authors": [
            "S Yuan",
            "H Wang",
            "L Xie"
          ],
          "url": "https://www.worldscientific.com/doi/abs/10.1142/S230138502150014X",
          "ref_texts": "[321] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. \u201cPvnet: Pixel-wise voting network for 6dof pose estimation.\u201d In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4561-4570.",
          "ref_ids": [
            "321"
          ],
          "1": "PVNet [321] proposed a deep feature extraction model for solving challenging localization problems in a dynamic and occlusion scene."
        },
        "Vs-net: Voting with segmentation for visual localization": {
          "authors": [
            "Zhaoyang Huang",
            "Han Zhou",
            "Yijin Li",
            "Bangbang Yang",
            "Yan Xu",
            "Xiaowei Zhou",
            "Hujun Bao",
            "Guofeng Zhang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.html",
          "ref_texts": "[38] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "38"
          ],
          "2": "Recently, PVNet [38] significantly improves robustness and accuracy of object pose estimation by detecting keypoints with pixel-wise votes, inspired by which, we propose to detect scene-specific landmarks with pixelwise votes."
        },
        "Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency": {
          "authors": [
            "Zongxin Yang",
            "Xin Yu",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_DSC-PoseNet_Learning_6DoF_Object_Pose_Estimation_via_Dual-Scale_Consistency_CVPR_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019. 2, 4, 7",
          "ref_ids": [
            "29"
          ],
          "2": "Fully-supervised deep model based methods: Deep learning based methods have demonstrated promising pose estimation performance [29, 43, 47, 46, 19, 22].",
          "4": "CPDN [49], DPOD [47] and Pix2Pose [18] output the 2D UV coordinates or 3D coordinates of 3D object models from images, while PoseCNN [43] and PVNet [29] employ Hough voting to localize object keypoints from estimated vector fields.",
          "6": "PVNet [29] predicts a vector field for each keypoint and employs voting to determine keypoint locations."
        },
        "Stereobj-1m: Large-scale stereo image dataset for 6d object pose estimation": {
          "authors": [
            "Xingyu Liu",
            "Shun Iwase",
            "Kris M. Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_StereOBJ-1M_Large-Scale_Stereo_Image_Dataset_for_6D_Object_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "3": "Specifically, we implement PVNet [27] and KeyPose [17], two classic keypoint-based 6D pose estimation frameworks that have achieved state-of-the-art performance on various datasets.",
          "4": "PVNet [27] is a single-RGB keypoint-based method.",
          "5": "25 Table 4: The results of PVNet [27] on single-object pose estimation in terms of ADD(-S) AUC and ADD(-S) accuracy on StereOBJ-1M dataset."
        },
        "Category-level metric scale object shape and pose estimation": {
          "authors": [
            "T Lee",
            "BU Lee",
            "M Kim",
            "IS Kweon"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9531548/",
          "ref_texts": "[19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "19"
          ],
          "1": "Recent RGB-based approaches [19], [25] have Manuscript received: April 30, 2021; Revised July 27, 2021; Accepted August, 23, 2021.",
          "2": "[19], [20], [25], [27] detected the keypoints in the projected 3D points on the image and then solved a non-linear problem using the Perspective-n-Point (PnP) algorithm [12].",
          "3": "1) Normalized Object Mesh Estimation (Mesh header):Instead of training a single network for each category [19], [20], [25], [27], we aim to cover all object categories with a single network [26], [30]."
        },
        "Keypoint-graph-driven learning framework for object pose estimation": {
          "authors": [
            "Shaobo Zhang",
            "Wanqing Zhao",
            "Ziyu Guan",
            "Xianlin Peng",
            "Jinye Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Keypoint-Graph-Driven_Learning_Framework_for_Object_Pose_Estimation_CVPR_2021_paper.html",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "Recently , deep learning approaches [13, 39, 4, 29, 34, 10, 26, 24, 25, 18, 3] have shown impressive results of pose estimation in RGB * Corresponding author images.",
          "2": "Some keypoint-based approaches [27, 25, 34, 10, 26, 31, 42] build the correspondence using sparse 2D keypoints on objects as an intermediate representation for pose estimation.",
          "3": "Synthetic data generation Given 3D models of the objects, first, we define the keypoints on the surface of them as proposed in PVnet [26] where K keypoints are selected using the farthest point sampling (FPS) algorithm.",
          "4": "W e use blender [26] to render these 3D models from different camera viewpoints to sufficiently cover the objects and project the keypoints to images under the viewpoints.",
          "5": "T o evaluate the accuracy of the estimated pose, we use two standard metrics for LINEMOD used in other related paper [36, 41, 26] which are ADD and ADD-S (for symmetric objects).",
          "6": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method AAE [33] 1 MHP [20] 1 DPOD [41] Self6D [36] 1 Ours YOLO6D [34] DPOD PVNet [26] CDPN [18] Ape 4.",
          "7": "W e compare our method with state-of-the-art 6D pose estimation methods (AAE [33], MHP [20], DPOD [41] Self6D [36]) that use the synthetic images generated by 3D CAD models and the methods (YOLD6D [34], DPOD [41], PVNet [26], CDPN[18]) using real images with manual 3D annotations for training.",
          "8": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method DPOD [41] CDPN [18] Self6D [36] Ours YOLO6D [34] HMap [23] PVNet [26] Mean 6.",
          "9": "W e use the model trained on the synthetic images for testing on the Occlusion dataset and compare our method with the three methods (DPOD [41], CDPN [18] and Self6D [36]) that do not require manual pose labels for training and three methods (YOLO6D [34], HMap [23] and PVNet [26]) using manual pose labels for training."
        },
        "Pr-gcn: A deep graph convolutional network with point refinement for 6d pose estimation": {
          "authors": [
            "Guangyuan Zhou",
            "Huiqun Wang",
            "Jiaxin Chen",
            "Di Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] proposes a deep offset prediction model to alleviate negative impacts of occlusions.",
          "2": "Similar to [9, 29, 38], we select the candidate with the highest confidence score as the estimated pose, formulated as: (\u02c6Ro, \u02c6to) = argmaxn (\u02c6R(k) o ,\u02c6t(k) o )|k=1,\u00b7\u00b7\u00b7,K o s(k) o .",
          "3": "RGB based methods RGB-D based methods Object PoseCNN* PVNet CDPN DPOD DPVL PF* SSD6D\u2020 DF* PVN3D G2L* Ours* Ours [40, 18] [29] [19] [44] [42] [41] [14] [38] [9] [4] ape 77.",
          "4": "Object PoseCNN [40] DeepHeat [26] SS [12] Pix2pose [28] PVNet [29] HybridPose [34] PVN3D [9] Ours Ape 9.",
          "5": "We first compare PR-GCN to the state-of-the-art methods on Linemod, including the RGB based models: PoseCNN (+DeepIM) [40, 18], PVNet [29], CDPN [19], DPOD [44] and DPVL [42] and the RGB-D based ones: Point Fusion [41], SSD6D (+ICP) [14], Dense Fusion [38], PVN3D [9] and G2L[4].",
          "6": "1 ness of PR-GCN to inter-object occlusions, we display detailed results on Occlusion Linemod, in comparison with PoseCNN [40], DeepHeat [26], SS [12], Pix2Pose [28], PVNet [29], HybridPose [34] and PVN3D [9]."
        },
        "Cloudaae: Learning 6d object pose regression with on-line data synthesis on point clouds": {
          "authors": [
            "G Gao",
            "M Lauri",
            "X Hu",
            "J Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561475/",
          "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "Depending on the applicability of the methods, for each dataset, we compare to a subset of stateof-the-art methods SSD-6D [29], EEPG-AAE [15], CloudPose [8], PVNet [30], PoseCNN [4], DenseFusion [12], PVN3D [13] and PointV oteNet [31].",
          "2": "9 PVNet [30] 47."
        },
        "Single-view robot pose and joint angle estimation via render & compare": {
          "authors": [
            "Yann Labbe",
            "Justin Carpentier",
            "Mathieu Aubry",
            "Josef Sivic"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Labbe_Single-View_Robot_Pose_and_Joint_Angle_Estimation_via_Render__CVPR_2021_paper.html",
          "ref_texts": "[44] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "44"
          ],
          "1": "For rigid objects, however, methods based on 2D keypoints [34, 3, 7, 6, 45, 52, 23, 50, 44, 43, 18] have been recently outperformed by render & compare methods that forgo explicit detection of 2D keypoints but instead use the entire shape of the object by comparing the rendered view of the 3D model to the input image and iteratively refining the object\u2019s 6D pose [59, 31, 25].",
          "2": "A set of sparse [45, 52, 23, 50, 44, 43, 18] or dense [56, 41, 49, 59] features is detected on the object in the image using a CNN and the resulting 2D-to-3D correspondences are used to recover the camera pose using PnP [29]."
        },
        "A survey on deep learning based methods and datasets for monocular 3D object detection": {
          "authors": [
            "S Kim",
            "Y Hwang"
          ],
          "url": "https://www.mdpi.com/2079-9292/10/4/517",
          "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "72"
          ],
          "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
          "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
          "3": "Overview of the keypoint localization in PVNet [72].",
          "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "T6d-direct: Transformers for multi-object 6d pose direct regression": {
          "authors": [
            "A Amini",
            "AS Periyasamy",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-92659-5_34",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: CVPR, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "20"
          ],
          "2": "We compare our results with PoseCNN [35], PVNet [20] and DeepIM [14]."
        },
        "Towards markerless surgical tool and hand pose estimation": {
          "authors": [
            "Jonas Hein"
          ],
          "url": "https://link.springer.com/article/10.1007/s11548-021-02369-2",
          "ref_texts": "26. Peng S, Liu Y , Huang Q, Zhou X, Bao H (2019) Pvnet: pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4561\u20134570",
          "ref_ids": [
            "26"
          ],
          "1": "One of the current state-of-the-art object tracking models, PVNet [26], utilizes this technique and performs well even under occlusions.",
          "2": "Baseline models PVNet We choose PVNet [26] as the first baseline since it is a state-of-the-art model for object-only pose estimation on single-shot RGB images.",
          "6": "Instead of directly regressing the 3D object pose via fully connected layers, such as employed in HandObjectNet, we propose to adopt the pose estimation method by using vector field encoded keypoints, similar to the method introduced for PVNet [26].",
          "8": "These results are in line with the results from the current state-of-the-art from computer vision applications, such as reported in [15,26]."
        },
        "A pose proposal and refinement network for better 6d object pose estimation": {
          "authors": [
            "Ameni Trabelsi",
            "Mohamed Chaabane",
            "Nathaniel Blanchard",
            "Ross Beveridge"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Trabelsi_A_Pose_Proposal_and_Refinement_Network_for_Better_6D_Object_WACV_2021_paper.html",
          "ref_texts": "[21] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "21"
          ],
          "1": "Most existing RGB-based methods [13, 20, 21, 23] take advantage of deep learning techniques used for object detection [5, 10, 16, 25] or image segmentation [17] and leverage them for 6D pose estimation.",
          "4": "W e report percentages of correctly estimated poses averaged over all object classes Method T ekin[28] PVNet[21] SSD6D\u2020 [13] DeepIM\u2020 [15] OURS\u2020 ADD(-S) 55.",
          "5": "W e report percentages of correctly estimated poses averaged over all object classes Method HMap[20] PVNet[21] BB8\u2020 [23] DeepIM\u2020 [15] OURS\u2020 ADD(-S) 30."
        },
        "Neural free-viewpoint performance rendering under complex human-object interactions": {
          "authors": [
            "G Sun",
            "X Chen",
            "Y Chen",
            "A Pang",
            "P Lin"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3475442",
          "ref_texts": "[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In CVPR.",
          "ref_ids": [
            "46"
          ],
          "1": "We believe the coarse-to-fine strategy [57] and the end-to-end 6DoF estimation [46] can accelerate human reconstruction and object tracking respectively."
        },
        "Self-supervised geometric perception": {
          "authors": [
            "Heng Yang",
            "Wei Dong",
            "Luca Carlone",
            "Vladlen Koltun"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_paper.html",
          "ref_texts": "[58] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conf. on Computer V ision and P attern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "58"
          ],
          "1": "Learned feature descriptors have been shown to consistently and significantly outperform their hand-crafted counterparts across applications such as relative camera pose estimation [69, 61], 3D point cloud registration [21, 32], and object detection and pose estimation [58, 86, 64, 72].",
          "2": "For example, ground-truth relative camera poses are needed for training image keypoint descriptors [69, 54, 27], pairwise rigid transformations are required for training point cloud descriptors [21, 32, 74, 85, 70], and object poses are used to train image keypoint predictors [58, 86].",
          "3": "F or example, we also present the formulation forobject detection and pose estimation [64, 58, 86, 15], and discuss the application of SGP in the Supplementary Material."
        },
        "Roft: Real-time optical flow-aided 6d object pose and velocity tracking": {
          "authors": [
            "NA Piga",
            "Y Onyshchuk",
            "G Pasquale"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9568706/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVnet: Pixel-wise voting network for 6DoF pose estimation,\u201d in2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2019, pp.",
          "ref_ids": [
            "2"
          ],
          "1": "Several approaches have been proposed to tackle the problems of 6D object pose estimation [1], [2], [3], refinement [4], [5] and tracking [6], [7].",
          "2": "Mk and Tk can be obtained using either separate deep learning-based networks for segmentation and 6D object pose estimation, or a single network which performs both tasks jointly [1], [2]."
        },
        "Multi-view fusion for multi-level robotic scene understanding": {
          "authors": [
            "Y Lin",
            "J Tremblay",
            "S Tyree",
            "PA Vela"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9635994/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "6"
          ],
          "1": "Despite the tremendous progress made in the computer vision community on solving problems such as 3D reconstruction [1], [2], [3], [4] and object pose estimation [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], existing deployed robotic manipulators have limited, if any, perception of their surroundings."
        },
        "Visual identification of articulated object parts": {
          "authors": [
            "V Zeng",
            "TE Lee",
            "J Liang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636054/",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "Of these works, PVNet [29] also regresses to a residual (pointing to object keypoints for pose estimation), whereas our motion residual is used to infer the kinematic constraint."
        },
        "Rede: End-to-end object 6d pose robust estimation using differentiable outliers elimination": {
          "authors": [
            "W Hua",
            "Z Zhou",
            "J Wu",
            "H Huang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9363576/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "Therefore, PVNet [13] employs the farthest point sampling (FPS) algorithm to select more representative keypoints.",
          "2": "As [13], we employ the farthest point sampling (FPS) algorithm to sample 3D keypoints {mk}K k=1 from the CAD model of each object.",
          "4": "10000 images using the \u201cCut and Paste\u201d strategy are further synthesized for training as [13].",
          "7": "4 1 With the same mask as PVNet [13].",
          "8": "4 1 With the same mask as PVNet [13] on Occlusion LineMOD dataset.",
          "9": "As for Occlusion LineMOD dataset with many hard cases, we use the same masks as PVNet [13].",
          "11": "2 APPENDIX A IMPLEMENTATION DETAILS During implementation, center point and 8 points selected by FPS algorithm are picked up as keypoints following PVNet [13].",
          "12": "10000 images using the \u201cCut and Paste\u201d strategy are further synthesized for training on LineMOD dataset as [13].",
          "13": "The second and third experiments extend PVNet [13] to 3D and the second experiment also employs SVD 3D-3D estimator."
        },
        "Sparse steerable convolutions: An efficient learning of se (3)-equivariant features for estimation and tracking of object poses in 3d space": {
          "authors": [
            "J Lin",
            "H Li",
            "K Chen",
            "J Lu",
            "K Jia"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/8c1b6fa97c4288a4514365198566c6fa-Abstract.html",
          "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "These works can be broadly categorized into three types: i) template matching [12] by constructing templates to search for the best matched poses; ii) 2D-3D correspondence methods [1, 14, 16, 19, 17], which establish 2D-3D correspondence via 2D keypoint detection [19, 17] or dense 3D coordinate predictions [1, 14, 16], followed by a PnP algorithm to obtain the target pose; iii) direct pose regression [26, 13, 23] via deep networks."
        },
        "6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping": {
          "authors": [
            "TT Le",
            "TS Le",
            "YR Chen",
            "J Vidal",
            "CY Lin"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0921889021000609",
          "ref_texts": "[44] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wisevotingnetworkfor6dofposeestimation, in: Proc. IEEE Conf. Comput. Vis. Pa\u0000ern Recognit., 2019: pp. 4561\u20134570.",
          "ref_ids": [
            "44"
          ],
          "1": "Some recent remarkable researches on deep learning-based 3D object recognition includePoseCNN[43]andPVNet[44]."
        },
        "Votehmr: Occlusion-aware voting network for robust 3d human mesh recovery from partial point clouds": {
          "authors": [
            "G Liu",
            "Y Rong",
            "L Sheng"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3475309",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 . Computer Vision Foundation / IEEE, Long Beach, 4561\u20134570. https: //doi.org/10.1109/CVPR.2019.00469",
          "ref_ids": [
            "32"
          ],
          "1": "PVN [32] predicted the unit vector field for a given image and PVN3D [12] predicted the keypoints offsets and instance semantic segmentation label for each pixel on a given RGBD image."
        },
        "Fast uncertainty quantification for deep object pose estimation": {
          "authors": [
            "G Shi",
            "Y Zhu",
            "J Tremblay",
            "S Birchfield"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561483/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "Recent leading methods rely on an approach similar to the one used in our work: A network is trained to predict object keypoints in the 2D image, followed by P nP [28] to estimate the pose of the object in the camera coordinate frame [2, 5, 24, 25, 27, 29]."
        },
        "Optimal pose and shape estimation for category-level 3d object perception": {
          "authors": [
            "J Shi",
            "H Yang",
            "L Carlone"
          ],
          "url": "https://arxiv.org/abs/2104.08383",
          "ref_texts": "[57] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "57"
          ],
          "1": "Such approaches first recover the position of semantic keypoints [56] in the images with neural networks, and then recover the 3D pose of the object by solving a geometric optimization problem [31, 53, 56, 57, 64]."
        },
        "Multi-view object pose refinement with differentiable renderer": {
          "authors": [
            "I Shugurov",
            "I Pavlov",
            "S Zakharov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9363552/",
          "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "In contrast, PVNet [14] used per-pixel Hough voting to allow all object\u2019s pixels to vote for the few keypoints lying on the object.",
          "2": "Train data Synthetic Real GT labels Method OURS SSD6D [12] OURS OURS Pix2Pose[16] PVNet[14] DPOD[4] HybPose[15]closest views random views farthest views Refinement DL [5] DR1 DR2 DR4 DR2 DR4 DR2 DR4 Ape 26.",
          "3": "Train dataSynt Real Autolabels Real GT labelsDR2 DR4 Method OURSOURS OURS DPOD [4] PVNet [14] OURS CDPN [17] HybP."
        },
        "Pyrapose: Feature pyramids for fast and accurate object pose estimation under domain shift": {
          "authors": [
            "S Thalhammer",
            "M Leitner",
            "T Patten"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9562108/",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "5"
          ],
          "1": "The best performing deep-learning approaches for single-shot object pose estimation employ encoder-decoder architectures [2], [3], [4], [5], [6].",
          "2": "The dominant strategy is to employ encoder-decoder architectures for dense hypotheses generation and subsequent pose estimation using the PnP algorithm [13], [14], [3], [15], [4], [5], [16], [6].",
          "3": "Therefore, only one model needs to be trained per dataset in contrast to the majority of state-of-the-art approaches [30], [4], [5], [17], [24], [27], [16], [6].",
          "4": "The dominant approach to recover the most likely reference frame transformation that aligns a set of 2D points to a set of noisy 3D points [2], [4], [5], [16], [6] is PnP."
        },
        "3D object tracking with adaptively weighted local bundles": {
          "authors": [
            "JC Li",
            "F Zhong",
            "SH Xu",
            "XY Qin"
          ],
          "url": "https://link.springer.com/article/10.1007/s11390-021-1272-5",
          "ref_texts": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE Conference on Computer Vision and Pattern Recognition, June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "6"
          ],
          "1": "3 this is different from the 3D object detection and 6DOF pose estimation from a single image, which has been greatly advanced using learning-based approaches [6,7]."
        },
        "Single-shot scene reconstruction": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://openreview.net/forum?id=CGn3XKSf7vf",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "7"
          ],
          "2": "The current state-of-the-art methods in object pose estimation almost exclusively belong to the latter group with such representatives as PVNet [7], CDPN [28], EPOS [27], Pix2Pose [4], GDR-Net [29] and DPOD [3, 6]."
        },
        "ARShoe: Real-time augmented reality shoe try-on system on smartphones": {
          "authors": [
            "S An",
            "G Che",
            "J Guo",
            "H Zhu",
            "J Ye",
            "F Zhou"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3481537",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 4556\u2013",
          "ref_ids": [
            "23"
          ],
          "1": "Pixel-wise voting network (PVNet) [23] predicts pixel-wise vectors pointing to the object keypoints and uses these vectors to vote for keypoint locations."
        },
        "Kdfnet: Learning keypoint distance field for 6d object pose estimation": {
          "authors": [
            "X Liu",
            "S Iwase",
            "KM Kitani"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636489/",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d inCVPR, 2019. 1, 2, 4, 5, 6",
          "ref_ids": [
            "5"
          ],
          "1": "There are mainly two types of methods for localizing 2D keypoints: heatmap-based [3, 4] and voting-based [2, 5, 6].",
          "2": "In voting-based methods, the visible parts of the object hallucinate and vote for the 2D locations of the invisible keypoints [5].",
          "3": "every object pixel predicts the 2D direction to the keypoints and the keypoint hypotheses are the intersections of the direction votes [2, 5, 6].",
          "4": "3%, significantly outperforms related baselines such as [5] and the current state-of-the-art HybridPose [6].",
          "5": "Besides heatmap, 2D direction field representation has also been proposed for localizing keypoints [2, 5].",
          "6": "Specifically, direction-based voting methods have been adopted by previous works to robustly localize object centers [2] or object keypoints [5, 10] on the images where the voting scores are based on the number of direction inliers.",
          "7": "The intuition behind this method is that given an object, the location of the invisible keypoint can be inferred from the visible parts [2, 5, 6].",
          "8": "Inspired by recent works [4, 5], we first predict KDF to localize 2D keypoints through voting, then compute object 6D pose by solving a PnP problem.",
          "9": "method PVNet [5] KDFNet (ours) GT mask 93.",
          "10": "DISTANCE -BASED VOTING : A TOY EXPERIMENT The key difference between our method and previous works [2, 5, 6] is the predicted representation used in voting, i.",
          "11": "The baseline we compare our KDFNet against is PVNet [5], a direction-based keypoint method.",
          "12": "The data used to train our model are the same as [5]: real images from LINEMOD and synthetically rendered images using the scanned 3D object models.",
          "13": "We adopt the same object keypoint set as [5], which are generated by Farthest Point Sampling (FPS) of the object 3D point set.",
          "14": "We evaluate and compare our model against previous baselines [2, 5, 6, 17, 24] on Occlusion LINEMOD dataset.",
          "15": "Among these baselines, the most relevant is PVNet [5], a direction-based keypoint voting method which was also compared against in the toy experiment in Section IV.",
          "16": "methods PoseCNN [2] Oberweger [24] Pix2Pose [17] PVNet [5] HybridPose [6] KDFNet (ours) ape 9.",
          "17": "3 methods PoseCNN [2] Oberweger [24] PVNet [5] KDFNet (ours) ape 34.",
          "18": "5 particular, our method outperforms PVNet [5] by a margin of 9."
        },
        "6D object pose estimation using keypoints and part affinity fields": {
          "authors": [
            "M Zappel",
            "S Bultmann",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-98682-7_7",
          "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "15"
          ],
          "1": "In recent years, two-stage approaches, which first detect keypoints and then solve a Perspective-n-Point (PnP) problem to infer the object pose [14,15,16], have been shown to provide robust and accurate results.",
          "2": "PVNet [15] also defines keypoints on the object surface but infers them in a dense manner: Each pixel in the object segmentation mask predicts vectors that point to every keypoint.",
          "3": "The automatically defined set of keypoints is chosen with the farthest-point-algorithm, inspired by PVNet [15]: Starting with the object center, points on the object surface which are farthest from the already chosen points are added to the keypoint set."
        },
        "End-to-end learning improves static object geo-localization from video": {
          "authors": [
            "Mohamed Chaabane",
            "Lionel Gueguen",
            "Ameni Trabelsi",
            "Ross Beveridge",
            "Stephen O"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Chaabane_End-to-End_Learning_Improves_Static_Object_Geo-Localization_From_Video_WACV_2021_paper.html",
          "ref_texts": "[24] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "5D Pose Estimation Many state-of-the-art methods for object pose estimation [15, 22, 24, 30, 35] use 3D models of the objects."
        },
        "L6dnet: Light 6 DoF network for robust and precise object pose estimation with small datasets": {
          "authors": [
            "M Gonzalez",
            "A Kacete",
            "A Murienne"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9364353/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF pose estimation,\u201d in IEEE Conf. on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "7"
          ],
          "4": "PVNet[7] proposes to apply an offset based approach to predict the 2D location of a set of keypoints on the object surface.",
          "5": "Inspired by [7], we select the keypoints using the farthest point sampling algorithm which allows us to get a good coverage of the object."
        },
        "Fast-learning grasping and pre-grasping via clutter quantization and Q-map masking": {
          "authors": [
            "D Ren",
            "X Ren",
            "X Wang",
            "ST Digumarti"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636165/",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u2013",
          "ref_ids": [
            "20"
          ],
          "1": "We also extend the FCN by utilizing a combination of skip connections [20] and upsampling to improve learning efficiency.",
          "2": "In addition, the combination of skip connections [20] and bilinear upsampling in our FCN further boosts accuracy."
        },
        "Deepflux for skeleton detection in the wild": {
          "authors": [
            "Y Xu",
            "Y Wang",
            "S Tsogkas",
            "J Wan",
            "X Bai"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-021-01430-6",
          "ref_texts": "45. Peng, S., Liu, Y ., Huang, Q., Zhou, X., & Bao, H. (2019). PVNet: Pixel-wise voting network for 6dof pose estimation. InProceedings of IEEE international conference on computer vision and pattern recognition (pp. 4561\u20134570).",
          "ref_ids": [
            "45"
          ],
          "1": "O u rw o r ki sa l s or e l a t e dt o the approaches in [1,2,6,9,27,38,45,67]w h i c hl e a r nd i r e c tion cues for edge detection, instance segmentation, and pose estimation.",
          "2": "Finally, direction cues are also used to improve instance segmentation in [1]a n dd i r e c t i o nfi e l d sp o i n t i n g towards keypoints are used for pose estimation in [27,45]."
        },
        "Strumononet: Structure-aware monocular 3d prediction": {
          "authors": [
            "Zhenpei Yang",
            "Li Erran",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_StruMonoNet_Structure-Aware_Monocular_3D_Prediction_CVPR_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer V ision and P attern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer V ision Foundation / IEEE, 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "Examples include learning a machine translator between two minor languages by composing machine translators via a mother language [19], solving 6D object pose prediction via intermediate keypoint detections [1, 31, 28, 36, 27, 29, 34], and predicting 3D human poses through 2D keypoint predictions [44]."
        },
        "Dynamical pose estimation": {
          "authors": [
            "Heng Yang",
            "Chris Doran",
            "Jacques Slotine"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Yang_Dynamical_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.2",
          "ref_ids": [
            "39"
          ],
          "1": "4 Problem (1), when specialized to the primitives 1-7, includes a broad class of fundamental perception problems concerning pose estimation from visual measurements, and finds extensive applications to object detection and localization [30, 39], motion estimation and 3D reconstruction [58, 56], and simultaneous localization and mapping [10, 52, 42]."
        },
        "DRNet: A depth-based regression network for 6D object pose estimation": {
          "authors": [
            "Lei Jin",
            "Xiaojuan Wang",
            "Mingshu He",
            "Jingyue Wang"
          ],
          "url": "https://www.mdpi.com/1424-8220/21/5/1692",
          "ref_texts": "28. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR, Long Beach, CA, USA, 15\u201321 June 2019; pp. 4561\u20134570. Sensors 2021, 21, 1692 14 of 14",
          "ref_ids": [
            "28"
          ],
          "1": "[28] proposed a Pixel-wise Voting Network (PVNet) to identify keypoints with the aid of RANSAC-based voting.",
          "3": "In addition, we also compare our method with PVNet [28], which uses key points to figure out poses.",
          "4": "4 for the average ADD(S) [28], which calculates the ADD AUC for asymmetric objects and the ADD-S AUC for symmetric objects."
        },
        "Instancepose: Fast 6dof pose estimation for multiple objects from a single rgb image": {
          "authors": [
            "Lee Aing",
            "Nung Lie",
            "Chiu Chiang",
            "Shiang Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.html",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "19"
          ],
          "4": "Metrics 2DPro ADD(S) Methods PoseCNN S-Driven PVNet S-Stage Ours PoseCNN S-Driven Pix2Pose PVNet S-Stage Ours[26] [10] [19] [9] [26] [10] [18] [19] [9] ape 34.",
          "6": "Metrics 5CMD 2CMD 5CMD 10CMD Methods DeepIM PVNet Ours[13] [19] ape 51."
        },
        "A multi-step CNN-based estimation of aircraft landing gear angles": {
          "authors": [
            "Fuyang Li",
            "Zhiguo Wu",
            "Jingyu Li",
            "Zhitong Lai",
            "Botong Zhao",
            "Chen Min"
          ],
          "url": "https://www.mdpi.com/1424-8220/21/24/8440",
          "ref_texts": "2. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DoF Object Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 4556\u20134565.",
          "ref_ids": [
            "2"
          ],
          "2": "In addition, there are two-stage attitude estimations, such as Deepmanta [23], YOLO-6D [24], PVNet [2], and other methods.",
          "4": "OpenPose [42] utilizes the vector field network to model the correspondence between key points, and PoseCNN [51] and PVNet [2] utilize it as an intermediate quantity to replace the regression of key points.",
          "5": "In this paper, an alterable vector field loss function is proposed to enhance the pixel learning of the principal amount as follows: LvecK = K \u00d7Lvec, (8) whenx p,k < \u03bb, K = xp,k/\u03bb, when, xp,k \u2265\u03bb, K = 1, (9) where Lver is the key point vector field loss function proposed in [2], K is the distancebased weight value, xp,k represents the distance between the current pixel point p and the corresponding key point k, and \u03bb is a constant.",
          "6": "The key points of the aircraft are selected as corner points, and points with unique characteristics [2] that can be easily identified."
        },
        "Investigations on output parameterizations of neural networks for single shot 6d object pose estimation": {
          "authors": [
            "K Kleeberger",
            "M V\u00f6lk",
            "R Bormann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561712/",
          "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "28"
          ],
          "1": "Other single shot approaches to object pose estimation output the 2D projections of the 3D bounding box and use a P nP algorithm [27] to compute the 6D pose [17], [18], [28], [29]."
        },
        "A 3d keypoints voting network for 6dof pose estimation in indoor scene": {
          "authors": [
            "Huikai Liu",
            "Gaorui Liu",
            "Yue Zhang",
            "Linjian Lei",
            "Hui Xie",
            "Yan Li",
            "Shengli Sun"
          ],
          "url": "https://www.mdpi.com/2075-1702/9/10/230",
          "ref_texts": "7. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "7"
          ],
          "1": "CNN is also used in pose estimation, PVNet [7] regress the 2d keypoint through the end-to-end network, and then use the PnP algorithm, estimate the 6d pose by calculating the 2d-3d correspondence relationship of the object.",
          "2": "PVNet [7] first votes the keypoints through RANSAC, then utilizes the 2D-3D correspondence to calculate the 6D pose.",
          "3": "We compare our method with the RGB based methods PoseCNN [9], PVNet [7] and RGDB based methods PointFusion [49], Densefusion [12], PVN3D [15]."
        },
        "A high-resolution network-based approach for 6D pose estimation of industrial parts": {
          "authors": [
            "J Fan",
            "S Li",
            "P Zheng",
            "CKM Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9551495/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "1": "To further improve the pose estimation accuracy, some studies decided to borrow the idea from feature points-based methods and predict the correspondence of key points via CNN models [23], [24], while others added an extra refinement stage upon the CNN-based 6D pose estimation model [25], [26]."
        },
        "From IR images to point clouds to pose: point cloud-based AR glasses pose estimation": {
          "authors": [
            "Ahmet Firintepe",
            "Carolin Vey",
            "Stylianos Asteriadis",
            "Alain Pagani",
            "Didier Stricker"
          ],
          "url": "https://www.mdpi.com/2313-433X/7/5/80",
          "ref_texts": "1. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
          "ref_ids": [
            "1"
          ],
          "1": "Given that depth information usually necessitates the use of dedicated hardware, image-only approaches for pose estimation have received significant attention over the last years [1,3,4,6,8].",
          "2": "Because handcrafting features is time-consuming and prone to errors, Deep Learning-based approaches have gained popularity and outperform traditional approaches [1,3,4,8].",
          "3": "Recent feature-based Deep Learning methods use Deep Neural Networks to estimate the objects\u2019 keypoints and combine them with PnP , partly relying on traditional methods [1,2,11].",
          "4": "[1] provide a state-of-the-art approach based on keypoint regression and further PnP execution.",
          "5": "Especially the definition of keypoints benefits pose estimation when dealing with occlusions and truncation [1]."
        },
        "Soft-jig-driven assembly operations": {
          "authors": [
            "T Kiyokawa",
            "T Sakuma",
            "J Takamatsu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9562008/",
          "ref_texts": "[36] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "36"
          ],
          "1": "To confirm the 6D pose estimation task for fixed parts, we apply PVNet [36], one of deep learning-based algorithms [37], [38]."
        },
        "Self-guided instance-aware network for depth completion and enhancement": {
          "authors": [
            "Z Luo",
            "F Zhang",
            "G Fu",
            "J Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561523/",
          "ref_texts": "[12] S. Y . Q. H.Bao and X.Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d IEEE Conference on Computer Vision and Pattern Recognition, pp. 4556\u20134565, 2018.",
          "ref_ids": [
            "12"
          ],
          "1": "In contrast, as convolutional neural network (CNN) have been successful in learning effective representations in object detection [9]\u2013[11], and pose estimation [12], [13]."
        },
        "Bridging the reality gap for pose estimation networks using sensor-based domain randomization": {
          "authors": [
            "Frederik Hagelskjaer",
            "Anders Glent"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.html",
          "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 2, 7",
          "ref_ids": [
            "24"
          ],
          "1": "In PVNet [24], the network instead locates keypoints by first segmenting the object and then letting all remaining pixels vote for keypoint locations.",
          "3": "The competing methods are DPOD [35], SSD-6D [19] (obtained from [32]), PVNet [24], DenseFusion [32], PointV oteNet [7] and PVN3D [10]."
        },
        "Automatic generation of machine learning synthetic data using ros": {
          "authors": [
            "Markus Richter"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-77772-2_21",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X.: PVNet: Pixel -wise Voting Network for ",
          "ref_ids": [
            "20"
          ],
          "1": "They are Darknet [14], a network implementation known as PVNet [20] that uses projected keypoints, and a custom format that uses the vertices projected into an arbitrary frame of reference."
        },
        "6D object pose estimation with pairwise compatible geometric features": {
          "authors": [
            "M Lin",
            "V Murali",
            "S Karaman"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561404/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "27"
          ],
          "1": "Recent work such as [27] and [25] predicts either a direction vector pointing to keypoints or offsets to the center of target objects for each point and achieves satisfactory accuracy.",
          "4": "Comparison with the State-of-the-Art on LineMOD and Occlusion LineMOD Datasets a) Performance on LineMOD dataset: We evaluate performance of our method and compare pose estimation recall against state-of-the-art methods [15], [27], [24], [42], [25], [40], [8].",
          "5": "The first observation we draw from the row of the mean recall is that, with a more consistent geometric representation, we can eliminate the gap with methods using RGB [15], [27] even without ICP refinement."
        },
        "Deep quaternion pose proposals for 6D object pose tracking": {
          "authors": [
            "Mateusz Majcher",
            "Bogdan Kwolek"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.html",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. CVPR, pages 4556\u20134565, 2019. 1, 4",
          "ref_ids": [
            "21"
          ],
          "2": "In [21], a Pixel-wise V oting Network (PVNet) to regress pixelwise unit vectors pointing to the keypoints and then using these vectors to vote for keypoint locations via RANSAC has been proposed."
        },
        "Attention voting network with prior distance augmented loss for 6DoF pose estimation": {
          "authors": [
            "Y He",
            "J Li",
            "X Zhou",
            "Z Chen",
            "X Liu"
          ],
          "url": "https://search.ieice.org/bin/summary.php?id=e104-d_7_1039",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp.4561\u20134570, 2019.",
          "ref_ids": [
            "7"
          ],
          "2": "Unit direction vector-field representation and Hough voting scheme were proposed for robust 2D keypoint localization and demonstrated its superiority in [6], [7], [17].",
          "6": "[7], [22] demonstrate that selecting the surface point of the 3D model as the keypoint can acquire more accurate poses.",
          "7": "[7] achieve state-of-the-art performance via their unit direction vector-field representation and Hough voting scheme.",
          "21": "Comparing our methods with PVNet [7], both PDAL and AFAM have significant improvements on most objects, especially on ape, cat, duck, etc."
        },
        "Joint Learning of Object Detection and Pose Estimation using Augmented Autoencoder": {
          "authors": [
            "RHAST Matsumoto",
            "N Ukita"
          ],
          "url": "https://www.mva-org.jp/Proceedings/2021/papers/P1-10.pdf",
          "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "13"
          ],
          "1": "local feature detection and matching, appearance matching, and pose refinement [7, 8, 9, 10, 11, 12, 13, 14, 15]) for pose estimation."
        },
        "Iterative optimisation with an innovation CNN for pose refinement": {
          "authors": [
            "G Kennedy",
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "https://arxiv.org/abs/2101.08895",
          "ref_texts": "[PLH+19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "PLH\\+19"
          ],
          "1": "Pose via an Intermediate Representation: A recent, popular approach to pose estimation is to first regress to an intermediate representation such as keypoints, from which pose can be obtained via 2D-3D correspondences and a PnP algorithm [TSF17, RL17, PLH+19, SSH20, ZSI19, lWJ19b, HBM20].",
          "2": "Alternatively, [PLH+19] predicts pixel-wise unit-vectors that in turn indicate direction to keypoints.",
          "3": "[SSH20] implements these vector directions from [PLH+19] while also estimating object edge vectors and symmetry correspondences.",
          "4": "We choose PVNet [PLH+19] to be the baseline 3 PREPRINT object pose estimation network.",
          "5": "The function h maps a vector field representation of keypoints to object pose via a RANSAC and uncertainty-driven PnP framework (EPnP), as discussed in [PLH+19].",
          "6": "We use a pretrained PVNet [PLH+19] to provide the initial estimate Xk ij(0) for each object.",
          "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [PLH+19].",
          "8": "7 PREPRINT Methods CDPN[lWJ19b] YOLO6D[RDGF16] PVNet[PLH+19] OURS ape 92.",
          "9": "Methods DPOD[ZSI19] Pix2Pose[PPV19] PVNet[PLH+19] OURS ape 22.",
          "10": "Methods DPOD[ZSI19] CDPN[lWJ19b] PVNet[PLH+19] OURS ape 53."
        },
        "End-to-end multi-instance robotic reaching from monocular vision": {
          "authors": [
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561518/",
          "ref_texts": "[3] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "3"
          ],
          "1": "Algorithms that first regress an intermediate representation, such as image keypoints, and then compute object pose by solving a PnP problem [3], [4], [5], have achieved impressive performance on popular monocular pose estimation datasets including LINEMOD [6] and Occlusion LINEMOD [7]."
        },
        "Experimental Evaluation of Affordance Detection Applied to 6-DoF Pose Estimation for Intelligent Robotic Grasping of Household Objects": {
          "authors": [
            "Aidan Keaveny"
          ],
          "url": "https://uwspace.uwaterloo.ca/handle/10012/17716",
          "ref_texts": "[19] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNET: Pixel-wise Voting Network for 6DoF Pose Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 4556\u20134565, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "19"
          ],
          "4": "An alternative to learning sparse keypoints is offered by dense methods in Pixel-wise Voting Network (PVNet) [19], which predict unit vectors pointing to keypoints for each pixel.",
          "5": "The latter has been more effective in estimating pose under heavy occlusion [19].",
          "6": "Thus, the P nP algorithm is still widely used in many deep learning frameworks for 6-DoF pose estimation today and such frameworks can be deployed in real-time [18, 19]."
        },
        "Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation": {
          "authors": [
            "J Wu",
            "L Liu",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://arxiv.org/abs/2109.12266",
          "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "To use more reliable correspondence, PVNet [14] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints.",
          "2": "Since feature extraction is not the focus of this paper, we follow [14] to build a multi-scale convolutional neural network, to extract features at multiple resolutions.",
          "4": "Implementation Details In implementation, we follow [14] to select 9 keypoints for every object, and perform the same data augmentation.",
          "6": "For fair comparison, we take the 2D predicted keypoints from PVNet [14], and triangulate the two keypoints to 3D space by classic method [29]."
        },
        "A Comprehensive Review on 3D Object Detection and 6D Pose Estimation With Deep Learning": {
          "authors": [
            "Sabera Hoque"
          ],
          "url": "https://figshare.utas.edu.au/ndownloader/files/40754126",
          "ref_texts": "[191] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u2018\u2018PVNet: Pixel-wise voting network for 6DoF pose estimation,\u2019\u2019 Tech. Rep., 2018.",
          "ref_ids": [
            "191"
          ],
          "1": "Also, [191] recently removed the ROI pooled orientation layer and introduced PVNet (Pixelwise Voting Network) to deny pixel-based vectors and use them for key-point positions."
        },
        "Learning Robot Skills from Few Demonstrations": {
          "authors": [
            "S Stev\u0161i\u0107"
          ],
          "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/478140/3/Stefan_Thesis.pdf",
          "ref_texts": "[131] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation\u201d, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u2013",
          "ref_ids": [
            "131"
          ],
          "3": "1 Neural Network Model At the initial step, we obtain the object pose from an existing one-shot pose estimation algorithm, such as PVNet [131].",
          "6": "We use the SoA in the one-shot setting, PVNet [131], for initialization."
        },
        "DEEP LEARNING FOR OBJECT DETECTION": {
          "authors": [
            "Akber Khan"
          ],
          "url": "https://trepo.tuni.fi/bitstream/handle/10024/135807/KhanAkberAli.pdf?sequence=4",
          "ref_texts": "[29] S. Peng, Y. Liu, Q. Huang, X. Zhou and H. Bao, \"PVNET: Pixel-wise voting network for 6dof pose estimation,\" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019 06-01, vol. 2019, pp. 4556\u20134565, 2019. ",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] is an example of an indirect voting-based technique and outperforms some of the earlier methods."
        },
        "Machine learning guided geometric analysis and pose estimation": {
          "authors": [
            "Philipp Ausserlechner"
          ],
          "url": "https://repositum.tuwien.at/handle/20.500.12708/18386",
          "ref_texts": "[PLH+19] Sida Peng, Yuan Liu,Q ixing Huang, XiaoweiZ hou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedingso ft he IEEE/CVF Conferenceo nC omputer Visiona nd Pattern Recognition,p ages 4561\u20134570, 2019.",
          "ref_ids": [
            "PLH\\+19"
          ],
          "1": "Peng and Liu [PLH+19] propose a Pixel-wise Voting Network (PVNet) to achieve a 6D object pose recovery (Fig."
        },
        "A Survey on Deep Learning Based Methods and Datasets for Monocular 3D Object Detection. Electronics 2021, 10, 517": {
          "authors": [
            "SH Kim",
            "Y Hwang"
          ],
          "url": "https://pdfs.semanticscholar.org/076b/052fe9aa43e1f8619cc9e8aab29966a32f6d.pdf",
          "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "72"
          ],
          "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
          "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11."
        },
        "Learning Innovations for State Estimation": {
          "authors": [
            "G Kennedy",
            "J Gao",
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "http://www.gerard-kennedy.com/files/iros-2021.pdf",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "31"
          ],
          "3": "State Estimation for Object Pose and Depth Refinement For object pose estimation we choose PVNet [31] to be the baseline network that provides \u02c6X(0).",
          "5": "For object pose estimation the baseline network is PVNet [31].",
          "6": "For object pose estimation we use a pretrained PVNet [31] to provide the initial estimate for each object."
        },
        "Supplementary Material-FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation": {
          "authors": [
            "Y He",
            "H Huang",
            "H Fan",
            "Q Chen",
            "J Sun"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/He_FFB6D_A_Full_CVPR_2021_supplemental.pdf",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 3",
          "ref_ids": [
            "15"
          ],
          "1": "RGB RGB-D PoseCNN DeepIM [18, 10] PVNet[15] CDPN[11] DPOD[20] PointFusion[19] DenseFusion[17] G2LNet[1] PVN3D[5] Our FFB6D ape 77.",
          "2": "[9] Pix2Pose [14] PVNet [15] DPOD [20] Hu et al."
        },
        "Self-supervised Geometric Perception": {
          "authors": [
            "CMU RI"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_supplemental.pdf",
          "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 3",
          "ref_ids": [
            "12"
          ],
          "1": "Recent works such as YOLO6D [14], PVNet [12], and DPOD [16] can all serve as the student network, despite using different methodologies.",
          "2": ", by rendering synthetic projections of the 3D models under different simulated poses, which is common in [16, 12, 14, 2].",
          "3": "2There are many different ways to establish 2D-3D correspondences, see PVNet [12], YOLO6D [14] and references therein."
        },
        "\ub85c\ubd07 \ud314\uc744 \ud65c\uc6a9\ud55c \uc815\ub9ac\uc791\uc5c5\uc744 \uc704\ud55c \ubb3c\uccb4 \uc790\uc138\ucd94\uc815 \ubc0f \uc774\ubbf8\uc9c0 \ub9e4\uce6d": {
          "authors": [
            "Media Contents"
          ],
          "url": "https://jkros.org/xml/31147/31147.pdf",
          "ref_texts": "[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 2019, DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "3"
          ],
          "1": "\uae30\uc874\uc758 \uc790\uc138\ucd94\uc815 \uc54c\uace0\ub9ac\uc998 \uc911\uc5d0\uc11c \uc2ec\uce35\ud559\uc2b5\uc5d0 \uae30\ubc18\ud55c \uc5f0\uad6c \ub4e4[2,3]\uc774 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\uba74\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4."
        },
        "A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators": {
          "authors": [
            "C Sahin",
            "G Garcia-Hernando",
            "J Sock",
            "TK Kim"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885620300305",
          "ref_texts": "[160] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation , In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4561-4570), 2019.",
          "ref_ids": [
            "160"
          ],
          "4": "PVNet [160] estimates full 6D poses of the objects of interest under severe occlusion or truncation."
        },
        "Cosypose: Consistent multi-view multi-object 6d pose estimation": {
          "authors": [
            "Y Labb\u00e9",
            "J Carpentier",
            "M Aubry",
            "J Sivic"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58520-4_34",
          "ref_texts": "5. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019) 4561\u20134570",
          "ref_ids": [
            "5"
          ],
          "1": "A convolutional neural network (CNN) can be used to detect object features in 2D [4,6,18,21,22] or to directly find 2D-to-3D correspondences [5,7,8,23].",
          "2": "1 PVNet [5] 73.",
          "3": "Following [5,10,18], we evaluate on a subset of 2949 keyframes from videos of the 12 testing scenes.",
          "4": "When testing, we follow previous works [5,10,18] and evaluate on a subset of 2949 keyframes."
        },
        "Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation": {
          "authors": [
            "Yisheng He",
            "Wei Sun",
            "Haibin Huang",
            "Jianran Liu",
            "Haoqiang Fan",
            "Jian Sun"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.html",
          "ref_texts": "[37] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer V ision and P attern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "37"
          ],
          "3": "T o better deal with truncated and occluded scenes, [37] proposes a pixel-wise voting network to vote for the 2D keypoints location.",
          "4": "PVNet [37] uses per-pixel voting for 2D Keypoints to combine the advantages of Dense methods and keypoint-based methods.",
          "5": "Therefore, we follow [37] and use the farthest point sampling (FPS) algorithm to select keypoints on the mesh.",
          "6": "Also, we follow [37] and add synthesis images into our training set.",
          "7": "4% 11636 RGB RGBD PoseCNN DeepIM [26, 52] PVNet [37] CDPN [27] Implicit ICP[45] SSD-6D ICP[22] PointFusion[50] DF(perpixel)[50] DF(iterative)[50] PVN3D ape 77.",
          "8": "DF(R T)[50] DF(3D KP)[50] Ours(R T) Ours(2D KPC) Ours(2D KP) PVNet[37] Ours(Corr) Ours(3D KP) ADD-S 92."
        },
        "Hybridpose: 6d object pose estimation under hybrid representations": {
          "authors": [
            "Chen Song",
            "Jiaru Song",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.html",
          "ref_texts": "[34] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation.CoRR, abs/1812.11788, 2018.",
          "ref_ids": [
            "34"
          ],
          "2": "T o express the geometric information in an RGB image, a prevalent intermediate representation is keypoints, which achieves state-of-the-art performance [34, 32, 36].",
          "6": "In our experiments, HybridPose incorporates an off-the-shelf architecture called PVNet [34], which is the state-of-the-art keypoint-based pose estimator that employs a voting scheme to predict both visible and invisible keypoints.",
          "10": "HybridPose outperforms PVNet [34], the backbone model we use to predict keypoints.",
          "12": "In terms of ADD(-S), our approach improves PVNet [34] from 40."
        },
        "Satellite pose estimation challenge: Dataset, competition design, and results": {
          "authors": [
            "M Kisantal",
            "S Sharma",
            "TH Park",
            "D Izzo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9076337/",
          "ref_texts": "[40] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral , 2019.",
          "ref_ids": [
            "40"
          ],
          "1": "While various DNN-based approaches have been proposed to perform pose estimation [30]\u2013[40], current state-of-the-art methods employ Convolutional Neural Networks (CNN) that either directly predict the 6D pose or an intermediate information that can be used to compute the 6D pose, notably a set of keypoints defined a priori.",
          "2": "Most recently, architectures like KPD [39] and PVNet [40] have been proposed to predict the locations of the 2D keypoints on the target\u2019s surface."
        },
        "Single-stage 6d object pose estimation": {
          "authors": [
            "Yinlin Hu",
            "Pascal Fua",
            "Wei Wang",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[36] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InConference on Computer V ision and P attern Recognition , 2019.",
          "ref_ids": [
            "36"
          ],
          "2": "W e then demonstrate the generality of this network by combining it with two state-of-the-art correspondenceextraction frameworks [13, 36].",
          "3": "W e show that these single-stage frameworks systematically outperform the original twostage ones [13, 36], in terms of both accuracy and runtime.",
          "5": "2(b), our formalism can handle 3D point to 2D vector correspondences, which have been shown to be better-suited to use in conjunction with a deep network [36].",
          "11": "Since one of the best current techniques [36] uses directions instead and infers poses from those using a voting-based PnP scheme, we feed the same 3D point to 2D vector correspondences to our own network.",
          "12": "3 noise level \u03c3 (outliers=30%) pose error Voting-based PnP Ours Figure 8: Comparison with PVNet\u2019s voting-based PnP [36].",
          "17": "W e evaluate two state-ofthe-art correspondence-extraction networks: SegDriven [13] and PVNet [36], by replacing their original RANSAC-based post processing with our small network.",
          "19": "1 Occluded-LINEMOD Results As discussed before, to demonstrate that our method is generic, we test it in conjunction with two correspondenceextraction networks SegDriven [13] and PVNet [36].",
          "20": "W e compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of both ADD-0.",
          "21": "W e compare the running times (in milliseconds) of PoseCNN [50], SegDriven [13], PVNet [36] and our method on a modern GPU (GTX1080 Ti).",
          "22": "W e compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of ADD-0.",
          "23": "In T able 2, we shown that our single-stage network outperform the state-of-the-art methods, PoseCNN [50], SegDriven [13] and PVNet [36].",
          "24": "2 YCB-Video Results T able 4 summarizes the results comparing against PoseCNN [50], SegDriven [13], and PVNet [36]."
        },
        "Learning canonical shape space for category-level 6d object pose and size estimation": {
          "authors": [
            "Dengsheng Chen",
            "Jun Li",
            "Zheng Wang",
            "Kai Xu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.html",
          "ref_texts": "[17] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InProc. CVPR , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "3": "PVNet [17] is a unique approach of feature point detection using CNNs: A vector field is estimated for the input RGB image based on which the feature points are voted."
        },
        "Self6d: Self-supervised monocular 6d object pose estimation": {
          "authors": [
            "G Wang",
            "F Manhardt",
            "J Shao",
            "X Ji",
            "N Navab"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58452-8_7",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "44"
          ],
          "3": "On the other hand, as for training with real pose labels, we are again on par with other recently published methods such as PVNet [44] and CDPN [30] reporting a mean average recall of 86 ."
        },
        "G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features": {
          "authors": [
            "Wei Chen",
            "Xi Jia",
            "Hyung Jin",
            "Jinming Duan",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[29] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation.arXiv preprint arXiv:1812.11788 , 2018.",
          "ref_ids": [
            "29"
          ],
          "1": "Introduction Real-time performance is important in many computer vision tasks, such as, object detection [35, 23], semantic segmentation [36, 10], object tracking [5, 11], and pose estimation [29, 38, 16].",
          "4": "Learning-based methods [33, 29, 34, 28, 15, 38] alleviate this problem by training their model to predict 2D keypoints and compute the object pose by the PnP algorithm [9, 20].",
          "5": "Another way is, as proposed in [29], to use the farthest point sampling (FPS) algorithm to sample the keypoints in each object model.",
          "7": "In experiments, we have found that our proposed method can make faster and more accurate predictions than the methods [29, 42, 4].",
          "8": "(3) When evaluating on YCB-V ideo dataset, same as [42, 29, 21], we use the ADD-S AUC metric proposed in [42], T able 1."
        },
        "Camera-to-robot pose estimation from a single image": {
          "authors": [
            "TE Lee",
            "J Tremblay",
            "T To",
            "J Cheng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9196596/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "10"
          ],
          "1": "Network Architecture Inspired by recent work on object pose estimation [9], [10], [11], we use an auto-encoder network to detect the keypoints.",
          "2": "Pose Estimation Given the 2D keypoint coordinates, robot joint configuration with forward kinematics, and camera intrinsics, P nP [7] is used to retrieve the pose of the robot, similar to [14], [15], [9], [10], [11].",
          "4": "Recent leading methods are similar to the approach proposed here: A network is trained to predict object keypoints in the 2D image, followed by P nP [7] to estimate the pose of the object in the camera coordinate frame [14], [10], [36], [11], [15], [37], or alternatively, a deformable shape model is fit to the detect keypoints [38].",
          "5": "[10], who showed that regressing to keypoints on the object is better than regressing to vertices of an enveloping cuboid."
        },
        "End-to-end learnable geometric vision by backpropagating pnp optimization": {
          "authors": [
            "Bo Chen",
            "Alvaro Parra",
            "Jiewei Cao",
            "Nan Li",
            "Jun Chin"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.html",
          "ref_texts": "[36] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "36"
          ],
          "1": "Other pose estimation approaches that combine deep learning with geometric optimization (PnP solver) [35, 37, 47, 36, 10] adopt a two-stage strategy: first learn to predict the 2D landmarks or fiducial points from the input image, then perform pose estimation by solving PnP on the 2D-3D correspondences.",
          "3": "W e provide the result of the current state-of-the-art PVNet [36] as a reference."
        },
        "Hot-net: Non-autoregressive transformer for 3d hand-object pose estimation": {
          "authors": [
            "L Huang",
            "J Tan",
            "J Meng",
            "J Liu",
            "J Yuan"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3394171.3413775",
          "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "37"
          ],
          "1": "In the last decade, we have witnessed a rapid advance towards both 3D hand pose estimation [4, 5, 11, 13, 14, 24, 30, 33, 41, 42, 47, 47, 51, 56, 57, 60] and object pose estimation [25, 26, 37, 38, 46, 52, 53, 55] in isolation."
        },
        "Gsnet: Joint vehicle pose and shape reconstruction with geometrical and scene-aware supervision": {
          "authors": [
            "L Ke",
            "S Li",
            "Y Sun",
            "YW Tai",
            "CK Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58555-6_31",
          "ref_texts": "39. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "39"
          ],
          "1": "Traditionally, 6D object pose estimation is handled by creating correspondences between the objects known 3D model and 2D pixel locations, followed by Perspective-n-Point (PnP) algorithm [45,54,39]."
        },
        "Single shot 6d object pose estimation": {
          "authors": [
            "K Kleeberger",
            "MF Huber"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9197207/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "10"
          ],
          "1": "Recent works regress the 2D image coordinates of the object\u2019s 3D bounding box and use a P nP algorithm to estimate the object\u2019s 6D pose [8], [9], [10]."
        },
        "Robust 6d object pose estimation by learning rgb-d features": {
          "authors": [
            "M Tian",
            "L Pan",
            "MH Ang",
            "GH Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9197555/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "7"
          ],
          "1": "To address the occlusion problem, dense prediction methods [5], [7]\u2013[10] are proposed.",
          "2": "[5] and [7] predict a unit vector at each pixel pointing towards keypoints.",
          "3": "Among them, [7] and [8] achieve top performances.",
          "4": "Translation In order to make use of the complementary depth information, we extend a RANSAC-based voting method [5], [7] from 2D to 3D space.",
          "5": "RGB w/o refinement RGB w/ refinement RGB-D w/o refinement RGB-D w/ refinement Method PoseCNN [5] PVNet [7] DPOD [8] DeepIM [41] DPOD+ [8] Per-Pixel DF [13] Ours Iterative DF [13] ape 21.",
          "6": "Comparison on LINEMOD We compare our method with the state-of-the-art object pose detectors [5], [7], [8], [13] on the LINEMOD dataset.",
          "7": "Of all those methods without pose refinement, PVNet [7] and Per-Pixel DF [13] are the state of the art on LINEMOD dataset."
        },
        "Guided uncertainty-aware policy optimization: Combining learning and model-based strategies for sample-efficient policy learning": {
          "authors": [
            "MA Lee",
            "C Florensa",
            "J Tremblay"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9197125/",
          "ref_texts": "[44] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "44"
          ],
          "1": "The problem of known object pose estimation is a vibrant subject within the robotics and computer vision communities [19], [39], [40], [41], [42], [43], [44], [45], [46].",
          "2": "[44] also explored the problem of using uncertainty by leveraging a ransac voting algorithm to find regions where a keypoint could be detected."
        },
        "MobilePose: Real-time pose estimation for unseen objects with weak shape supervision": {
          "authors": [
            "T Hou",
            "A Ahmadyan",
            "L Zhang",
            "J Wei"
          ],
          "url": "https://arxiv.org/abs/2003.03522",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X.: Pvnet: Pixel-wise voting network for 6DoF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)",
          "ref_ids": [
            "20"
          ],
          "3": "PVNet [20] finds 2D-3D correspondence of object features, and formulates pose estimation as a PnP problem."
        },
        "PointPoseNet: Point pose network for robust 6D object pose estimation": {
          "authors": [
            "Wei Chen",
            "Jinming Duan",
            "Hector Basevi",
            "Hyung Jin",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content_WACV_2020/html/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.html",
          "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
          "ref_ids": [
            "22"
          ],
          "3": "Our method shares features of PVNet [22]: both methods use unit vector regression to estimate pose, however, our method takes point cloud, and instead of using 2D keypoints we use 3D keypoints, then we use our proposed scoring mechanism to access the final pose which is different to the optimization based method in [22].",
          "10": "From T able 2, we can see that our method outperforms its 2D counterpart PVNet [22], the baseline and other state-of-the-art methods, which shows that our method can better utilize 3D information from depth image."
        },
        "Pointvotenet: Accurate object detection and 6 dof pose estimation in point clouds": {
          "authors": [
            "F Hagelskj\u00e6r",
            "AG Buch"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9191119/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "6"
          ],
          "4": "Both PoseCNN [5] and PVNet [6], which both use image data, produce much less accurate poses for this dataset.",
          "6": "The competing methods are SSD-6D [8], BB8 [2], PVNet [6], and DenseFusion [10]."
        },
        "Reconstruct locally, localize globally: A model free method for object pose estimation": {
          "authors": [
            "Ming Cai",
            "Ian Reid"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.html",
          "ref_texts": "[40] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "40"
          ],
          "3": "PVNet[40] proposes a method that automatically discovers a set of keypoints on the 3D object surface based on the physical structure, to ensure that their 2D projection are all within the silhouette.",
          "4": "[40, 42, 35] use the textured object model and random poses to generate a large amount of synthetic images to augment (or replace) the limited training images, preventing the network from overfitting.",
          "6": "Our method outperforms more than half of the learning-based methods and achieves comparable result with the state-of-the-art method, which use a large amount of synthetic training images from new viewpoints [40] and/or 3D model for refinement [52, 27].",
          "7": "ADD-10 results are shown in T able 3 following the test scheme of [40]."
        },
        "6dof object pose estimation via differentiable proxy voting loss": {
          "authors": [
            "X Yu",
            "Z Zhuang",
            "P Koniusz",
            "H Li"
          ],
          "url": "https://arxiv.org/abs/2002.03923",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "21"
          ],
          "3": "Deep model based methods: Due to the powerful feature representation ability of deep neural networks, deep learning based methods have demonstrated impressive results on object pose estimation [13, 21, 30].",
          "5": "Rather than only estimating a centroid, PVNet [21] votes several features of interest, while the work [12] votes the corners of a 3D boundingbox from each segmentation grid.",
          "6": "Since voting based methods [21, 30] have demonstrated their robustness to occlusions and view changes, we therefore follow the voting based pose estimation pipeline.",
          "8": "3 Network architecture and training strategy To demonstrate the effectiveness of our proposed loss, we adopt the same architecture as PVNet [21], as illustrated in Fig.",
          "9": "Following [21], we render 10,000 images and synthesize 10,000 images by \u201cCut and Paste\" for each object.",
          "10": "Comparisons on LINEMOD: We compare our algorithm with BB8 [22], SSD6D [13], YOLO6D [27], DPOD [31], Pix2Pose [14], CDPN [32], PoseCNN [30], and PVNet [21] on ADD(-S) scores (in Table 1) and 2D projection errors (in Table 2)."
        },
        "3d object detection and pose estimation of unseen objects in color images with local surface embeddings": {
          "authors": [
            "Giorgia Pitteri",
            "Aurelie Bugeau",
            "Slobodan Ilic",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content/ACCV2020/html/Pitteri_3D_Object_Detection_and_Pose_Estimation_of_Unseen_Objects_in_ACCV_2020_paper.html",
          "ref_texts": "8. Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X.: PVNet: Pixe l-Wise Voting Network for 6DoF Pose Estimation. CoRR abs/1812.11788 (2018)",
          "ref_ids": [
            "8"
          ],
          "1": "1 Introduction Deep Learning (DL) provides powerful techniques to estimate the6D pose of an object from color images, and impressive results have been achieve d over the last years [1,2,3,4,5,6], including in the presence of occlusions [7,8,9], in the absence of texture, and for objects with symmetries (which create p ose ambiguities) [10,11]."
        },
        "Occlusion-aware region-based 3D pose tracking of objects with temporally consistent polar-based local partitioning": {
          "authors": [
            "L Zhong",
            "X Zhao",
            "Y Zhang",
            "S Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9003503/",
          "ref_texts": "[46] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u2013",
          "ref_ids": [
            "46"
          ],
          "3": "When GPU is available, deep learning-based methods (such as [44], [46]) could be incorporated for better performance."
        },
        "Super-BPD: Super boundary-to-pixel direction for fast image segmentation": {
          "authors": [
            "Jianqiang Wan",
            "Yang Liu",
            "Donglai Wei",
            "Xiang Bai",
            "Yongchao Xu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Wan_Super-BPD_Super_Boundary-to-Pixel_Direction_for_Fast_Image_Segmentation_CVPR_2020_paper.html",
          "ref_texts": "[32] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InProc. of CVPR , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "32"
          ],
          "1": "PifPaf [24] and PVNet [32] leverage direction cue for 2D human pose estimation and 6 DoF pose estimation, respectively."
        },
        "Deepurl: Deep pose estimation framework for underwater relative localization": {
          "authors": [
            "B Joshi",
            "M Modasshir",
            "T Manderson"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9341201/",
          "ref_texts": "[32] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proc. CVPR, 2019.",
          "ref_ids": [
            "32"
          ],
          "1": "[32] used a pixel-wise voting network to regress pixel-wise unit vectors pointing to the keypoints and used these vectors to vote for keypoint locations using RANSAC.",
          "3": "[15] and PVNet [32] trained on a synthetic dataset and tested on real pool dataset.",
          "4": "[15] and PVNet [32] in terms of rotation and translation errors along with REP-10px and ADD-0."
        },
        "MaskedFusion: Mask-based 6D object pose estimation": {
          "authors": [
            "N Pereira",
            "LA Alexandre"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9356139/",
          "ref_texts": "17. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "17"
          ],
          "3": "One of the most accurate method in 6D pose using RGB images is PVNet [17]."
        },
        "Edge enhanced implicit orientation learning with geometric prior for 6D pose estimation": {
          "authors": [
            "Y Wen",
            "H Pan",
            "L Yang",
            "W Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9126189/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "27"
          ],
          "1": "While [28] and [33] use bounding box corners as keypoints, a recent work [27] explores using designated surface keypoints for more robust 2D keypoint localization."
        },
        "Lit: Light-field inference of transparency for refractive object localization": {
          "authors": [
            "Z Zhou",
            "X Chen",
            "OC Jenkins"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9113653/",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "Other end-to-end method methods have explored using synthetic data in training [3], [13], pixel-wise voting over keypoints [14], [15], and residual networks to iteratively refine object poses [5], [2].",
          "2": "In addition, the center point estimation branch does not regress multiple keypoints which is common in texture-rich object pose estimation networks [14], [15]."
        },
        "Neural object learning for 6d pose estimation using a few cluttered images": {
          "authors": [
            "K Park",
            "T Patten",
            "M Vincze"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58548-8_38",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 2, 3",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23,24,28].",
          "2": "To overcome this limitation, both real images and synthetic images are used for training [23,24,28,45], which currently achieves state-of-the-art performance."
        },
        "6 dof pose estimation of textureless objects from multiple rgb frames": {
          "authors": [
            "R Kaskman",
            "I Shugurov",
            "S Zakharov",
            "S Ilic"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_41",
          "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "40"
          ],
          "1": "In Pixel-wise Voting Network (PVNet) [40].",
          "2": "The network outputs per-seed point classification labels as well as estimation of the keypoint direction vectors, which are further used for the RANSAC-based voting for the object keypoint locations, similar to PVNet [40]."
        },
        "Spatial attention improves iterative 6D object pose estimation": {
          "authors": [
            "S Stev\u0161i\u010d",
            "O Hilliges"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9320380/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2, 3, 5, 6, 16, 17",
          "ref_ids": [
            "21"
          ],
          "1": "To solve this challenging task, a variety of methods such as BB8 [22], PoseCNN [28], PVNet [21] and DPOD [29] have been proposed recently.",
          "6": "State-of-the-art methods [29, 21] do not use hand-designed keypoints.",
          "7": "Contrary, papers for initial pose estimation use advanced techniques, such as RANSAC for keypoint selection [21] or masking for removing occlusions [12].",
          "8": "Neural Network Model At the initial step, we obtain the object pose from an existing one-shot pose estimation algorithm, such as PVNet [21].",
          "9": "Evaluation Metric For evaluation, we use the standard ADD(-S) evaluation metric as is done in most 6D pose estimation papers [21, 20, 29, 17, 11, 1].",
          "10": "The red object outlines show the initial pose obtained from PVNet [21].",
          "11": "We use the SoA in the one-shot setting, PVNet [21], for initialization.",
          "12": "Note that here the accuracy of the initialization method is significantly lower than PVNet [21] which implies that some initial estimates maybe outside of the distribution used during training of our method.",
          "14": "For initialization, we use PVNet [21].",
          "15": "For initialization, we use PVNet [21]."
        },
        "Dronepose: photorealistic uav-assistant dataset synthesis for 3d pose estimation via a smooth silhouette loss": {
          "authors": [
            "G Albanis",
            "N Zioulis",
            "A Dimou",
            "D Zarpalas"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_44",
          "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "48"
          ],
          "1": "PVNet [48] votes for keypoints, which are then used to estimate the object\u2019s 6DOF pose using PnP."
        },
        "Prima6d: Rotational primitive reconstruction for enhanced and robust 6d pose estimation": {
          "authors": [
            "MH Jeon",
            "A Kim"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9123683/",
          "ref_texts": "[1] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d 12 2018.",
          "ref_ids": [
            "1"
          ],
          "3": "To alleviate this issue, PVNet [1] predicted the unit vectors that point to predefined keypoints for each pixel in an object.",
          "4": "Evaluation Metric (i) 2D Projection error metric To evaluate the pose estimation in terms of the 2D projection error we also use the same metric as in [1] and measure the mean pixel distance between projection of the 3D model and the image pixel points.",
          "5": "We chose the average distance metric (ADD) [28] as the evaluation metric, which computes the distance of transformed 3D model points methods Holistic Approach PnP based Approach w/o refinement w/ refinement PoseCNN [19] Deep-6D Pose [16]BB8 [11] Tekin [12] Pix2Pose [15] DPOD [26] PVNet [1] PrimA6D-S PrimA6D-SRBB8 Tekin HybridPose [27] ape\u2020 38.",
          "6": "All values are imported from [1] except pix2pose and hybridpose."
        },
        "Active 6d multi-object pose estimation in cluttered scenarios with deep reinforcement learning": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9340842/",
          "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "To handle occlusion and truncation, keypoint detection with PnP [13, 14] or per-pixel regression/patch-based approaches [6, 9, 7] followed by Hough voting [15, 7] or RANSAC have been proposed."
        },
        "Robust rgb-based 6-dof pose estimation without real pose annotations": {
          "authors": [
            "Z Li",
            "Y Hu",
            "M Salzmann",
            "X Ji"
          ],
          "url": "https://arxiv.org/abs/2008.08391",
          "ref_texts": "23. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "23"
          ],
          "1": "Therefore, recent advances in the field have focused on a deeplearning-based approach [10,32,28,22,24,29,7,23,14,3,33,31,20].",
          "2": "To construct the correspondences, the network is trained to either detect pre-defined object keypoints [24,29,7,23], or regress 3D object coordinates from the image [14,3,33,31,20].",
          "4": "For keypoints-based methods, such as [7,23], where the model is trained to predict a keypoint-relevant representation, self-supervision can be applied using the MCI or MCV loss, as in our approach.",
          "5": "4 PVNet[23] 43.",
          "6": "4 PVNet[23] 15."
        },
        "Multi-view shape estimation of transparent containers": {
          "authors": [
            "A Xompero",
            "R Sanchez-Matilla",
            "A Modas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9054112/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d inProc. IEEE Conf. Comput. Vis. Pattern Recognit., Long Beach, CA, USA, 16\u201320 June 2019.",
          "ref_ids": [
            "6"
          ],
          "2": "[2] LGP \u2713 \u2713 \u2713 \u2713\n[18] DeepIM \u2713 \u2713 3DM\n[7] StoCS \u2713 \u2713 3DM\n[6] PVNet \u2713 \u2713 \u2713 3DM\n[5] DenseFusion \u2713 \u2713 3DM\n[16] SegOPE \u2713 \u2713 \u2713 3DM\n[15] NOCS \u2713 \u2713 \u2713 \u2713 LoDE \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 to estimate the 6 DoF object pose quite accurately, but their training requires large amount of data, usually annotated only for the highlevel object class [14], including depth information and/or known dense 3D models in addition to colour images [5, 6, 7, 15, 16].",
          "4": "Pixel-wise V oting Network (PVNet) [6] estimates the pose of occluded or truncated objects with an uncertainty-driven PnP, learning a vector-field representation to localise a sparse set of 2D keypoints and their spatial uncertainty.",
          "5": "As most of these works target object pose estimation, related comprehensive reviews can be found in [5, 6, 15, 16]."
        },
        "Towards an egocentric framework for rigid and articulated object tracking in virtual reality": {
          "authors": [
            "C Taylor",
            "R McNicholas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9090593/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Comp. Vision and Pattern Recognition, 2019.",
          "ref_ids": [
            "24"
          ],
          "1": "Works such as PoseCNN [39] and PVNet [24] demonstrate accurate 6DoF pose predictions from RGB images, even in complex, uncontrolled environments [1, 21, 38, 40]."
        },
        "3d-aware ellipse prediction for object-based camera pose estimation": {
          "authors": [
            "M Zins",
            "G Simon",
            "MO Berger"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9320405/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.",
          "ref_ids": [
            "24"
          ],
          "1": "Many works exist on this subjects [12, 18, 24, 25, 30, 31, 36]."
        },
        "Pixel-pair occlusion relationship map (p2orm): formulation, inference and application": {
          "authors": [
            "X Qiu",
            "Y Xiao",
            "C Wang",
            "R Marlet"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58548-8_40",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019) P2ORM: Formulation, Inference & Application 17",
          "ref_ids": [
            "36"
          ],
          "1": "Besides the joint treatment of occlusion when developing techniques for specific tasks [40,19,54,36,35,37,18], task-independent occlusion reasoning [42,24,49,53,51,30] offers valuable occlusion-related features for high-level scene understanding tasks."
        },
        "Fully convolutional geometric features for category-level object alignment": {
          "authors": [
            "Qiaojun Feng",
            "Nikolay Atanasov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9341550/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation,\u201d inIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "PVNet [13] estimates sparse object keypoints on the RGB image via voting mechanism."
        },
        "Deep soft procrustes for markerless volumetric sensor alignment": {
          "authors": [
            "V Sterzentsenko",
            "A Doumanoglou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9089541/",
          "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "PVNet [30] densely regresses vectors pointing at the keypoints to improve robustness to occlusions."
        },
        "End-to-end differentiable 6DoF object pose estimation with local and global constraints": {
          "authors": [
            "A Gupta",
            "J Medhi",
            "A Chattopadhyay"
          ],
          "url": "https://arxiv.org/abs/2011.11078",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "9"
          ],
          "1": "A recent study regressed direction vectors to the 2D keypoints from the segmented regions of the object [9].",
          "2": "Their end-to-end trainable model showed improved results compared to the two stage approach as validated with two state of the art correspondence estimators [8] [9].",
          "3": "We follow up on their model with [9] as the correspondence estimator as it shows superior performance and refer to it as SSPE.",
          "4": "This is because a 2D keypoint is given by the intersection of a pair of direction vectors pointing to that keypoint [9].",
          "5": "Part I: Occlusion Linemod Part II: Linemod PVNet [9] DPVR [13] SSPE [10] SSPE-r2 SSPE-ours PVNet [9] DPVR [13] SSPE-r SSPE-ours Ape 15.",
          "6": "The final loss Lto optimize is a linear combination of the cross entropy segmentation loss Ls and L1 vector regression loss Lk from the correspondence estimator [9], and the pose loss Lp and triplet regularization term Lt from the pose estimator.",
          "7": "As per previous studies [9][10], we train separate models for each object.",
          "8": "Similar to previous approaches [9][10], we augment the Linemod train data using synthetic data.",
          "9": "We generate 10000 images containing multiple objects using the cut and paste [17] technique, and 8 \u00d710000 images of single objects using the rendering technique in [9]."
        },
        "Pose proposal critic: Robust pose refinement by learning reprojection errors": {
          "authors": [
            "L Brynte",
            "F Kahl"
          ],
          "url": "https://arxiv.org/abs/2005.06262",
          "ref_texts": "[18] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "18"
          ],
          "2": "[18] does indeed prove robust to partial occlusions, and yields accurate estimates of rotation as well as lateral translation on the Occlusion LINEMOD benchmark.",
          "6": "On the Occlusion LINEMOD benchmark, we initialize our method with pose proposals from PVNet [18], yielding state-of-the-art results for two out of three metrics on this competitive benchmark, while performing on-par with previous methods for the third metric.",
          "7": "Despite the sub-optimal pose proposals from PVNet [18], the poses are accurately recovered."
        },
        "3D point-to-keypoint voting network for 6D pose estimation": {
          "authors": [
            "W Hua",
            "J Guo",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9305322/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
          "ref_ids": [
            "6"
          ],
          "3": "To overcome this problem, PVNet [6] selects kepoints using the farthest point sampling (FPS) algorithm.",
          "4": "Inspired by recent 2D methods [5], [6], we estimate pose by 3D keypoints instead of regression directly.",
          "5": "Before training, FPS algorithm is employed to sample K 3D keypoints on CAD model surface like PVNet [6]."
        },
        "Pose estimation of specular and symmetrical objects": {
          "authors": [
            "J Hu",
            "H Ling",
            "P Parashar",
            "A Naik"
          ],
          "url": "https://arxiv.org/abs/2011.00372",
          "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "17"
          ],
          "1": "Deep learning method Recently, deep learning methods[17][9][7][18] have been used to detect robust keypoint."
        },
        "Kosnet: A unified keypoint, orientation and scale network for probabilistic 6d pose estimation": {
          "authors": [
            "K Hashimoto",
            "DN Ta",
            "E Cousineau",
            "R Tedrake"
          ],
          "url": "https://groups.csail.mit.edu/robotics-center/public_papers/Hashimoto20.pdf",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "19"
          ],
          "1": "More recent works focus on fixing this problem by using local patches to reduce the effect of occlusion [17] or adding a segmentation head to aggregate information only from pixels in the object regions [18], [19].",
          "2": "Several other works [15], [19] realize the benefits of heat maps of keypoints in enabling probabilistic fusion."
        },
        "3DPVNet: Patch-level 3D Hough voting network for 6D pose estimation": {
          "authors": [
            "Y Liu",
            "J Zhou",
            "Y Zhang",
            "C Ding",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2009.06887",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "26"
          ],
          "1": "5D images, such as PVNet [26].",
          "3": "PVNet [26] predicts vectors pointing to 2D keypoints, forming dense correspondences between 2D and 3D data.",
          "4": "Our work is inspired by PVNet [26] and V oteNet [27], and employs deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation."
        },
        "Model-free Bin-Picking: Food Processing and Parcel Processing Use Cases": {
          "authors": [
            "N Castaman",
            "A Cenzato",
            "S Tonello",
            "E Menegatti"
          ],
          "url": "https://i-rim.it/wp-content/uploads/2020/12/I-RIM_2020_paper_153.pdf",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "5"
          ],
          "1": "This problem, often referred to asrandom bin picking, rely on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [6], with an increasingly trend towards datadriven approcehs based on deep models [3], [5]."
        },
        "Instance-specific 6-dof object pose estimation from minimal annotations": {
          "authors": [
            "RP Singh",
            "I Kumagai",
            "A Gabas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9026239/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "2"
          ],
          "3": "The more recent PVNet method [2] regresses to vector fields for each of the pre-defined object keypoints and uses these vectors to vote for keypoint locations using RANSAC."
        },
        "Automatic 3D Object Recognition and Localization for Robotic Grasping": {
          "authors": [
            "BMSE Santo"
          ],
          "url": "https://search.proquest.com/openview/a645f0a6e44b8f38da9372b7d267a074/1?pq-origsite=gscholar&cbl=2026366&diss=y",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019. URL: http://dx.doi.org/10.1109/CVPR.",
          "ref_ids": [
            "29"
          ],
          "3": "It is concluded that PVNet achieves the best results in terms of the ADD(-S) metric when compared to other approaches (Table 3, 5, and 7 in [29])."
        },
        "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images": {
          "authors": [
            "M Vincze"
          ],
          "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490630.pdf",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4556\u20134565 (June 2019)",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23, 24, 28].",
          "2": "To overcome this limitation, both real images and synthetic images are used for training [23, 24, 28, 45], which currently achieves state-of-the-art performance."
        },
        "Model-based 3D Tracking for Augmented Orthopedic Surgery": {
          "authors": [
            "A Murienne",
            "B Labb\u00e9",
            "L Launay"
          ],
          "url": "https://hal.science/hal-03022939/",
          "ref_texts": "[9] S Peng, Y Liu, Q Huang, X Zhou, H Bao. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018). Pvnet: Pixel-wise voting network for 6dof pose estimation. ",
          "ref_ids": [
            "9"
          ],
          "1": "Machine learning based pose inference [9] could also be investigated to initialize registration."
        },
        "Supplement to \u201cPose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors\u201d": {
          "authors": [
            "L Brynte",
            "F Kahl"
          ],
          "url": "https://research.chalmers.se/publication/538336/file/538336_AdditionalFile_dad0f2b5.pdf",
          "ref_texts": "[9] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 9",
          "ref_ids": [
            "9"
          ],
          "1": "Despite the sub-optimal pose proposals from PVNet [9], the poses are accurately recovered.",
          "2": "[8] PVNet [9] PoseCNN [11] + DeepIM [6] PVNet [9] + PPC (Ours) ape 17.",
          "3": "[8] PVNet [9] PoseCNN [11] + DeepIM [6] PVNet [9] + PPC (Ours) ape 69.",
          "4": "BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 5 PVNet [9] PoseCNN [11] + DeepIM [6] PVNet [9] + PPC (Ours) ape 37.",
          "5": "1 Negative Depth Correction of Pose Proposals We observed that the pose proposals from PVNet [9] sometimes have negative depth, and in this case we switched sign for the object center position (in the camera frame), and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera).",
          "6": "This correction is done both when reporting the results of [9], and when reporting the results of our refinement."
        },
        "Review on 6D Object Pose Estimation With the Focus on Indoor Scene Understanding. 2022\u037e 2 (4): 41": {
          "authors": [
            "N Nejatishahidin",
            "P Fayyazsanavi"
          ],
          "url": "https://www.oajaiml.com/uploads/archivepdf/24821141.pdf",
          "ref_texts": "[41] Peng S, Liu Y , Huang Q, Zhou X, Bao H. Pvnet: Pixel-Wise V oting Network for 6DOF Pose Estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019:4561-4570.",
          "ref_ids": [
            "41"
          ],
          "2": "To address the occlusion problem for keypoint detection, PVNet [41] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
          "3": "3DPVNet [72], inspired from V oteNet [73] and pvnet [41], employed deep learning 600 https://www.",
          "4": "The promise of voting-based techniques [41, 72] are being robust to these challenges."
        },
        "Learning to Estimate 3D Object Pose from Synthetic Data": {
          "authors": [
            "Sergey Zakharov"
          ],
          "url": "https://mediatum.ub.tum.de/1550255",
          "ref_texts": "[93] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "93"
          ],
          "5": "If trained on real data, our method is the second best after [93].",
          "7": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [40, 101], by a large margin and performs similarly to [93]."
        },
        "Dpod: 6d pose object detector and refiner": {
          "authors": [
            "Sergey Zakharov",
            "Ivan Shugurov",
            "Slobodan Ilic"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html",
          "ref_texts": "[25] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "Recent deep learning-based approaches, such as SSD6D [15], YOLO6D [33], AAE [31], PoseCNN [34] and PVNet [25], are the current top performers for this task in RGB images.",
          "4": "Among the methods that are specifically designed to be robust to occlusions we would like to highlight iPose [14], PoseCNN [34], and PVNet [25].",
          "5": "PVNet [25] takes a different approach and designs a network which for every pixel in the image regresses an offset to some predefined keypoints.",
          "9": "If trained on real data, our method is the second best after [25].",
          "11": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [33, 34], by a large margin and performs similarly to [25]."
        },
        "Satellite pose estimation with deep landmark regression and nonlinear pose refinement": {
          "authors": [
            "Bo Chen",
            "Jiewei Cao",
            "Alvaro Parra",
            "Jun Chin"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.html",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "Inspired by works that combine the strength of deep neural networks and geometric optimisation [26, 25, 35], our approach contains three main components: 1.",
          "2": "While the keypoint matching problem can be solved using machine learning, deep CNN-based feature learning methods typically fix the 2D-3D keypoint associations and learn to predict the image locations of each corresponding 3D keypoint such as [26, 25, 35]."
        },
        "ConvPoseCNN: Dense convolutional 6D object pose estimation": {
          "authors": [
            "C Capellen",
            "M Schwarz",
            "S Behnke"
          ],
          "url": "https://arxiv.org/abs/1912.07333",
          "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "22"
          ],
          "1": "[22] also removed the RoIpooled orientation prediction branch, but with a different method: Here, 2D directions to a fixed number of keypoints are densely predicted."
        },
        "Cullnet: Calibrated and pose aware confidence scores for object pose estimation": {
          "authors": [
            "Kartik Gupta",
            "Lars Petersson",
            "Richard Hartley"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.html",
          "ref_texts": "[17] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "Recently proposed PV -Net [17] tries to address the problem of partial occlusion in RGB based object pose estimation by regressing for dense pixel-wise unit vectors pointing to the keypoints, which are combined together using RANSAC like voting scheme."
        },
        "CorNet: generic 3D corners for 6D pose estimation of new objects without retraining": {
          "authors": [
            "Giorgia Pitteri",
            "Slobodan Ilic",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.html",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Figure 9: Some qualitative results on Object #20 in Scene #13 of the T -LESS dataset. Figure 10: Some qualitative results on Object #20 in Scene #14 of the T -LESS dataset. Figure 11: Some qualitative results on Object #26 and Object #29 in Scene #15 of the T -LESS dataset. Pixel-Wise V oting Network for 6DoF Pose Estimation. CoRR, abs/1812.11788, 2018.",
          "ref_ids": [
            "24"
          ],
          "2": "Also focusing on occlusion handling, PVNet [24] proposed a network that for each pixel regresses an offset to predefined keypoints.",
          "3": "Somewhat related to our approach, [18, 4, 37, 24] first predict the 3D coordinates of the image locations lying on the objects, in the object coordinate system, and predict the 3D object pose through hypotheses sampling with preemptive RANSAC."
        },
        "W-posenet: Dense correspondence regularized pixel pair pose regression": {
          "authors": [
            "Z Xu",
            "K Chen",
            "K Jia"
          ],
          "url": "https://arxiv.org/abs/1912.11888",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "Intuitively, compared to sparse semantic keypoints pre-defined in keypoint-based methods [16], [17], [18], [19], [20], dense correspondence mapping in our scheme treats each pixel as a keypoint to regress its corresponding 3D coordinate in object model space, which makes each pixel-wise feature more discriminative, and thus our pixel-wise feature encoding is more robust to occlusion.",
          "2": "Recently, PVNet [20] is proposed to detect keypoints via voting on pixel-wise predictions of the directional vector that points to keypoints and is robust to truncation and occlusion."
        },
        "Car Pose in Context: Accurate Pose Estimation with Ground Plane Constraints": {
          "authors": [
            "P Li",
            "W Qiu",
            "M Peven",
            "GD Hager",
            "AL Yuille"
          ],
          "url": "https://arxiv.org/abs/1912.04363",
          "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "36"
          ],
          "1": "More recent methods, including SOTA method [36] in 6-DoF pose estimation, adopts a two-stage method."
        }
      }
    },
    {
      "title": "prior guided dropout for robust visual localization in dynamic environments",
      "id": 17,
      "valid_pdf_number": "30/42",
      "matched_pdf_number": "25/30",
      "matched_rate": 0.8333333333333334,
      "citations": {
        "A survey on deep learning for localization and mapping: Towards the age of spatial machine intelligence": {
          "authors": [
            "C Chen",
            "B Wang",
            "CX Lu",
            "N Trigoni"
          ],
          "url": "https://arxiv.org/abs/2006.12567",
          "ref_texts": "[148] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u201cPrior guided dropout for robust visual localization in dynamic environments,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV) , pp. 2791\u20132800, 2019.",
          "ref_ids": [
            "148"
          ],
          "2": "Similarly, a prior guided dropout mask is additionally adopted in RVL [148] to further eliminate the uncertainty caused by dynamic objects."
        },
        "Diffloc: Diffusion model for outdoor lidar localization": {
          "authors": [
            "Wen Li",
            "Yuyang Yang",
            "Shangshu Yu",
            "Guosheng Hu",
            "Chenglu Wen",
            "Ming Cheng",
            "Cheng Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Li_DiffLoc_Diffusion_Model_for_Outdoor_LiDAR_Localization_CVPR_2024_paper.html",
          "ref_texts": "[11] Zhaoyang Huang, Y an Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior guided dropout for robust visual localization in dynamic environments. InICCV, pages 2791\u20132800, 2019.",
          "ref_ids": [
            "11"
          ],
          "2": "Previous studies [1, 11, 28, 36] suggest that foundation models in images can improve the performance of point cloud tasks, inspiring us to investigate foundation models for LiDAR localization."
        },
        "Hypliloc: Towards effective lidar pose regression with hyperbolic fusion": {
          "authors": [
            "Sijie Wang",
            "Qiyu Kang",
            "Rui She",
            "Wei Wang",
            "Kai Zhao",
            "Yang Song",
            "Wee Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_HypLiLoc_Towards_Effective_LiDAR_Pose_Regression_With_Hyperbolic_Fusion_CVPR_2023_paper.html",
          "ref_texts": "[16] Zhaoyang Huang, Yan Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior guided dropout for robust visual localization in dynamic environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2791\u20132800, 2019. 2, 7",
          "ref_ids": [
            "16"
          ],
          "1": "AD-PoseNet [16] leverages semantic masks to drop out the dynamic area in the image."
        },
        "SGLoc: Scene geometry encoding for outdoor LiDAR localization": {
          "authors": [
            "Wen Li",
            "Shangshu Yu",
            "Cheng Wang",
            "Guosheng Hu",
            "Siqi Shen",
            "Chenglu Wen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2023_paper.html",
          "ref_texts": "[21] Zhaoyang Huang, Yan Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior guided dropout for robust visual localization in dynamic environments. In ICCV, pages 2791\u20132800, 2019. 1",
          "ref_ids": [
            "21"
          ],
          "1": "Prior efforts have tried to minimize the relative pose or photometric errors to add geometry constraints by pose graph optimization (PGO) [4, 21] or novel view synthesis (NVS) [10,11]."
        },
        "Regression-based camera pose estimation through multi-level local features and global features": {
          "authors": [
            "M Xu",
            "Z Zhang",
            "Y Gong",
            "S Poslad"
          ],
          "url": "https://www.mdpi.com/1424-8220/23/8/4063",
          "ref_texts": "13. Huang, Z.; Xu, Y.; Shi, J.; Zhou, X.; Bao, H.; Zhang, G. Prior guided dropout for robust visual localization in dynamic environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, 27 October\u20132 November 2019; pp. 2791\u20132800.",
          "ref_ids": [
            "13"
          ],
          "1": "Regression-based methods for camera pose estimation can either learn the mapping from image pixels to absolute poses [11] or learn the relative poses of a pair of images, as in MapNet [12], PVL [13], and other methods.",
          "2": "Later, the adversarial network [41] and novel DNN [13] are added to share the same loss function."
        },
        "A critical analysis of image-based camera pose estimation techniques": {
          "authors": [
            "Meng Xu",
            "Youchen Wang",
            "Bin Xu",
            "Jun Zhang",
            "Jian Ren",
            "Stefan Poslad",
            "Pengfei Xu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231223012481",
          "ref_texts": "[106] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, G. Zhang, Prior guided dropout for robust visual localization in dynamic environments, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp.",
          "ref_ids": [
            "106"
          ],
          "1": "PVL [106] adopted a priorguided dropout mask to avoid the influence of uncertainty of dynamic objects in dynamic environments.",
          "2": "PVL [106] adopted a priorguided dropout mask to avoid the influence of uncertainty of dynamic objects in dynamic environments.",
          "3": "PoseNet [103] GoogLeNet 1FC+1normalisation layer for orientation to unit length AtLoc [99] ResNet-34 1FC AdPR [104] ResNet-18 1FC PVL [106] Prior Guided Dropout + Resnet34 Composite Self-Attention + 1FC APANet [105] ResNet-34 1FC SPPNet [107] 3\u00d7 (4 layers of 1 \u00d71 convolutions) + Spatial Pyramid maxpooling units 3FC Other Loss Geo.",
          "4": "PoseNet [103] 2017-CVPR-384 l\u03c3(I) =lx(I)exp(\u2212\u02c6sx) + \u02c6sx +lq(I)exp(\u2212\u02c6sq) + \u02c6sq AtLoc [99] 2019-AAAI-6 AdPR [104] 2019-ICCVW-5 PVL [106] 2019-ICCV-9 APANet [105] 2020-ECCVW-0 SPPNet [107] 2018-BMVC-6 Other Loss Geo.",
          "6": "3 / \u2713 AdPR [104] Motion blur, repeating structures, texture-less surfaces Nvidia GeForce RTX 2080 50 / / PVL [106] Dynamic environments / / / \u2713 APANet [105] Lighting, viewpoint / / / / SPPNet [107] Unevenly distributed image features Nvidia Titan X 2 36."
        },
        "Lisa: Lidar localization with semantic awareness": {
          "authors": [
            "Bochun Yang",
            "Zijun Li",
            "Wen Li",
            "Zhipeng Cai",
            "Chenglu Wen",
            "Yu Zang",
            "Matthias Muller",
            "Cheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Yang_LiSA_LiDAR_Localization_with_Semantic_Awareness_CVPR_2024_paper.html",
          "ref_texts": "[17] Zhaoyang Huang, Y an Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior guided dropout for robust visual localization in dynamic environments. InICCV, pages 2791\u20132800, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "For instance, AD-PoseNet[17] utilizes Mask RCNN[12] to minimize the impact of dynamic foreground objects on scene understanding."
        },
        "Hr-apr: Apr-agnostic framework with uncertainty estimation and hierarchical refinement for camera relocalisation": {
          "authors": [
            "C Liu",
            "S Chen",
            "Y Zhao",
            "H Huang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610903/",
          "ref_texts": "[18] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u201cPrior guided dropout for robust visual localization in dynamic environments,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 2791\u20132800.",
          "ref_ids": [
            "18"
          ],
          "1": "In AD-PoseNet [18], uncertainty is quantified via prior guided dropout.",
          "3": "Similarly, AD-PoseNet [18] evaluates pose distribution by generating multiple hypotheses via prior guided dropout."
        },
        "Coordinet: uncertainty-aware pose regressor for reliable vehicle localization": {
          "authors": [
            "Arthur Moreau",
            "Nathan Piasco",
            "Dzmitry Tsishkou",
            "Bogdan Stanciulescu",
            "La Fortelle"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2022/html/Moreau_CoordiNet_Uncertainty-Aware_Pose_Regressor_for_Reliable_Vehicle_Localization_WACV_2022_paper.html",
          "ref_texts": "[9] Zhaoyang Huang, Yan Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior guided dropout for robust visual localization in dynamic environments. In Proceedings of the IEEE International Conference on Computer Vision, volume 2019-Octob, pages 2791\u20132800, 2019.",
          "ref_ids": [
            "9"
          ],
          "1": "They generate multiple hypothesis for each single image at inference time, and then compute mean and variance to estimate pose and uncertainty [10, 9].",
          "2": "Several innovations have been proposed to improve the architecture of the network: Hourglass network [17] proposes an encoder-decoder architecture, AtLoc [31] and RVL [9] use an attention module before the fully-connected layers.",
          "4": "RVL [9] proposes a prior guided dropout on input image: it removes areas where dynamic objects appear and allows to generate a Monte Carlo sample too.",
          "6": "[9] to compare performances with related methods (ADPoseNet) and report results in table 3.",
          "7": "CoordiNet (ours) AD PoseNet [9] Oxford median 1.",
          "8": "These results outperform methods with pose graph optimization reported in [3, 9]."
        },
        "Unloc: a universal localization method for autonomous vehicles using lidar, radar and/or camera input": {
          "authors": [
            "M Ibrahim",
            "N Akhtar",
            "S Anwar"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10342046/",
          "ref_texts": "[26] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u201cPrior guided dropout for robust visual localization in dynamic environments,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2791\u20132800.",
          "ref_ids": [
            "26"
          ],
          "1": "The state-of-the-art camera-based localization methods [25], [17], [26] utilize a pre-trained ResNet model [27] as a features extractor."
        },
        "Rethinking generic camera models for deep single image camera calibration to recover rotation and fisheye distortion": {
          "authors": [
            "N Wakai",
            "S Sato",
            "Y Ishii",
            "T Yamashita"
          ],
          "url": "https://arxiv.org/abs/2111.12927",
          "ref_texts": "27. Huang, Z., Xu, Y., Shi, J., Zhou, X., Bao, H., Zhang, G.: Prior guided dropout for robust visual localization in dynamic environments. In: Proceedings of IEEE/CVF Rethinking Generic Camera Models 17 International Conference on Computer Vision (ICCV). pp. 2791\u20132800 (2019). https://doi.org/10.1109/ICCV.2019.00288",
          "ref_ids": [
            "27"
          ],
          "1": "Calibration methods for only extrinsic parameters have been proposed that are aimed at narrow view cameras [27,48,56,57,62,63] and panoramic360\u25e6 images [16]."
        },
        "Mobilearloc: On-device robust absolute localisation for pervasive markerless mobile ar": {
          "authors": [
            "C Liu",
            "Y Zhao",
            "T Braud"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10503320/",
          "ref_texts": "[16] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u201cPrior guided dropout for robust visual localization in dynamic environments,\u201d in IEEE/CVF international conference on computer vision , 2019.",
          "ref_ids": [
            "16"
          ],
          "1": "Bayesian PoseNet [15] and AD-PoseNet [16] model the uncertainty by measuring the variance of several inferences of the same input data."
        },
        "SIRe-Networks: Convolutional neural networks architectural extension for information preservation via skip/residual connections and interlaced auto-encoders": {
          "authors": [
            "Danilo Avola",
            "Luigi Cinque",
            "Alessio Fagioli",
            "Gian Luca"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0893608022002453",
          "ref_texts": "[40] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, G. Zhang, Prior guided dropout for robust visual localization in dynamic environments, in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 2791\u20132800.",
          "ref_ids": [
            "40"
          ],
          "1": "Well-known examples of such approaches comprise a dropout strategy [39], to reduce overfitting over the training dataset distribution by omitting random units at training time for more robust input representations [40]; and batch normalization [41, 42], to facilitate deep networks training by normalizing a given batch according to a specific strategy such as the batch mean and variance [43]."
        },
        "KS-APR: Keyframe Selection for Robust Absolute Pose Regression": {
          "authors": [
            "C Liu",
            "Y Zhao",
            "T Braud"
          ],
          "url": "https://arxiv.org/abs/2308.05459",
          "ref_texts": "[19] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang. Prior guided dropout for robust visual localization in dynamic environments. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2791\u20132800, 2019.",
          "ref_ids": [
            "19"
          ],
          "1": "Similarly, AD-PoseNet [19] evaluates pose distribution by generating multiple poses via prior guided dropout."
        },
        "Benchmarking visual-inertial deep multimodal fusion for relative pose regression and odometry-aided absolute pose regression": {
          "authors": [
            "F Ott",
            "NL Raichur",
            "D R\u00fcgamer",
            "T Feigl"
          ],
          "url": "https://arxiv.org/abs/2208.00919",
          "ref_texts": "[59] Zhaoyang Huang, Yan Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, and Guofeng Zhang. Prior Guided Dropout for Robust Visual Localization in Dynamic Environments. In ICCV, pages 2791\u20132800, Seoul, Korea, October 2019.",
          "ref_ids": [
            "59"
          ],
          "1": "[59] add a prior guided dropout module before PoseNet with spatial and channel attention modules to guide CNNs to ignore foreground objects.",
          "2": "The dropout module by [59] enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified."
        },
        "Robust Localization with Visual-Inertial Odometry Constraints for Markerless Mobile AR": {
          "authors": [
            "C Liu",
            "Y Zhao",
            "T Braud"
          ],
          "url": "https://arxiv.org/abs/2308.05394",
          "ref_texts": "[21] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang. Prior guided dropout for robust visual localization in dynamic environments. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2791\u20132800, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "Bayesian PoseNet [22], AD-PoseNet [21], BMDN [7, 13], VaPoR [53] and CoordiNet [29] can output poses and uncertainty simultaneously, but they are less accurate than many APR methods that only provide estimated poses.",
          "2": "AD-PoseNet [21] proposes an uncertainty-aware PGO that only optimizes unreliable poses using multiple images."
        },
        "LieposeNet: Heterogeneous loss function based on lie group for significant speed-up of PoseNet training process": {
          "authors": [
            "M Kurenkov",
            "I Kalinov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10011660/",
          "ref_texts": "[28] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u201cPrior guided dropout for robust visual localization in dynamic environments,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2791\u20132800.",
          "ref_ids": [
            "28"
          ],
          "1": "improved the PoseNet architecture to work with dynamic objects by using a neural network that ignored the front objects in the image because they were often dynamic [28]."
        },
        "On the Application of Efficient Neural Mapping to Real-Time Indoor Localisation for Unmanned Ground Vehicles": {
          "authors": [
            "Christopher Holder"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10601279/",
          "ref_texts": "[26] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao and G. Zhang, \"Prior guided dropout for robust visual localization in dynamic environments,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. ",
          "ref_ids": [
            "26"
          ],
          "1": "exploiting temporal constraints in image sequences [23], geometry aware loss functions [24], joint prediction of pose and visual odometry between image pairs [25], masking of dynamic objects that could otherwise confound localisation [26], and joint prediction of pose and semantic segmentation [27]."
        },
        "A Review of Recurrent Neural Network Based Camera Localization for Indoor Environments": {
          "authors": [
            "Muhammad Shamsul"
          ],
          "url": "http://eprints.utm.my/104837/1/MuhammadShamsulAlamFarhanMohamedAliSelamat2023_AReviewofRecurrentNeuralNetworkBasedCamera.pdf",
          "ref_texts": "[140] Z. Huang, Y . Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, \u2018\u2018Prior guided dropout for robust visual localization in dynamic environments,\u2019\u2019 in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2791\u20132800.",
          "ref_ids": [
            "140"
          ],
          "1": "To minimize the impact of the uncertainty of dynamic objects in dynamic contexts, PVL [140] implements a prior guided dropout mask."
        }
      }
    },
    {
      "title": "generating human motion in 3d scenes from text descriptions",
      "id": 36,
      "valid_pdf_number": "12/12",
      "matched_pdf_number": "6/12",
      "matched_rate": 0.5,
      "citations": {
        "Smoodi: Stylized motion diffusion model": {
          "authors": [
            "L Zhong",
            "Y Xie",
            "V Jampani",
            "D Sun",
            "H Jiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73232-4_23",
          "ref_texts": "4. Cen, Z., Pi, H., Peng, S., Shen, Z., Yang, M., Zhu, S., Bao, H., Zhou, X.: Generating human motion in 3d scenes from text descriptions. In: CVPR (2024)",
          "ref_ids": [
            "4"
          ],
          "1": "1 Human Motion Generation Human motion generation has attracted great attention [4,5,7,9,14,16,20,32, 34\u201336,47,50,51,57,58,63]."
        },
        "SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control": {
          "authors": [
            "X Zhang",
            "S Starke",
            "V Guzov",
            "Z Zhang"
          ],
          "url": "https://arxiv.org/abs/2412.15664",
          "ref_texts": "[6] Cen, Z., Pi, H., Peng, S., Shen, Z., Yang, M., Shuai, Z., Bao, H., Zhou, X.: Generating human motion in 3d scenes from text descriptions. In: CVPR (2024) 3, 4",
          "ref_ids": [
            "6"
          ],
          "1": "In the context of human-scene interactions, a significant portion of the work is dedicated to generating short-term motion within 3D scenes [6, 71, 72]."
        },
        "Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation": {
          "authors": [
            "H Pi",
            "R Guo",
            "Z Shen",
            "Q Shuai",
            "Z Hu",
            "Z Wang"
          ],
          "url": "https://arxiv.org/abs/2412.13111",
          "ref_texts": "[10] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1855\u20131866, 2024. 3",
          "ref_ids": [
            "10"
          ],
          "1": "Additionally, [10, 55, 78, 82] generate motion with scene information."
        },
        "InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions": {
          "authors": [
            "S Xu",
            "HY Ling",
            "YX Wang",
            "LY Gui"
          ],
          "url": "https://arxiv.org/abs/2502.20390",
          "ref_texts": "[8] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In CVPR, 2024. 2",
          "ref_ids": [
            "8"
          ],
          "1": "Related Work Significant progress has been made in human interaction animation and control, with advancements in areas such as human-human interactions [31, 44, 55, 75, 81, 84, 144, 158, 161], hand-object interactions [1, 7, 14, 94, 101, 121, 128, 128, 139, 145, 159, 168\u2013170, 178, 183, 195, 197], single-frame interactions [20, 43, 56, 60, 73, 107, 141, 149, 153, 163\u2013167, 181, 184, 190, 192], human interactions with static scenes [8, 9, 16, 36, 57, 65, 69, 85, 95, 99, 125, 136, 137, 140, 152, 185, 188, 193], and real-world humanoid control for object manipulation [4, 11, 12, 15, 23, 25, 27, 28, 33, 38, 39, 48, 52, 54, 72, 76, 86, 115, 135, 175, 177]."
        },
        "Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis": {
          "authors": [
            "J Gong",
            "C Zhang",
            "F Liu",
            "K Fan",
            "Q Zhou",
            "X Tan"
          ],
          "url": "https://arxiv.org/abs/2412.02261",
          "ref_texts": "[5] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1855\u20131866, 2024. 3",
          "ref_ids": [
            "5"
          ],
          "1": "Synthesizing realistic human motion in various scenes has garnered much attention in recent years [5, 16, 19, 23, 26, 50, 56]."
        },
        "MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control Flow": {
          "authors": [
            "Z Li",
            "Y He",
            "L Zhong",
            "W Shen",
            "Q Zuo",
            "L Qiu"
          ],
          "url": "https://arxiv.org/abs/2412.09901",
          "ref_texts": "[4] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1855\u20131866, 2024. 3",
          "ref_ids": [
            "4"
          ],
          "1": "Inspired by the efficacy of diffusion models for control and conditioning, several works have controlled pretrained motion diffusion models to follow constraints such as trajectories [20, 36, 41], 3D scenes [4, 43], and object interactions [7, 26, 39]."
        }
      }
    },
    {
      "title": "smap: single-shot multi-person absolute 3d pose estimation",
      "id": 11,
      "valid_pdf_number": "65/88",
      "matched_pdf_number": "51/65",
      "matched_rate": 0.7846153846153846,
      "citations": {
        "Deep learning-based human pose estimation: A survey": {
          "authors": [
            "C Zheng",
            "W Wu",
            "C Chen",
            "T Yang",
            "S Zhu",
            "J Shen"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3603618",
          "ref_texts": "[300] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. 2020. SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation. In ECCV.",
          "ref_ids": [
            "300"
          ],
          "2": "[300] leveraged a depth-aware part association algorithm to assign joints to individuals by reasoning about inter-person occlusion and bone-length constraints."
        },
        "Recent advances of monocular 2D and 3D human pose estimation: A deep learning perspective": {
          "authors": [
            "W Liu",
            "Q Bao",
            "Y Sun",
            "T Mei"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3524497",
          "ref_texts": "[177] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, \u201cSmap: Single-shot multi-person absolute 3d pose estimation,\u201d in ECCV, 2020.",
          "ref_ids": [
            "177"
          ],
          "2": "In addition, SMAP [177] estimates multiple maps, representing the body root depth and part relative-depth at each position."
        },
        "Putting people in their place: Monocular regression of 3d people in depth": {
          "authors": [
            "Yu Sun",
            "Wu Liu",
            "Qian Bao",
            "Yili Fu",
            "Tao Mei",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.html",
          "ref_texts": "[48] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3D pose estimation. In ECCV, pages 550\u2013",
          "ref_ids": [
            "48"
          ],
          "5": "SMAP [48] and HMOR [38] employ a 2D depth map to represent the root depth of 3D pose at each pixel.",
          "8": "We first compare with the most competitive methods [11,25,48], which solve depth relations in monocular images."
        },
        "Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization": {
          "authors": [
            "Yu Zhan",
            "Fenghai Li",
            "Renliang Weng",
            "Wongun Choi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhan_Ray3D_Ray-Based_3D_Human_Pose_Estimation_for_Monocular_Absolute_3D_CVPR_2022_paper.html",
          "ref_texts": "[47] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: single-shot multiperson absolute 3d pose estimation. In ECCV, volume 12360, pages 550\u2013566, 2020. 2, 3",
          "ref_ids": [
            "47"
          ],
          "1": "To resolve these ambiguities, a number of monocular 3D human estimation approaches have been proposed [4,10,29, 32, 44, 47].",
          "2": "In contrast, [29, 47] rely on image-based human depth estimation for absolute root-keypoint localization.",
          "3": "Image based 3D human pose estimation Image based approaches aim to improve the 3D estimation accuracy by directly utilizing image features [1, 17, 18, 20,21,37\u201339,45,47,49].",
          "4": "Alternatively, [37\u201339, 47] estimate the root depth from the full image up to a scale."
        },
        "Vision-based human pose estimation via deep learning: A survey": {
          "authors": [
            "G Lan",
            "Y Wu",
            "F Hu",
            "Q Hao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9955393/",
          "ref_texts": "[148] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, \u201cSmap: Single-shot multi-person absolute 3D pose estimation,\u201d in European Conference on Computer Vision . Springer, 2020, pp. 550\u2013",
          "ref_ids": [
            "148"
          ],
          "2": "io/learnable-triangulation/ A SOTA multi-view approach SMAP [148] https://github."
        },
        "Psvt: End-to-end multi-person 3d pose and shape estimation with progressive video transformers": {
          "authors": [
            "Zhongwei Qiu",
            "Qiansheng Yang",
            "Jian Wang",
            "Haocheng Feng",
            "Junyu Han",
            "Errui Ding",
            "Chang Xu",
            "Dongmei Fu",
            "Jingdong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Qiu_PSVT_End-to-End_Multi-Person_3D_Pose_and_Shape_Estimation_With_Progressive_CVPR_2023_paper.html",
          "ref_texts": "[56] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. In ECCV, pages 550\u2013",
          "ref_ids": [
            "56"
          ],
          "2": "83 SMAP [56] 31."
        },
        "Learning to estimate robust 3d human mesh from in-the-wild crowded scenes": {
          "authors": [
            "Hongsuk Choi",
            "Gyeongsik Moon",
            "Kyu Park",
            "Kyoung Mu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Choi_Learning_To_Estimate_Robust_3D_Human_Mesh_From_In-the-Wild_Crowded_CVPR_2022_paper.html",
          "ref_texts": "[53] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3D pose estimation. In ECCV, 2020. 3",
          "ref_ids": [
            "53"
          ],
          "1": "Several methods [29, 47, 53] have shown reasonable results on multi-person 3D benchmarks [16, 26]."
        },
        "Single-stage is enough: Multi-person absolute 3D pose estimation": {
          "authors": [
            "Lei Jin",
            "Chenyang Xu",
            "Xiaojuan Wang",
            "Yabo Xiao",
            "Yandong Guo",
            "Xuecheng Nie",
            "Jian Zhao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Jin_Single-Stage_Is_Enough_Multi-Person_Absolute_3D_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[39] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InECCV, pages 550\u2013566, 2020. 1, 2, 3, 4, 6, 7, 8",
          "ref_ids": [
            "39"
          ],
          "2": "\u2022 DRM achieves comparable performance with the most top-down methods and significantly outperforms the state-of-the-art bottom-up method [39] by 4.",
          "10": "point compared to SMAP [39], which is current the state-of-the-art bottom-up method.",
          "14": "It can be observed that our model significantly outperforms the state-of-the-art bottom-up method SMAP [39] in terms of RtError by a large margin, i."
        },
        "Body meshes as points": {
          "authors": [
            "Jianfeng Zhang",
            "Dongdong Yu",
            "Jun Hao",
            "Xuecheng Nie",
            "Jiashi Feng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Body_Meshes_as_Points_CVPR_2021_paper.html",
          "ref_texts": "[73] Jianan Zhen, Qi Fang, Jiaming Sun, W entao Liu, W ei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InECCV, 2020.",
          "ref_ids": [
            "73"
          ],
          "1": "Inspired by the recent success of depth estimation for human body joints [42, 73], we propose to take the depth of each person (center point) predicted by a model pre-trained on 3D datasets with depth annotations as the pseudo ordinal relation for model training on the in-the-wild data, which is experimentally proved beneficial to depth-coherent body mesh reconstruction."
        },
        "Loose inertial poser: Motion capture with IMU-attached loose-wear jacket": {
          "authors": [
            "Chengxu Zuo",
            "Yiming Wang",
            "Lishuang Zhan",
            "Shihui Guo",
            "Xinyu Yi",
            "Feng Xu",
            "Yipeng Qin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zuo_Loose_Inertial_Poser_Motion_Capture_with_IMU-attached_Loose-Wear_Jacket_CVPR_2024_paper.html",
          "ref_texts": "[59] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, pages 550\u2013566. Springer, 2020. 2",
          "ref_ids": [
            "59"
          ],
          "1": "Single-camera 2D/3D pose estimation methods such as HRNet [45], SMAP [59], PARE [20], ViTPose[51] and others [23, 51, 52, 58] have had remarkable progress on human pose estimation."
        },
        "Reconstructing 3d human pose by watching humans in the mirror": {
          "authors": [
            "Qi Fang",
            "Qing Shuai",
            "Junting Dong",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.html",
          "ref_texts": "[66] Jianan Zhen, Qi Fang, Jiaming Sun, W entao Liu, W ei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InECCV, 2020.",
          "ref_ids": [
            "66"
          ],
          "1": "Some recent works [36, 66, 29, 30, 12] explore the representation of the absolute depth in the camera coordinate system.",
          "2": "4 SMAP [66] 37.",
          "3": "4 SMAP [66]+MiHu 42.",
          "4": "W e choose the top-down method [36] and the bottom-up method [66] for evaluation.",
          "5": "Following previous methods [36, 66], AP 25 root, PCK rel and PCK abs are measured.",
          "6": "For bottom-up methods, we also improve the performance of [66] apparently."
        },
        "Escnet: Gaze target detection with the understanding of 3d scenes": {
          "authors": [
            "Jun Bao",
            "Buyu Liu",
            "Jun Yu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Bao_ESCNet_Gaze_Target_Detection_With_the_Understanding_of_3D_Scenes_CVPR_2022_paper.html",
          "ref_texts": "[45] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InEuropean Conference on Computer Vision, pages 550\u2013566. Springer, 2020. 7",
          "ref_ids": [
            "45"
          ],
          "1": "We adopt pre-trained SMAP [45] as fkd3."
        },
        "Monocular 3D multi-person pose estimation by integrating top-down and bottom-up networks": {
          "authors": [
            "Yu Cheng",
            "Bo Wang",
            "Bo Yang",
            "Robby T. Tan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Cheng_Monocular_3D_Multi-Person_Pose_Estimation_by_Integrating_Top-Down_and_Bottom-Up_CVPR_2021_paper.html",
          "ref_texts": "[53] Jianan Zhen, Qi Fang, Jiaming Sun, W entao Liu, W ei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InProceedings of the European Conference on Computer V ision (ECCV) , 2020.",
          "ref_ids": [
            "53"
          ],
          "2": "[53, 24, 22]",
          "5": "[53]",
          "9": "[53]"
        },
        "Explicit occlusion reasoning for multi-person 3d human pose estimation": {
          "authors": [
            "Q Liu",
            "Y Zhang",
            "S Bai",
            "A Yuille"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20065-6_29",
          "ref_texts": "84. Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H., Zhou, X.: Smap: Singleshot multi-person absolute 3d pose estimation. In: European Conference on Computer Vision. pp. 550\u2013566. Springer (2020) 1, 5, 6, 8, 10, 11, 12",
          "ref_ids": [
            "84"
          ],
          "3": "[84]",
          "4": "[84]",
          "6": "[84]",
          "8": "[84]",
          "10": "[84]",
          "11": "[84]"
        },
        "Deep learning for 3d human pose estimation and mesh recovery: A survey": {
          "authors": [
            "Y Liu",
            "C Qiu",
            "Z Zhang"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231224008208",
          "ref_texts": "[116] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, X. Zhou, Smap: Single-shot multi-person absolute 3d pose estimation, in: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, Springer, 2020, pp. 550\u2013566.",
          "ref_ids": [
            "116"
          ],
          "2": "[116]"
        },
        "Dual networks based 3d multi-person pose estimation from monocular video": {
          "authors": [
            "Y Cheng",
            "B Wang",
            "RT Tan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9763389/",
          "ref_texts": "[55] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, \u201cSmap: Single-shot multi-person absolute 3d pose estimation,\u201d in Proceedings of the European Conference on Computer Vision (ECCV) , 2020.",
          "ref_ids": [
            "55"
          ],
          "3": ", [53], [54], [55]) achieve comparable performance in multi-person datasets without using human detection.",
          "4": "The top-down method is RootNet [35], the bottom-up method is SMAP [55].",
          "5": "Bottom-Up Monocular 3D Human Pose EstimationA few bottom-up methods have been proposed [34], [36], [37], [54], [55].",
          "9": "6 shows the comparison among a SOTA bottom-up method SMAP [55], our bottom-up network, top-down network, and full model.",
          "10": "First row shows the images from two video clips; second row shows the results from SMAP [55]; third row shows the result of of our bottom-up (BU) network; fourth row shows the results of our top-down (TD) network; last row shows the results of our full model.",
          "11": "To further demonstrate the performance of our method compared with the SOTA 3D multi-person pose estimation methods, we provide additional qualitative results of our method compared with that of the SOTA bottom-up method SMAP [55] and the SOTA top-down method RootNet [35] on four video clips from MuPoTS dataset, as shown in Fig."
        },
        "Learning human mesh recovery in 3D scenes": {
          "authors": [
            "Zehong Shen",
            "Zhi Cen",
            "Sida Peng",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.html",
          "ref_texts": "[33] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3d pose estimation. In ECCV, 2020. 3, 7",
          "ref_ids": [
            "33"
          ],
          "1": "5D manner following SMAP [33].",
          "2": "3 shows that the predicted human root position is improved progressively by the refinement and scene-aware HMR modules, where the initial prediction [33] is improved 44% / 52% in RICH, and 64% / 69% in PROX."
        },
        "Virtualpose: Learning generalizable 3d human pose models from virtual data": {
          "authors": [
            "J Su",
            "C Wang",
            "X Ma",
            "W Zeng",
            "Y Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_4",
          "ref_texts": "47. Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H., Zhou, X.: Smap: Single-shot multi-person absolute 3d pose estimation. In: ECCV. pp. 550\u2013566. Springer (2020)",
          "ref_ids": [
            "47"
          ],
          "5": "(SMAP) [47] 63.",
          "6": "(SMAP) [47] 38."
        },
        "Instance-aware Contrastive Learning for Occluded Human Mesh Reconstruction": {
          "authors": [
            "Gyeong Gwon",
            "Mun Um",
            "Sik Cheong",
            "Wonjun Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Gwon_Instance-aware_Contrastive_Learning_for_Occluded_Human_Mesh_Reconstruction_CVPR_2024_paper.html",
          "ref_texts": "[49] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. InProc. Eur. Conf. Comput. Vis., pages 550\u2013566, 2020. 3",
          "ref_ids": [
            "49"
          ],
          "1": "[31] proposed to apply the knowledge transfer technique to the 3D keypoint detection [49] for successfully reasoning invisible body parts under occlusions."
        },
        "Snipper: A spatiotemporal transformer for simultaneous multi-person 3d pose estimation tracking and forecasting on a video snippet": {
          "authors": [
            "S Zou",
            "Y Xu",
            "C Li",
            "L Ma",
            "L Cheng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10041951/",
          "ref_texts": "[45] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, \u201cSmap: Single-shot multi-person absolute 3d pose estimation,\u201d in European Conference on Computer Vision , 2020.",
          "ref_ids": [
            "45"
          ],
          "1": "8, AUGUST 2015 3 Single-stage methods are emerging in recent years for both pose estimation [11], [36], [37], [40]\u2013[42], [45], [46] and parametric body mesh estimation [38]."
        },
        "Crowd3D: Towards hundreds of people reconstruction from a single image": {
          "authors": [
            "Hao Wen",
            "Jing Huang",
            "Huili Cui",
            "Haozhe Lin",
            "Kun Lai",
            "Lu Fang",
            "Kun Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wen_Crowd3D_Towards_Hundreds_of_People_Reconstruction_From_a_Single_Image_CVPR_2023_paper.html",
          "ref_texts": "[45] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3D pose estimation. InProc. European Conference on Computer Vision, pages 550\u2013566, 2020. 2, 6, 8",
          "ref_ids": [
            "45"
          ],
          "2": "Inspired by monocular depth estimation methods, SMAP [45] utilizes a deep convolutional neural network (CNN) to estimate a normalized root depth map and part relative-depth maps.",
          "3": "Method PPDS\u2191 PA-PPDS\u2191 PCOD\u2191 OKS\u2191 SMAP [45]-Large 58.",
          "4": "The percentage of correct ordinal depth (PCOD) [45] is used to evaluate the ordinal depth relations between all pairs of people in the image.",
          "5": "Comparison Because no existing methods can directly handle largescene images with hundreds of people, we compare our method with three baselines that are modified from the state-of-the-art methods: SMAP [45], CRMH [13], and 8942",
          "6": "Both SMAP [45] and BEV [35] infer the locations through perspective camera models."
        },
        "Accurate and steady inertial pose estimation through sequence structure learning and modulation": {
          "authors": [
            "Y Wu",
            "L Yin",
            "S Guo",
            "Y Qin"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/4ae74049d83240399df9772017bfa046-Abstract-Conference.html",
          "ref_texts": "[62] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multi-person absolute 3d pose estimation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, pages 550\u2013566. Springer, 2020.",
          "ref_ids": [
            "62"
          ],
          "1": "With the popularity of deep learning, an increasing number of methods are using a single camera to capture human 2D/3D poses, such as CPN [8], HRNet [44] and others [62, 22, 54, 51, 61, 45, 39, 28, 67, 58, 25], achieving significant success."
        },
        "Ivt: An end-to-end instance-guided video transformer for 3d pose estimation": {
          "authors": [
            "Z Qiu",
            "Q Yang",
            "J Wang",
            "D Fu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3547871",
          "ref_texts": "[50] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. 2020. Smap: Single-shot multi-person absolute 3d pose estimation. In ECCV. Springer, 550\u2013566.",
          "ref_ids": [
            "50"
          ],
          "4": "1 SMAP [50] ECCV\u201920 73."
        },
        "Dynamic graph reasoning for multi-person 3d pose estimation": {
          "authors": [
            "Z Qiu",
            "Q Yang",
            "J Wang",
            "D Fu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3547846",
          "ref_texts": "[54] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. 2020. Smap: Single-shot multi-person absolute 3d pose estimation. In ECCV. Springer, 550\u2013566.",
          "ref_ids": [
            "54"
          ],
          "7": "8 SMAP [54] 80.",
          "9": "1 SMAP [54] 71."
        },
        "A compact and powerful single-stage network for multi-person pose estimation": {
          "authors": [
            "Yabo Xiao",
            "Xiaojuan Wang",
            "Mingshu He",
            "Lei Jin",
            "Mei Song",
            "Jian Zhao"
          ],
          "url": "https://www.mdpi.com/2079-9292/12/4/857",
          "ref_texts": "55. Zhen, J.; Fang, Q.; Sun, J.; Liu, W.; Jiang, W.; Bao, H.; Zhou, X. Smap: Single-shot multi-person absolute 3d pose estimation. In Proceedings of the European Conference on Computer Vision, Glasgow, UK, 23\u201328 August 2020.",
          "ref_ids": [
            "55"
          ],
          "4": "0 Bottom-Up Methods SMAP [55] 80."
        },
        "GigaVision: When Computer Vision Meets Gigapixel Videography": {
          "authors": [
            "Lu Fang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-6915-5_6",
          "ref_texts": "171. Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multi-person absolute 3D pose estimation. In proceedings European Conference on Computer Vision, pages 550\u2013566, 2020. ",
          "ref_ids": [
            "171"
          ],
          "1": "SMAP [171] can recover multiperson 3D poses with absolute depth information in small scenes.",
          "2": "1 Classical Methods for Crowd Reconstruction There are many methods to address multiperson 3D pose estimation, which can be divided into top-down [172\u2013175] and bottom-up [171, 176\u2013180] paradigms.",
          "3": "Inspired by monocular depth estimation methods, SMAP [171] directly utilizes a deep convolutional neural network (CNN) [3, 181] to estimate the root depth map and part relative depth maps.",
          "4": "Crowd Reconstruction Performance with Gigapixel Image Evaluation Metrics In addition to the mean per joint position error (MPJPE) metric used for 3D pose evaluation, we use the root error (RtError) [171], percentage of the correct ordinal depth (PCOD) [171], and pairwise percentage distance estimation error (PPDError) [197] to evaluate the accuracy of the spatial locations.",
          "5": "Comparison of Large Scene We evaluate the performance of the proposed method based on the large-scene dataset Crowd-Location and compare the results with those of four state-of-the-art methods: SMAP [171], CRMH [168], ROMP [191], and BEV [170].",
          "6": "10 Comparisons to the state-of-the-art methods on Crowd-Location Method S1_scene S2_scene Matched PCOD PPDError OKS Matched PCOD PPDError OKS SMAP [171] 96.",
          "7": "SMAP [171] and BEV [170] both infer the depths through perspective camera models; however, they use only the cropped images.",
          "8": "For the SMAP model [171], we use the model provided by the authors that was not trained based on the Panoptic dataset for a fair comparison.",
          "11": "\u2191 SMAP [171] 83."
        },
        "Permutation-invariant relational network for multi-person 3d pose estimation": {
          "authors": [
            "N Ugrinovic",
            "A Ruiz",
            "A Agudo",
            "A Sanfeliu"
          ],
          "url": "https://arxiv.org/abs/2204.04913",
          "ref_texts": "[42] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou. Smap: Single-shot multiperson absolute 3d pose estimation. In Computer Vision \u2013 ECCV 2020. Springer International Publishing, 2020.",
          "ref_ids": [
            "42"
          ],
          "1": "2 MuCoSMAP [42]* 73.",
          "3": "9 SMAP [42] 63."
        },
        "Human Pose Estimation and Shape Modeling in 3D: New Cameras, Datasets and Approaches": {
          "authors": [
            "S Zou"
          ],
          "url": "https://era.library.ualberta.ca/items/adee6f13-92cf-417b-be5e-3052f9b4b430",
          "ref_texts": "[254] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, \u201cSmap: Single-shot multi-person absolute 3d pose estimation,\u201d in European Conference on Computer Vision , 2020.",
          "ref_ids": [
            "254"
          ],
          "1": "Single-stage methods are emerging in recent years for both pose estimation [14], [15], [86], [127], [145], [181], [217], [254] and parametric human shape estimation [189]."
        },
        "End-to-end 3D Human Pose Estimation using Dual Decoders": {
          "authors": [
            "Z Wang",
            "T Wang",
            "M Song",
            "L Jin"
          ],
          "url": "https://www.preprints.org/manuscript/202306.0033/download/final_file",
          "ref_texts": "23. Zhen, J.; Fang, Q.; Sun, J.; Liu, W .; Jiang, W .; Bao, H.; Zhou, X. Smap: Single-shot multi-person absolute 3d pose estimation. Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow , UK, August 23\u201328, 2020, Proceedings, Part XV 16. Springer , 2020, pp. 550\u2013566.",
          "ref_ids": [
            "23"
          ],
          "1": "Experimental T o fully train the network, a method of mixed dataset is used during training, referring to SMAP [23]."
        },
        "An overview of state-of-the-art methods for 3D human pose estimation from monocular video": {
          "authors": [
            "C P\u00e9rez Millar"
          ],
          "url": "https://upcommons.upc.edu/handle/2117/401727",
          "ref_texts": "[89] Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H. and Zhou, X., \u201dSMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation\u201d. https://arxiv.org/pdf/2008.11469.pdf, (2020).",
          "ref_ids": [
            "89"
          ],
          "1": "\u2022 Resources: \u2013 SMAP Paper[89] \u2013 SMAP Project[90] Among the most recent works featuring models created expressly for the worlds of film and animation are the following: 1."
        },
        "Supplementary Document for Crowd3D: Towards Hundreds of People Reconstruction from a Single Image": {
          "authors": [
            "H Wen",
            "J Huang",
            "H Cui",
            "H Lin",
            "YK Lai",
            "L Fang",
            "K Li"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Wen_Crowd3D_Towards_Hundreds_CVPR_2023_supplemental.pdf",
          "ref_texts": "[11] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3D pose estimation. InProc. European Conference on Computer Vision, pages 550\u2013566, 2020. 2",
          "ref_ids": [
            "11"
          ],
          "1": "For SMAP [11] and BEV [8], we use the method in [1] to implement the local-to-global depth conversion.",
          "2": "We use the SMAP [11] model provided by the authors that is not trained on Panoptic for fair comparison.",
          "3": "Method Haggling Ultim Pizza Mean MPJPE \u2193 SMAP [11] 128.",
          "4": "0 RtError \u2193 SMAP [11] 432.",
          "5": "7 PCOD \u2191 SMAP [11] 83."
        },
        "Supplementary Material: Learning Human Mesh Recovery in 3D Scenes": {
          "authors": [
            "ZSZCS Peng",
            "QSHBX Zhou"
          ],
          "url": "https://zju3dv.github.io/sahmr/files/supplementary.pdf",
          "ref_texts": "[3] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. SMAP: Single-shot multiperson absolute 3d pose estimation. In ECCV, 2020. 1",
          "ref_ids": [
            "3"
          ],
          "1": "We follow SMAP [3] to estimate the initial human root from f1 and f2."
        }
      }
    },
    {
      "title": "im4d: high-fidelity and real-time novel view synthesis for dynamic scenes",
      "id": 37,
      "valid_pdf_number": "18/20",
      "matched_pdf_number": "15/18",
      "matched_rate": 0.8333333333333334,
      "citations": {
        "4d gaussian splatting for real-time dynamic scene rendering": {
          "authors": [
            "Guanjun Wu",
            "Taoran Yi",
            "Jiemin Fang",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Qi Tian",
            "Xinggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[27] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2, 5, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "1": "[14, 27, 38, 50, 51, 53] are efficient methods to handle multi-view setups.",
          "3": "To assess the quality of novel view synthesis, we conducted benchmarking against several state-of-the-art methods in the field, including [5, 8, 11, 12, 17, 19, 27, 49].",
          "4": "Though [27] addresses the high quality in comparison to ours, the need for multi-cam setups makes it hard to model monocular scenes and other methods also limit freeview rendering speed and storage."
        },
        "4k4d: Real-time 4d view synthesis at 4k resolution": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Haotong Lin",
            "Guangzhao He",
            "Jiaming Sun",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html",
          "ref_texts": "[48] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2, 6",
          "ref_ids": [
            "48"
          ],
          "4": "Other image-based methods [48, 49, 90] produce high-quality appearance."
        },
        "Hifi4g: High-fidelity human performance rendering via compact gaussian splatting": {
          "authors": [
            "Yuheng Jiang",
            "Zhehao Shen",
            "Penghao Wang",
            "Zhuo Su",
            "Yu Hong",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jiang_HiFi4G_High-Fidelity_Human_Performance_Rendering_via_Compact_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[36] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, New York, NY , USA, 2023. Association for Computing Machinery. 3",
          "ref_ids": [
            "36"
          ],
          "1": "Method Given human performance videos captured by multi-view RGB cameras, HiFi4G integrates recent advancements in differentiable rasterization with traditional non-rigid tracking, significantly outperforming existing rendering approaches [22, 36, 69, 81] in terms of optimization speed, rendering quality, and storage overhead."
        },
        "Vivid-zoo: Multi-view video generation with diffusion model": {
          "authors": [
            "B Li",
            "C Zheng",
            "W Zhu",
            "J Mai",
            "B Zhang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/71c9eb0913e6c7fda3afd69c914b1a0c-Abstract-Conference.html",
          "ref_texts": "[48] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023.",
          "ref_ids": [
            "48"
          ],
          "1": "More importantly, the availability of such data holds substantial promise for facilitating progress in research areas such as 4D reconstruction [44, 48], 4D generation [3, 49], and long video generation [9, 101] with 3D consistency."
        },
        "Neural parametric gaussians for monocular non-rigid object reconstruction": {
          "authors": [
            "D Das",
            "C Wewer",
            "R Yunus",
            "E Ilg"
          ],
          "url": "https://arxiv.org/abs/2312.01196",
          "ref_texts": "[26] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In ACM SIGGRAPH, 2023. 2",
          "ref_ids": [
            "26"
          ],
          "1": "One approach, dubbed Space-Time Neural Fields, directly adds an extra time dimension to the scene representation to reconstruct the dynamic scene from multi-view [1, 21, 23, 26, 42, 50, 59\u201361, 63, 70] or monocular [4, 8, 10, 11, 24, 25, 48, 51, 67] video.",
          "2": "Hybrid methods further achieve acceleration by parameterizing the 4D scene representation using voxel grids [21, 42, 51, 59\u201361, 63] or planar factorization [1, 4, 10, 26, 48, 70]."
        },
        "Fast View Synthesis of Casual Videos with Soup-of-Planes": {
          "authors": [
            "YC Lee",
            "Z Zhang",
            "K Blackburn-Matzen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72920-1_16",
          "ref_texts": "38. Lin, H., Peng, S., Xu, Z., Xie, T., He, X., Bao, H., Zhou, X.: High-fidelity and realtime novel view synthesis for dynamic scenes. In: SIGGRAPH Asia Conference Proceedings (2023)",
          "ref_ids": [
            "38"
          ],
          "1": "To make this problem more tractable, many existing methods [2,3,5,8,12,31,38,39,45,58,68,92] reconstruct 4D scenes from multiple cameras capturing the dynamic scene simultaneously."
        },
        "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos": {
          "authors": [
            "S Girish",
            "T Li",
            "A Mazumdar"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html",
          "ref_texts": "[46] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. Highfidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u20139, 2023.",
          "ref_ids": [
            "46"
          ],
          "1": "[1, 46, 77, 87, 93] incorporate efficient NeRF representations [9, 19, 91] for higher fidelity."
        },
        "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time rendering of temporally complex dynamic scenes": {
          "authors": [
            "J Yan",
            "R Peng",
            "L Tang",
            "R Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681463",
          "ref_texts": "[28] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. 2023. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers . 1\u20139.",
          "ref_ids": [
            "28"
          ],
          "1": "Many extensions of NeRF to dynamic scenes either utilize deformation fields and canonical fields to model the motion of objects relative to canonical frames over time[18, 29, 36, 42, 51, 53], or decompose the 4D volume into spatial-only and spatial-temporal spaces[7, 13, 28, 49], representing space through combinations of dimensionally reduced features.",
          "2": "Other methods[7, 13, 28, 49, 56] reduce the dimensionality of the 4D space by decomposing it into a set of planar grids or hash grids."
        },
        "Denser: 3d gaussians splatting for scene reconstruction of dynamic urban environments": {
          "authors": [
            "MA Mohamad",
            "G Elghazaly",
            "A Hubert"
          ],
          "url": "https://arxiv.org/abs/2409.10041",
          "ref_texts": "[21] H. Lin, S. Peng, Z. Xu, T . Xie, X. He, H. Bao, and X. Zhou, \u201cHighfidelity and real-time novel view synthesis for dynamic scenes,\u201d in SIGGRAPH Asia 2023 Conference Proceedings, pp. 1\u20139, 2023.",
          "ref_ids": [
            "21"
          ],
          "1": "R ELATED WORK Dynamic scene representation has seen remarkable progress, especially in the domain of 4D neural scene representations focusing on scenes of single dynamic object, where time is considered as an additional dimension besides spatial ones [18], [19], [20], [21], [22], [23], [24]."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[33] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 1, 3",
          "ref_ids": [
            "33"
          ],
          "1": "Other approaches [33, 51] utilize temporal embeddings instead of SMPL poses to model each frame independently, achieving high-quality rendering but making it challenging to animate avatars.",
          "3": "Motivated by these, a stream of human researches [33, 51] focus on pure rendering quality, employing temporal embeddings instead of human pose to encode each frame of human videos."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[25] H. Lin, S. Peng, Z. Xu, T. Xie, X. He, H. Bao, and X. Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023.",
          "ref_ids": [
            "25"
          ],
          "1": "Although these methods don\u2019t guarantee an animatable human avatar, they can achieve high-quality rendering, among which, we compare the rendering quality of Im4D [25] with our method.",
          "2": "Im4d* [25] achieves high-quality rendering but cannot be animated."
        },
        "Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction": {
          "authors": [
            "D Chen",
            "B Oberson",
            "I Feldmann",
            "O Schreer"
          ],
          "url": "https://arxiv.org/abs/2411.06602",
          "ref_texts": "[42] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u20139, 2023. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Building on this foundation, numerous subsequent works [3,17,37,42,51,53,55] have further explored the synthesis of free-viewpoint videos for dynamic scenes.",
          "2": "To enhance training and rendering efficiency, recent works factorize the 4D space into lowerdimensional components, such as planes, thereby reducing computational complexity [3,17,26,42,55]."
        }
      }
    },
    {
      "title": "novel view synthesis of human interactions from sparse multi-view videos",
      "id": 22,
      "valid_pdf_number": "33/37",
      "matched_pdf_number": "25/33",
      "matched_rate": 0.7575757575757576,
      "citations": {
        "Learning neural volumetric representations of dynamic humans in minutes": {
          "authors": [
            "Chen Geng",
            "Sida Peng",
            "Zhen Xu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.html",
          "ref_texts": "[74] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 8",
          "ref_ids": [
            "74"
          ],
          "1": "It might be plausible to combine our method with STNeRF [29] or MultiNB [74] to quickly reconstruct dynamic scenes containing foreground and background objects."
        },
        "Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering": {
          "authors": [
            "Wei Cheng",
            "Ruixiang Chen",
            "Siming Fan",
            "Wanqi Yin",
            "Keyu Chen",
            "Zhongang Cai",
            "Jingbo Wang",
            "Yang Gao",
            "Zhengming Yu",
            "Zhengyu Lin",
            "Daxuan Ren",
            "Lei Yang",
            "Ziwei Liu",
            "Chen Change",
            "Chen Qian",
            "Wayne Wu",
            "Dahua Lin",
            "Bo Dai",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[52] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH, 2022.",
          "ref_ids": [
            "52"
          ],
          "1": "A thorough comparison of our fitting pipeline with other fitting methods [52, 70, 10] is described in Sec."
        },
        "Hosnerf: Dynamic human-object-scene neural radiance fields from a single video": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Tianyuan Yang",
            "Zhongcong Xu",
            "Jussi Keppo",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html",
          "ref_texts": "[37] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings , pages 1\u2013",
          "ref_ids": [
            "37"
          ],
          "1": "To support multi-person modeling, recent works [55, 37] have proposed to segment each human into 3D bounding boxes and learn a separate layered dynamic NeRF for each person."
        },
        "Multiply: Reconstruction of multiple people from monocular video in the wild": {
          "authors": [
            "Zeren Jiang",
            "Chen Guo",
            "Manuel Kaufmann",
            "Tianjian Jiang",
            "Julien Valentin",
            "Otmar Hilliges",
            "Jie Song"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_MultiPly_Reconstruction_of_Multiple_People_from_Monocular_Video_in_the_CVPR_2024_paper.html",
          "ref_texts": "[36] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 1, 2, 3, 7",
          "ref_ids": [
            "36"
          ],
          "4": "[36], which is a state-of-the-art multi-person novel view synthesis approach from multi-view videos, to the monocular setting for a fair comparison.",
          "6": "3 our method outperforms [36] on all metrics."
        },
        "Neuraldome: A neural modeling pipeline on multi-view human-object interactions": {
          "authors": [
            "Juze Zhang",
            "Haimin Luo",
            "Hongdi Yang",
            "Xinru Xu",
            "Qianyang Wu",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.html",
          "ref_texts": "[52] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings , pages 1\u2013",
          "ref_ids": [
            "52"
          ],
          "1": "Besides, existing neural techniques [52,77] suffer from tedious training procedures due to the human-object occlusion, and hence infeasible for building a large-scale dataset."
        },
        "Guess the unseen: Dynamic 3d scene reconstruction from partial 2d glimpses": {
          "authors": [
            "Inhee Lee",
            "Byungjun Kim",
            "Hanbyul Joo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lee_Guess_The_Unseen_Dynamic_3D_Scene_Reconstruction_from_Partial_2D_CVPR_2024_paper.html",
          "ref_texts": "[46] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH, 2022. 3, 6, 7",
          "ref_ids": [
            "46"
          ],
          "1": "Recent human reconstruction methods also use compositional approaches to reconstruct a human from videos [11, 17, 46, 59].",
          "2": "[46] tackles more practical situations where multiple people interact with objects.",
          "4": "[46] is a compositional method similar to ours, which optimizes implicit functions of people and background from sparse view input [46]."
        },
        "MetaCap: Meta-learning Priors from Multi-view Imagery for Sparse-View Human Performance Capture and Rendering": {
          "authors": [
            "G Sun",
            "R Dabral",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72952-2_20",
          "ref_texts": "73. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Novel view synthesis of human interactions from sparse multi-view videos. In: SIGGRAPH Conference Proceedings (2022)",
          "ref_ids": [
            "73"
          ],
          "1": "The highest-quality results are obtained using many\u20148 or more\u2014 calibrated cameras in a dome-like configuration [11,20,70,73,99]."
        },
        "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos": {
          "authors": [
            "F Lu",
            "Z Dong",
            "J Song",
            "O Hilliges"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73668-1_13.pdf",
          "ref_texts": "53. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Novel view synthesis of human interactions from sparse multi-view videos. In: ACM SIGGRAPH 2022 Conference Proceedings. SIGGRAPH \u201922, Association for Computing Machinery, New York, NY, USA (2022).https://doi.org/10.1145/3528233.",
          "ref_ids": [
            "53"
          ],
          "1": "To learn and render avatar models of multiple people, we adapt layered volume rendering [53,70] to our avatar model.",
          "2": "Recent methods including ST-NeRF [70] and [53] leverage layered neural representation to model multiple humans with sparse multi-view videos and thus can generate novel view synthesis of dynamic multiple humans."
        },
        "SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking": {
          "authors": [
            "TC Yu",
            "MM Zhang",
            "P He",
            "CJ Lee",
            "C Cheesman"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3654777.3676341",
          "ref_texts": "[49] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. 2022. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings . 1\u201310.",
          "ref_ids": [
            "49"
          ],
          "1": "For real-time joint visualization, we use the visualizer provided by EasyMocap [49]."
        },
        "3D Human Scan With A Moving Event Camera": {
          "authors": [
            "Kai Kohyama",
            "Shintaro Shiba",
            "Yoshimitsu Aoki"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/CV4MR/html/Kohyama_3D_Human_Scan_With_A_Moving_Event_Camera_CVPRW_2024_paper.html",
          "ref_texts": "[44] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings , pages 1\u2013",
          "ref_ids": [
            "44"
          ],
          "1": "These approaches also consist of a wide range of problem settings, from using a single camera [5, 35, 36] to employing multiple cameras [1, 7, 44, 47], and utilizing depth sensors as additional information [32, 52]."
        },
        "Hdhuman: High-quality human novel-view rendering from sparse views": {
          "authors": [
            "T Zhou",
            "J Huang",
            "T Yu",
            "R Shao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10168294/",
          "ref_texts": "[37] Q. Shuai, C. Geng, Q. Fang, S. Peng, W. Shen, X. Zhou, and H. Bao, \u201cNovel view synthesis of human interactions from sparse multiview videos,\u201d in Proc. ACM SIGGRAIPH, 2022, p. 10. 2.2",
          "ref_ids": [
            "37"
          ],
          "1": "For human rendering from sparse views, some works [33], [34], [35], [36], [37] use SMPL template as a prior, which helps to constrain the motion space and improve the rendering quality."
        },
        "Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects": {
          "authors": [
            "S Gopal",
            "R Dabral",
            "V Golyanik",
            "C Theobalt"
          ],
          "url": "https://arxiv.org/abs/2502.13968",
          "ref_texts": "[32] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 2",
          "ref_ids": [
            "32"
          ],
          "1": "Some works also extend it to multi-person scenarios [11, 22, 32]."
        },
        "MV2MP: Segmentation Free Performance Capture of Humans in Direct Physical Contact from Sparse Multi-Cam Setups": {
          "authors": [
            "Sergei Eliseev",
            "Leonid Shtanko",
            "Rasim Akhunzianov",
            "Yaroslav Romanenko",
            "Anatoly Starostin"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2024/html/Eliseev_MV2MP_Segmentation_Free_Performance_Capture_of_Humans_in_Direct_Physical_ACCV_2024_paper.html",
          "ref_texts": "25. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Novel view synthesis of human interactions from sparse multi-view videos. In: ACM SIGGRAPH 2022 Conference Proceedings. SIGGRAPH \u201922, Association for Computing Machinery, New York, NY, USA (2022).https://doi.org/10.1145/3528233.",
          "ref_ids": [
            "25"
          ],
          "1": "Approaches presented in [25,31,35] achieve good results in performance capture and free view point video generation of humans in close interactions, but either only work with dense camera setups [31] or are limited to the setups with relatively distant performers [25] Sec.",
          "3": "Therefore, to ensure realistic evaluation, we also estimate SMPL parameters using the widely-used open-source tool [25] and generate instance segmentation masks using nearly state of the art the Mask2Former [4] model.",
          "4": "Method [25] which we refer as MultiNB is based on a layered neural scene representation."
        },
        "Few-Shot Multi-Human Neural Rendering Using Geometry Constraints": {
          "authors": [
            "VF Abrevaya",
            "F Multon",
            "A Boukhayma"
          ],
          "url": "https://arxiv.org/abs/2502.07140",
          "ref_texts": "[80] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022. 2",
          "ref_ids": [
            "80"
          ],
          "1": "More recently, deep learning-based approaches were proposed, but they either require temporal information [30, 80, 115, 116], pretraining on a large dataset [116] which cannot work on general scenes, or a coarse body model [30, 80, 115] which lacks geometric detail."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[46] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 2",
          "ref_ids": [
            "46"
          ],
          "1": "The widespread adoption of SMPL(-X) has further been boosted for human body animation, driven by methods [7, 46] that estimate parameters of SMPL(-X) from 2D image inputs."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[39] Q. Shuai, C. Geng, Q. Fang, S. Peng, W. Shen, X. Zhou, and H. Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Furthermore, it has been widely adopted for human body animation, thanks to the methods [6, 39] of estimating SMPL parameters from 2D inputs."
        },
        "GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis": {
          "authors": [
            "Y Abdelkareem",
            "S Shehata",
            "F Karray"
          ],
          "url": "https://arxiv.org/abs/2309.11627",
          "ref_texts": "[30] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. ACM SIGGRAPH, 2022. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "30"
          ],
          "1": "[30] utilized a layered architecture by representing the human entities using NeuralBody [25] and weakly supervising the human instance segmentation.",
          "4": "[30] extended ST-NeRF by modeling the human subjects using NeuralBody [25] and predicted human segmentation masks as part of the network training.",
          "5": "L-NeRF [30] introduced a time-synchronization step that accounts for the multi-view image de-synchronization by producing a per-view body model using predicted time offsets.",
          "6": "Existing layered scene representations [30] follow the approach of NeuralBody [25] by encoding the vertices of human layers using learnable embeddings that are unique to each layer in each training scene.",
          "7": "We evaluate our performance compared to the multi-human layered scene representation approach [30], denoted as L-NeRF.",
          "8": "Comparison with a per-scene multi-human method [30] on seen models/unseen poses on the DeepMultiSyn Dataset."
        },
        "GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Human View Synthesis": {
          "authors": [
            "Y Abdelkareem",
            "S Shehata",
            "F Karray"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-54605-1_11",
          "ref_texts": "29. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Easymocap make human motion capture easier. Github (2021), https://github.com/zju3dv/EasyMocap 30. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Novel view synthesis of human interactions from sparse multi-view videos. ACM SIGGRAPH (2022)",
          "ref_ids": [
            "29"
          ],
          "1": "Another issue with existing Human NVS methods [12, 30, 44] is the reliance on multi-step optimization methods [2, 29, 43] for the estimation of pre-fitted 3D body models.",
          "2": "Existing Human NVS approaches [3,12,20,25,44] utilize pre-fitted 3D observations computed using multi-step optimization approaches [29, 43].",
          "3": "Following NeuralBody [25], we use EasyMoCap [29] to fit the SMPL human models for all the subjects in the available frames.",
          "4": "The video sequence was published online [29] with the calibration files.",
          "5": "Similar to DeepMultiSyn, we predict the SMPL models and segmentation masks utilizing [14, 29]."
        },
        "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses Supplemental Material": {
          "authors": [
            "I Lee",
            "B Kim",
            "H Joo"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Guess_The_Unseen_CVPR_2024_supplemental.pdf",
          "ref_texts": "[15] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH, 2022. 1",
          "ref_ids": [
            "15"
          ],
          "1": "[15] represents the scene as a composition of a background model and human model, both represented by a variant of NeRF [10, 11].",
          "2": "The remaining settings are the same as the original paper [15]."
        },
        "Scene Representations for Generalizable Novel View Synthesis": {
          "authors": [
            "Y Abdelkareem"
          ],
          "url": "https://uwspace.uwaterloo.ca/handle/10012/19247",
          "ref_texts": "[55] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. ACM SIGGRAPH 2022 Conference Proceedings, 2022.",
          "ref_ids": [
            "55"
          ],
          "1": "The red boxes highlight areas where our method is better at representing the texture details compared to L-NeRF [55].",
          "2": "10 Comparison with a per-scene multi-human method [55] on seen models, and unseen poses on the DeepMultiSyn Dataset.",
          "3": "2 Motivations The research community has developed methods that achieve photo-realistic view synthesis results on a variety of synthetic and real-world scenes [42, 45, 47, 55, 1].",
          "6": "Recently, [55] extended ST-NeRF by modeling the human subjects using NeuralBody [45] and predicted human segmentation masks as part of the network training.",
          "9": "Human-anchored Feature Generation Existing layered scene representations [55] follow the approach of NeuralBody [45] by encoding the vertices of human layers using learnable embeddings that are unique to each layer in each training scene.",
          "10": "The red boxes highlight areas where our method is better at representing the texture details compared to L-NeRF [55].",
          "13": "The highlighted areas indicate that our proposed method is capable of representing more texture details compared to L-NeRF [55].",
          "17": "Our proposed method performs at par with the state-of-the-art per-scene baseline (L-NeRF [55]), while effectively saving computational and time resources.",
          "18": "The pixel-level combination takes the form of multi-plane neural radiance fields (MINE) [34], while the object-level combination represents each object in the scene with an independent neural radiance field [55].",
          "19": "This was done by exploring the capabilities of combining explicit [67, 55] and implicit [72, 32] 3D scene representations in the form of scene layers at the pixel level or the object level."
        },
        "Free-Viewpoint Video in the Wild Using a Flying Camera": {
          "authors": [
            "Z Hong",
            "W Shen"
          ],
          "url": "https://openreview.net/forum?id=AsPHD00Wer",
          "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., W ang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "34"
          ],
          "1": "NeRF [29] and its follow-ups [28, 34, 48] have demonstrated promising performances in rendering both static scene and dynamic human.",
          "4": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 42, 45] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity.",
          "5": "Human Deformation Inspired by [34,42], W e learn to unwrap the performer\u2019s observed poses in the observation space to a canonical T-pose represented by SMPL [26]."
        },
        "Free-Viewpoint Video of Outdoor Sports Using a Drone": {
          "authors": [
            "Z Hong"
          ],
          "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02178.pdf",
          "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., W ang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "34"
          ],
          "1": "NeRF [29] and its follow-ups [28, 34, 47] have demonstrated promising performances in rendering both static scene and dynamic human.",
          "4": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 41, 44] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity.",
          "5": "Human Deformation Inspired by [34,41], W e learn to unwrap the performer\u2019s observed poses in the observation space to a canonical T-pose represented by SMPL [26]."
        }
      }
    },
    {
      "title": "deep snake for real-time instance segmentation",
      "id": 5,
      "valid_pdf_number": "149/243",
      "matched_pdf_number": "113/149",
      "matched_rate": 0.7583892617449665,
      "citations": {
        "Maptrv2: An end-to-end framework for online vectorized hd map construction": {
          "authors": [
            "B Liao",
            "S Chen",
            "Y Zhang",
            "B Jiang",
            "Q Zhang"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-024-02235-z",
          "ref_texts": "[69] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: CVPR (2020)",
          "ref_ids": [
            "69"
          ],
          "1": "Deepsnake [69] proposes a two-stage contour evolution process and designs circular convolution to exploit the features of the contour."
        },
        "Polyformer: Referring image segmentation as sequential polygon generation": {
          "authors": [
            "Jiang Liu",
            "Hui Ding",
            "Zhaowei Cai",
            "Yuting Zhang",
            "Ravi Kumar",
            "Vijay Mahadevan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.html",
          "ref_texts": "[63] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In CVPR, pages 8530\u20138539, 2020. 3",
          "ref_ids": [
            "63"
          ],
          "1": "Deep Snake [63] extends the classic snake algorithm [82] and uses a neural network to iteratively deform an initial contour to match the object boundary."
        },
        "You only segment once: Towards real-time panoptic segmentation": {
          "authors": [
            "Jie Hu",
            "Linyan Huang",
            "Tianhe Ren",
            "Shengchuan Zhang",
            "Rongrong Ji",
            "Liujuan Cao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.html",
          "ref_texts": "[40] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2",
          "ref_ids": [
            "40"
          ],
          "1": "CenterMask [30] employs an efficient anchor-free framework, and DeepSnake [40] explores the use of object contours for fast segmentation of instances."
        },
        "E2ec: An end-to-end contour-based method for high-quality high-speed instance segmentation": {
          "authors": [
            "Tao Zhang",
            "Shiqing Wei",
            "Shunping Ji"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_E2EC_An_End-to-End_Contour-Based_Method_for_High-Quality_High-Speed_Instance_Segmentation_CVPR_2022_paper.html",
          "ref_texts": "[27] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "27"
          ],
          "1": "Examples of such methods are Curve GCN [21], Deep Snake [27], Point-Set Anchors [31], DANCE [25], PolarMask [33], and LSNet [9].",
          "2": ", N = 128 ) vertices is sufficient to describe most of the instances well [27].",
          "4": "Curve GCN [21], Deep Snake [27], Point-Set Anchors [31], and DANCE [25] use the vertex features of the contour for the boundary regression, which greatly improves the performance.",
          "5": "The deformation modules [27] then deform the coarse contour twice to the final contour.",
          "6": "Dynamic matching loss (DML) As the pre-fixed vertex pairing used in previous studies is not optimal and can cause learning difficulty, we propose DML, which dynamically adjusts the relationship of the vertex pairing to supervise output of the last deformation module [27] as shown in Figure 2.",
          "7": "Baseline method is Deep Snake [27]."
        },
        "VPFNet: Improving 3D object detection with virtual point based LiDAR and stereo data fusion": {
          "authors": [
            "H Zhu",
            "J Deng",
            "Y Zhang",
            "J Ji",
            "Q Mao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9826439/",
          "ref_texts": "[57] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020. 9",
          "ref_ids": [
            "57"
          ],
          "1": "We generate the foreground object mask list using the pretrained Deep Snake network [57]."
        },
        "Solq: Segmenting objects by learning queries": {
          "authors": [
            "B Dong",
            "F Zeng",
            "T Wang",
            "X Zhang"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/b7087c1f4f89e63af8d46f3b20271153-Abstract.html",
          "ref_texts": "[11] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020.",
          "ref_ids": [
            "11"
          ],
          "1": "Some recent works [8, 9, 10, 11] build instance segmentation frameworks on top of anchorfree object detectors [12, 13] to remove the ROI-Cropping operation, reducing the effect of feature misalignment."
        },
        "Box-supervised instance segmentation with level set evolution": {
          "authors": [
            "W Li",
            "W Liu",
            "J Zhu",
            "M Cui",
            "XS Hua",
            "L Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19818-2_1",
          "ref_texts": "38. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: Proc. of IEEE Conf. Comp. Vis. Patt. Recogn. pp.",
          "ref_ids": [
            "38"
          ],
          "3": "As shown in Table 3, our method achieves comparable results to the fully supervised variational-based methods, and even outperforms DeepSnake [38] and Levelset R-CNN [15]."
        },
        "Adaptive boundary proposal network for arbitrary shape text detection": {
          "authors": [
            "Xue Zhang",
            "Xiaobin Zhu",
            "Chun Yang",
            "Hongfa Wang",
            "Cheng Yin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Adaptive_Boundary_Proposal_Network_for_Arbitrary_Shape_Text_Detection_ICCV_2021_paper.html",
          "ref_texts": "[23] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In CVPR, pages 8530\u20138539. IEEE, 2020.",
          "ref_ids": [
            "23"
          ],
          "2": "Inspired by [1, 15], DeepSnake [23] performs object segmentation by deforming an initial contour to object boundary with circular convolution which consists of multi-layer 1-D convolutions with 1 \u00d7N kernel size."
        },
        "Point-set anchors for object detection, instance segmentation and pose estimation": {
          "authors": [
            "F Wei",
            "X Sun",
            "H Li",
            "J Wang",
            "S Lin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58607-2_31",
          "ref_texts": "34. Peng, S., Jiang, W., Pi, H., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. arXiv preprint arXiv:2001.01629 (2020)",
          "ref_ids": [
            "34"
          ],
          "2": "The recent Deep Snake [34] proposes a two-stage pipeline based on initial contours and contour deformation."
        },
        "BuildMapper: A fully learnable framework for vectorized building contour extraction": {
          "authors": [
            "S Wei",
            "T Zhang",
            "S Ji",
            "M Luo",
            "J Gong"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0924271623000217",
          "ref_texts": "[27] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \"Deep Snake for Real-Time Instance Segmentation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 8533-8542. ",
          "ref_ids": [
            "27"
          ],
          "1": "The shapes of the initial contours used in the current contour-based methods are generally hand-designed, such as ellipses for Curve GCN [26], rectangles for DANCE [28], and octagons for Deep Snake [27].",
          "3": "Tables 4\u20136 list the quantitative comparison of BuildMapper, Mask R-CNN [17], SOLO [18], YOLACT [19], PolarMask [40], Deep Snake [27], and CLP-CNN [29] for mask AP, boundary AP, and speed (frames per second, FPS) on the WHU aerial building dataset and WHU-Mix (vector) dataset."
        },
        "Boxsnake: Polygonal instance segmentation with box supervision": {
          "authors": [
            "Rui Yang",
            "Lin Song",
            "Yixiao Ge",
            "Xiu Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yang_BoxSnake_Polygonal_Instance_Segmentation_with_Box_Supervision_ICCV_2023_paper.html",
          "ref_texts": "[54] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InCVPR, 2020.",
          "ref_ids": [
            "54"
          ],
          "4": "Two-stage Deep Snake [54] created initial octagon contours using a detector and then iteratively deformed them through a circular convolution network.",
          "7": "For example, when using ResNet-50, BoxSnake surpasses PolarMask [72] and Deep Snake [54] by 2."
        },
        "Polarmask++: Enhanced polar representation for single-shot instance segmentation and beyond": {
          "authors": [
            "E Xie",
            "W Wang",
            "M Ding",
            "R Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9431650/",
          "ref_texts": "[48] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou. Deep snake for realtime instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "48"
          ],
          "1": "To address the above issue, recent trend has spent more effort in designing simpler single-stage pipeline for bounding box detection [13], [27], [30], [37], [56], [66], [73] and instance segmentation [2], [5], [48], [75]."
        },
        "Yolactedge: Real-time instance segmentation on the edge": {
          "authors": [
            "H Liu",
            "RAR Soto",
            "F Xiao",
            "YJ Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561858/",
          "ref_texts": "[7] Sida Peng, Wen Jiang, Huaijin Pi, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. arXiv preprint arXiv:2001.01629, 2020.",
          "ref_ids": [
            "7"
          ],
          "1": "However, while there has been great progress in real-time instance segmentation research [1], [2], [3], [4], [5], [6], [7], thus far, there is no method that can run accurately at real-time speeds on small edge devices like the Jetson AGX Xavier."
        },
        "Instance segmentation with mask-supervised polygonal boundary transformers": {
          "authors": [
            "Justin Lazarow",
            "Weijian Xu",
            "Zhuowen Tu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Lazarow_Instance_Segmentation_With_Mask-Supervised_Polygonal_Boundary_Transformers_CVPR_2022_paper.html",
          "ref_texts": "[25] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In CVPR, 2020. 2, 3, 7",
          "ref_ids": [
            "25"
          ],
          "1": "As one of the first works built on modern architectures, DeepSnake [25] considers an initial octagonal polygon derived from four extreme points, after which an iterative process of refinement is carried out using circular convolutions.",
          "2": "We use Bi as a means to initialize an ellipsoidal polygon Vi(0) inscribed in Bi, similar to other contour-based methods [14, 20, 25].",
          "3": "2 DeepSnake [25] R50-FPN CenterNet polygons polygons 30.",
          "4": "5 PolarMask [25] R101-FPN FCOS 2\u00d7 polygons polygons 32.",
          "5": "0 DeepSnake [25] ImageNet extreme pts \u2713 poly 28."
        },
        "Sign language translation with iterative prototype": {
          "authors": [
            "Huijie Yao",
            "Wengang Zhou",
            "Hao Feng",
            "Hezhen Hu",
            "Hao Zhou",
            "Houqiang Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yao_Sign_Language_Translation_with_Iterative_Prototype_ICCV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 2",
          "ref_ids": [
            "34"
          ],
          "1": "The idea of iterative refinement is applied to various computer vision tasks, such as image generation [1, 18, 37], instance segmentation [34, 28, 29, 47], image rectification [15], etc.",
          "2": "DeepSanke [34] uses the deep network to iteratively enclose the object boundary based on an initial contour."
        },
        "Box2mask: Box-supervised instance segmentation via level-set evolution": {
          "authors": [
            "W Li",
            "W Liu",
            "J Zhu",
            "M Cui",
            "R Yu",
            "X Hua"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10423160/",
          "ref_texts": "[57] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "57"
          ],
          "1": "AP AP 50 AP75 DeepSnake [57] DLA-34 [86] M 30.",
          "2": "Deep Variational-based Instance Segmentation To further evaluate the proposed level-set based approach, we compare it with other deep variationalbased instance segmentation approaches, including DeepSnake [57], Levelset R-CNN [22] and DVIS700 [87].",
          "3": "Among them, DeepSnake [57] is based on the classical snake method [28].",
          "4": "As shown in Table 7, Box2Mask-C achieves comparable results to the fully supervised variationalbased methods, even outperforming DeepSnake [57] by 0."
        },
        "Dance: A deep attentive contour model for efficient instance segmentation": {
          "authors": [
            "Zichen Liu",
            "Jun Hao",
            "Xiangyu Chen",
            "Jiashi Feng"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Liu_DANCE_A_Deep_Attentive_Contour_Model_for_Efficient_Instance_Segmentation_WACV_2021_paper.html",
          "ref_texts": "[23] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , 2020.",
          "ref_ids": [
            "23"
          ],
          "6": "Among them, the more recently published DeepSnake [23] demonstrates state-of-the-art performance on several datasets [5, 24, 9].",
          "22": "W e adopt the same backbone, detector, training and testing configuration as DeepSnake [23], and validate that our method outperforms all other methods (T able 4)."
        },
        "SharpContour: A contour-based boundary refinement approach for efficient and accurate instance segmentation": {
          "authors": [
            "Chenming Zhu",
            "Xuanye Zhang",
            "Yanran Li",
            "Liangdong Qiu",
            "Kai Han",
            "Xiaoguang Han"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_SharpContour_A_Contour-Based_Boundary_Refinement_Approach_for_Efficient_and_Accurate_CVPR_2022_paper.html",
          "ref_texts": "[29] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "29"
          ],
          "4": "Comparison with the state-of-the-art Effectiveness for contour-based model We apply SharpContour to the DANCE [26], which follows the idea of DeepSnake [29] and achieves current state-of-the-art performance for contour-based instance segmentation meth4397 Figure 5."
        },
        "Location-sensitive visual recognition with cross-iou loss": {
          "authors": [
            "K Duan",
            "L Xie",
            "H Qi",
            "S Bai",
            "Q Huang",
            "Q Tian"
          ],
          "url": "https://arxiv.org/abs/2104.04899",
          "ref_texts": "[44] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020. 1, 2, 7",
          "ref_ids": [
            "44"
          ],
          "1": "powerful deep networks as the backbone [25, 60, 20], the designs of the head module for detection [38, 48, 49, 52, 65, 31, 30, 63, 6], segmentation [59, 24, 44, 7], and pose estimation [51, 57, 5, 40, 66] have fallen into individual subfields.",
          "2": "The early representative methods are the snake series [27, 11, 23, 12] and the recent efforts include using deep neural network [22] and the idea of anchor-free to improve the features, such as DeepSnake [44] and PolarMask [59]."
        },
        "Muva: A new large-scale benchmark for multi-view amodal instance segmentation in the shopping scenario": {
          "authors": [
            "Zhixuan Li",
            "Weining Ye",
            "Juan Terven",
            "Zachary Bennett",
            "Ying Zheng",
            "Tingting Jiang",
            "Tiejun Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_MUVA_A_New_Large-Scale_Benchmark_for_Multi-View_Amodal_Instance_Segmentation_ICCV_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.",
          "ref_ids": [
            "26"
          ],
          "1": "9 DeepSnake [26] CVPR\u201920 7."
        },
        "Deep learning-based computer vision methods for complex traffic environments perception: A review": {
          "authors": [
            "T Azfar",
            "J Li",
            "H Yu",
            "RL Cheu",
            "Y Lv",
            "R Ke"
          ],
          "url": "https://link.springer.com/article/10.1007/s42421-023-00086-7",
          "ref_texts": "[135] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp.",
          "ref_ids": [
            "135"
          ],
          "1": "A learning-based snake algorithm [135] is proposed for real-time instance segmentation, which introduces the circular convolution for efficient feature learning on the contour and regresses vertexwise offsets for the contour deformation."
        },
        "Inverse-like antagonistic scene text spotting via reading-order estimation and dynamic sampling": {
          "authors": [
            "SX Zhang",
            "C Yang",
            "X Zhu",
            "H Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10403760/",
          "ref_texts": "[61] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in CVPR, 2020, pp. 8530\u20138539.",
          "ref_ids": [
            "61"
          ],
          "1": "Circular convolution will encode the cyclicity of points along the closed contour effectively, building on the success of DeepSnake [61]."
        },
        "Eigencontours: Novel contour descriptors based on low-rank approximation": {
          "authors": [
            "Wonhui Park",
            "Dongkwon Jin",
            "Su Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Park_Eigencontours_Novel_Contour_Descriptors_Based_on_Low-Rank_Approximation_CVPR_2022_paper.html",
          "ref_texts": "[19] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProc. CVPR, 2020. 1",
          "ref_ids": [
            "19"
          ],
          "1": "The boundary of an object in an image is encoded in contour description, which is useful in various applications, such as image retrieval [4, 31, 32], recognition [17,25,29], and segmentation [16,19,27,28,34]."
        },
        "Unifs: universal few-shot instance perception with point representations": {
          "authors": [
            "S Jin",
            "R Yao",
            "L Xu",
            "W Liu",
            "C Qian",
            "J Wu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73397-0_27.pdf",
          "ref_texts": "41. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 8533\u20138542",
          "ref_ids": [
            "41"
          ],
          "1": "Following the methodology outlined in Deep Snake [41], we uniformly add or remove points from the initial contour until the desired number of points is reached."
        },
        "Dense reppoints: Representing visual objects with dense point sets": {
          "authors": [
            "Z Yang",
            "Y Xu",
            "H Xue",
            "Z Zhang",
            "R Urtasun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58589-1_14",
          "ref_texts": "[33] Sida Peng, Wen Jiang, Huaijin Pi, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation.arXiv preprint arXiv:2001.01629, 2020. 2, 5, 7",
          "ref_ids": [
            "33"
          ],
          "2": "Contours and binary masks are two typical representations widely used in previous works [18, 7, 45, 33].",
          "3": "Point-to-point is a common and intuitive supervision strategy and it is widely used by other methods [45, 50, 33]."
        },
        "Panoramic panoptic segmentation: Insights into surrounding parsing for mobile agents via unsupervised contrastive learning": {
          "authors": [
            "A Jaus",
            "K Yang",
            "R Stiefelhagen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10012449/",
          "ref_texts": "[4] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proc. CVPR, 2020, pp. 8533\u20138542.",
          "ref_ids": [
            "4"
          ],
          "2": "Moreover, additional methods tackle instance-specific image segmentation via clustering [38], [39], class-agnostic mask generation [40], [41], and contour-based techniques [4], [42]."
        },
        "Pointins: Point-based instance segmentation": {
          "authors": [
            "L Qi",
            "Y Wang",
            "Y Chen",
            "YC Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9444808/",
          "ref_texts": "[35] S. Peng, W. Jiang, H. Pi, H. Bao, and X. Zhou. Deep snake for real-time instance segmentation. In CVPR, 2020.",
          "ref_ids": [
            "35"
          ],
          "1": "Most state-of-the-art (SOTA) instance segmentation methods [15], [31], [5], [19], [23], [35] construct mask representations using Region-of-Interests (RoIs) features.",
          "2": "[43] and detection-based [15], [31], [5], [19], [23], [35] methods.",
          "3": "Popular instance segmentation frameworks are mainly detection-based [13], [14], [40], [11], [27], [35]."
        },
        "Apanet: Auto-path aggregation for future instance segmentation prediction": {
          "authors": [
            "JF Hu",
            "J Sun",
            "Z Lin",
            "JH Lai",
            "W Zeng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9353241/",
          "ref_texts": "[52] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2020.",
          "ref_ids": [
            "52"
          ],
          "1": "These methods simultaneously detect object locations and predict mask representations [32], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54]."
        },
        "DeepMesh: Differentiable iso-surface extraction": {
          "authors": [
            "B Guillard",
            "E Remelli",
            "A Lukoianov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10506652/",
          "ref_texts": "[32] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep Snake for Real-Time Instance Segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.",
          "ref_ids": [
            "32"
          ],
          "1": "This representation has been successfully used for single view 3D reconstruction [2], [3], [30], and 3D shape completion [31], [32]."
        },
        "Real-time instance segmentation with discriminative orientation maps": {
          "authors": [
            "Wentao Du",
            "Zhiyu Xiang",
            "Shuya Chen",
            "Chengyu Qiao",
            "Yiman Chen",
            "Tingming Bai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Du_Real-Time_Instance_Segmentation_With_Discriminative_Orientation_Maps_ICCV_2021_paper.html",
          "ref_texts": "[25] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "[25] implement the snake algorithm in a learning-based form and the circular convolution is proposed to iteratively regress sampling points towards contour positions."
        },
        "ST-CenterNet: Small target detection algorithm with adaptive data enhancement": {
          "authors": [
            "Yujie Guo",
            "Xu Lu"
          ],
          "url": "https://www.mdpi.com/1099-4300/25/3/509",
          "ref_texts": "3. Peng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; Zhou, X. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020; pp. 8533\u20138542.",
          "ref_ids": [
            "3"
          ],
          "1": "Target detection has become one of the most essential topics in the CV community, requiring object classification and localization [3]."
        },
        "PolyRoom: Room-aware Transformer for Floorplan Reconstruction": {
          "authors": [
            "Y Liu",
            "L Zhu",
            "X Ma",
            "H Ye",
            "X Gao",
            "X Zheng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72973-7_19",
          "ref_texts": "32. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8530\u20138539 (2020)",
          "ref_ids": [
            "32"
          ],
          "1": "DeepSnake [32] implements the snake algorithm by iteratively deforming an initial contour to match the object boundary."
        },
        "Prior to segment: Foreground cues for weakly annotated classes in partially supervised instance segmentation": {
          "authors": [
            "David Biertimpel",
            "Sindi Shkodrani",
            "Anil S. Baslamisli",
            "Nora Baka"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Biertimpel_Prior_to_Segment_Foreground_Cues_for_Weakly_Annotated_Classes_in_ICCV_2021_paper.html",
          "ref_texts": "[30] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
          "ref_ids": [
            "30"
          ],
          "1": "Introduction Instance segmentation is an essential task in computer vision with applications ranging from autonomous vehicles to robotics and medical imaging [5, 6, 13, 22, 26, 30].",
          "2": "Contour based approaches [27, 30, 38] segment objects by refining a sequence of vertices to match the object shape."
        },
        "BARS: a benchmark for airport runway segmentation": {
          "authors": [
            "W Chen",
            "Z Zhang",
            "L Yu",
            "Y Tai"
          ],
          "url": "https://link.springer.com/article/10.1007/s10489-023-04586-5",
          "ref_texts": "41. Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8530\u20138539, 2020.",
          "ref_ids": [
            "41"
          ],
          "1": "Deep snake [41], DANCE [22], and Point-Set Anchors [42] deform an initial contour to match the object boundary."
        },
        "Deeply shape-guided cascade for instance segmentation": {
          "authors": [
            "Hao Ding",
            "Siyuan Qiao",
            "Alan Yuille",
            "Wei Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Ding_Deeply_Shape-Guided_Cascade_for_Instance_Segmentation_CVPR_2021_paper.html",
          "ref_texts": "[42] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In2020 IEEE/CVF Conference on Computer V ision and P attern Recognition, CVPR 2020, Seattle, W A, USA, June 13-19, 2020, pages 8530\u20138539. IEEE, 2020.",
          "ref_ids": [
            "42"
          ],
          "1": "Deep Snake [42] and PolyTransform [29] predicted position offsets w ."
        },
        "Recurrent generic contour-based instance segmentation with progressive learning": {
          "authors": [
            "H Feng",
            "K Zhou",
            "W Zhou",
            "Y Yin",
            "J Deng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10487946/",
          "ref_texts": "[26] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in CVPR, 2020, pp. 8533\u20138542. 2, 3, 4, 7, 8, 9, 10, 11",
          "ref_ids": [
            "26"
          ],
          "1": "Another pioneering method is DeepSnake [26] which directly regresses the coordinates of contour vertices in the Cartesian coordinate system.",
          "2": "Inspired by the classic Snake algorithm [27], DeepSnake [26] devises a neural network to evolve an initial contour to enclose the object boundary.",
          "4": "Some other works [26], [30], [32], [51], [33] apply Cartesian coordinate representation for vertices, and regress them towards ground-truth object boundaries.",
          "5": "Typically, DeepSnake [26] first initializes the contours based on the box predictions, and then applies several contour deformation steps for segmentation.",
          "6": "Based on DeepSnake[26], Dance [32] improves the matching scheme between predicted and target contours and introduces an attentive deformation mechanism.",
          "7": "Leveraging the principles of CenterNet [53], both DeepSnake [26] and Dance [32] initialize a contour from the detected bounding box with a rectangle, whereas E2EC [33] adopts a direct regression of an ordered vertex set to form the initial contour.",
          "8": "Following [26], [32], [33], [38], we use the circle-convolution to fuse the feature of the contour.",
          "9": "Note that we set R = 4 by default, following DeepSnake [26] and E2EC [33].",
          "10": "Following previous methods [26], [33], all the ablations are studied on the SBD [34] dataset with 20 semantic categories, which well evaluates the algorithm capability to handle various object shapes.",
          "11": "For one thing, this can be attributed to the increased training difficulty of the large model size, which is verified in DeepSnake [26].",
          "13": "We further compare our contour deformation pipeline with the classic DeepSnake [26].",
          "14": "10, we present the results of PolySnake and DeepSnake [26] in terms of the performance (left) and the parameter numbers (right) at different iterations on the SBD val set [34].",
          "15": "In DeepSnake [26], the iteration number is fixed as three by default.",
          "16": "Results of DeepSnake [26] and our PolySnake on the performance (left) and the parameter numbers (right) at different time steps on the SBD val set [34].",
          "17": "\u201c 0\u201d denotes the initial contour for DeepSnake [26] and PolySnake.",
          "18": "1 DeepSnake [26] CVPR\u201920 54.",
          "19": "8 DeepSnake [26] CVPR\u201920 DLA-34 30.",
          "20": "9 DeepSnake [26] CVPR\u201920 31.",
          "21": "The network is trained and tested at a single scale of 512\u00d7512, following [26], [33], following DeepSnake [26].",
          "22": "In Table IV, we compare the proposed PolySnake with other contour-based methods [28], [29], [26], [32], [52], [33] on the SBD val set [34].",
          "23": "Compared with the classic DeepSnake [26], PolySnake yields 5.",
          "24": "It outperforms the classic DeepSnake [26] by 4.",
          "25": "As suggested in DeepSnake [26], we apply the component detection strategy [14] to handle the fragmented instances, which are frequently occurred in the dataset.",
          "26": "We choose the model performing best on the validation set and the evaluation is conducted at a single resolution of 1216 \u00d7 2432, following DeepSnake [26].",
          "27": "THE INFERENCE TIME OF METHOD [26], [32], [33] AND POLYSNAKE ARE MEASURED ON A SINGLE RTX 2080T I GPU, WHILE WE REPORT THE RESULTS OF THE OTHER METHODS FROM THEIR ORIGINAL PAPERS .",
          "28": "6 DeepSnake [26] fine 4.",
          "29": "We outperform DeepSnake [26] by 3.",
          "30": "Following the classic DeepSnake [26], the models are trained and tested at a single resolution of 512 \u00d7 512 and 768 \u00d7 2496, respectively.",
          "31": "Besides, our PolySnake outperforms DeepSnake [26] and E2EC [33] by 3.",
          "32": "3, we employ the standard 1-D conventional convolution instead of circle-convolution [26] to perform the vertex feature aggregation."
        },
        "Centerpoly: Real-time instance segmentation using bounding polygons": {
          "authors": [
            "Hughes Perreault",
            "Alexandre Bilodeau",
            "Nicolas Saunier",
            "Maguelonne Heritier"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/AVVision/html/Perreault_CenterPoly_Real-Time_Instance_Segmentation_Using_Bounding_Polygons_ICCVW_2021_paper.html",
          "ref_texts": "[24] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "Finally, Deep Snake [24] is a fast instance segmentation method that learns to deform the generated contour of an object to iteratively better match the contour of the object."
        },
        "Iterative next boundary detection for instance segmentation of tree rings in microscopy images of shrub cross sections": {
          "authors": [
            "Alexander Gillert",
            "Giulia Resente",
            "Alba Anadon",
            "Martin Wilmking",
            "Uwe Freiherr"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Gillert_Iterative_Next_Boundary_Detection_for_Instance_Segmentation_of_Tree_Rings_CVPR_2023_paper.html",
          "ref_texts": "[17] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 8533\u2013",
          "ref_ids": [
            "17"
          ],
          "1": "Contour methods such as Deep Snake [17] or DANCE [13] can generate masks with higher precision but still require an upstream object detector.",
          "2": "Method On a high level, INBD simply modifies and extends the various contour based methods like Deep Snake [17] or PolarMask [26] with an iterative inference procedure.",
          "3": "By choosing a 2D network as opposed to a 1D one, as in many contour methods such as Deep Snake [17], we can leverage transfer learning since we are working in a low data regime and in addition to that we can reject and interpolate ambiguous predictions (see below , Eq.",
          "4": "This second network has mostly the same architecture as the first one, except that we replace the normal 2D convolutions with circular convolutions to wrap around the full circle, as also used in Deep Snake [17].",
          "5": "From the top-down category we compare with Mask-R-CNN [8] and Deep Snake [17]."
        },
        "GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model for Multi-organ Segmentation": {
          "authors": [
            "R Zhang",
            "H Guo",
            "Z Zhang",
            "P Yan",
            "S Zhao"
          ],
          "url": "https://arxiv.org/abs/2501.12844",
          "ref_texts": "[5] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2020. [Online]. Available: http://dx.doi.org/10.1109/cvpr42600.2020.",
          "ref_ids": [
            "5",
            "Online"
          ],
          "1": "Most existing approaches [5]\u2013[8] conceptualize the contour as a graph and employ graph convolutional networks to model snake evolution as a topological problem.",
          "2": "[5] propose a two-stage deep snake pipeline that utilizes a novel circular convolution for efficient feature learning to enhance snake evolution.",
          "3": "METHOD Inspired by [5], GAMED-Snake performs segmentation by iteratively deforming an initial contour to align with the target organ boundary.",
          "4": "Inspired by [5]\u2013[7], we handle contour evolution in an iterative optimization fashion.",
          "5": "The outputs are processed through multi-layer circular 1D convolutions [5] to produce the final contour-point offset vectors."
        },
        "Deep active surface models": {
          "authors": [
            "Udaranga Wickramasinghe",
            "Pascal Fua",
            "Graham Knott"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Wickramasinghe_Deep_Active_Surface_Models_CVPR_2021_paper.html",
          "ref_texts": "[34] S. Peng, W . Jiang, H. Pi, X. Li, H. Bao, and X. Zhou. Deep Snake for Real-Time Instance Segmentation. In Conference on Computer V ision and P attern Recognition , 2020. 2",
          "ref_ids": [
            "34"
          ],
          "1": "[24, 34] use graph-convolution networks to perform instance segmentation by deforming a contour."
        },
        "Object Segmentation from Open-Vocabulary Manipulation Instructions Based on Optimal Transport Polygon Matching with Multimodal Foundation Models": {
          "authors": [
            "T Nishimura",
            "K Kuyo",
            "M Kambara"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10802596/",
          "ref_texts": "[5] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep Snake for Real-Time Instance Segmentation,\u201d in CVPR, 2020, pp. 8530\u20138539.",
          "ref_ids": [
            "5"
          ],
          "1": "Recently, some studies show that proposed polygon-based mask generation methods can achieve shorter inference times compared with traditional pixel-based mask generation methods [3], [5]\u2013[7]."
        },
        "Unsupervised contour tracking of live cells by mechanical and cycle consistency losses": {
          "authors": [
            "Junbong Jang",
            "Kwonmoo Lee",
            "Kyun Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jang_Unsupervised_Contour_Tracking_of_Live_Cells_by_Mechanical_and_Cycle_CVPR_2023_paper.html",
          "ref_texts": "[26] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE/CVF Conference 235",
          "ref_ids": [
            "26"
          ],
          "1": "The proposed architectural design achieves the best accuracy among variants, including circular convolutions [26], and correspondence matrix [4].",
          "2": "There are conventional contour prediction methods such as Snake [15] or deep learning-based models such as DeepSnake [26] and DANCE [19], which performs real-time objection detection.",
          "3": "From DeepSnake [26] and PoST [24], circular convolution is known to be effective for point regression given a sequence of contour points."
        },
        "Circle Representation for Medical Instance Object Segmentation": {
          "authors": [
            "J Xiong",
            "EH Nguyen",
            "Y Liu",
            "R Deng",
            "RN Tyree"
          ],
          "url": "https://arxiv.org/abs/2403.11507",
          "ref_texts": "[30] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.",
          "ref_ids": [
            "30"
          ],
          "1": "2) Contour-based Methods: Contour-based methods such as DeepSnake [30] have the potential to be faster and simpler.",
          "2": "Snake [30] to right ventricular segmentation.",
          "3": "3, is an instance segmentation method inspired by the DeepSnake [30] method.",
          "4": "Adhering to the guidelines in [30], the value of N is fixed at 128, facilitating a consistent and streamlined process.",
          "5": "In line with the approach detailed in [30], circular convolution is employed for the learning of features.",
          "6": "Following [30], the kernel size of the circular convolution is fixed to be nine.",
          "7": "The CircleSnake model incorporates a Graph Convolutional Network (GCN), drawing inspiration from the work presented in [30], for its convolutional implementation.",
          "8": "Following [30], we regress the N offsets in 3 iterations.",
          "9": "As baseline methods, Faster-RCNN [38], Mask-RCNN [10], CenterNet [33], DeepSnake [30], were chosen for their superior object detection and object instance segmentation performance."
        },
        "Reconstructing floorplans from point clouds using GAN": {
          "authors": [
            "Tianxing Jin",
            "Jiayan Zhuang",
            "Jiangjian Xiao",
            "Ningyuan Xu",
            "Shihao Qin"
          ],
          "url": "https://www.mdpi.com/2313-433X/9/2/39",
          "ref_texts": "26. Peng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; Zhou, X. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020; pp. 8533\u20138542. J. Imaging2023, 9, 39 14 of 14",
          "ref_ids": [
            "26"
          ],
          "1": "introduced the snake algorithm into the target detection task; they use the bounding box predicted by center-net as the initial value of the contour, and then obtain instance masks by iterative deformation [26]."
        },
        "Learning panoptic segmentation from instance contours": {
          "authors": [
            "S Chennupati",
            "V Narayanan",
            "G Sistu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9560798/",
          "ref_texts": "[44] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
          "ref_ids": [
            "44"
          ],
          "1": "Deep Snake [44] recently proposed to predict instance contours by learning contours from object detection.",
          "2": "Other two-stage methods UPSNet [49] and DeepSnake [44] are lighter compared to Mask R-CNN and operate at \u223c4 fps for instance segmentation task.",
          "3": "3 DeepSnake [44] 37."
        },
        "Boundarysqueeze: Image segmentation as boundary squeezing": {
          "authors": [
            "H He",
            "X Li",
            "Y Yang",
            "G Cheng",
            "Y Tong",
            "L Weng"
          ],
          "url": "https://arxiv.org/abs/2105.11668",
          "ref_texts": "[56] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "56"
          ],
          "1": "On top of that, Snake [34] and Deep Snake [56] refine the initial object contours recursively through some specific operations such as circular convolution."
        },
        "Fast object segmentation learning with kernel-based methods for robotics": {
          "authors": [
            "F Ceola",
            "E Maiettini",
            "G Pasquale"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561758/",
          "ref_texts": "[35] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "35"
          ],
          "1": "In [35], instead, the object\u2019s contour is not treated as a general graph, but it leverages the cycle graph topology and uses the circular convolution for efficient feature learning on a contour."
        },
        "EffSeg: Efficient fine-grained instance segmentation using structure-preserving sparsity": {
          "authors": [
            "C Picron",
            "T Tuytelaars"
          ],
          "url": "https://arxiv.org/abs/2307.01545",
          "ref_texts": "[25] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "Another family of methods obtaining fine-grained segmentation masks, are contour-based methods [25, 21, 39]."
        },
        "Weakly supervised volumetric image segmentation with deformed templates": {
          "authors": [
            "U Wickramasinghe",
            "P Jensen",
            "M Shah",
            "J Yang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-16443-9_41",
          "ref_texts": "[20] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep Snake for RealTime Instance Segmentation. In: Conference on Computer Vision and Pattern Recognition (2020) 3",
          "ref_ids": [
            "20"
          ],
          "1": "This is addressed in [20] by introducing deep deformable contours that only need a simple approximate contour for initialization purposes."
        },
        "CircleSnake: instance segmentation with circle representation": {
          "authors": [
            "EH Nguyen",
            "H Yang",
            "Z Asad",
            "R Deng",
            "AB Fogo"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-21014-3_31",
          "ref_texts": "16. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8533\u20138542 (2020)",
          "ref_ids": [
            "16"
          ],
          "1": "In contrast, DeepSnake [16] addresses localization errors from the detector by deforming the detected bounding octagon to object boundaries.",
          "2": "Inspired by [16], the object segmentation is implemented by deforming an initial rough contour to match object boundary.",
          "3": "The N is set to 128 based on [16].",
          "4": "We utilize the circular convolution for the feature learning, as illustrated [16].",
          "5": "Following [16], the kernel size of the circular convolution is fixed to be nine.",
          "6": "In CircleSnake, the above convolution is implemented via the graph convolutional network (GCN) inspired by [16].",
          "7": "Following [16], we regress the N offsets in 3 iterations.",
          "8": "3 Experimental Design For baseline methods, DeepSnake [16] was utilized for their superior performance in instance segmentation.",
          "9": "647 DeepSnake [16] (Detection) DLA 0.",
          "10": "635 DeepSnake [16] (Segmentation) DLA 0.",
          "11": "Methods Backbone Glomeruli (Dice) DeepSnake [16] DLA 0."
        },
        "Cracks identification using mask region-based denoised deformable convolutional network": {
          "authors": [
            "Kia Wei"
          ],
          "url": "https://link.springer.com/article/10.1007/s11042-022-13422-w",
          "ref_texts": "28. Peng S, Jiang W, Pi H, Li X, Bao H, Zhou X (2020) Deep snake for real-time instance segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 8533\u20138542",
          "ref_ids": [
            "28"
          ],
          "1": "On the other hand, DeepSnake network [28] applies the classic contour-based approach to deform an initial contour to match the object boundary using deep snake algorithm."
        },
        "Real-time instance segmentation with polygons using an Intersection-over-Union loss": {
          "authors": [
            "K Jodogne-del Litto",
            "GA Bilodeau"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10229851/",
          "ref_texts": "[18] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep Snake for Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Apr. 2020.",
          "ref_ids": [
            "18"
          ],
          "1": "Deep Snake [18] uses contour generation and iterative contour deformation to segment instances.",
          "2": "00 > 1 DeepSnake [18] Outline Hourglass-104 31."
        },
        "End-to-end instance edge detection": {
          "authors": [
            "X Zou",
            "H Liu",
            "YJ Lee"
          ],
          "url": "https://arxiv.org/abs/2204.02898",
          "ref_texts": "50. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8533\u20138542 (2020) 4",
          "ref_ids": [
            "50"
          ],
          "1": "Since then, researchers have explored various directions to improve efficiency [4,50] and effectiveness [29,31,13]."
        },
        "GEA-MSNet: A Novel Model for Segmenting Remote Sensing Images of Lakes Based on the Global Efficient Attention Module and Multi-Scale Feature Extraction": {
          "authors": [
            "Qiyan Li",
            "Zhi Weng",
            "Zhiqiang Zheng",
            "Lixin Wang"
          ],
          "url": "https://www.mdpi.com/2076-3417/14/5/2144",
          "ref_texts": "59. Peng, S.; Jiang, W.B.; Pi, H.; Bao, H.; Zhou, X. Deep Snake for Real-Time Instance Segmentation. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 13\u201319 June 2020; pp. 8530\u20138539. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
          "ref_ids": [
            "59"
          ],
          "1": "Discussion In the research of semantic segmentation algorithms, ablation experiments serve as an effective approach to validate the significance of each module [59].",
          "2": "Discussion In the research of semantic segmentation algorithms, ablation experiments serve as an effective approach to validate the significance of each module [59]."
        },
        "Eosinophils instance object segmentation on whole slide imaging using multi-label circle representation": {
          "authors": [
            "Y Liu",
            "R Deng",
            "J Xiong",
            "RN Tyree"
          ],
          "url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12933/129330I/Eosinophils-instance-object-segmentation-on-whole-slide-imaging-using-multi/10.1117/12.3005995.short",
          "ref_texts": "[31] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., and Zhou, X., \u201cDeep snake for real-time instance segmentation,\u201d in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition], 8533\u20138542 (2020).",
          "ref_ids": [
            "31",
            "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          "2": ", \u201cDeep snake for real-time instance segmentation,\u201d in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition], 8533\u20138542 (2020)."
        },
        "Unifying visual perception by dispersible points learning": {
          "authors": [
            "J Liang",
            "G Song",
            "B Leng",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_26",
          "ref_texts": "31. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: CVPR. pp. 8533\u20138542 (2020)",
          "ref_ids": [
            "31"
          ],
          "1": "PolarMask [45] and DeepSnake [31] are two typical works using this idea.",
          "2": "To align the point number between scattered points and the contour points in training data, we uniformly add new points, or delete points with the shortest edge until the target number is met, which is similar to Deep Snake [31]."
        },
        "Polyline generative navigable space segmentation for autonomous visual navigation": {
          "authors": [
            "Z Chen",
            "Z Ding",
            "DJ Crandall"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10048510/",
          "ref_texts": "[10] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.",
          "ref_ids": [
            "10"
          ],
          "1": "The boundary could be represented by polylines/splines (represented by vertices or control points) [9], [10], which are naturally robust to noise in images.",
          "2": "The polygon/spline representation is used in [10], [17]\u2013[20] to achieve fast and potentially interactive instance segmentation."
        },
        "FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting": {
          "authors": [
            "A Das",
            "S Biswas",
            "U Pal",
            "J Llad\u00f3s"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-78498-9_10",
          "ref_texts": "31. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real-time instance segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8533\u20138542 (2020) FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting 15",
          "ref_ids": [
            "31"
          ],
          "1": "To optimize training, we introduce a novel attention module, SAC2 (Self-Attention with Circular Convolutions), inspired by [44,31].",
          "2": "To address this, we incorporate local circular convolution [31] to bolster factorized self-attention (FSA)."
        },
        "Deep ContourFlow: Advancing Active Contours with Deep Learning": {
          "authors": [
            "A Habis",
            "V Meas-Yedid",
            "E Angelini"
          ],
          "url": "https://arxiv.org/abs/2407.10696",
          "ref_texts": "[1] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. , pp. 8530\u20138539, 2020, doi: 10.1109/CVPR42600.2020.00856.",
          "ref_ids": [
            "1"
          ],
          "1": "To our knowledge, unlike [1], [2], [3], [4], this is the first paper to use the relevance of CNN\u2019s features while retaining the spirit of active contours that require no training."
        },
        "Segmenting objects with Bayesian fusion of active contour models and convnet priors": {
          "authors": [
            "P Polewski",
            "J Shelton",
            "W Yao",
            "M Heurich"
          ],
          "url": "https://arxiv.org/abs/2410.07421",
          "ref_texts": "[26] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d 2020. [Online]. Available: https://arxiv.org/abs/2001.01629",
          "ref_ids": [
            "26",
            "Online"
          ],
          "2": "In the DeepSnakes approach [26], the contour is initialized to N vertices sampled from the bounding octagon, and the CNN extracts offsets to the object boundary based on convolutional feature maps and the circular convolution operator."
        },
        "Pix2Poly: A Sequence Prediction Method for End-to-end Polygonal Building Footprint Extraction from Remote Sensing Imagery": {
          "authors": [
            "YK Adimoolam",
            "C Poullis",
            "M Averkiou"
          ],
          "url": "https://arxiv.org/abs/2412.07899",
          "ref_texts": "[29] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In CVPR, 2020. 14",
          "ref_ids": [
            "29"
          ],
          "1": "00 DeepSnake [29] 93."
        },
        "Polygonal point set tracking": {
          "authors": [
            "Gunhee Nam",
            "Miran Heo",
            "Seoung Wug",
            "Young Lee",
            "Seon Joo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Nam_Polygonal_Point_Set_Tracking_CVPR_2021_paper.html",
          "ref_texts": "[36] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition, 2020.",
          "ref_ids": [
            "36"
          ],
          "1": "Point set representation is also employed for image instance segmentation as well [36, 29].",
          "2": "Other options to handle the representation are proposed in those methods such as circular convolution [36] and transformer [29].",
          "3": "Backbone is constructed by stacking 8 base blocks where each block consists of two circular convolutions [36] followed by multi-head attention (MHA) [43]."
        },
        "CenterDisks: Real-time instance segmentation with disk covering": {
          "authors": [
            "KJD Litto",
            "GA Bilodeau"
          ],
          "url": "https://arxiv.org/abs/2403.03296",
          "ref_texts": "[Online]. Available: [4] H. Perreault, G.-A. Bilodeau, N. Saunier, and M. H \u00b4eritier, \u201cCenterPoly: Real-Time Instance Segmentation Using Bounding Polygons,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2982\u20132991. [Online]. Available: [5] P. Hurtik, V . Molek, J. Hula, M. Vajgl, P. Vlasanek, and T. Nejezchleba, \u201cPoly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3,\u201d Neural Computing and Applications , vol. 34, no. 10, pp. 8275\u20138290, May 2022. [Online]. Available: [6] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep Snake for Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Apr.",
          "ref_ids": [
            "Online",
            "4",
            "Online",
            "5",
            "Online",
            "6"
          ],
          "5": "CV] 5 Mar 2024 Our contributions are the following: \u2022 We propose a new approach for mask approximation with disk covering; \u2022 We show that a deep neural network can learn to cover a surface by positioning a set of disks; \u2022 Our proposed method is real-time and achieves state-of-the art results on the IDD and KITTI test set.",
          "39": "Except for CenterPoly, our results for AP50% are competitive with other real-time method.",
          "42": "90 in AP50% for IDD, and for KITTI, we improve the state-of-the-art method by 3 points for the AP metric and 10.",
          "43": "5 points for the AP50% metric.",
          "125": "The accuracy is thus improved by 4 points for the AP50%."
        },
        "Se-psnet: Silhouette-based enhancement feature for panoptic segmentation network": {
          "authors": [
            "SE Chang",
            "Y Chen",
            "YC Yang",
            "ET Lin",
            "PY Hsiao"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1047320322002565",
          "ref_texts": "[24] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8533\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "DeepSnake [24] performs instance segmentation by deforming an initial contour to match object boundary with proposed circular convolution.",
          "2": "Silhouette Features Inspired by PolarMask [27] and Deep snake [24] which formulate the instance segmentation problem as predicting contour, the silhouette can be viewed as a critical feature in the segmentation task."
        },
        "A coarse-to-fine instance segmentation network with learning boundary representation": {
          "authors": [
            "F Luo",
            "X Li",
            "BB Gao",
            "J Yan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9533399/",
          "ref_texts": "[6] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d inProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2020, pp. 8533\u20138542.",
          "ref_ids": [
            "6"
          ],
          "3": "Similarly, Deep Snake [6] employs CenterNet [23] to locate instances and then gradually deforms four midpoints of bounding boxes\u2019 edges to the target positions.",
          "5": "Moreover, with simple end-to-end training, our method outperforms Deep Snake [6]."
        },
        "ContourFormer: Real-Time Contour-Based End-to-End Instance Segmentation Transformer": {
          "authors": [
            "C Li",
            "M Xiong",
            "W Dong",
            "H Chen",
            "X Xiao"
          ],
          "url": "https://arxiv.org/abs/2501.17688",
          "ref_texts": "[23] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "23"
          ],
          "2": "For instance, PolarMask [12] reformulated instance segmentation as a contour regression problem in polar coordinates, while DeepSnake[23] initializes contours using bounding box predictions, followed by multiple deformation steps to complete the segmentation task.",
          "3": "Method Venue APvol AP50 AP70 DeepSnake[23] CVPR\u20192020 54.",
          "4": "Method Venue Backbone APval APtest\u2212dev DeepSnake[23] CV PR\u20322020 DLA-34 30.",
          "5": "Method Venue AP DeepSnake[23] CV PR\u20322020 31."
        },
        "Instance segmentation with unsupervised adaptation to different domains for autonomous vehicles": {
          "authors": [
            "M Diaz-Zapata",
            "\u00d6 Erkent"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9305452/",
          "ref_texts": "[17] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.",
          "ref_ids": [
            "17"
          ],
          "1": "In the case of the deep snake [17] approach, the network takes a contour as input and outputs vertex-wise offsets to deform the contour in order to match the boundary of the detected instance."
        },
        "DArch: Dental Arch Prior-assisted 3D Tooth Instance Segmentation": {
          "authors": [
            "L Qiu",
            "C Ye",
            "P Chen",
            "Y Liu",
            "X Han",
            "S Cui"
          ],
          "url": "https://arxiv.org/abs/2204.11911",
          "ref_texts": "[17] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8533\u2013",
          "ref_ids": [
            "17"
          ],
          "1": "To estimate the dental arch of each dental model, we first formulate the dental arch by representing it as a curve passing through teeth centroids and then adopt a lightweight 1-D convolutional network to refine the dental arch [10, 17]."
        },
        "Enhancing Polygonal Building Segmentation via Oriented Corners": {
          "authors": [
            "MM Sheikholeslami",
            "M Kamran",
            "A Wichmann"
          ],
          "url": "https://arxiv.org/abs/2407.12256",
          "ref_texts": "[12] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8533\u20138542, 2020. 3",
          "ref_ids": [
            "12"
          ],
          "1": "Initialization Module Many methods in literature [9, 10, 12, 17] use a fixed number of predefined corners, e."
        },
        "Deep level set for box-supervised instance segmentation in aerial images": {
          "authors": [
            "W Li",
            "Y Chen",
            "W Liu",
            "J Zhu"
          ],
          "url": "https://arxiv.org/abs/2112.03451",
          "ref_texts": "[31] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8533\u20138542, 2020. 3",
          "ref_ids": [
            "31"
          ],
          "1": "Inspired by DeepSnake [31], we regard the rotated RoIs as the initial contour explicitly and K vertices {(xk,yk)}k=1,.",
          "2": "The separate snake modules [31] are used to process the K vertices to iteratively refine the contour vertices towards the groundtruth rotated bounding box."
        },
        "Weakly-supervised arbitrary-shaped text detection with expectation-maximization algorithm": {
          "authors": [
            "M Zhao",
            "W Feng",
            "F Yin",
            "XY Zhang",
            "CL Liu"
          ],
          "url": "https://arxiv.org/abs/2012.00424",
          "ref_texts": "[20] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3, 4",
          "ref_ids": [
            "20"
          ],
          "1": "Contour-Based Text Detection Inspired by [20], we propose a contour based arbitrary-shaped text detector.",
          "2": "In [39] and [20], octagon enclose the arbitrary-shaped object much tighter than the rectangle.",
          "3": "After that, we adopt the Deep Snake, a contour regression model proposed by [20]."
        },
        "Synthetic Instance Segmentation from Semantic Image Segmentation Masks": {
          "authors": [
            "Y Shen",
            "D Zhang",
            "Z Zhang",
            "L Fu",
            "Q Ye"
          ],
          "url": "https://arxiv.org/abs/2308.00949",
          "ref_texts": "[8] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition , 2020.",
          "ref_ids": [
            "8"
          ],
          "1": "In particular, it is worth mentioning that the vision community has made remarkable progress in instance segmentation by leveraging both instance-level object detection and semantic segmentation tasks [8]\u2013[10].",
          "2": "For the second category , the methods first predict pixel-wise masks and then group instance-level objects together based on a set of instance cues [8], [18], e.",
          "3": "As shown in the first block of Table III, without instance-level mask supervision, our proposed \u201cSISeg (PSPNet)\u201d can still achieve over 75% of performance of fully-supervised approaches [4], [8], [10], [11], [18] under weak supervision.",
          "4": "AP (%) AP 50 (%) AP 75 (%) FPS (a) Fully-supervised Deep Snake [8] CVPR 2020 \u2013 F \u2013 62."
        },
        "BezierSeg: Parametric shape representation for fast object segmentation in medical images": {
          "authors": [
            "Haichou Chen",
            "Yishu Deng",
            "Bin Li",
            "Zeqin Li",
            "Haohua Chen",
            "Bingzhong Jing",
            "Chaofeng Li"
          ],
          "url": "https://www.mdpi.com/2075-1729/13/3/743",
          "ref_texts": "20. Peng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; Zhou, X. Deep Snake for Real-time Instance Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13\u201319 June 2020; pp. 8533\u20138542.",
          "ref_ids": [
            "20"
          ],
          "1": "DeepSnake [20] exploits the special topology of the object boundary as prior knowledge, and adopts circular convolution to predict the vertices along the object boundary."
        },
        "Learning Stixel-based Instance Segmentation": {
          "authors": [
            "M Santarossa",
            "L Schneider",
            "C Zelenka"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9575565/",
          "ref_texts": "[18] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "18"
          ],
          "1": "While over the last years instance segmentation of pixel images has been addressed with increasingly high accuracy [14]\u2013[18], it remains a costly procedure, with state-of-the-art CNN-based methods reaching less than 5 fps on Cityscapes [17], [18].",
          "3": "4 Deep Snake [18] 217 4.",
          "4": "00GHz) \u2022 Deep Snake [18] with 4.",
          "5": "StixelPointNet is also shown to be considerably faster than pixel-based approaches [17], [18] submitted to the same benchmark."
        },
        "PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction in High-definition Maps": {
          "authors": [
            "H Peng",
            "Y Zhan",
            "B Wang",
            "H Zhang"
          ],
          "url": "https://arxiv.org/abs/2401.14024",
          "ref_texts": "[13] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep Snake for Real-Time Instance Segmentation,\u201d arXiv preprint arXiv:2001.01629, ",
          "ref_ids": [
            "13"
          ],
          "1": "Furthermore , we compare PLCNet with a modified Deepsnake method [13] to showcase its superior performance .",
          "2": "We use focal loss and smooth-L1 loss applied in [13] to optimize the binary segmentation map and lane correction process, respectively.",
          "3": "W e make two modifications for the lane correction task: (1) setting initial lanes as the initial contours and (2) only optimizing the center and smooth-L1 loss [13]."
        },
        "Deep Learning in Medical Image Analysis": {
          "authors": [
            "Hao Tang"
          ],
          "url": "https://search.proquest.com/openview/b8effb61a6560440b195ff2a27719a47/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[99] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "99"
          ],
          "1": "This design draws inspiration from recent works [140, 99, 55] that employ iterative refinement."
        },
        "SCR: Smooth Contour Regression with Geometric Priors": {
          "authors": [
            "G Bahl",
            "L Daniel",
            "F Lafarge"
          ],
          "url": "https://arxiv.org/abs/2202.03784",
          "ref_texts": "[23] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In CVPR, 2020. 1, 2, 5, 6",
          "ref_ids": [
            "23"
          ],
          "2": "Snake-based methods [22, 23] represent shapes as polygons in pixel coordinates and deform them iteratively using circular convolutions.",
          "3": "Quantitative comparison We compare our method to existing shape encoding methods [31, 35, 37], as well as state-of-the-art snake-based [22, 23] and mask-based methods [8, 20, 34].",
          "4": "Method Ncoeff mAP FPS SEC OES Snake DeepSnake [23] 256 31."
        },
        "Growing Instance Mask on Leaf": {
          "authors": [
            "C Yang",
            "H Ma",
            "Q Wang"
          ],
          "url": "https://arxiv.org/abs/2211.16738",
          "ref_texts": "[20] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proc. CVPR, 2020, pp. 8533\u20138542.",
          "ref_ids": [
            "20"
          ],
          "1": "To pursue a straightforward and effective instance segmentation pipeline, recent methods [17], [18], [19], [20], [21], [11], [10], [22] formulated instance segmentation problem as contour points prediction.",
          "2": "Some researches [18], [20], [21], [19], [11] proposed to regress ordered contour point sequences.",
          "3": "It avoids multiple stages serial feature enhancement process in [20], [11], which brings gains with negligible costs.",
          "4": "4 contour-based multi-stage DeepSnake [20] 128 30.",
          "5": "As depicted in Table IIIb, for multi-stage contour-based methods (DeepSnake [20] and E2EC [11], the proposed method can achieve 32."
        },
        "Occlusion-Ordered Semantic Instance Segmentation": {
          "authors": [
            "Soroosh Baselizadeh"
          ],
          "url": "https://uwspace.uwaterloo.ca/handle/10012/19747",
          "ref_texts": "[81] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8533\u20138542, 2020.",
          "ref_ids": [
            "81"
          ],
          "1": "2 Contour-based Instance Segmentation Contour-based methods, including Curve GCN [64], Deep Snake [81], PolarMask [120], and LSNet [28], have recently gained attention and demonstrated promising results."
        },
        "ContourRender: Detecting arbitrary contour shape for instance segmentation in one pass": {
          "authors": [
            "T Tang",
            "W Xu",
            "R Ye",
            "YF Wang",
            "C Lu"
          ],
          "url": "https://arxiv.org/abs/2106.03382",
          "ref_texts": "[37] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u20138542, 2020.",
          "ref_ids": [
            "37"
          ],
          "5": "DeepSnake [37] proposed a circle convolution operator to adapt to the polygon representation.",
          "6": "2 Contour-based DeepSnake [37] DLA-34 30.",
          "8": "8 DeepSnake* [37] DLA-34 28.",
          "9": "2 DeepSnake [37] DLA-34 31.",
          "10": "Besides, our methods also beat the iterative approach DeepSnake [37], even though the latter approach use the iterative optimization to refine the contour while ours do not.",
          "11": "We notice that a few methods [37, 1] report AP metric on Cityscapes, to compare with them, we also report the AP metric on Cityscapes in Table 3."
        },
        "Glacier Segmentation in Satellite Images for Hindu Kush Himalaya Region": {
          "authors": [
            "Bibek Aryal"
          ],
          "url": "https://search.proquest.com/openview/a7e750a8296184a3653abed92ccaa1a9/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[42] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., and Zhou, X. Deep Snake for Real-Time Instance Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020), pp. 8533\u20138542.",
          "ref_ids": [
            "42"
          ],
          "1": "Combination of deep learning and contour models have also been shown to perform very well for the general segmentation tasks [42]."
        },
        "Semantic SLAM system for mobile robots based on large visual model in complex environments": {
          "authors": [
            "C ZHENG",
            "P ZHANG",
            "Y LI"
          ],
          "url": "https://www.researchsquare.com/article/rs-4634722/latest",
          "ref_texts": "[33] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H. and Zhou, X. \u201cDeep snake for real-time instance segmentation,\u201d Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2117-2125 (2020). ",
          "ref_ids": [
            "33"
          ],
          "1": "There are three commonly used semantic information extraction methods in semantic vSLAM, including object detection [29,30], semantic segmentation [31,32], and instance segmentation [33,34]."
        },
        "Instance Segmentation with Mask-supervised Polygonal Regression Transformers": {
          "authors": [
            "Justin Lazarow"
          ],
          "url": "https://par.nsf.gov/biblio/10350146",
          "ref_texts": "[25] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InCVPR, 2020. 2, 3, 7",
          "ref_ids": [
            "25"
          ],
          "1": "As one of the first works built on modern architectures, DeepSnake [25] considers an initial octagonal polygon derived from four extreme points, after which an iterative process of refinement is carried out using circular convolutions.",
          "2": "We use Bi as a means to initialize an ellipsoidal polygon Vi(0) inscribed in Bi, similar to other contour-based methods [14, 20, 25].",
          "3": "2 DeepSnake [25] R50-FPN CenterNet polygons polygons 30.",
          "4": "5 --PolarMask [25] R101-FPN FCOS 2\u00d7 polygons polygons 32.",
          "5": "0 DeepSnake [25] ImageNet extreme pts \u2713 poly 28."
        },
        "Chinese Character Stroke Extraction Method based on Hierarchical Multi-Model": {
          "authors": [
            "S Ma",
            "Y Xu"
          ],
          "url": "https://www.researchsquare.com/article/rs-4534522/latest",
          "ref_texts": "[16] S. Peng, W. Jiang, H. Pi, H. Bao, and X. Zhou. Deep snake for real-time instance segmentation.IEEE, 2020.",
          "ref_ids": [
            "16"
          ],
          "1": "instance segmentation methods, such as YOLACT [15], DeepSnake [16] and so on."
        },
        "Sparse Shape Encoding for Improved Instance Segmentation": {
          "authors": [
            "K Liu"
          ],
          "url": "https://yorkspace.library.yorku.ca/items/862eabaa-e883-4c88-a584-2a708bef3366",
          "ref_texts": "[36] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8530\u20138539, 2020.",
          "ref_ids": [
            "36"
          ],
          "1": "Alternatively, additional post-hoc computations can be used to fix topological errors: DeepSnake [36] uses an extra part detector to infer disconnected shape components first and perform instance segmentation on each detected component.",
          "2": "DeepSnake [36] relaxes the polar assumption and estimates shapes by recurrent deformation of boundary vertices, disconnected components are handled independently with the aid of a part detector.",
          "3": "DeepSnake [36] is also a contour-based method but is trained with a double-stack hourglass backbone for 160 epochs, and reports a 0.",
          "4": "Class System # Epochs Mask AP Contour DeepSnake[36] 160 0."
        },
        "U2-ONet: A two-level nested octave U-structure with multiscale attention mechanism for moving instances segmentation": {
          "authors": [
            "C Wang",
            "C Li",
            "B Luo"
          ],
          "url": "https://arxiv.org/abs/2007.13092",
          "ref_texts": "[27] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp.",
          "ref_ids": [
            "27"
          ],
          "1": "Recently, a few works propose some novel contourbased approaches for real-time instance segmentation [27], [28].",
          "2": "Deep Snake [27] uses the circular convolution for feature learning on the contour and proposes a two-stage pipeline including initial contour proposal and contour deformation for instance segmentation."
        },
        "Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections": {
          "authors": [
            "MWUF von Lukas"
          ],
          "url": "https://publica.fraunhofer.de/bitstreams/39866c0d-c305-407e-92b0-adcdc3c0f1c0/download",
          "ref_texts": "[17] Sida Peng, W en Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 8533\u2013",
          "ref_ids": [
            "17"
          ],
          "1": "Contour methods such as Deep Snake [17] or DANCE [13] can generate masks with higher precision but still require an upstream object detector.",
          "2": "Method On a high level, INBD simply modifies and extends the various contour based methods like Deep Snake [17] or PolarMask [26] with an iterative inference procedure.",
          "3": "By choosing a 2D network as opposed to a 1D one, as in many contour methods such as Deep Snake [17], we can leverage transfer learning since we are working in a low data regime and in addition to that we can reject and interpolate ambiguous predictions (see below , Eq.",
          "4": "This second network has mostly the same architecture as the first one, except that we replace the normal 2D convolutions with circular convolutions to wrap around the full circle, as also used in Deep Snake [17].",
          "5": "From the top-down category we compare with Mask-R-CNN [8] and Deep Snake [17]."
        },
        "Research Article An Elliptic Centerness for Object Instance Segmentation in Aerial Images": {
          "authors": [
            "Y Luo",
            "J Han",
            "Z Liu",
            "M Wang",
            "GS Xia"
          ],
          "url": "https://spj.science.org/doi/pdf/10.34133/2022/9809505?adobe_mc=MCMID%3D13000005405683999525849378418609464876%7CMCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1709856000",
          "ref_texts": "[10] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation, \u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 8533 \u20138542, Seattle, USA, 2020.",
          "ref_ids": [
            "10"
          ],
          "1": "DeepSnake [10] introduces circular convolution, which finishes segmentation by transforming the box detected by CenterNet [19] into a polygon."
        },
        "Applications of Computer Image Recognition Technologies for Animal Behavior Analysis": {
          "authors": [
            "Bowen Shen"
          ],
          "url": "https://books.google.com/books?hl=en&lr=&id=RpUjEQAAQBAJ&oi=fnd&pg=PA276&ots=z78en3S3Vt&sig=rasS2KZ7xvfETqKC6N_9lHBQs_I",
          "ref_texts": "4. Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., & Zhou, X.: Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8533-8542. IEEE, Seattle (2020).",
          "ref_ids": [
            "4"
          ],
          "1": "The Deep Snake model is a feature extraction model that utilizes contour extraction and gated transformer network to identify animal features, using contour features to characterize the spatial differences in various behaviors [4,5]."
        },
        "TopDiG: Class-agnostic Topological Directional Graph Extraction from Remote Sensing Images\u2013Supplementary Material": {
          "authors": [
            "B Yang",
            "M Zhang",
            "Z Zhang",
            "Z Zhang",
            "X Hu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Yang_TopDiG_Class-Agnostic_Topological_CVPR_2023_supplemental.pdf",
          "ref_texts": "[10] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8533\u2013",
          "ref_ids": [
            "10"
          ],
          "1": "In terms of contour-based approaches, two influential workflows called Curve-GCN [8] and Deep Snake [10] are evaluated on polygon-shape targets."
        },
        "Discovering and Segmenting Unseen Objects for Robot Perception": {
          "authors": [
            "C Xie"
          ],
          "url": "https://search.proquest.com/openview/b7adbc66fecd0daf9688f798b6778830/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[119] Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, and Xiaowei Zhou. Deep snake for real-time instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
          "ref_ids": [
            "119"
          ],
          "1": "Other methods examine dense sliding-window instance segmentation on 4D tensors [32], combining top-down and bottom-up methods via blending modules [27], and alternative mask representations such as contours [119]."
        },
        "Multi-task Learning for Visual Perception in Automated Driving": {
          "authors": [
            "S Chennupati"
          ],
          "url": "https://deepblue.lib.umich.edu/handle/2027.42/167355",
          "ref_texts": "[30] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, \u201cDeep snake for real-time instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
          "ref_ids": [
            "30"
          ],
          "1": "Panoptic segmentation [22, 23, 24, 25], a joint semantic [5, 6, 7] and instance segmentation [26, 27, 28, 29, 30] has provided complete scene understanding by categorizing a pixel into distinct categories and instances.",
          "2": "Deep Snake [30] recently proposed to predict instance contours by learning contours from object detection.",
          "4": "Other two-stage methods UPSNet [25], and DeepSnake [30] are lighter compared to Mask R-CNN and operate at \u223c4 fps, for instance segmentation task."
        },
        "\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u52d5\u753b\u89e3\u6790\u3092\u7528\u3044\u305f 4 \u8db3\u52d5\u7269\u306e\u884c\u52d5\u306e\u7570\u5e38\u691c\u77e5\u624b\u6cd5\u306e\u691c\u8a0e": {
          "authors": [
            "\u9ad8\u4e45\u512a\u5178\uff0c \u7530\u6751\u4ec1"
          ],
          "url": "https://www.ieice.org/publications/conference-FIT-DVDs/FIT2023/data/html/program/pdf/H-007.pdf",
          "ref_texts": "[12] Peng, S., Jiang, W., Pi, H., Li, X., Bao, H., Zhou, X.: Deep snake for real -time instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8533\u20138542, 2020 ",
          "ref_ids": [
            "12"
          ],
          "1": "\u52d5\u7269\u306e \u95a2 \u7bc0 \u4f4d \u7f6e \u5ea7 \u6a19 \u306f DeepLabCut[11]\u3084 DeepSnake[12],MMpose[13]\u306a \u3069\u3092\u7528\u3044\u3066\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b ."
        }
      }
    },
    {
      "title": "easyvolcap: accelerating neural volumetric video research",
      "id": 52,
      "valid_pdf_number": "1/1",
      "matched_pdf_number": "1/1",
      "matched_rate": 1.0,
      "citations": {
        "Envgs: Modeling view-dependent appearance with environment gaussian": {
          "authors": [
            "T Xie",
            "X Chen",
            "Z Xu",
            "Y Xie",
            "Y Jin",
            "Y Shen"
          ],
          "url": "https://arxiv.org/abs/2412.15215",
          "ref_texts": "[40] Zhen Xu, Tao Xie, Sida Peng, Haotong Lin, Qing Shuai, Zhiyuan Yu, Guangzhao He, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Easyvolcap: Accelerating neural volumetric video research. In SIGGRAPH Asia 2023 Technical Communications, pages 1\u20134. 2023. 6",
          "ref_ids": [
            "40"
          ],
          "1": "Implementation Details We implement EnvGS with custom OptiX kernels and optimize our model using the PyTorch framework [28, 40] with the Adam optimizer [17]."
        }
      }
    },
    {
      "title": "pvnet: pixel-wise voting network for 6dof pose estimation. in 2019 ieee",
      "id": 41,
      "valid_pdf_number": "625/840",
      "matched_pdf_number": "513/625",
      "matched_rate": 0.8208,
      "citations": {
        "End-to-end probabilistic geometry-guided regression for 6dof object pose estimation": {
          "authors": [
            "T P\u00f6llabauer",
            "J Li",
            "V Knauthe",
            "S Berkei"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10896093/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q.-X. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 4556\u20134565, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID: 57189382",
          "ref_ids": [
            "10",
            "Online"
          ],
          "1": "Among these are SingleShotPose [9], PVNet [10], HybridPose [11], CDPN [12], EPOS [13], SurfEmb [14], ZebraPose [15], GDR-Net [16], and its improved version GDRNPP [17]."
        },
        "Novel Object 6D Pose Estimation with a Single Reference View": {
          "authors": [
            "J Liu",
            "W Sun",
            "K Zeng",
            "J Zheng",
            "H Yang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2503.05578",
          "ref_texts": "[58] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "58"
          ],
          "1": "Currently, instance-level methods [10, 11, 26, 58, 67, 68, 71, 78] have attained high precision but are limited to objects encountered during training."
        },
        "GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation": {
          "authors": [
            "W Li",
            "H Xu",
            "J Huang",
            "H Jung",
            "PKT Yu",
            "N Navab"
          ],
          "url": "https://arxiv.org/abs/2502.04293",
          "ref_texts": "[48] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "48"
          ],
          "1": "Early visual feature extractors relied on CNN backbones [28, 32, 48, 58, 72, 76] to predict or refine object poses from single RGB images."
        },
        "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation": {
          "authors": [
            "J Liu",
            "W Sun",
            "H Yang",
            "P Deng",
            "C Liu",
            "N Sebe"
          ],
          "url": "https://arxiv.org/abs/2502.02525",
          "ref_texts": "[6] S. Peng, X. Zhou, Y. Liu, H. Lin, Q. Huang, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof object pose estimation,\u201d inIEEE. Trans. Pattern. Anal. Mach. Intell., vol. 44, pp. 3212\u20133223, 2022.",
          "ref_ids": [
            "6"
          ],
          "1": "Instance-level methods [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] are restricted to specific objects the model has been trained on, which greatly limits their practical applicability."
        },
        "Diffusion Suction Grasping with Large-Scale Parcel Dataset": {
          "authors": [
            "DT Huang",
            "X He",
            "D Hua",
            "D Yu",
            "ET Lin"
          ],
          "url": "https://arxiv.org/abs/2502.07238",
          "ref_texts": "[8] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "8"
          ],
          "1": "Many object 6D pose estimation models [6], [7], [8], [9] project the pre-defined suction configuration onto the scene.",
          "2": "[6] estimated the 6D pose of the object [7], [8], [9] and projected the pre-defined suction configuration onto the objects in the scene."
        },
        "Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks": {
          "authors": [
            "Y Jin",
            "Y Zhang",
            "Z Xu",
            "W Zhang",
            "J Xu"
          ],
          "url": "https://arxiv.org/abs/2502.03877",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "5"
          ],
          "1": "[5] addressed some of these issues with PVNet, a pixel-wise voting network that improved robustness in cluttered environments."
        },
        "A survey of 6dof object pose estimation methods for different application scenarios": {
          "authors": [
            "Jian Guan",
            "Yingming Hao",
            "Qingxiao Wu",
            "Sicong Li",
            "Yingjian Fang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/4/1076",
          "ref_texts": "46. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6Dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "46"
          ],
          "1": "The second stage refers to the voting-based PVNet [46] method for template matching.",
          "2": "To address the challenges posed by severe occlusion, PVNet [46] builds upon the symmetry handling approach introduced in [60].",
          "3": "The proposed pose estimation framework in PVNet [46] regresses pixel-level vectors that point to the keypoints.",
          "4": "PVNet [46] effectively solves the problem of occlusion and has laid the foundation for much of the subsequent work in the field.",
          "6": "Hybridpose [68] proposes a network architecture based on PVNet [46], leveraging a prediction network with three intermediate representations using ResNet [69].",
          "9": "A typical example is PVNet [46], which utilizes the principles of PointNet [89].",
          "10": "There are already some methods that improve accuracy through refinement, the usage of PoseCNN [20] results in DeepIM [33] and the combination of Repose [74] and PVNet [46] are successful examples."
        },
        "6d-diff: A keypoint diffusion framework for 6d object pose estimation": {
          "authors": [
            "Li Xu",
            "Haoxuan Qu",
            "Yujun Cai",
            "Jun Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_6D-Diff_A_Keypoint_Diffusion_Framework_for_6D_Object_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u2013",
          "ref_ids": [
            "44"
          ],
          "7": "Compared to this type of direct methods, correspondence-based methods[5, 19, 43, 44, 46, 53, 56] often demonstrate better performance, which estimate 6D object poses via learning 2D-3D correspondences between the observed image and the object 3D model.",
          "9": "Later, PVNet [44] achieved better performance by estimating 2D keypoints for sampled points on the surface of the object 3D model via pixel-wise voting."
        },
        "Object pose estimation via the aggregation of diffusion features": {
          "authors": [
            "Tianfu Wang",
            "Guosheng Hu",
            "Hongguang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_Object_Pose_Estimation_via_the_Aggregation_of_Diffusion_Features_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "PVNet [28] regresses pixel-wise unit vectors pointing to 2D projections of a set of 3D keypoints."
        },
        "RDPN6D: Residual-based dense point-wise network for 6Dof object pose estimation based on RGB-D images": {
          "authors": [
            "Wei Hong",
            "Yang Hung",
            "Song Chen"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/DLGC/html/Hong_RDPN6D_Residual-based_Dense_Point-wise_Network_for_6Dof_Object_Pose_Estimation_CVPRW_2024_paper.html",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "31"
          ],
          "1": "These methods typically provide pixel-level correspondences [11, 19, 25, 41] or predict 2D image locations for predefined 3D keypoints [4, 31], resulting in more robust results.",
          "2": "PVN3D [17] extends PVNet [31] to predict the keypoints in the 3D space because errors that may appear small in the projection can significantly impact the real world.",
          "4": "Our approach follows the protocol established in [11, 31, 41\u201343], which employs the standard 15%/85% training/testing split.",
          "5": "For Linemod and Occlusion LineMOD datasets, we follow [23, 31] to report ADD(-S) 0."
        },
        "Challenges for monocular 6d object pose estimation in robotics": {
          "authors": [
            "D Bauer",
            "P H\u00f6nig",
            "JB Weibel"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10609560/",
          "ref_texts": "[125] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019.",
          "ref_ids": [
            "125"
          ],
          "3": "For the sparse keypoints, improvements are proposed by assigning keypoints to geometrically relevant positions on the object surface and through more sophisticated keypoint location voting schemes [31], [125], [135]."
        },
        "Advancing 6-DoF instrument pose estimation in variable X-ray imaging geometries": {
          "authors": [
            "CGA Viviers",
            "L Filatova",
            "M Termeer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10478293/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "3": "We compare the proposed approach against seven competitive object pose estimation methods on the LINEMOD dataset: YOLO6D [7], PoseCNN [22], PVNet [23], Gen6D [15], EfficientPose [51], RNNPose [21] and EPro-PnPv2 [8]."
        },
        "Hoisdf: Constraining 3d hand-object pose estimation with global signed distance fields": {
          "authors": [
            "H Qi",
            "C Zhao",
            "M Salzmann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10657272/",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "41"
          ],
          "1": "Many excellent 3D hand [32, 45, 51, 58] and object [8, 25, 41] pose estimation algorithms have been developed."
        },
        "Ar overlay: Training image pose estimation on curved surface in a synthetic way": {
          "authors": [
            "S Huang",
            "Y Song",
            "Y Kang",
            "C Yu"
          ],
          "url": "https://arxiv.org/abs/2409.14577",
          "ref_texts": "[34] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\u201d Dec. 2018, arXiv:1812.11788 [cs].",
          "ref_ids": [
            "34",
            "cs"
          ],
          "1": "Hybrid methods like DenseFusion [33] and PVNet [34] integrate RGB data with depth information, improving accuracy and robustness in complex scenes [35\u201337]."
        },
        "Hipose: Hierarchical binary surface encoding and correspondence pruning for rgb-d 6dof object pose estimation": {
          "authors": [
            "Yongliang Lin",
            "Yongzhi Su",
            "Praveen Nathan",
            "Sandeep Inuganti",
            "Yan Di",
            "Martin Sundermeyer",
            "Fabian Manhardt",
            "Didier Stricker",
            "Jason Rambach",
            "Yu Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lin_HiPose_Hierarchical_Binary_Surface_Encoding_and_Correspondence_Pruning_for_RGB-D_CVPR_2024_paper.html",
          "ref_texts": "[42] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "42"
          ],
          "1": "Dense correspondence-based methods have been shown to outperform the keypoint-based methods [36, 40, 42, 46, 71] and holistic approaches [8, 23, 49, 63] nowadays, as also demonstrated in the BOP challenge results [54]."
        },
        "Transpose: 6d object pose estimation with geometry-aware transformer": {
          "authors": [
            "X Lin",
            "D Wang",
            "G Zhou",
            "C Liu",
            "Q Chen"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231224004235",
          "ref_texts": "[32] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H., 2019. Pvnet: Pixelwise voting network for 6dof pose estimation, in: Proceedings of the IEEE/CVFConferenceonComputerVisionandPatternRecognition, pp. 4561\u20134570.",
          "ref_ids": [
            "32"
          ],
          "1": "Recently, some researchers have applied deep neural networks to estimate 6D object pose from a single RGB image[32,31,49,23]andachievedpromisingresults.",
          "2": "PVNet[32] can learn a vector field representation directed to the 2D keypoints.",
          "4": "Wefollowpriorworks[32,31] and directly utilize the pre-trained model on the LineMod dataset for testing."
        },
        "Acr-pose: Adversarial canonical representation reconstruction network for category level 6d object pose estimation": {
          "authors": [
            "Z Fan",
            "Z Song",
            "Z Wang",
            "J Xu",
            "K Wu",
            "H Liu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3652583.3658050",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "29"
          ],
          "1": "Over the last decade, numerous 6D object pose estimation works [13, 21, 29, 34, 37, 47, 51] have emerged and been deployed in industrial products, demonstrating the usefulness of this line of research."
        },
        "Gs-pose: Category-level object pose estimation via geometric and semantic correspondence": {
          "authors": [
            "P Wang",
            "T Ikeda",
            "R Lee",
            "K Nishiwaki"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73383-3_7",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "23"
          ],
          "1": "With the advancement of deep learning methods, various learning-based pose estimation approaches have proven effective for instance-level pose estimation [3, 9\u201314, 19, 20, 22, 23, 29\u201331, 35, 39].",
          "2": "Category-Level Object Pose Estimation In the past few years, instance-level object pose estimation based on Deep Neural Networks (DNNs) has made great progress in computer vision and robotics fields [3, 9\u201314, 19, 20, 22, 23, 29\u201331, 35, 39]."
        },
        "Closure: Fast quantification of pose uncertainty sets": {
          "authors": [
            "Y Gao",
            "Y Tang",
            "H Qi",
            "H Yang"
          ],
          "url": "https://arxiv.org/abs/2403.09990",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "39"
          ],
          "1": "The first paradigm is to start by detecting salient keypoints in the sensor data \u2013often done using deep neural networks [17, 55, 39, 38]\u2013 and then leverage the maximum likelihood estimation (MLE) framework to estimate the \u2217 equal contribution."
        },
        "GBOT: Graph-based 3d object tracking for augmented reality-assisted assembly guidance": {
          "authors": [
            "S Li",
            "H Schieber",
            "N Corell",
            "B Egger"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10494181/",
          "ref_texts": "[36] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "36"
          ],
          "2": "Most deep-learning methods combine object detection or semantic segmentation with a consecutive pose estimation [22, 36, 58].",
          "6": "Generally, more keypoints slightly improve the accuracy but also increase the computational cost [36]."
        },
        "Marrying nerf with feature matching for one-step pose estimation": {
          "authors": [
            "R Chen",
            "Y Cong",
            "Y Ren"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610766/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "7"
          ],
          "1": "However, they can only estimate poses of known instances [5]\u2013[7] or similar ones from the same category [8]\u2013[11], and have to retrain on novel objects for hours."
        },
        "A robust CoS-PVNet pose estimation network in complex scenarios": {
          "authors": [
            "Jiu Yong",
            "Xiaomei Lei",
            "Jianwu Dang",
            "Yangping Wang"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/11/2089",
          "ref_texts": "21. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6D of pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "Inspired by the U-Net network, the PVNet [21] uses residual blocks and bilinear interpolation to reconstruct a lightweight U-Net as a feature extraction network with an inference speed of 40 ms.",
          "2": "On the LineMod dataset and Occlusion LineMod dataset, CoS-PVNet is compared quantitatively with BB8 [30], YOLO-6D [12], and PVNet [21] in 2D projection metrics.",
          "3": "On the LineMod dataset and Occlusion LineMod dataset, CoS-PVNet is compared quantitatively with BB8 [30], YOLO-6D [12], and PVNet [21] in 2D projection metrics.",
          "4": "Comparative Experiment of CoS-PVNet Algorithm ADD (-S) (1) LineMod Dataset ADD (-S) Comparative Experiment Experiments are conducted on the LineMod dataset, comparing CoS-PVNet with algorithms such as YOLO-6D [12], PoseCNN [17], DenseFusion [31], Dual Stream [32], and PVNet [21].",
          "5": "(2) Comparison Experiment of the Occupation LineMod Dataset ADD (-S) Experiments are conducted on the Occlusion LineMod dataset to compare CoSPVNet with HybridPose [33], SSPE [34], RePOSE [35], SegDriven [36], PoseCNN [17] and PVNet [21]."
        },
        "Confronting ambiguity in 6d object pose estimation via score-based diffusion on se (3)": {
          "authors": [
            "Ching Hsiao",
            "Wei Chen",
            "Kung Yang",
            "Yi Lee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hsiao_Confronting_Ambiguity_in_6D_Object_Pose_Estimation_via_Score-Based_Diffusion_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 3",
          "ref_ids": [
            "44"
          ],
          "1": "Some researchers [44, 46], on the other hand, introduce constraints to the regression targets (especially regarding rotation angles) to mitigate ambiguity."
        },
        "Uncertainty-aware 3d object-level mapping with deep shape priors": {
          "authors": [
            "Z Liao",
            "J Yang",
            "J Qian",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611206/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "23"
          ],
          "1": "To this end, in many recent CAD-based approaches, the uncertainties are incorporated when estimating object poses [23], [24], [8], [7]."
        },
        "Towards Co-Evaluation of Cameras HDR and Algorithms for Industrial-Grade 6DoF Pose Estimation": {
          "authors": [
            "Agastya Kalra",
            "Guy Stoppi",
            "Dmitrii Marin",
            "Vage Taamazyan",
            "Aarrushi Shandilya",
            "Rishav Agarwal",
            "Anton Boykov",
            "Tze Hao",
            "Michael Stark"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Kalra_Towards_Co-Evaluation_of_Cameras_HDR_and_Algorithms_for_Industrial-Grade_6DoF_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "53"
          ],
          "1": "This mechanism depends on the camera setup and consists either of some form of PnP (for multi-view images, candidates include [18, 23, 40, 41, 59]) or ICP (for structured light sensors, candidates include [23, 30, 53, 60, 72])."
        },
        "Asdf: Assembly state detection utilizing late fusion by integrating 6d pose estimation": {
          "authors": [
            "H Schieber",
            "S Li",
            "N Corell",
            "P Beckerle"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10765472/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570. IEEE/CVF, 2019. 3",
          "ref_ids": [
            "27"
          ],
          "1": "The constant progress in 6D pose estimation is promising for single objects [45, 40, 27, 26, 10, 50], the combination of 6D pose estimation and assembly state detection is even more promising for AR guidance approaches [21, 25]."
        },
        "FocalPose++: Focal Length and Object Pose Estimation via Render and Compare": {
          "authors": [
            "M C\u00edfka",
            "G Ponimatkin",
            "Y Labb\u00e9"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10706831/",
          "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "Previous approaches for this task mainly rely on establishing local 2D-3D correspondences between an image and a 3D model using either hand-crafted [1], [5], [6], [7], [8], [9] or CNN features [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], followed by robust camera pose estimation using PnP [22].",
          "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs) [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21]."
        },
        "Research on Six-Degree-of-Freedom Refueling Robotic Arm Positioning and Docking Based on RGB-D Visual Guidance": {
          "authors": [
            "Mingbo Yang",
            "Jiapeng Liu"
          ],
          "url": "https://www.mdpi.com/2076-3417/14/11/4904",
          "ref_texts": "25. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp.",
          "ref_ids": [
            "25"
          ],
          "1": "These include direct estimation methods like PoseNet [19], SSD-6D [20], Deep-6DPose [21], PoseCNN [22], and keypoint methods such as BB8 [23], YOLO-6D [24], PVNet [25], etc."
        },
        "Bridging Domain Gap for Flight-Ready Spaceborne Vision": {
          "authors": [
            "TH Park",
            "S D'Amico"
          ],
          "url": "https://arxiv.org/abs/2409.11661",
          "ref_texts": "[99] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4556\u20134565. https://doi.org/10.1109/ CVPR.2019.00469.",
          "ref_ids": [
            "99"
          ],
          "1": "These uncertainties can be leveraged to better perform state estimation via navigation filter [16] or even incorporated into the uncertainty-aware P\ud835\udc5bP algorithm [38, 99]."
        },
        "Robust Category-Level 3D Pose Estimation from Diffusion-Enhanced Synthetic Data": {
          "authors": [
            "Jiahao Yang",
            "Wufei Ma",
            "Angtian Wang",
            "Xiaoding Yuan",
            "Alan Yuille",
            "Adam Kortylewski"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.html",
          "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1",
          "ref_ids": [
            "25"
          ],
          "1": "Pose estimation has been studied in depth on the instance level [14, 17, 19, 25, 38], and on the category-level for very specific object classes like cars [11] and faces [26]."
        },
        "An analysis of precision: occlusion and perspective geometry's role in 6D pose estimation": {
          "authors": [
            "Jeffrey Choate"
          ],
          "url": "https://link.springer.com/article/10.1007/s00521-023-09094-8",
          "ref_texts": "34. Peng S, Liu Y, Huang Q, Zhou X, Bao H (2019) Pvnet: pixelwise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 4561\u20134570",
          "ref_ids": [
            "34"
          ],
          "1": "[4, 7, 32, 34, 35, 47, 55, 56] are very similar to these approach with slight variations, all basically doing direct pose regression from a CNN."
        },
        "NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models": {
          "authors": [
            "F Milano",
            "JJ Chung",
            "H Blum"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801399/",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in CVPR, 2019. 2, 4",
          "ref_ids": [
            "29"
          ],
          "1": "For instance, keypoint-based methods [27]\u2013[29] predict the 2D location of pre-defined salient points, and use the object model to estimate the object pose based on 2D-3D correspondences.",
          "3": "Both with RGB-only and with RGB-D inputs, NeuSurfEmb achieves comparable performance to several CAD-model-based baselines [29], [46] and is outperformed by a small margin by others [2], [11], [12]."
        },
        "FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization": {
          "authors": [
            "N Ma",
            "M Wang",
            "Y Han",
            "YJ Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610549/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "24"
          ],
          "1": "Since the purpose is to capture the maximal geometric feature of the entire point cloud with the minimal number of points, we adopt the approach outlined in PVNET [24], utilizing the Farthest Point Sampling (FPS) algorithm to select n key points."
        },
        "FAFA: Frequency-Aware Flow-Aided Self-supervision for Underwater Object Pose Estimation": {
          "authors": [
            "J Tang",
            "G Wang",
            "Z Chen",
            "S Li",
            "X Li",
            "X Ji"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73021-4_21",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019) FAFA for Self-Supervised Underwater Object Pose Estimation 17",
          "ref_ids": [
            "36"
          ],
          "1": "The first category involves indirect methods [17,25,36,38,45], which identify target keypoints and then employ a PnP solver to calculate the pose."
        },
        "Mrc-net: 6-dof pose estimation with multiscale residual correlation": {
          "authors": [
            "Yuelong Li",
            "Yafei Mao",
            "Raja Bala",
            "Sunil Hadap"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_MRC-Net_6-DoF_Pose_Estimation_with_MultiScale_Residual_Correlation_CVPR_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 6, 7",
          "ref_ids": [
            "40"
          ],
          "1": "Earlier works focused on finding sparse corresponding geometric [41] or semantic [31, 40, 46] keypoints with iterative refinement strategies [3].",
          "2": "We benchmark our method against state-of-the-art techniques spanning a variety of recent approaches: EPOS [17], CDPNv2 [29], DPODv2 [44], PVNet [40], CosyPose [26], SurfEmb [13], SC6D [2], SCFlow [11], CIR [33], PFA [22], SO-Pose [7], NCF [23], CRT-6D [3], GDR-Net [52], ZebraPose [47], DProST [39], RePose [24], SegDriven [19], SingleStage [20], and CheckerPose [31].",
          "3": "4 PVNet [40] 57."
        },
        "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios": {
          "authors": [
            "DT Huang",
            "ET Lin",
            "L Chen",
            "LF Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10802595/",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "4"
          ],
          "1": "These methods primarily fall into two categories: holistic methods [1], [3] and keypoint-based methods [4], [5].",
          "2": "Some other works [5], [4] employs the farthest point sampling (FPS) algorithm to sample keypoints on the object\u2019s surface according to their relative proximity.",
          "3": "Therefore, PVNet [4] and PVN3D [5] use the farthest point sampling algorithm to select keypoint to reduce position errors."
        },
        "Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models": {
          "authors": [
            "H Ding",
            "L Seenivasan",
            "H Shu",
            "G Byrd",
            "H Zhang"
          ],
          "url": "https://arxiv.org/abs/2409.13107",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "26"
          ],
          "1": "While advancements in deep learning algorithms for computer vision, such as instance segmentation [21], [22], [23], [24], [25] and pose estimation [26], [27], [28], [29], [30], [31], offer an alternate vision-based, marker-less approach to extract the digital twin-based scene representation, these methods lack generalizability and fail when the observed scenario differs from the training data [32], [33], [34]."
        },
        "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with 3D Gaussian Splatting": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://arxiv.org/abs/2403.10683",
          "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "38"
          ],
          "1": "Most existing pose estimation methods [2, 3, 6, 13, 17, 19, 25, 38, 45, 50, 53] are object-specific pose estimators, which are specialized for pre-defined objects and cannot generalize to previously unseen objects without retraining.",
          "2": "While other approaches[6, 13, 17, 25, 36, 38, 45] establish 2D-3D correspondences between 2D images and 3D object models to estimate the 6D pose by solving the Perspective-n-Point (PnP)[22] problem."
        },
        "Embedded 3d reconstruction of dynamic objects in real time for maritime situational awareness pictures": {
          "authors": [
            "Felix Sattler"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-023-02802-4",
          "ref_texts": "35. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: pixelwise voting network for 6dof pose estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4556\u20134565. IEEE Computer Society, Los Alamitos, CA, USA",
          "ref_ids": [
            "35"
          ],
          "1": "Recent work shown by [35]a l s o demonstrates that the tasks of object detection, instance segmentation and pose estimation can be jointly estimated using a single neural network."
        },
        "Effects of Markers in Training Datasets on the Accuracy of 6D Pose Estimation": {
          "authors": [
            "Janis Rosskamp",
            "Rene Weller",
            "Gabriel Zachmann"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.html",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "The YCB-V dataset [34], comprising approximately 130,000 frames, is a popular choice for benchmarking [11, 17, 21, 31] and will thus be used in this paper."
        },
        "Learning better keypoints for multi-object 6dof pose estimation": {
          "authors": [
            "Yangzheng Wu",
            "Michael Greenspan"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "40"
          ],
          "1": "The second approach are keypoint-based methods [21, 22, 40, 52, 53], which are the main alternative to direct regression methods.",
          "2": "Despite the success of keypoint methods, the selection of the pre-defined object-centric keypoint locations has been overwhelmingly based on just two approaches, the first of which is Farthest Point Sampling (FPS) [22, 40].",
          "3": "FPS samples points on an object surface based on their relative proximities, and was originally developed for progressive image sampling [15] and subsequently repurposed for keypoint selection [40].",
          "4": "Both approaches are also heuristic, with their main objective being to produce keypoints that are geometrically dispersed, and which fall on [22, 40], or close to [52, 53], the objects\u2019 surfaces.",
          "5": "A graph network is trained to optimize a disperse set of keypoints with similarly distributed votes for keypoint voting 6DoF PE methods [22,40,52,53].",
          "6": "Motivation Existing keypoint methods [22,40,52,53] use regression networks to estimate a quantity that geometrically relates each image pixel (and/or point) to each keypoint.",
          "7": "A variety of such quantities have been explored in the literature, including the offset [22], direction vector [40], and radial distance [52, 53] between points.",
          "8": "As the majority of methods [22,40,52] train a single network for all keypoints, a larger variance between the quantities regressed for each keypoint results in a scenario similar to class imbalance [19] in a classification network.",
          "9": "Keypoint-based 6DoF PE Methods[22, 40, 52, 53] exhibit relatively good accuracy compared to viewpoint-based methods [32, 38, 49] or direct regression [28, 36, 54] methods.",
          "10": "Keypoint voting-based methods [21,22,40,52,53], however, can better accommodate noise.",
          "11": "Implementation Details The proposed KeyGNet is trained to generate a set of optimized keypoints, which we tested on a variety of stateof-the-art PE methods [22, 40, 53].",
          "12": "We test the optimized keypoints on three keypoint-based 6DoF PE voting methods, RCVPose [53], PVNet [40] and PVN3D [22].",
          "13": "radial [53], vector [40], and offset [22] respectively.",
          "14": "The majority of previous keypoint-based methods [22, 40, 52, 53] were designed to address the less challenging SISO case by only processing a single object at a time, which allows the training of unique network parameters for each distinct object.",
          "15": "Some [22, 40, 50] argue that more keypoints provide redundancy to the least square fitting algorithm ultimately used in the final transformation estimation, whereas others [52, 53] use as few as three keypoints to ease the estimation task of the backbone network.",
          "16": "PVNet [40] and PVN3D [22] exhibited a slight overall improvement in ADD(s) as the number of keypoints increased, which saturated when there are more than eight keypoints."
        },
        "Category Level Object Pose Estimation via Global High-Order Pooling": {
          "authors": [
            "Changhong Jiang",
            "Xiaoqiao Mu",
            "Bingbing Zhang",
            "Mujun Xie",
            "Chao Liang"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/9/1720",
          "ref_texts": "6. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201317 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "6"
          ],
          "1": "Currently, there are two types of 6D object pose estimation methods: instance-level pose estimation [2,6] and category-level pose estimation [7\u201311]."
        },
        "Rhaml: Rendezvous-based hierarchical architecture for mutual localization": {
          "authors": [
            "G Chen",
            "K Song",
            "X Xu",
            "W Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10540183/",
          "ref_texts": "[25] S. Peng, X. Zhou, Y . Liu, H. Lin, Q. Huang, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof object pose estimation,\u201d IEEE Trans. Pattern Anal. Machine Intell. , vol. 44, no. 6, pp. 3212\u20133223, 2022.",
          "ref_ids": [
            "25"
          ],
          "1": "In [25], keypoint localization is achieved through a vector-field representation.",
          "2": "As baseline methods, FrontNet [13], DOPE [14], and PVNet [25] are all retrained with our dataset."
        },
        "Pseudo-keypoint RKHS learning for self-supervised 6DoF pose estimation": {
          "authors": [
            "Y Wu",
            "M Greenspan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73027-6_3",
          "ref_texts": "61. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019) 3, 4, 6, 9",
          "ref_ids": [
            "61"
          ],
          "3": "Unlikefeature-matchingmethods,keypoint-basedmethods are typically more accurate due to redundancies encountered through voting schemes [32,61,83] and by generating confidence hypotheses of keypoints [31,87].",
          "4": "Inspired by recent voting techniques [32,61,82,83,90], Mr estimates an intermediate voting quantity, which is a radial distance mapVr [83], by using a modified Fully Connected ResNet 18 (FCN-ResNet-18)."
        },
        "KVN: Keypoints voting network with differentiable RANSAC for stereo pose estimation": {
          "authors": [
            "I Donadi",
            "A Pretto"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10440415/",
          "ref_texts": "[3] S. Peng, X. Zhou, Y . Liu, H. Lin, Q. Huang, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof object pose estimation,\u201dIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp. 3212\u2013",
          "ref_ids": [
            "3"
          ],
          "4": "Inspired by [18], [19], we integrated PVNet [3], an established monocular pose estimation network, with a differentiable RANSAC layer that enables the network to directly infer 2D-3D keypoint correspondences.",
          "5": "Contributions Our contributions are the following: i) A novel 6D stereo object pose estimation pipeline that extends PVNet [3] with a differentiable RANSAC layer, turning it into a keypoint prediction network without losing the robustness granted by its pixel-wise voting approach; ii) a novel sub-differentiable hypotheses\u2019 scoring function along with an ablation study that supports our choice over previous proposals; iii) a challenging, real-world and fully annotated stereo object pose estimation dataset (TTD: Transparent Tableware Dataset 1); iv) an extensive performance evaluation on the challenging task of transparent object pose estimation, showing that our method achieves state-of-the-art performance on both the public TOD dataset [12] and TTD; v) An open-source implementation of KVN2.",
          "6": "PVNet [3] improved this method by predicting a unit vector pointing to each keypoint projection for each pixel in the image corresponding to the target object, and in [4] this approach is improved by accounting for the distance between pixels and keypoints.",
          "7": "PVNet: Pixel-Wise Voting Network Our work builds upon PVNet [3], a keypoint-based monocular pose estimation network."
        },
        "Keymatchnet: Zero-shot pose estimation in 3d point clouds by generalized keypoint matching": {
          "authors": [
            "F Hagelskj\u00e6r",
            "RL Haugaard"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10711403/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "PVNet [21] compute keypoint locations by first segmentation the object, and then computing the relative position of keypoint for all pixels belonging to the object."
        },
        "Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices": {
          "authors": [
            "X Yang",
            "Z Yu",
            "AG Banerjee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10711289/",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "9"
          ],
          "1": "Deep learning approaches, including direct pose regression [7], [8], keypoint detection [9], and hybrid variants, have improved accuracy and efficiency, especially when addressing the synthetic-to-real domain gap [10], [11].",
          "2": "Correspondence methods predict the 2D projections of the coordinates or keypoints to match the 3D models [9], [25], [26], avoiding regression difficulties by decomposing the problem.",
          "3": "V oting schemes improve occlusion robustness by allowing the visible portions to contribute to keypoint localization [9].",
          "4": "We synthesize an additional 3,000 images per LINEMOD object using the PVNet [9] rendering approach to expand the training data.",
          "5": "*I NDICATES SYMMETRIC OBJECTS EVALUATED WITH ADD-S METRIC , \u2020INDICATES SYMMETRIC OBJECTS EVALUATED WITH ADD-S\u2019 METRIC PVNet [9] SingleShot [25] PoseCNN [7] Pix2Pose [13] CDPN [27] DPOD [26] HybridPose [35] COPE [16] SCCN (Ours) ape 43.",
          "6": "3 TABLE II EXPERIMENT ON LINEMOD OCCLUSION DATASET PVNet [9] PoseCNN [7] Pix2Pose [13] HybridPose [35] Seg-Driven6D [15] COPE [16] GDR-Net [38] SCCN (Ours) ape 15.",
          "7": "1 TABLE III SPEED PERFORMANCE ON NVIDIA JETSON AGX XAVIER PVNet [9] SingleShot [25] DPOD [26] Pix2Pose [13] HybridPose [35] Seg-Driven6D [15] COPE [16] GDR-Net [38] SCCN (Ours) LINEMOD 3."
        },
        "Extending 6d object pose estimators for stereo vision": {
          "authors": [
            "T P\u00f6llabauer",
            "J Emrich",
            "V Knauthe",
            "A Kuijper"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-8705-0_8",
          "ref_texts": "22. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "22"
          ],
          "2": "Furthermore, the dense PV-Net [22] can be applied to stereo scenarios by predicting keypoints for each image and subsequently triangulating the object using keypoints from both images."
        },
        "Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion Error for Improved Object Pose Estimation": {
          "authors": [
            "A Li",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10810662/",
          "ref_texts": "[8] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, (2019)",
          "ref_ids": [
            "8"
          ],
          "2": "Single-view Keypoint and Heatmap Estimation The keypoint detection network is based off PVNet, a pixel-wise voting network for 6D pose estimation of a known object with a CAD model[8]."
        },
        "A Common Knowledge-Driven Generic Vision Inspection Framework for Adaptation to Multiple Scenarios, Tasks, and Objects": {
          "authors": [
            "Delong Zhao",
            "Feifei Kong",
            "Nengbin Lv",
            "Zhangmao Xu",
            "Fuzhou Du"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/13/4120",
          "ref_texts": "31. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6D of pose estimation. IEEE Trans. Pattern Anal. Mach. Intell. 2022, 44, 3212\u20133223. [CrossRef] [PubMed]",
          "ref_ids": [
            "31"
          ],
          "2": "Group IV employed the advanced estimator PVNet [31], in which semantic labels were generated from the minimum bounding rectangle of significant geometric feature."
        },
        "Multi-Modal Pose Representations for 6-DOF Object Tracking": {
          "authors": [
            "M Majcher",
            "B Kwolek"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02181-5",
          "ref_texts": "15. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: \u201cPVNet: PixelWise V oting Network for 6DoF Pose Estimation.\u201d in IEEE Conf. CVPR, pp. 4556\u20134565. (2019)",
          "ref_ids": [
            "15"
          ],
          "1": "Instead of estimating the object center, a Pixel-wise Voting Network (PVNet) [15] votes for several features of interest.",
          "2": "In order to deal with annotations required for supervised training of deep models, most of recent methods rely on training such networks on synthetic objects rendered from 3D models [15, 60].",
          "3": "For instance, 10000 images for each object have been rendered to train PVNet models [15]."
        },
        "RayEmb: Arbitrary Landmark Detection in X-Ray Images Using Ray Embedding Subspace": {
          "authors": [
            "Pragyan Shrestha",
            "Chun Xie",
            "Yuichi Yoshii",
            "Itaru Kitahara"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2024/html/Shrestha_RayEmb_Arbitrary_Landmark_Detection_in_X-Ray_Images_Using_Ray_Embedding_ACCV_2024_paper.html",
          "ref_texts": "30. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "30"
          ],
          "1": "To address the challenge of truncated objects where keypoints may lie outside the image, PVNet [30] regresses vectors that point towards the keypoints, determining the final location through a voting mechanism."
        },
        "Flying Projectile Attitude Determination from Ground-Based Monocular Imagery with a Priori Knowledge": {
          "authors": [
            "Huamei Chen",
            "Zhigang Zhu",
            "Hao Tang",
            "Erik Blasch",
            "Khanh D. Pham",
            "Genshe Chen"
          ],
          "url": "https://www.mdpi.com/2078-2489/15/4/201",
          "ref_texts": "34. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019.",
          "ref_ids": [
            "34"
          ],
          "1": "To additionally handle the case of truncated objects, a pixel-wise voting network (PVNet) [34] was proposed to learn a vector-field representation for robust 2D keypoint localization."
        },
        "AnnotateXR: An Extended Reality Workflow for Automating Data Annotation to Support Computer Vision Applications": {
          "authors": [
            "Subramanian Chidambaram",
            "Rahul Jain",
            "Sai Swarup",
            "Asim Unmesh",
            "Karthik Ramani"
          ],
          "url": "https://asmedigitalcollection.asme.org/computingengineering/article/24/12/121001/1202055",
          "ref_texts": "[27] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., 2019,\u201cPvnet: Pixel-Wise Voting Network for 6DOF Pose Estimation,\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, June 16\u201320, pp. 4561\u20134570.",
          "ref_ids": [
            "27"
          ],
          "1": "[25], video action segmentation [26], 6DOF predictions [27], H\u2013O interaction [28], object\u2013object (O\u2013O) interaction [20,29], and rich 3D scene recording [30] are supported by AnnotateXR.",
          "2": "[27] Peng, S."
        },
        "VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object Pose Estimation": {
          "authors": [
            "R Lian",
            "Y Lin",
            "LJ Latecki",
            "H Ling"
          ],
          "url": "https://arxiv.org/abs/2403.14559",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: pixelwise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "To increase the robustness under various imaging conditions, most existing methods [9], [10], [11], [12], [13], [14], [15], [16], [17], [18] first generate correspondences between 2D image pixels and 3D object points, and then regress the pose via any available Perspective-n-Point (PnP) solver [19], [20], [21].",
          "2": "The second kind of methods [9], [10], [13], [23], [18] localize predefined 3D keypoints in the input image to obtain 3D-2D correspondences, which more efficiently encode the object geometry information and facilitate the pose estimation process."
        },
        "CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation": {
          "authors": [
            "J Zhao",
            "W Quan",
            "BJ Matuszewski"
          ],
          "url": "https://arxiv.org/abs/2410.09010",
          "ref_texts": "[32] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):3212\u20133223, 2020.",
          "ref_ids": [
            "32"
          ],
          "3": "The method effectively addresses various challenging scenarios, including texture-less objects, occlusion, truncation [32], and clutter."
        },
        "Weakly Supervised Pose Estimation of Surgical Instrument from a Single Endoscopic Image": {
          "authors": [
            "Lihua Hu",
            "Shida Feng",
            "Bo Wang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/11/3355",
          "ref_texts": "26. Peng, S.; Liu, Y.; Huang, Q.; Bao, H. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "26"
          ],
          "1": "Pixelwise Voting Network (PVNet) [26] locates feature points through a voting mechanism, demonstrating strong occlusion robustness."
        },
        "Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation": {
          "authors": [
            "J Park",
            "J Kim",
            "NI Cho"
          ],
          "url": "https://arxiv.org/abs/2401.16284",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "Typically, these approaches identify sparse keypoints [30, 35, 42], bounding box corners [10, 31], dense 2D-3D correspondence [11, 28], or UV maps [46], which are then used in PnP-RANSAC [18] for pose estimation."
        },
        "GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos": {
          "authors": [
            "Z Chen",
            "F Lu",
            "G Yu",
            "B Li",
            "S Qu",
            "Y Huang",
            "C Fu"
          ],
          "url": "https://arxiv.org/abs/2412.02267",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xibin Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "Early 6DoF object pose estimation or tracking approaches assume access to 3D models [28, 42] or category templates [13, 33] and rely on feature matching algorithms *Corresponding author: guangchen@tongji."
        },
        "A Learnable Viewpoint Evolution Method for Accurate Pose Estimation of Complex Assembled Product": {
          "authors": [
            "Delong Zhao",
            "Feifei Kong",
            "Fuzhou Du"
          ],
          "url": "https://www.mdpi.com/2076-3417/14/11/4405",
          "ref_texts": "30. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6dof pose estimation. IEEE Trans. Pattern Anal. Mach. Intell.2022, 44, 3212\u20133223. [CrossRef] [PubMed]",
          "ref_ids": [
            "30"
          ],
          "2": "DL-based methods employed in hierarchical strategy include PoseNet2 [29], single-shot pose [47], and state-of-the-art estimator PVNet [30] and MegaPose [34]."
        },
        "Six-Degree-of-Freedom Pose Estimation Method for Multi-Source Feature Points Based on Fully Convolutional Neural Network": {
          "authors": [
            "J Wang",
            "P Wu",
            "X Zhang",
            "R Xu",
            "T Wang"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02154-8",
          "ref_texts": "33. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixelwise voting network for 6dof pose estimation. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4556\u20134565 (2019).https://doi.org/10.1109/CVPR.2019.00469",
          "ref_ids": [
            "33"
          ],
          "1": "8: ADD \u2212 S = 1 N N\u2211 i =1 min j \u2208[1,n] \u2225(R \u00b7 pi + t ) \u2212 (\u02c6R \u00b7 pi + \u02c6t )\u2225 (8) 123 131 Page 8 of 12 Journal of Intelligent & Robotic Systems (2024) 110:131 Table 2 ADD(-S) Performance Om Linemod Dataset Feature select method Suppression method Other method B-box FPS BOTH NMS Ours Overall BB8 [32] PVNet [33] GDR-Net [34] ape 82.",
          "2": "In terms of attitude solving, we Table 3 ADD(-S) Performance On Occlusion Linemod Dataset method B-box FPS BOTH PVNet [33] ape 24."
        },
        "Multi-View Metal Parts Pose Estimation Based on a Single Camera": {
          "authors": [
            "Chen Chen",
            "Xin Jiang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/11/3408",
          "ref_texts": "11. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "11"
          ],
          "3": "Hybridpose [24] extends the PVNet [11], which not only estimates keypoints but also predicts edge vectors and symmetry correspondences.",
          "4": "m = avg x\u2208M \u0000 Rgtx + Tgt \u0001 \u2212 (Rest x + Test ) (4) Figure 8 compares the qualitative results of metal parts pose estimations between our proposed method and PVNet [11]."
        },
        "Particle-based 6D Object Pose Estimation from Point Clouds using Diffusion Models": {
          "authors": [
            "C M\u00f6ller",
            "N Funk",
            "J Peters"
          ],
          "url": "https://arxiv.org/abs/2412.00835",
          "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019. 1, 2, 7",
          "ref_ids": [
            "24"
          ],
          "2": "Exemplarily, the Pixel-wise V oting Network (PVNet) [24] regresses unit vectors pointing to keypoints, HybridPose [29] leverages a multi-folded intermediate presentation consisting of keypoints, edge vectors between keypoints, and symmetry correspondences, and YOLO-6D+ [18] predicts an object\u2019s 3D bounding box and uses the its 2D projections as PnP input."
        },
        "SEMPose: A Single End-to-end Network for Multi-object Pose Estimation": {
          "authors": [
            "X Liu",
            "H Wang",
            "S Xue",
            "D Zhao"
          ],
          "url": "https://arxiv.org/abs/2411.14002",
          "ref_texts": "[8] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d inProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "8"
          ],
          "4": "For example, PVNet [8] predicts pixel-level vectors pointing to key points, then uses these vectors to determine the key points\u2019 positions through a RANSAC-basedvotingmechanism."
        },
        "Memory Efficient Deep Learning-Based Grasping Point Detection of Nontrivial Objects for Robotic Bin Picking": {
          "authors": [
            "P Dolezel",
            "D Stursa",
            "D Kopecky"
          ],
          "url": "https://link.springer.com/article/10.1007/s10846-024-02153-9",
          "ref_texts": "29. Peng, S., Zhou, X., Liu, Y ., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Trans. Pattern Anal. Mach. Intell. (2020).https://doi.org/10.1109/ TPAMI.2020.3047388",
          "ref_ids": [
            "29"
          ],
          "1": "This approach can be further improved using pixel-wise predictions that provide the position of occluded key points [17, 29]."
        },
        "RoCap: A Robotic Data Collection Pipeline for the Pose Estimation of Appearance-Changing Objects": {
          "authors": [
            "JN Li",
            "T Chong",
            "Z Zhou",
            "H Yoshida",
            "K Yatani"
          ],
          "url": "https://arxiv.org/abs/2407.08081",
          "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4561\u2013",
          "ref_ids": [
            "38"
          ],
          "1": "More recently, data-driven deep learning methods [38, 47] demonstrated accurate predictions of the 6D pose of pre-defined sets of object included in carefully crafted datasets [5, 26]."
        },
        "EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models": {
          "authors": [
            "Z Hong",
            "K Zheng",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801359/",
          "ref_texts": "[42] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp. 4561\u2013",
          "ref_ids": [
            "42"
          ],
          "1": "EasyHeC trained a PVNet [42] on synthetic data to initialize the camera pose at the robot arm\u2019s zero joint pose."
        },
        "HPPS: A Hierarchical Progressive Perception System for Luggage Trolley Detection and Localization at Airports": {
          "authors": [
            "Z Sun",
            "Z Zhang",
            "J Zhao",
            "H Ye",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2405.05514",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] uses keypoints and a voting mechanism to estimate poses, handling partial occlusions and various viewing angles effectively."
        },
        "GMFlow: Global Motion-Guided Recurrent Flow for 6D Object Pose Estimation": {
          "authors": [
            "X Liu",
            "S Xue",
            "D Zhao",
            "S Ma",
            "M Jiang"
          ],
          "url": "https://arxiv.org/abs/2411.17174",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "19"
          ],
          "3": "Comparison with State of the Art We compare our proposed method with current state-of-the-art techniques, including PoseCNN [3], PVNet [19], SO-Pose [28], DeepIM [6], GDR-Net [17], RePose [26], RNNPose [25], PFA [8], and SCFlow [10]."
        },
        "A Lightweight 6D Pose Estimation Network Based on Improved Atrous Spatial Pyramid Pooling": {
          "authors": [
            "Fupan Wang",
            "Xiaohang Tang",
            "Yadong Wu",
            "Yinfan Wang",
            "Huarong Chen",
            "Guijuan Wang",
            "Jing Liao"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/7/1321",
          "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "13"
          ],
          "1": "This was improved upon in PVNet, where sparse key points are discarded as reference points, pixel-to-key point vectors are introduced through a semantic segmentation network, and the estimation accuracy is enhanced in scenarios with occlusion and symmetric objects [13].",
          "2": "This was improved upon in PVNet, where sparse key points are discarded as reference points, pixel-to-key point vectors are introduced through a semantic segmentation network, and the estimation accuracy is enhanced in scenarios with occlusion and symmetric objects [13]."
        },
        "SurgeoNet: Realtime 3D Pose Estimation of Articulated Surgical Instruments from Stereo Images using a Synthetically-trained Network": {
          "authors": [
            "AT Aboukhadra",
            "N Robertini",
            "J Malik",
            "A Elhayek"
          ],
          "url": "https://arxiv.org/abs/2410.01293",
          "ref_texts": "14. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4561\u20134570 (2019) 3, 10, 11",
          "ref_ids": [
            "14"
          ],
          "1": "In their experiments, they compare the performance of PVNet [14] and HandObjectNet [7].",
          "2": "8 PVNet [14] 39.",
          "3": "5 TR50 WS PVNet [14] 42."
        },
        "Synthetic Dataset Generation and Learning From Demonstration Applied to Industrial Manipulation": {
          "authors": [
            "A Barekatain",
            "HR Nohooji",
            "H Voos"
          ],
          "url": "https://arxiv.org/abs/2404.00447",
          "ref_texts": "[3] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u2013",
          "ref_ids": [
            "3"
          ],
          "1": "The output dataset has 500 images, which is subsequently used to train a state-of-the-art pose estimation method, namely PVNet [3]."
        },
        "FastPoseCNN: Real-Time Monocular Category-Level Pose and Size Estimation Framework": {
          "authors": [
            "E Davalos",
            "M Aminian"
          ],
          "url": "https://arxiv.org/abs/2406.11063",
          "ref_texts": "[38] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, page arXiv:1812.11788, 12 2020.",
          "ref_ids": [
            "38"
          ],
          "4": "For the hough voting scheme, we adapted the CUDA-accelerated implementation proposed by PVNet [38] for our problem as it provides a fast and accurate method to process multiple centroids in a batched fashion.",
          "5": "Our model adapted PVNet\u2019s CUDA-accelerated hough voting scheme [38]."
        },
        "Sim-to-Real Dataset of Industrial Metal Objects": {
          "authors": [
            "Peter De",
            "Steven Moonen",
            "Nick Michiels"
          ],
          "url": "https://www.mdpi.com/2075-1702/12/2/99",
          "ref_texts": "35. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570. Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
          "ref_ids": [
            "35"
          ],
          "1": "The authors first trained PVNet [35], a popular 6D object pose estimation method, on our dataset."
        },
        "Accurate Robot Arm Attitude Estimation Based on Multi-View Images and Super-Resolution Keypoint Detection Networks": {
          "authors": [
            "Ling Zhou",
            "Ruilin Wang",
            "Liyan Zhang"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/1/305",
          "ref_texts": "10. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4556\u20134565.",
          "ref_ids": [
            "10"
          ],
          "1": "[10] proposed PVNet (Pixel-wise Voting Network), which can obtain superior keypoint detection results when the target object is partially blocked."
        },
        "PCKRF: Point Cloud Completion and Keypoint Refinement With Fusion Data for 6D Pose Estimation": {
          "authors": [
            "Y Han",
            "IH Zhan",
            "L Zeng",
            "YP Wang",
            "R Yi"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10504632/",
          "ref_texts": "[11] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "11"
          ],
          "1": "[11]\u2013[13] utilize DNNs to detect the keypoints of each object and subsequently compute the 6D pose parameters using Perspective-n-Point (PnP) for 2D keypoints or Least Squares methods for 3D keypoints.",
          "2": "PVNet [11] predicted a unit vector to each keypoint for each pixel, then voted the 2D location for each keypoint and calculated the final pose using the PnP algorithm."
        },
        "Vision-Based 6D Pose Estimation and Tracking: From Known to Novel Objects": {
          "authors": [
            "L Tian"
          ],
          "url": "https://qmro.qmul.ac.uk/xmlui/handle/123456789/99018",
          "ref_texts": "[101] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In IEEE conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "101"
          ],
          "1": "3 2D voting in PVNet [101], which takes RGB image as input and predicts vectors pointing to keypoints for each pixel, and localises 2D keypoints in a RANSAC-based voting scheme.",
          "2": "Instance-level methods focus on estimating the 6D pose of specific objects, which can be achieved through direct pose regression from images [156, 72, 29, 10, 115] constructing 2D-3D correspondences and solving poses with PnP algorithm [101, 162, 66, 100, 83] or finding 3D-3D matching and recovering poses with Least-squares algorithm [54, 53, 82, 70, 154].",
          "3": "3: 2D voting in PVNet [101], which takes RGB image as input and predicts vectors pointing to keypoints for each pixel, and localises 2D keypoints in a RANSAC-based voting scheme.",
          "4": "In the 2D case, PVNet [101] votes 2D feature points and then finds the corresponding 2D-3D correspondences to obtain the 6D object pose.",
          "5": "Approach Learning-level TA Methods Estimation Instance-level # PVNet [101], PVN3D [54] DPOD [162] Estimation Category-level# NOCS [140], SGPA [21], FS-Net [23] Tracking Instance-level # PoseRBPF [29], SE(3)-TrackNet [148], DART [115] Tracking Category-level # 6-PACK [137], iCaps [28], BundleTrack [146] Estimation Instance-level \" Xinkeet al.",
          "6": "It is widely used by recent deep learning-based methods [101, 54, 53].",
          "7": "The FPS algorithm is used to select points that are farthest from each other in the 3D space, which is widely used in point cloud related methods [105, 103, 101, 29].",
          "8": "Following other keypoint-based 6D object pose estimation methods [101, 54, 137], I train my model to find 8 pairs of matched 3D keypoints from two frames for changed 6D pose estimation."
        },
        "FruitBin: a tunable large-scale dataset for advancing 6D pose estimation in fruit bin-picking automation": {
          "authors": [
            "G Duret",
            "M Ali",
            "N Cazin",
            "D Mazurak"
          ],
          "url": "https://hal.science/hal-04683842/",
          "ref_texts": "30. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "30"
          ],
          "1": "We evaluated the performance of three foundational 6D pose estimation models: PVNet [30], DenseFusion [39], and GDRNPP [23,40].",
          "2": "The first method, known as PVNet [30], employs an RGB image and 3D model information of objects as input to predict the 6D pose."
        },
        "Active Perception for Estimating 6D Poses of Textureless Shiny Objects": {
          "authors": [
            "J Yang"
          ],
          "url": "https://search.proquest.com/openview/a1c3d29634ebff5d42101124599298a2/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation\u201d. In: IEEE/CVF Conference on Computer Vision and Pattern recognition (CVPR). 2019.",
          "ref_ids": [
            "27"
          ],
          "1": "Due to the advancements in deep learning, many learning-based approaches have been recently shown to significantly boost the pose estimation performance on textureless objects using only RGB images [24, 25, 26, 27, 28].",
          "2": "With recent advances in deep learning, many recent approaches employ deep neural networks, such as convolutional neural networks (CNNs), to tackle the challenge of RGB-based object pose estimation [24, 64, 65, 25, 26, 66, 27, 67, 68, 28, 69].",
          "3": "Instead of directly estimating the object\u2019s 6D pose from the input image, recent solutions revisit the feature matching-based solutions with deep learning [64, 65, 27, 66, 67, 68, 28, 71, 69].",
          "4": "To handle textureless objects, these approaches leverage CNNs to first predict 2D object keypoints [64, 65, 27, 71] or dense 2D-3D correspondences [66, 67, 68, 28, 69], and then compute the 6D object pose through 2D-3D correspondences with a PnP algorithm.",
          "5": "As the most representative keypoint-based approach, PVNet [27] introduces a pixel-wise voting network to find object keypoints\u2019 pixel locations in the image and estimate the object pose using an uncertainty-driven PnP solver.",
          "6": "Instead of directly estimating the object pose, PVN3D [18] extends PVNet [27] and estimates the 3D keypoints of the object from the RGB-D image.",
          "7": "To evaluate the object pose performance, most existing works use the average model distance (ADD) metric [62, 24, 64, 65, 25, 26, 67, 68, 27, 18, 35, 15, 3, 33], which considers a pose estimate as correct if the average distance of model points between the estimated pose and ground truth is less than 10% of the object diameter.",
          "8": "Due to the recent advancements in deep learning, many RGB-based solutions have been proposed and shown the high performance [24, 25, 26, 27, 28].",
          "9": "Our network architecture is based on PVNet [27].",
          "10": "For more details of the object center localization prediction, we refer the reader to [27]."
        },
        "Mapping Real-World Objects into Virtual Reality to Facilitate Interaction using 6DoF Pose Estimation": {
          "authors": [
            "S Pelser"
          ],
          "url": "https://scholar.sun.ac.za/bitstreams/ed797c6c-035f-4b97-a177-9bd22d3234db/download",
          "ref_texts": "[55] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4556\u20134565.",
          "ref_ids": [
            "55"
          ],
          "1": "Examples of deep learning methods are Posenet [53], Densefusion [54], PVNet [55] and PoseCNN [56].",
          "2": "Many DL models have been designed and tested on the LineMOD dataset used for 6DoF pose estimation, introducing CNN-based approaches that outperformed traditional methods by a significant margin, with PoseCNN [56] and PVNet [55] being some of the most well-known models."
        },
        "Visibility aware human-object interaction tracking from single rgb camera": {
          "authors": [
            "Xianghui Xie",
            "Bharat Lal",
            "Gerard Pons"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xie_Visibility_Aware_Human-Object_Interaction_Tracking_From_Single_RGB_Camera_CVPR_2023_paper.html",
          "ref_texts": "[52] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4556\u20134565, Long Beach, CA, USA, June 2019. IEEE. 2",
          "ref_ids": [
            "52"
          ],
          "1": "On the other hand, deep learning method has also significantly improved object 6D pose estimation from single RGB images [22,25,33,45,50,52,72].",
          "2": "For object pose estimation, pixel-wise voting [52] and self-occlusion [22] are explored for more robust prediction under occlusions."
        },
        "Hs-pose: Hybrid scope feature extraction for category-level object pose estimation": {
          "authors": [
            "Linfang Zheng",
            "Chen Wang",
            "Yinghan Sun",
            "Esha Dasgupta",
            "Hua Chen",
            "Ales Leonardis",
            "Wei Zhang",
            "Hyung Jin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zheng_HS-Pose_Hybrid_Scope_Feature_Extraction_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 2",
          "ref_ids": [
            "32"
          ],
          "1": "The correspondences can be sparse bounding box corners [33, 41], or distinguishable points on the object\u2019s surface [19, 31, 32]."
        },
        "Deep fusion transformer network with weighted vector-wise keypoints voting for robust 6d object pose estimation": {
          "authors": [
            "Jun Zhou",
            "Kai Chen",
            "Linlin Xu",
            "Qi Dou",
            "Jing Qin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Deep_Fusion_Transformer_Network_with_Weighted_Vector-Wise_Keypoints_Voting_for_ICCV_2023_paper.html",
          "ref_texts": "[47] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 2, 5, 6, 7",
          "ref_ids": [
            "47"
          ],
          "1": "For better robustness to highly occluded scenes, furthermore, the pixel/point-wise voting methods [20, 47, 49, 62] are proposed to vote for the keypoints position.",
          "2": "In this way, instead of regressing point-wise offsets to the predefined keypoints directly, we propose to predict the unit vector that represents the direction from the point pi to a 3D keypoint kj of the object, like [47] in 2D.",
          "3": "We use synthesis images in the training phase following [19, 20, 47] and follow previous works [47, 61] to split the training and testing set.",
          "4": "1d) as in [22, 47].",
          "5": "RGB RGB-D PoseCNN DeepIM [38, 61] PVNet [47] CDPN [39] DPOD [64] PointFusion [63] DenseFusion [58] G2L-Net [13] PVN3D [20] FFB6D [19] Ours ape 77.",
          "6": "Method PoseCNN [61] Pix2Pose [46] PVNet [47] Hu et al."
        },
        "Full-body articulated human-object interaction": {
          "authors": [
            "Nan Jiang",
            "Tengyu Liu",
            "Zhexuan Cao",
            "Jieming Cui",
            "Zhiyuan Zhang",
            "Yixin Chen",
            "He Wang",
            "Yixin Zhu",
            "Siyuan Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Full-Body_Articulated_Human-Object_Interaction_ICCV_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 3",
          "ref_ids": [
            "39"
          ],
          "2": "Articulated Object Pose Estimation The estimation of 6-DOF poses for rigid objects has garnered attention [25, 19, 3, 48, 39, 37, 9]."
        },
        "Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation": {
          "authors": [
            "Heng Yang",
            "Marco Pavone"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.html",
          "ref_texts": "[72] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation.IEEE Trans. Pattern Anal. Machine Intell., 2022. 1, 2, 8",
          "ref_ids": [
            "72"
          ],
          "2": "PVNet [72]",
          "3": "voting [72]"
        },
        "Se (3) diffusion model-based point cloud registration for robust 6d object pose estimation": {
          "authors": [
            "H Jiang",
            "M Salzmann",
            "Z Dang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/43069caa6776eac8bca4bfd74d4a476d-Abstract-Conference.html",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "41"
          ],
          "2": "PVNet[41]"
        },
        "Shape-constraint recurrent flow for 6d object pose estimation": {
          "authors": [
            "Yang Hai",
            "Rui Song",
            "Jiaojiao Li",
            "Yinlin Hu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition, 2019. 2, 5, 6",
          "ref_ids": [
            "36"
          ],
          "1": "Related Work Object pose estimation , has shown significant improvement [36, 47, 50] after the utilization of deep learning techniques [13, 51].",
          "4": "Comparison to the State of the Art We compare our method with most state-of-the-art methods, including PoseCNN [50], PVNet [36], SO-Pose [10], DeepIM [28], RePose [23], RNNPose [52], and PFA [16]."
        },
        "Texpose: Neural texture learning for self-supervised 6d object pose estimation": {
          "authors": [
            "Hanzhi Chen",
            "Fabian Manhardt",
            "Nassir Navab",
            "Benjamin Busam"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_TexPose_Neural_Texture_Learning_for_Self-Supervised_6D_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.1, 2",
          "ref_ids": [
            "39"
          ],
          "1": "Noteworthy, accuracy and runtime have both recently made a huge leap forward thanks to deep learning [19, 23, 31, 39, 40]."
        },
        "Learning symmetry-aware geometry correspondences for 6d object pose estimation": {
          "authors": [
            "Heng Zhao",
            "Shenxing Wei",
            "Dahu Shi",
            "Wenming Tan",
            "Zheyang Li",
            "Ye Ren",
            "Xing Wei",
            "Yi Yang",
            "Shiliang Pu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "43"
          ],
          "2": "For instance, PVNet [43] selects K 3D keypoints from the object surface."
        },
        "Crt-6d: Fast 6d object pose estimation with cascaded refinement transformers": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "34"
          ],
          "4": "This shortfall was noticed by PVNet [34], which suggests the use of the surface region to find suitable keypoints.",
          "6": "1 8 1 8 1 1 8 8 Method PVNet [34] GDR [47] GDR [47] SO-Pose [9] ZebraPose [40] RePose [20] DeepIM [28] CRT-6D Ape 15.",
          "7": "Repose [20] proposed a faster refinement method at 18ms with 5 iterations however they require a good initialization (they use PVNet [34] which itself takes over 25ms) and it only support a single object per model."
        },
        "PoET: Pose estimation transformer for single-view, multi-object 6D pose estimation": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://proceedings.mlr.press/v205/jantos23a.html",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "In recent years, advancements in deep learning for computer vision tasks have been applied to singleview, image-based 6D pose estimation, either to replace components of classical approaches [17, 18, 19, 20], or as end-to-end learned methods, where the 6D pose is directly estimated from the input using convolutional neural networks (CNNs)."
        },
        "Rigidity-aware detection for 6d object pose estimation": {
          "authors": [
            "Yang Hai",
            "Rui Song",
            "Jiaojiao Li",
            "Mathieu Salzmann",
            "Yinlin Hu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hai_Rigidity-Aware_Detection_for_6D_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition, 2019. 2",
          "ref_ids": [
            "33"
          ],
          "1": "Related Work Object pose estimation, whose goal is to estimate the 3D rotation and 3D translation of a target object with respect to the camera, nowadays typically involves a pose regression network to establish 3D-to-2D correspondences [12,15\u201317, 19, 20, 33]."
        },
        "Nerf-loc: Visual localization with conditional neural radiance field": {
          "authors": [
            "J Liu",
            "Q Nie",
            "Y Liu",
            "C Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161420/",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "PVNet[21] 25."
        },
        "Center-based decoupled point-cloud registration for 6D object pose estimation": {
          "authors": [
            "Haobo Jiang",
            "Zheng Dang",
            "Shuo Gu",
            "Jin Xie",
            "Mathieu Salzmann",
            "Jian Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[51] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1",
          "ref_ids": [
            "51"
          ],
          "1": "While great progress has been made when exploiting RGB or RGB-D data as input [34, 54, 51, 59, 49, 69, 58], the advances in 3D sensors and deep point-cloud learning architectures have led to the development of increasingly accurate point cloud registration algorithms [61, 67, 32, 19, 14]."
        },
        "Query6dof: Learning sparse queries as implicit shape prior for category-level 6dof pose estimation": {
          "authors": [
            "Ruiqi Wang",
            "Xinggang Wang",
            "Te Li",
            "Rong Yang",
            "Minhong Wan",
            "Wenyu Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Query6DoF_Learning_Sparse_Queries_as_Implicit_Shape_Prior_for_Category-Level_ICCV_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "Instance-Level 6D Object Pose Estimation Based on the input data format, existing methods can be divided into two categories: RGB-based [15, 28, 25, 22, 37, 23] and RGB-D-based [14, 32, 10, 13] approaches."
        },
        "Easyhec: Accurate and automatic hand-eye calibration via differentiable rendering and space exploration": {
          "authors": [
            "L Chen",
            "Y Qin",
            "X Zhou",
            "H Su"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10251600/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "In our work, we adopt the PVNet [25] to perform the pose initialization.",
          "2": "We rendered 10,000 images for the PointRend [20] and another 10,000 images for the PVNet [25] training to obtain the observed segmentation mask and initial camera pose for our method.",
          "3": "Specifically, we use PVNet [25] to estimate the object poses in the camera coordinate system and then use the handeye calibration results to transform the object poses from the camera coordinate system to the base coordinate system as the input states to the CoTPC network."
        },
        "ContourPose: Monocular 6-D pose estimation method for reflective textureless metal parts": {
          "authors": [
            "Z He",
            "Q Li",
            "X Zhao",
            "J Wang",
            "H Shen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10189174/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4556\u20134565.",
          "ref_ids": [
            "10"
          ],
          "2": "PVNet[10] regresses pixelwise vectors point at the keypoints and uses these vectors to vote for the location of the keypoints.",
          "3": "HybridPose[35] extends the approach of PVNet [10] by utilizing a hybrid intermediate representation to express different geometric information in the input image, including keypoints, edge vectors, and symmetry correspondences.",
          "5": "Many methods such as PVNet[10] and PSGMN [37] will output both the keypoints information and the mask using only one decoder.",
          "6": "Common methods use the farthest point sampling (FPS) [40] algorithm to select several points at the farthest Euclidean distance on the model, such as PVNet[10].",
          "9": "Our proposed method trains a specific network to estimate a single object similar to PVNet[10],C D P N[12], and PSGMN [37]."
        },
        "SMOC-Net: leveraging camera pose for self-supervised monocular object pose estimation": {
          "authors": [
            "Tao Tan",
            "Qiulei Dong"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[22] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "22"
          ],
          "4": "[22] proposed a pixel level voting network (PVNet) by using the direction vector field to predict keypoints, which achieved good performance under severe truncation and occlusion.",
          "6": "Here, we evaluate the proposed SMOC-Net on the LineMOD dataset in comparison to some state-of-the-art methods, including three fullysupervised methods (DPOD [41], PVNet [22], CDPN [17]), three self-supervised methods that are trained with only synthetic data (AAE [31], MHP [20], DPOD [41]), one selfsupervised method that is trained with both synthetic data and un-annotated real images (DSC-PoseNet [39]), and two self-supervised methods that are trained with synthetic data + un-annotated real images + depth images (Self6D [34], Self6D++ [33])."
        },
        "Posematcher: One-shot 6d object pose estimation by deep feature matching": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Castro_PoseMatcher_One-Shot_6D_Object_Pose_Estimation_by_Deep_Feature_Matching_ICCVW_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2, 6, 7, 8",
          "ref_ids": [
            "30"
          ],
          "1": "PVNet [30] found that choosing keypoints that lie within the object\u2019s silhouette would yield better results.",
          "3": "It is interesting to observe that we also outperform PVNet[30], an instance-level pose estimator."
        },
        "Generative category-level shape and pose estimation with semantic primitives": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://proceedings.mlr.press/v205/li23d.html",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "9"
          ],
          "3": "For example, PVNet [9] predicts the 3D keypoints on the RGB image by a voting scheme."
        },
        "Revisiting fully convolutional geometric features for object 6d pose estimation": {
          "authors": [
            "Jaime Corsetti",
            "Davide Boscaini",
            "Fabio Poiesi"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Corsetti_Revisiting_Fully_Convolutional_Geometric_Features_for_Object_6D_Pose_Estimation_ICCVW_2023_paper.html",
          "ref_texts": "[31] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "31"
          ],
          "1": "PVN3D [13] extends PVNet [31] by incorporating 3D point cloud information."
        },
        "Multi-object manipulation via object-centric neural scattering functions": {
          "authors": [
            "Stephen Tian",
            "Yancheng Cai",
            "Xing Yu",
            "Sergey Zakharov",
            "Katherine Liu",
            "Adrien Gaidon",
            "Yunzhu Li",
            "Jiajun Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR, pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "41"
          ],
          "1": "Many prior works investigate the problem of estimating rigid object poses from RGB images, including deep-learning approaches based on correspondences [24, 35, 40, 41, 47, 58] as well as direct regression [9, 13, 29, 53, 59]."
        },
        "Checkerpose: Progressive dense keypoint localization for object pose estimation with graph neural network": {
          "authors": [
            "Ruyi Lian",
            "Haibin Ling"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lian_CheckerPose_Progressive_Dense_Keypoint_Localization_for_Object_Pose_Estimation_with_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2, 4, 5, 7",
          "ref_ids": [
            "43"
          ],
          "1": "Instead of direct estimation, correspondence guided methods [42, 48, 59, 38, 18, 43, 17, 19, 72, 40, 33, 66, 8, 56] follow a two-stage framework: they first predict a set of correspondences between 3D object frame coordinates and 2D image plane coordinates, and then recover the pose from the 3D-2D correspondences with a PnP algorithm [27, 25, 9, 64, 4].",
          "2": "Keypoint-localization based methods [42, 48, 59, 38, 18, 43, 17, 19] estimate the 2D coordinates for a sparse set of predefined 3D keypoints, while dense methods [72, 40, 33, 66, 8, 56] predict the 3D object frame coordinate of each 2D image pixel.",
          "3": ", heatmaps [42, 38] and vector-fields [43, 18]), our representation needs only 2d + 1binary bits for each keypoint, thus greatly reduces the memory usage for dense keypoint localization.",
          "4": ", voting for the vector-field representations [43].",
          "5": "As shown in Table 1, the accuracy decreases significantly without 14027 Method PVNet [43] S."
        },
        "Cad2render: A modular toolkit for gpu-accelerated photorealistic synthetic data generation for the manufacturing industry": {
          "authors": [
            "Steven Moonen",
            "Bram Vanherle",
            "Taoufik Bourgana",
            "Abdellatif Bey",
            "Nick Michiels"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/html/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.html",
          "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "The identification of the objects and subsequent position and pose estimation was done with two state of the art networks: YoloV4 [2] for object detection and PVNET [17] for pose estimation."
        },
        "Digital twin tracking dataset (dttd): A new rgb+ depth 3d dataset for longer-range object tracking applications": {
          "authors": [
            "Weiyu Feng",
            "Seth Z. Zhao",
            "Chuanyu Pan",
            "Adam Chang",
            "Yichen Chen",
            "Zekun Wang",
            "Allen Y. Yang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/html/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.html",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3",
          "ref_ids": [
            "23"
          ],
          "1": "6 DoF Object Pose Estimation Most data-driven methods for object pose estimation take RGB [18, 23, 29, 30] or RGB-D images [10, 11, 15, 22, 27] as input."
        },
        "Stereopose: Category-level 6d transparent object pose estimation from stereo images via back-view nocs": {
          "authors": [
            "K Chen",
            "S James",
            "C Sui",
            "YH Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160780/",
          "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "10"
          ],
          "1": "instance-level voting field [10] and category-level NOCS map [1]) used on nontransparent objects only describe dense correspondences for the front-view1 of the object."
        },
        "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching": {
          "authors": [
            "Y Sun",
            "X Wang",
            "Y Zhang",
            "J Zhang",
            "C Jiang"
          ],
          "url": "https://arxiv.org/abs/2312.09031",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019) 1",
          "ref_ids": [
            "28"
          ],
          "1": "Common pose estimation methods often rely on detailed geometric models related to the target object [18,28,36,39]."
        },
        "Interacting hand-object pose estimation via dense mutual attention": {
          "authors": [
            "Rong Wang",
            "Wei Mao",
            "Hongdong Li"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "31"
          ],
          "1": "Hand-Object Pose Estimation Most previous works tackle 3D hand pose estimation [17, 25, 40, 50, 47] and object pose estimation [27, 31, 44, 49] separately."
        },
        "Generalizable pose estimation using implicit scene representations": {
          "authors": [
            "V Saxena",
            "KR Malekshan",
            "L Tran"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161162/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "13"
          ],
          "1": "The majority of approaches can only be used for a specific object or for categories ([8], [9], [10], [11], [12], [13], [14]) that are similar to the ones in the training data.",
          "2": "b) Instanceand Category-Specific Pose Estimation: Most state-of-the-art object pose estimators are either instance-specific ([8], [9], [10], [11], [12], [13], [14]) or category-specific ([29], [30]).",
          "3": "Keypoint-based approaches ([13], [14]) utilized deep neural networks to detect 2D keypoints of an object and computed 6D pose parameters with Perspective-n-Point (PnP) algorithms, improving pose estimates by a large margin."
        },
        "Complementary bi-directional feature compression for indoor 360deg semantic segmentation with self-distillation": {
          "authors": [
            "Zishuo Zheng",
            "Chunyu Lin",
            "Lang Nie",
            "Kang Liao",
            "Zhijie Shen",
            "Yao Zhao"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "26"
          ],
          "1": "mantic segmentation aims to assign each pixel in the image a category label and is critical for various applications such as pose estimation [26], autonomous vehicles [31], augmented reality [2]."
        },
        "6d pose estimation for textureless objects on rgb frames using multi-view optimization": {
          "authors": [
            "J Yang",
            "W Xue",
            "S Ghavidel"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160529/",
          "ref_texts": "[26] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "com boost the object pose estimation performance using only RGB images [23], [24], [25], [26], [27].",
          "2": "In comparison, some recent works leverage CNNs to first predict 2D object keypoints [36], [37], [26] or dense 2D-3D correspondences [38], [39], [27], [40], and then compute the pose through 2D-3D correspondences with a PnP algorithm [41].",
          "3": "Our network architecture is based on PVNet [26].",
          "4": "For more details of the object center localization prediction, we refer the reader to [26]."
        },
        "Perceiving unseen 3d objects by poking the objects": {
          "authors": [
            "L Chen",
            "Y Song",
            "H Bao",
            "X Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160338/",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "4"
          ],
          "2": "Here, we use the PVNet [4] to demonstrate how to learn an object pose estimator based on the reconstructed object model.",
          "3": "3, given the reconstructed object model, we use the analytic method Graspit! [16] to compute Tgo and PVNet [4] to estimate Toc."
        },
        "Depth-based 6dof object pose estimation using swin transformer": {
          "authors": [
            "Z Li",
            "I Stamos"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10342215/",
          "ref_texts": "[10] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DOF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "10"
          ],
          "2": "INPUTS RGB RGB-D Depth-Only METHODSPVNet [10]Pix2Pose [42]RNNPose [43]PVN3D [15]DenseFusion [13]KPD [44]CloudAAE [16]CATRE [19]OVE6D [17](w Mask R-CNN)OVE6D [17](w GT Masks) OURS(w/o GT Mask)OURS(w GT Masks) ape 43.",
          "3": "INPUTS RGB RGB-D Depth-Only METHODSPVNet [10]Pix2Pose [42]Keypoint [45]Point-to-Keypoint [46]FFB6D [14]KPD [44] CloudAAE [16]OVE6D [17](w Mask R-CNN)OVE6D [17](w GT Masks) OURS(w/o GT Mask) OURS(w GT Masks) ape 15."
        },
        "Linear-covariance loss for end-to-end learning of 6d pose estimation": {
          "authors": [
            "Fulin Liu",
            "Yinlin Hu",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Linear-Covariance_Loss_for_End-to-End_Learning_of_6D_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1, 2",
          "ref_ids": [
            "41"
          ],
          "1": "More recently, most works [5, 21, 23, 33, 35, 38, 39, 41, 42, 43, 44] draw inspiration from geometry and seek to predict 2D-3D corresponImageNoisy 2D-3D matchingPosteriordistribution Residual variances of correspondencesSolution distribution loss pose p g.",
          "2": ", via the farthest point sampling algorithm [41].",
          "3": "[23] aggregate the 2D keypoint predictions from all pixels belonging to the given target; Similarly, PVNet [41] regresses the vector-field pointing from each object pixel to the 2D locations."
        },
        "Ikea-manual: Seeing shape assembly step by step": {
          "authors": [
            "R Wang",
            "Y Zhang",
            "J Mao",
            "R Zhang",
            "CY Cheng"
          ],
          "url": "https://arxiv.org/abs/2302.01881",
          "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DOF pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "37"
          ],
          "1": "Combined with the annotation of assembly parts, this annotation can be leveraged in pose estimation [36, 37] and single-view 3D reconstruction [38, 39] tasks."
        },
        "YOLOPose V2: Understanding and improving transformer-based 6D pose estimation": {
          "authors": [
            "AS Periyasamy",
            "A Amini",
            "V Tsaturyan"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S092188902300129X",
          "ref_texts": "[40] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "40"
          ],
          "1": "Keypointbased include [22, 23, 40, 44, 53].",
          "2": "[40] for RGB images to 3D point clouds by learning point-wise 3D keypoint offset and using a deep Hough voting network.",
          "3": "[40] instead used the Farthest Point Sampling (FPS) algorithm to sample eight keypoints on the surface of the object meshes, which are also spread out on the object to help the P nP algorithm find a more stable solution."
        },
        "Multi-view keypoints for reliable 6d object pose estimation": {
          "authors": [
            "A Li",
            "AP Schoellig"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160354/",
          "ref_texts": "[7] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019)",
          "ref_ids": [
            "7"
          ],
          "1": "These methods have been revisited using the advantages of CNNs to automatically learn features and implicitly estimate pose [9] [7].",
          "2": "The network is derived from YOLO object detection [15], specifically trained using multi-view RGB-D data that is rendered synthetically using domain randomization [7].",
          "3": "Single-view Keypoint and Heatmap Estimation The keypoint detection network is based off PVNet, a pixel-wise voting network for 6D pose estimation [7]."
        },
        "DR-pose: A two-stage deformation-and-registration pipeline for category-level 6D object pose estimation": {
          "authors": [
            "L Zhou",
            "Z Liu",
            "R Gan",
            "H Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10341552/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "7"
          ],
          "1": "While most current object pose estimation networks focus on instancelevel object pose estimation [5][6][7][8], which requires exact instance CAD models and their sizes beforehand, this approach can be limiting in real-world scenarios where such information may not be available.",
          "2": "The second approach uses 2D-3D [7][16][17][18][19] or 3D3D [20] correspondences correspondences to solve a PnP [21][22] problem and obtain the 6D pose."
        },
        "Learning bifunctional push-grasping synergistic strategy for goal-agnostic and goal-oriented tasks": {
          "authors": [
            "D Ren",
            "S Wu",
            "X Wang",
            "Y Peng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10342533/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "2) Grasp module establishes contact with the perception module through skip connections [24] and outputs a dense pixel-wise map of Qvalues as Grasp Qmap for the grasping action."
        },
        "SD-pose: structural discrepancy aware category-level 6D object pose estimation": {
          "authors": [
            "Guowei Li",
            "Dongchen Zhu",
            "Guanghui Zhang",
            "Wenjun Shi",
            "Tianyu Zhang",
            "Xiaolin Zhang",
            "Jiamao Li"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "27"
          ],
          "1": "So far, instance-level 6D pose estimation works [19, 29, 22, 27, 38, 17, 16] have made considerable progress.",
          "2": "For the correspondence between 2D and 3D [27, 29, 30], the pose is obtained by solving a PnP problem [21].",
          "3": "Indirect voting [27, 17] first selects key point positions through RANSAC [10] voting and then calculates the 6D pose of the object according to the correspondence between key points."
        },
        "PViT-6D: Overclocking vision transformers for 6D pose estimation with confidence-level prediction and pose tokens": {
          "authors": [
            "S Stapf",
            "T Bauernfeind",
            "M Riboldi"
          ],
          "url": "https://arxiv.org/abs/2311.17504",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CoRR, abs/1812.11788, 2018. 1, 2, 8",
          "ref_ids": [
            "33"
          ],
          "1": "This has resulted in significant improvements in accuracy, as seen in models like PVNet [33], GDRNet [45], and the latest ZebraPose [38].",
          "2": "PVNet [33] introduced a twostage method that initially estimates the 2D keypoints of objects in the image.",
          "4": "Method N ADD(-S) AUC of ADD(-S) PVNet[33] 21 73."
        },
        "Deeprm: Deep recurrent matching for 6d pose refinement": {
          "authors": [
            "Alexander Avery",
            "Andreas Savakis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/html/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.html",
          "ref_texts": "[22] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNET: Pixel-wise voting network for 6dof pose estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019June:4556\u20134565, dec 2019. 2, 5, 6",
          "ref_ids": [
            "22"
          ],
          "1": "To further address the problem of occlusion, PVNet [22] introduced a pixel-wise voting network using RANSAC, resulting in an estimator that is capable of detecting keypoints, even when they are occluded.",
          "5": "Initial predictions are obtained from PVNet [22], where DeepRM outperforms all existing methods except for ZebraPose [27] and CRT-6D [4]."
        },
        "Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction": {
          "authors": [
            "Y Yang",
            "J Wu",
            "Y Wang",
            "G Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10265177/",
          "ref_texts": "[9] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "9"
          ],
          "2": "PVNet [9], employs farthest point sampling to vote for key points on the target object, predicting the direction vector pointing from each pixel to the projection point using a RANSAC voting strategy to locate the projection point."
        },
        "MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_31",
          "ref_texts": "23. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)",
          "ref_ids": [
            "23"
          ],
          "1": "Different from BB8, PVNet [23] selects the pre-defined 3D keypoints from the surface of the 3D object CAD model and then localizes the 2D pixel coordinates of these 3D keypoints in RGB images based on the pixel-wise voting schema."
        },
        "For a more comprehensive evaluation of 6dof object pose tracking": {
          "authors": [
            "Y Li",
            "F Zhong",
            "X Wang",
            "S Song",
            "J Li",
            "X Qin"
          ],
          "url": "https://arxiv.org/abs/2309.07796",
          "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE/CVF Conference on CVPR, pages 4556\u20134565, Long Beach, CA, USA, June 2019. IEEE.",
          "ref_ids": [
            "24"
          ],
          "1": "This approach enables automatic initialization and re-initialization for the tracking, which are usually requires to be done with 6DoF pose estimation [43, 24, 33].",
          "3": "Although in this paper we focus on only 6DoF tracking, similar problems also exist for 6DoF pose refinement [19, 14, 20, 44] and 6DoF pose estimation [43, 24, 33], which also commonly use YCBV for their evaluations, and thus can benefit from our work."
        },
        "Transpose: A transformer-based 6d object pose estimation network with depth refinement": {
          "authors": [
            "M Abdulsalam",
            "N Aouf"
          ],
          "url": "https://arxiv.org/abs/2307.05561",
          "ref_texts": "[49] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "49"
          ],
          "1": "Some other methods have considered using models encompassing classical algorithm such as PnP algorithm to increase the accuracy of estimation [44], [49], [50]."
        },
        "Rigidity preserving image transformations and equivariance in perspective": {
          "authors": [
            "L Brynte",
            "G B\u00f6kman",
            "A Flinth",
            "F Kahl"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_5",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conf. Computer Vision and Pattern Recognition (2019)",
          "ref_ids": [
            "44"
          ],
          "2": "As expected, there is some discrepancy between the baseline trained by us and the original reported results, in particular Rigidity Preserving Image Transformations and Equivariance in Perspective 13 EP:Baseline EP:PY EP:RHaug PVNet[44] CDPN[38] BPnP[11] RNNPose[55] DFPN-6D[12] 95."
        },
        "NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios": {
          "authors": [
            "ET Lin",
            "WJ Lv",
            "DT Huang",
            "L Zeng"
          ],
          "url": "https://arxiv.org/abs/2311.09269",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "To address the non-linear problem in the rotation space, some works [10], [21] propose to predict corresponding keypoints and recover the 6D pose by least-squares fitting."
        },
        "AttentionPose: Attention-driven end-to-end model for precise 6D pose estimation": {
          "authors": [
            "MA Rasheed",
            "RN Farhan",
            "WM Jasim"
          ],
          "url": "https://www.degruyter.com/document/doi/10.1515/jisys-2023-0153/html",
          "ref_texts": "[13] Peng S, Zhou X, Liu Y, Lin H, Huang Q, Bao H. PVNet: Pixel-wise voting network for 6DoF object pose estimation. IEEE Trans Pattern Anal Mach Intell. 2022;44(6):3212\u201323.",
          "ref_ids": [
            "13"
          ],
          "2": "A 2019 study [13] introduces a DL solution for 6D pose estimation using DL.",
          "3": "PVnet [13] is a neural network developed to estimate the posture of 6D objects.",
          "4": "Table 2:Comparison with some state-of-the-art methods Object The proposed method PVNet [13] Hybrid pose [32] YOLO6D [33] DPOD [34] DPOD +[34] Efficient Pose [35] Duck 99."
        },
        "A Benchmark for Cycling Close Pass Near Miss Event Detection from Video Streams": {
          "authors": [
            "M Li",
            "T Rathnayake",
            "B Beck",
            "L Meng",
            "Z Chen"
          ],
          "url": "https://arxiv.org/abs/2304.11868",
          "ref_texts": "[40] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570",
          "ref_ids": [
            "40"
          ],
          "1": "On the other hand, end-to-end approaches [54], [37], [40] aim to directly return the 3D information and pose parameters of the camera [26], avoiding the nonlinear space for rotation regression and improving efficiency."
        },
        "An open-source recipe for building simulated robot manipulation benchmarks": {
          "authors": [
            "J Gu",
            "L Chen",
            "Z Jia",
            "F Xiang",
            "H Su"
          ],
          "url": "https://cseweb.ucsd.edu/~jigu/pdf/icra23-compare.pdf",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "1": "In the real-world setup, we first estimate the 6-DoF object poses in the camera space by PVNet [25] and then transform the object poses to the robot base using the relative pose between the camera and the base of the robot arm obtained by hand-eye-calibration."
        },
        "Detection and Pose Estimation of Flat, Texture-Less Industry Objects on HoloLens Using Synthetic Training": {
          "authors": [
            "T P\u00f6llabauer",
            "F R\u00fccker",
            "A Franek"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-31438-4_37",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019)",
          "ref_ids": [
            "32"
          ],
          "2": "PVNet [32] introduces a voting scheme, requiring pixels to vote for keypoint locations, making the predictions more robust to occlusion and truncation."
        },
        "OLF: RGB-D Adaptive Late Fusion for Robust 6D Pose Estimation": {
          "authors": [
            "P Th\u00e9o",
            "Z Wu",
            "C Demonceaux",
            "O Laligant"
          ],
          "url": "https://hal.science/hal-04085729/",
          "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR], (2019).",
          "ref_ids": [
            "25",
            "CVPR"
          ],
          "14": "Pvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR], (2019)."
        },
        "Affordance-grounded Robot Perception and Manipulation in Adversarial, Translucent, and Cluttered Environments": {
          "authors": [
            "Xiaotong Chen"
          ],
          "url": "https://deepblue.lib.umich.edu/handle/2027.42/177867",
          "ref_texts": "[147] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "147"
          ],
          "1": "Then there comes pure endto-end deep-learning pipelines from PoseCNN [192], BB8 [151], and SSD-6D [92] started the trend, followed by a large set of methods using RGB images [182, 14, 105, 147, 100, 70], or RGB-depth images [187, 73, 74] as input.",
          "2": "StereObj1M [111] benchmarked KeyPose and another RGB-based object pose estimator, PVNet [147], on more challenging objects and scenes, where both methods achieved lower accuracy with respect to the ADD-S AUC metric (introduced in [192]) with both monocular and stereo input."
        },
        "Learning Embodied AI Agents with Task Decomposition": {
          "authors": [
            "Z Jia"
          ],
          "url": "https://search.proquest.com/openview/54c5288fb3d998ed6b1314b68a1eb510/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[118] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "118"
          ],
          "1": "With an off-the-shelf pose estimation framework such as PVNet [118], we can achieve reasonable performance using the state-based CoTPC policy learned purely from simulated data."
        },
        "Vision-guided object pose estimation for robotic pushing in real-time": {
          "authors": [
            "PM van der Burg"
          ],
          "url": "https://repository.tudelft.nl/record/uuid:8155dc98-21ec-4603-8be0-cd13eec6f6ad",
          "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, June 2019.",
          "ref_ids": [
            "33"
          ],
          "1": "Additionally, pose refinement methods rely on initial pose estimates obtained from models such as PoseCNN [48] or PVNet [33]."
        },
        "6D Pose Estimation of Weakly Textured Object Driven by Decoupling Analysis and Algorithm Fusion Strategy": {
          "authors": [
            "\u6c5f\u82cf\u79d1\u6280\u5927\u5b66\uff0c \u90d1\u5929\u5b87"
          ],
          "url": "https://www.researchsquare.com/article/rs-3105669/latest",
          "ref_texts": "32. Peng S, Liu Y , Huang Q, Zhou X, Bao H (2019) Pvnet: Pixel-wise voting netwo rk for 6dof pose estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561-4570 ",
          "ref_ids": [
            "32"
          ],
          "1": "developed the PVNet [32] network, which combines 3D shape information and 2D projection information for pose estimation."
        },
        "Image-based Object Pose Estimation for Robotic Manipulation: A Cost-effective Approach in Virtual Environment": {
          "authors": [
            "\u9ec4\u6d69\u6668"
          ],
          "url": "https://gunma-u.repo.nii.ac.jp/record/10029/files/T201D604.pdf",
          "ref_texts": "[10] Peng, S., Liu, Y ., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570). ",
          "ref_ids": [
            "10"
          ],
          "1": "In recent years, deep learning methods based on 2D image [2, 9, 10, 11] or 3D point cloud [12, 13] are proposed to solve the pose estimation problem, and achieved a good result benefiting from the powerful feature extraction ability of neural network.",
          "2": "Keypoint-based methods [10, 11]: Compared with directly predicting pose-related parameters, building 2D-3D correspondences for object pose detection is more accurate."
        },
        "Single-view articulated robot state estimation": {
          "authors": [
            "I Bilic"
          ],
          "url": "https://www.fer.unizg.hr/_download/repository/Ivan_Bilic_-_rad_za_KI.pdf",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4556\u2013",
          "ref_ids": [
            "6"
          ],
          "1": "Keypoint-based object pose estimation methods [2], [3], [4], [5], [6], [7] typically detect a set of sparse features on the object in the image using a CNN and the resulting 2D-to-3D correspondences are used to recover the camera pose using Perspective-n-point (PnP) solver [8], as shown in Fig.",
          "2": "In [6], 6-DoF pose estimation under severe occlusions is addressed."
        },
        "DAPO: Self-Supervised Domain Adaptation for 6DoF Pose Estimation": {
          "authors": [
            "J Jin",
            "E Jeong",
            "J Cho",
            "JH Park",
            "YG Kim"
          ],
          "url": "https://sslneurips23.github.io/paper_pdfs/paper_21.pdf",
          "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "12"
          ],
          "1": "For instance, PVNet [12] utilizes a regression-based approach to predict pixel-wise unit vectors pointing to key points."
        },
        "Rilevamento di oggetti 3D da immagini 2D: metodi e applicazioni": {
          "authors": [
            "Zlatko Kovachev"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/57086",
          "ref_texts": "[13] S. Peng, Y. Liu, Q. Huang, X. Zhou e H. Bao, \u00abPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u00bb in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
          "ref_ids": [
            "13"
          ],
          "4": "[13], \u00e8 un ottimo esempio di metodo Feature-Based per la stima della posa 6D, concentrandosi specificamente su scene che presentano grandi occlusioni (g) 22 e oggetti troncati (h) (Figura 3."
        },
        "Utilizzo di reti GAN per la creazione di immagini di addestramento per l'object pose estimation": {
          "authors": [
            "R DEL BEN"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/54924",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "2"
          ],
          "1": "Finally, the best GAN models have been combined with Pixel-wise Voting Network (PVNet [2]) to compare their performance against the PVNet model trained using the conventional image Superimposing Method (PVNet-SM)."
        },
        "Deep learning on monocular object pose detection and tracking: A comprehensive overview": {
          "authors": [
            "Z Fan",
            "Y Zhu",
            "Y He",
            "Q Sun",
            "H Liu",
            "J He"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3524496",
          "ref_texts": "[118] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4561\u20134570.",
          "ref_ids": [
            "118"
          ],
          "1": "To solve this problem, PVNet [118] adopts the strategy of voting-based keypoint localization.",
          "6": "Beyond leveraging image-level consistency for self-supervised learning, inspired by recent keypoint-based methods [118, 137], DSC-PoseNet [168] develops a weakly supervised and a self-supervised learning-based pose estimation framework that enforces dual-scale keypoint consistency without using pose annotations."
        },
        "Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation": {
          "authors": [
            "Hansheng Chen",
            "Pichao Wang",
            "Fan Wang",
            "Wei Tian",
            "Lu Xiong",
            "Hao Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 4, 7",
          "ref_ids": [
            "31"
          ],
          "3": "Comparison to the State of the Art As shown in Table 2, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE [20], which adds extra overhead to the PnP-based initial estimator PVNet [31]."
        },
        "Onepose: One-shot object pose estimation without cad models": {
          "authors": [
            "Jiaming Sun",
            "Zihao Wang",
            "Siyu Zhang",
            "Xingyi He",
            "Hongcheng Zhao",
            "Guofeng Zhang",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "26"
          ],
          "1": "Most established works in object pose estimation [16, 26, 46] assume that the CAD model of the object is known a priori.",
          "2": "In contrast, the latter type of methods first find correspondences between image pixels and 3D object coordinates either by regression [22, 24, 25] or by voting [26, 27], and then compute the pose with Perspective-n-Points (PnP).",
          "3": "2) Instance-level method PVNet [26, 27].",
          "4": "Our method is compared with PVNet [26] on selected objects from the OnePose dataset with the 5cm-5deg metric.",
          "5": "The proposed method is compared with PVNet [26] with 5cm-5deg on selected objects from our OnePose dataset and the results are as presented in Tab."
        },
        "Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation": {
          "authors": [
            "Yongzhi Su",
            "Mahdi Saleh",
            "Torben Fetzer",
            "Jason Rambach",
            "Nassir Navab",
            "Benjamin Busam",
            "Didier Stricker",
            "Federico Tombari"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.html",
          "ref_texts": "[49] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "49"
          ],
          "2": "BB8 [52] firstly defines the 3D object bounding box corners as the keypoints and PVNet [49] reaches high recall rate in LM [27] dataset by predicting the keypoints with a dense pixel-wise voting for sampled keypoints on the object."
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
          "authors": [
            "X He",
            "J Sun",
            "Y Wang",
            "D Huang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/e43f900f571de6c96a70d5724a0fb565-Abstract-Conference.html",
          "ref_texts": "[39] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: pixel-wise voting network for 6dof object pose estimation. T-PAMI, 2020. 1, 2, 3, 8, 9",
          "ref_ids": [
            "39"
          ],
          "4": "Our method is compared with PVNet[39] on objects with CAD models in the OnePose-LowTexture dataset using the ADD(S)-0.",
          "6": "For the comparison with PVNet [39], we follow its original training setting, which first samples 8 keypoints on the object surface and then trains a network using 5000 synthetic images for each object.",
          "7": "On the OnePose-LowTexture dataset, the proposed method is compared with PVNet [39] on the subset objects with scanned models.",
          "8": "4 Results on LINEMOD We compare the proposed method with OnePose [48] and Gen6D [33] which are under the One-shot setting, and Instance-level methods PVNet [39] and CDPN [29] on ADD(S)-0.",
          "9": "Our method has lower or comparable performance with instance-level methods [39, 29], which are trained to fit each object instance, and thus perform well naturally, at the expense of the tedious training for each object."
        },
        "Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images": {
          "authors": [
            "Y Liu",
            "Y Wen",
            "S Peng",
            "C Lin",
            "X Long",
            "T Komura"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_18",
          "ref_texts": "42. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019) Gen6D Pose Estimator 17",
          "ref_ids": [
            "42"
          ],
          "2": "Experiments show that without training on these objects, our method still outperforms instance-specific estimator PVNet [42] on the GenMOP dataset and another model-free MOPED [41] dataset.",
          "4": "3 Results on GenMOP For comparison, we choose the generalizable image-matching based ObjDesc [69] and two instance-specific estimators PVNet [42] and RLLG [6] as baseline methods.",
          "11": "3) However, Gen6D performs worse than instance-specific estimators [42,70,74] with real training.",
          "12": "4) With ground-truth bounding box, Gen6D achieves comparable results as the instance-specific estimators [42,70,74] with real training because such ground-truth bounding boxes provide correct depths.",
          "13": "5 Results on MOPED [41] On the MOPED dataset, we compare Gen6D with Latent-Fusion [41] and PVNet [42]."
        },
        "Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings": {
          "authors": [
            "Rasmus Laurvig",
            "Anders Glent"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Haugaard_SurfEmb_Dense_and_Continuous_Correspondence_Distributions_for_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "26"
          ],
          "4": "PVNet [26] regresses vector fields toward the 2D projections of a set of fixed 3D key points and handles symmetries like BB8."
        },
        "Osop: A multi-stage one shot object pose estimation framework": {
          "authors": [
            "Ivan Shugurov",
            "Fu Li",
            "Benjamin Busam",
            "Slobodan Ilic"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Shugurov_OSOP_A_Multi-Stage_One_Shot_Object_Pose_Estimation_Framework_CVPR_2022_paper.html",
          "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "37"
          ],
          "1": "According to the BOP challenge [14], which combines publicly available 6 DoF pose estimation datasets and offers standardized evaluation and comparison procedures, the field is dominated by deep learning methods [2, 12, 16, 19, 21, 22, 22, 24\u201327, 36, 37, 47, 49\u201352, 59].",
          "2": "In particular, IPose [16], YOLO6D [53], PVNet [37], HybridPose [50] and [19] predict a sparse set of the pre-defined keypoints."
        },
        "Category-level 6d object pose estimation in the wild: A semi-supervised learning approach and a new dataset": {
          "authors": [
            "Y Fu",
            "X Wang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/afe99e55be23b3523818da1fefa33494-Abstract-Conference.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "34"
          ],
          "1": "One is performing instance-level 6D pose estimation, where a model is trained to estimate the pose of one exact instance with an existing 3D model [13, 34, 22, 50, 32, 5, 14]."
        },
        "Rnnpose: Recurrent 6-dof object pose refinement with robust correspondence field estimation and pose optimization": {
          "authors": [
            "Yan Xu",
            "Yee Lin",
            "Guofeng Zhang",
            "Xiaogang Wang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "34"
          ],
          "1": "These methods may estimate the object\u2019s bounding box corners [35, 45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
          "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit (estimated by any direct methods [34,52]).",
          "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
          "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
          "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
          "6": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
          "7": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [58], DeepIM [23] and RePOSE [20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
          "8": "ObjectPoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51]DPOD [58] RePOSE [20]OursApe 9.",
          "9": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [58] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
          "10": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Shapo: Implicit representations for multi-object shape, appearance, and pose optimization": {
          "authors": [
            "MZ Irshad",
            "S Zakharov",
            "R Ambrus",
            "T Kollar"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_16",
          "ref_texts": "41. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "41"
          ],
          "1": "Such methods [41,52,55], while achieving impressive results, rely on provided 3D reconstructions or prior CAD models for successful detection and pose estimation."
        },
        "Sar-net: Shape alignment and recovery network for category-level 6d object pose and size estimation": {
          "authors": [
            "Haitao Lin",
            "Zichang Liu",
            "Chilam Cheang",
            "Yanwei Fu",
            "Guodong Guo",
            "Xiangyang Xue"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Lin_SAR-Net_Shape_Alignment_and_Recovery_Network_for_Category-Level_6D_Object_CVPR_2022_paper.html",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 3, 4, 5, 6, 7",
          "ref_ids": [
            "39"
          ],
          "7": "Compared with RGB(-D) methods [15, 39] or depth-only method [12, 13], our SAR-Net achieves comparable results in terms of ADD(-S) metric as in Tab."
        },
        "Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation": {
          "authors": [
            "Dingding Cai",
            "Janne Heikkila",
            "Esa Rahtu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cai_OVE6D_Object_Viewpoint_Encoding_for_Depth-Based_6D_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 1, 2, 6, 7, 8",
          "ref_ids": [
            "36"
          ],
          "5": "2 PVNet [36] RGBD \u2713 79."
        },
        "Tracking objects as pixel-wise distributions": {
          "authors": [
            "Z Zhao",
            "Z Wu",
            "Y Zhuang",
            "B Li",
            "J Jia"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_5",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "44"
          ],
          "2": "Dense fusion [57] and pixel-wise voting network [44,18] are proposed to overcome occlusions in the object pose estimation [19]."
        },
        "Ifor: Iterative flow minimization for robotic object rearrangement": {
          "authors": [
            "Ankit Goyal",
            "Arsalan Mousavian",
            "Chris Paxton",
            "Wei Chao",
            "Brian Okorn",
            "Jia Deng",
            "Dieter Fox"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Goyal_IFOR_Iterative_Flow_Minimization_for_Robotic_Object_Rearrangement_CVPR_2022_paper.html",
          "ref_texts": "[50] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "50"
          ],
          "1": "This makes explicit object pose estimation [33, 35, 50, 61, 66] a necessary part of the pipeline, and the full system susceptible to pose estimation error from real vision systems.",
          "2": ", detecting and segmenting objects [4, 6, 22, 38, 70] and estimating their 6D poses [33, 35, 50, 61, 66]."
        },
        "Uni6d: A unified cnn framework without projection breakdown for 6d pose estimation": {
          "authors": [
            "Xiaoke Jiang",
            "Donghai Li",
            "Hao Chen",
            "Ye Zheng",
            "Rui Zhao",
            "Liwei Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Jiang_Uni6D_A_Unified_CNN_Framework_Without_Projection_Breakdown_for_6D_CVPR_2022_paper.html",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "We follow previous work [30,53] to split the training and testing sets, and we also obtain synthesis images for the training set as the same with [6, 53].",
          "2": "For LineMOD dataset, we follow [16,30] to report the accuracy of distance less than 10% of the objects\u2019 diameter (ADD-0."
        },
        "Catre: Iterative point clouds alignment for category-level object pose refinement": {
          "authors": [
            "X Liu",
            "G Wang",
            "Y Li",
            "X Ji"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_29",
          "ref_texts": "38. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "38"
          ],
          "1": "The vast majority of previous works [30,64,45,62,53,54,31,52,38] study with instance-level object pose estimation, which can be decomposed by two procedures: initial pose estimation and pose refinement."
        },
        "Clearpose: Large-scale transparent object dataset and benchmark": {
          "authors": [
            "X Chen",
            "H Zhang",
            "Z Yu",
            "A Opipari"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20074-8_22",
          "ref_texts": "14. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "14"
          ],
          "1": "StereObj1M [12] benchmarked KeyPose and another RGB-based object pose estimator, PVNet [14], on more challenging objects and scenes, where both methods achieved lower accuracy with respect to the ADD-S AUC metric (introduced in [19]) with both monocular and stereo input."
        },
        "Perspective flow aggregation for data-limited 6d object pose estimation": {
          "authors": [
            "Y Hu",
            "P Fua",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_6",
          "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition (2019)",
          "ref_ids": [
            "32"
          ],
          "1": "When ample amounts of annotated real images are available, deep learning-based methods now deliver excellent results [7,32,31,46,38].",
          "2": "2 Related Work 6D pose estimation is currently dominated by neural network-based methods [11,32,38,37,14,2].",
          "4": "Most methods train their models for LINEMOD and Occluded-LINEMOD separately [11,26], sometimes even one model per object [32,47], which yields better accuracy but is less flexible and does not scale well.",
          "6": "1 Comparison with the State of the Art We now compare our method to the state-of-the-art ones, PoseCNN [49], SegDriven [11], PVNet [32], GDR-Net [47], DeepIM [25], and CosyPose [23], where DeepIM and CosyPose are two refinement methods based on an iterative strategy."
        },
        "DGECN: A depth-guided edge convolutional network for end-to-end 6D pose estimation": {
          "authors": [
            "Tuo Cao",
            "Fei Luo",
            "Yanping Fu",
            "Wenxiao Zhang",
            "Shengjie Zheng",
            "Chunxia Xiao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cao_DGECN_A_Depth-Guided_Edge_Convolutional_Network_for_End-to-End_6D_Pose_CVPR_2022_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "28"
          ],
          "3": "PVNet [28] and Seg-Driven [17] conducted segmentation coupled with voting for each correspondence to make the estimation more robust.",
          "4": "Afterwards, like GDR-Net [42] and PVNet [28], we locate each object in the image with the method of FCN [24].",
          "6": "The 3D keypoints are selected from the 3D object model as in [14, 28].",
          "7": "We follow [28] and adopt the farthest point sampling (FPS) algorithm to select keypoints on object surface."
        },
        "Refine-net: Normal refinement neural network for noisy point clouds": {
          "authors": [
            "H Zhou",
            "H Chen",
            "Y Zhang",
            "M Wei",
            "H Xie"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9693131/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "2"
          ],
          "1": "As standard outputs of these 3D sensors, point clouds have been flexibly used in various applications, ranging from 6-degree virtual reality [1], [2], robotics [3] to autonomous driving [4], [5]."
        },
        "Ssp-pose: Symmetry-aware shape prior deformation for direct category-level object pose estimation": {
          "authors": [
            "R Zhang",
            "Y Di",
            "F Manhardt"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981506/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "As for RGBonly methods, end-to-end methods [18], [19], [20], [21], [22], [23], [6] regress the pose parameters directly, while two-stage methods [24], [25], [26], [27], [28] first establish 2D-3D correspondences by predicting the 3D coordinate for each pixel or detecting pre-defined keypoints, and then utilize PnP/RANSAC algorithm to solve the pose from intermediate results."
        },
        "6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning": {
          "authors": [
            "L Zou",
            "Z Huang",
            "N Gu",
            "G Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9933183/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "7"
          ],
          "1": "[7] presented the prediction of a unit vector for each pixel pointing toward the keypoints."
        },
        "UDA-COPE: Unsupervised domain adaptation for category-level object pose estimation": {
          "authors": [
            "Taeyeop Lee",
            "Uk Lee",
            "Inkyu Shin",
            "Jaesung Choe",
            "Ukcheol Shin",
            "In So",
            "Jin Yoon"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Lee_UDA-COPE_Unsupervised_Domain_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "27"
          ],
          "1": "Previous 6D object pose estimation methods follow the instance-level pose estimation schemes [12, 13, 25, 27, 31, 34, 38] that rely on given 3D CAD model information (e.g., [12, 13, 25, 27, 31, 34, 38])."
        },
        "Semantic segmentation of outdoor panoramic images": {
          "authors": [
            "Semih Orhan"
          ],
          "url": "https://link.springer.com/article/10.1007/s11760-021-02003-3",
          "ref_texts": "19. Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019) Network for 6dof Pose Estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "19"
          ],
          "1": "Many computer vision applications benefit from it, such as pedestrian detection [6,16], autonomous vehicles [22,26], pose estimation [19,27] and remote sensing [13,24]."
        },
        "Es6d: A computation efficient and symmetry-aware 6d pose regression framework": {
          "authors": [
            "Ningkai Mo",
            "Wanshui Gan",
            "Naoto Yokoya",
            "Shifeng Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Mo_ES6D_A_Computation_Efficient_and_Symmetry-Aware_6D_Pose_Regression_Framework_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1",
          "ref_ids": [
            "26"
          ],
          "1": "In recent years, methods based on the deep neural network (DNN) have gradually emerged [17, 22, 25, 26, 40]."
        },
        "Vote from the center: 6 dof pose estimation in rgb-d images by radial keypoint voting": {
          "authors": [
            "Y Wu",
            "M Zand",
            "A Etemad",
            "M Greenspan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20080-9_20",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "36"
          ],
          "3": "As an alternate to directly regressing keypoint coordinates, methods which vote for keypoints have been shown to be highly effective [36,49,18,37], especially when objects are partially occluded.",
          "4": "While recent voting methods have shown great promise and leading performance, they require the regression of either a 2-channel (for 2D voting) [36] or 3-channel (for 3D voting ) [14] activation map where voting quantities are accumulated in order to vote for keypoints.",
          "5": "Notably, RCVPose requires only 3 keypoints per object, which is fewer than existing methods that use 4 or more keypoints [36,14,37].",
          "9": "Specifically, our method is inspired by PVNet [36], and is most closely related to the recently proposed PVN3D of He et al.",
          "12": "This is analogous to the approach of [14], and is efficient compared to previous pure RGB approaches [36] which employ an iterative PnP method.",
          "16": "5 Keypoint Dispersion Impact on Transformation Estimation: It was suggested in [36] that 6 DoF pose estimation accuracy is improved by selecting keypoints that lie on the object surface, rather than the bounding box corners which lie just beyond the object surface."
        },
        "Neural correspondence field for object pose estimation": {
          "authors": [
            "L Huang",
            "T Hodan",
            "L Ma",
            "L Zhang",
            "L Tran"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20080-9_34",
          "ref_texts": "61. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. CVPR (2019) 2, 3",
          "ref_ids": [
            "61"
          ],
          "2": ", by predicting the 2D projections of a fixed set of 3D keypoints pre-selected for each object model, have also been proposed [63, 60, 54, 73, 76, 27, 61]."
        },
        "YOLOPose: Transformer-based multi-object 6D pose estimation using keypoint regression": {
          "authors": [
            "A Amini",
            "A Selvam Periyasamy",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-22216-0_27",
          "ref_texts": "[21] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "21"
          ],
          "1": "Some examples for direct regression methods include [1, 22, 29, 30] and examples for keypoint-based methods include [8, 9, 21, 24, 28].",
          "3": "Method PoseCNN [30]PVNet [21]GDR-Net [29]T6D-Direct [1]YOLOPose (Ours)DeepIM [15] P."
        },
        "Robust category-level 6d pose estimation with coarse-to-fine rendering of neural features": {
          "authors": [
            "W Ma",
            "A Wang",
            "A Yuille",
            "A Kortylewski"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_29",
          "ref_texts": "23. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)",
          "ref_ids": [
            "23"
          ],
          "1": "However, most prior work on 6D pose estimation focused on the \u201cinstance-level\u201d task, where exact CAD models of the object instances are available [35,23,19,10,12]."
        },
        "Sim-to-real 6d object pose estimation via iterative self-training for robotic bin picking": {
          "authors": [
            "K Chen",
            "R Cao",
            "S James",
            "Y Li",
            "YH Liu",
            "P Abbeel"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19842-7_31",
          "ref_texts": "37. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. TPAMI (2020) 1, 3",
          "ref_ids": [
            "37"
          ],
          "1": "Recently, learning-based models [22,37,44,51] that arXiv:2204.",
          "2": "1 6D Object Pose Estimation for Bin-Picking Though recent works [10,26,37,40] show superior performance on household object datasets (e."
        },
        "Focal length and object pose estimation via render and compare": {
          "authors": [
            "Georgy Ponimatkin",
            "Yann Labbe",
            "Bryan Russell",
            "Mathieu Aubry",
            "Josef Sivic"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ponimatkin_Focal_Length_and_Object_Pose_Estimation_via_Render_and_Compare_CVPR_2022_paper.html",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. InCVPR, pages 4561\u20134570, 2019.2",
          "ref_ids": [
            "34"
          ],
          "1": "Previous approaches for this task primarily rely on establishing local 2D-3D correspondences between an image 3825 and a 3D model using either hand-crafted [2, 3, 7, 8, 17, 27] or CNN features [12,19,20,31,32,34,35,38,41,42,47,48], followed by robust camera pose estimation using PnP [23].",
          "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs) [19,20,31,32,34,35,38,41,42,47,48]."
        },
        "Polarimetric pose prediction": {
          "authors": [
            "D Gao",
            "Y Li",
            "P Ruhkamp",
            "I Skobleva",
            "M Wysocki"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_43",
          "ref_texts": "41. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "41"
          ],
          "1": "Some methods in this field use sparse correspondences [43,41,47,25], while others establish dense 2D-3D pairs [57,40,36,22]."
        },
        "Object level depth reconstruction for category level 6d object pose estimation from monocular rgb image": {
          "authors": [
            "Z Fan",
            "Z Song",
            "J Xu",
            "Z Wang",
            "K Wu",
            "H Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_13",
          "ref_texts": "19. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "19"
          ],
          "1": "To improve keypoint detection performance, PVNet [19] formulates a voting scheme, which is more robust towards occlusion and truncation."
        },
        "Sc6d: Symmetry-agnostic and correspondence-free 6d object pose estimation": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044457/",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "32"
          ],
          "2": "proposed an occlusion-robust approach PVNet [32], which predicts the pixel-wise voting vectors to localize the keypoints defined based on the object 3D model instead of on the 3D bounding box."
        },
        "Learning-based point cloud registration for 6d object pose estimation in the real world": {
          "authors": [
            "Z Dang",
            "L Wang",
            "Y Guo",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_2",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570. Long Beach, California (2019) 1",
          "ref_ids": [
            "44"
          ],
          "1": "In this context, great progress has been made by learning-based methods operating on RGB(D) images [32,48,44,58,42,74,62,36,61,57]."
        },
        "Trans6D: Transformer-based 6D object pose estimation and refinement": {
          "authors": [
            "Z Zhang",
            "W Chen",
            "L Zheng",
            "A Leonardis"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25085-9_7",
          "ref_texts": "31. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: Pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 1\u20131 (2020). https://doi.org/10.1109/TPAMI.2020.3047388",
          "ref_ids": [
            "31"
          ],
          "3": "For example, Pixel2Pose [30] used an auto-encoder architecture to estimate the 3D coordinates per pixel to build dense correspondences, while PVNet [31], PVN3D [15], and PointPoseNet [7] adopted a voting net to select 2D keypoint or 3D keypoint respectively to build dense correspondences.",
          "4": "Compared approaches: PVNet [31], DPOD [43], DeepIm [25] PVNet +DPOD +DeepIm +Trans6D+ 85.",
          "5": "Baseline approaches: BB8 [32], Pix2Pose [30], DPOD [43], PVNet [31], CDPN [26], Hybrid [35], GDRN [40].",
          "6": "Baseline approaches: PoseCNN [41], Pix2Pose [30], DPOD [43], PVNet [31], Single-Stage [19], HybridPose [35], GDRN [40].",
          "7": "[36], PVNet [31], Singel-Stage [20], GDR-Net [40] Methods PoseCNNDeepIMPVNetSingle-StageGDR-NetAmeni et al.",
          "8": "Table 5 compares our method with other state-of-the-art methods [31,40] on Occlusion LINEMOD dataset in terms of ADD metric."
        },
        "Pixel2mesh++: 3d mesh generation and refinement from multi-view images": {
          "authors": [
            "C Wen",
            "Y Zhang",
            "C Cao",
            "Z Li",
            "X Xue"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9763061/",
          "ref_texts": "[68] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "68"
          ],
          "1": "[68] regard camera pose estimation as a key point regression problem, and use voting strategy to densely estimate the key point offset.",
          "2": "We use four types of standard metrics to evaluate our camera pose estimation method: 2D reprojection error d2D, mean distance d3D [4], 2D reprojection accuracy Acc2D and average 3D distance of model points (ADD) accuracy metric ADD3D [68]."
        },
        "Occlusion-robust object pose estimation with holistic representation": {
          "authors": [
            "Bo Chen",
            "Jun Chin",
            "Marius Klimavicius"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2022/html/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.html",
          "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 5, 7, 8",
          "ref_ids": [
            "44"
          ],
          "3": "PVNet [44] predicts the object mask and, for each pixel within the mask, unit vectors that points to the landmarks.",
          "5": "For the YCB-Video dataset we also report the AUC metric proposed in [62] and adopted in [41, 44].",
          "10": "For example, PVNet [44] renders 20000 images for each object and the same strategy is adopted in [52]."
        },
        "A visual navigation perspective for category-level object pose estimation": {
          "authors": [
            "J Guo",
            "F Zhong",
            "R Xiong",
            "Y Liu",
            "Y Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_8",
          "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019) 3",
          "ref_ids": [
            "40"
          ],
          "1": "2 Related Work Object Pose Estimation: Extensive studies have been conducted for object pose estimation of known instances [10, 12, 19, 22, 26\u201328, 35, 39, 40, 53]."
        },
        "Dcl-net: Deep correspondence learning network for 6d pose estimation": {
          "authors": [
            "H Li",
            "J Lin",
            "K Jia"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_22",
          "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019) 1, 3, 14 DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation 17",
          "ref_ids": [
            "31"
          ],
          "1": "Many of the data-driven methods [3,14,20,23,28,31,33,34,38,41] thus achieve the estimation by learning point correspondence between camera and object coordinate systems.",
          "2": "2 Related Work 6D Pose Estimation from RGB Data This body of works can be broadly categorized into three types: i) holistic methods [11,15,18] for directly estimating object poses; ii) keypoint-based methods [28,33,34], which establish 2D-3D correspondence via 2D keypoint detection, followed by a PnP/RANSAC algorithm to solve the poses; iii) dense correspondence methods [3, 20, 23, 31], which make dense pixel-wise predictions and vote for the final results.",
          "3": "PoseCNN [45] DeepHeat [29] SS [17] Pix2pose [30] PVNet [31] HybridPose [35] PVN3D [14] PR-GCN [47] FFB6D [13] DCL-Net ape 9."
        },
        "Fusing local similarities for retrieval-based 3d orientation estimation of unseen objects": {
          "authors": [
            "C Zhao",
            "Y Hu",
            "M Salzmann"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_7",
          "ref_texts": "24. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "24"
          ],
          "1": "Motivated by the tremendous success of deep learning, much effort [36,24,32] has been dedicated to developing deep networks able to recognize the objects depicted in the input image and estimate their 3D orientation.",
          "2": "PVNet [24] estimates the 2D projections of 3D points using a voting network."
        },
        "A survey of 6d object detection based on 3d models for industrial applications": {
          "authors": [
            "F Gorschl\u00fcter",
            "P Rojtberg",
            "T P\u00f6llabauer"
          ],
          "url": "https://www.mdpi.com/2313-433X/8/3/53",
          "ref_texts": "49. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNET: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; IEEE Computer Society: Washington, DC, USA, 2019; pp. 4556\u20134565. [CrossRef]",
          "ref_ids": [
            "49"
          ],
          "1": "[49] 2019 RGB Learned Local Cont.",
          "2": "3 [33] PVNet [49] (ICP) RGBD 50.",
          "3": "2 [53] PVNet [49] RGB 42."
        },
        "Photo-realistic neural domain randomization": {
          "authors": [
            "S Zakharov",
            "R Ambru\u0219",
            "V Guizilini",
            "W Kehl"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19806-9_18",
          "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "48"
          ],
          "1": "Correspondence-based methods [69,37,27,23,46,48] tend to show superior generalization performance in terms of adapting to different pose distributions."
        },
        "Unseen object 6D pose estimation: a benchmark and baselines": {
          "authors": [
            "M Gou",
            "H Pan",
            "HS Fang",
            "Z Liu",
            "C Lu",
            "P Tan"
          ],
          "url": "https://arxiv.org/abs/2206.11808",
          "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "41"
          ],
          "1": "Recently, deep learning methods based on 2D image [31, 41, 44, 56] or 3D point cloud [21, 22, 54] are proposed to tackle this problem and yield better performances, benefiting from the powerful feature extraction ability of neural network.",
          "2": "Prior knowledge of object models such as keypoint location [22,41] or voting offsets [22,41] is also encoded by the networks."
        },
        "Spatial feature mapping for 6dof object pose estimation": {
          "authors": [
            "Jianhan Mei",
            "Xudong Jiang",
            "Henghui Ding"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0031320322003168",
          "ref_texts": "[25] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wise voting network for 6dof pose estimation, in: IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp.",
          "ref_ids": [
            "25"
          ],
          "3": "[6, 25] learn the keypoints of 2D to 3D matching through the instance segmentation framework to enhance the description of the object pose, which brings the system performance to a new level.",
          "6": "Methods only based on RGB image (DeepIM [57], PVNet [25], CDPN [27]) and methods using the depth information (Point-Fusion [9], DF (perpixel) [9], DF (iterative) [9], PVN3D [28]) are compared.",
          "7": "So, the 24 proposed method still achieves comparable results with the state-of-the-art three methods [57, 25, 27]."
        },
        "Stability-driven contact reconstruction from monocular color images": {
          "authors": [
            "Zimeng Zhao",
            "Binghui Zuo",
            "Wei Xie",
            "Yangang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Stability-Driven_Contact_Reconstruction_From_Monocular_Color_Images_CVPR_2022_paper.html",
          "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "43"
          ],
          "1": "With the rapid increase of 3D hand datasets [16,33,61,64,68] and object datasets [23, 33, 61], data-driven methods [2, 15, 25, 26, 30, 37, 43, 54, 60, 63, 66, 67] become popular in the community."
        },
        "Bcot: A markerless high-precision 3d object tracking benchmark": {
          "authors": [
            "Jiachen Li",
            "Bin Wang",
            "Shiqiang Zhu",
            "Xin Cao",
            "Fan Zhong",
            "Wenxuan Chen",
            "Te Li",
            "Jason Gu",
            "Xueying Qin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_BCOT_A_Markerless_High-Precision_3D_Object_Tracking_Benchmark_CVPR_2022_paper.html",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 1",
          "ref_ids": [
            "32"
          ],
          "1": "Despite the rapid development of single-frame 6DOF pose estimation methods [32, 41], for video analysis 3D tracking can be more accurate and more efficient, and thus is indispensable."
        },
        "Large-displacement 3D object tracking with hybrid non-local optimization": {
          "authors": [
            "X Tian",
            "X Lin",
            "F Zhong",
            "X Qin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_36",
          "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: IEEE/CVF Conference on CVPR. pp. 4556\u20134565. IEEE, Long Beach, CA, USA (Jun 2019). https://doi.org/10.1109/CVPR.2019.00469",
          "ref_ids": [
            "15"
          ],
          "1": "This approach actually bridges 3D tracking with detection-based 6D pose estimation [11, 15, 27]."
        },
        "A 6D pose estimation for robotic bin-picking using point-pair features with curvature (Cur-PPF)": {
          "authors": [
            "Xining Cui",
            "Menghui Yu",
            "Linqigao Wu",
            "Shiqian Wu"
          ],
          "url": "https://www.mdpi.com/1424-8220/22/5/1805",
          "ref_texts": "16. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNET: Pixel-Wise Voting Network for 6dof Pose Estimation. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Los Angeles, CA, USA, 15\u201321 June 2019. Sensors 2022, 22, 1805 20 of 20",
          "ref_ids": [
            "16"
          ],
          "1": "[16] proposed to use Pixel-wise Voting Network (PVNet) to return unit vectors to key points, then used RANdom SAmple Consensus (RANSAC) to vote for key points, and finally used PnP algorithm to derive accurate poses."
        },
        "HMD-EgoPose: Head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance": {
          "authors": [
            "M Doughty",
            "NR Ghugre"
          ],
          "url": "https://link.springer.com/article/10.1007/s11548-022-02688-y",
          "ref_texts": "[18] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "18"
          ],
          "1": "have presented several strategies for surgical drill and hand pose estimation from monocular RGB data for the synthetic drill dataset based off of the PVNet [18] and HandObjectNet [26] frameworks."
        },
        "Sequential voting with relational box fields for active object detection": {
          "authors": [
            "Qichen Fu",
            "Xingyu Liu",
            "Kris Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Fu_Sequential_Voting_With_Relational_Box_Fields_for_Active_Object_Detection_CVPR_2022_paper.html",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "26"
          ],
          "2": "[26, 36] use pixel-wise predictions with Hough voting to localize keypoint for pose estimation."
        },
        "Video based object 6D pose estimation using transformers": {
          "authors": [
            "A Beedu",
            "H Alamri",
            "I Essa"
          ],
          "url": "https://arxiv.org/abs/2210.13540",
          "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "40"
          ],
          "3": "They have enabled the emergence of novel network designs such as PoseCNN [56], DPOD [59], PVNet [40], and others [8, 52, 16, 10]."
        },
        "Template-based category-agnostic instance detection for robotic manipulation": {
          "authors": [
            "Z Hu",
            "R Tan",
            "Y Zhou",
            "J Woon"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9935113/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "[21] introduced a pixel-wise voting network to enhance the representation of the key points to optimize the matching performance."
        },
        "A dynamic keypoint selection network for 6dof pose estimation": {
          "authors": [
            "H Sun",
            "T Wang",
            "E Yu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885622000014",
          "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570). ",
          "ref_ids": [
            "25"
          ],
          "1": "Recently, the explosive growth of deep learning techniques motivates several works to tackle this problem by convolution neural networks (CNNs) on RGB images [19, 25, 40] and reveal promising improvements."
        },
        "6d robotic assembly based on rgb-only object pose estimation": {
          "authors": [
            "B Fu",
            "SK Leong",
            "X Lian",
            "X Ji"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9982262/",
          "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "17"
          ],
          "1": "To enhance the robustness, SegDriven [16] and PVNet [17] employ segmentation paired with voting for each correspondence."
        },
        "Sim2real instance-level style transfer for 6d pose estimation": {
          "authors": [
            "T Ikeda",
            "S Tanishige",
            "A Amma"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981878/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "25"
          ],
          "2": "Training We selected PVNet [25] as a PoseNet for our experiments."
        },
        "Parapose: Parameter and domain randomization optimization for pose estimation using synthetic data": {
          "authors": [
            "F Hagelskj\u00e6r",
            "AG Buch"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981511/",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u2013",
          "ref_ids": [
            "27"
          ],
          "2": "In PVNet [27] 10000 images are rendered and cut and pasted onto images from the SUN397 [37] dataset.",
          "3": "PVN3D [16] use the training data and domain randomization from PVNet [27], but expand with 3D pointclouds."
        },
        "Dfbvs: Deep feature-based visual servo": {
          "authors": [
            "N Adrian",
            "VT Do",
            "QC Pham"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9926560/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "[24], [25] are some of the best performing methods which score highly on the LINEMOD dataset [26].",
          "2": "Here, we use accuracy in the image space as the accuracy metric as inspired from pose estimation field [24], [25]."
        },
        "Canonical voting: Towards robust oriented bounding box detection in 3d scenes": {
          "authors": [
            "Yang You",
            "Zelin Ye",
            "Yujing Lou",
            "Chengkun Li",
            "Lu Li",
            "Lizhuang Ma",
            "Weiming Wang",
            "Cewu Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/You_Canonical_Voting_Towards_Robust_Oriented_Bounding_Box_Detection_in_3D_CVPR_2022_paper.html",
          "ref_texts": "[14] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "PVNet [14] regresses pixel-wise unit vectors pointing to the predefined keypoints and solves a Perspective-n-Point (PnP) problem for pose estimation in RGB images."
        },
        "Dprost: Dynamic projective spatial transformer network for 6d pose estimation": {
          "authors": [
            "J Park",
            "NI Cho"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_21",
          "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "31"
          ],
          "1": "Therefore, researchers have proposed methods for applying deep learning to the object pose estimation problem with great performance [2,8,14,15,19,21,22,26,28,31,32,37,40,41].",
          "5": "Method PoseCNN [42] DeepIM [21] PVNet [31] S.",
          "6": "Method PoseCNN [42] PVNet [31] DeepIM [21] Cosypose [19] GDR-Net [40] SO-Pose [8] RePOSE [16] DProST DProST A."
        },
        "Welsa: Learning to predict 6d pose from weakly labeled data using shape alignment": {
          "authors": [
            "SR Vutukur",
            "I Shugurov",
            "B Busam",
            "A Hutter"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20074-8_37",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "28"
          ],
          "1": "Alternatively, [33,28,16] estimate a predefined set of sparse keypoints instead of dense correspondences, which has proven to be more robust to occlusions."
        },
        "Reflective texture-less object registration using multiple edge features for augmented reality assembly": {
          "authors": [
            "Z He",
            "J Zhao",
            "X Zhao",
            "W Feng",
            "Q Wang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00170-022-10333-w",
          "ref_texts": "23. S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\" 2018.",
          "ref_ids": [
            "23"
          ],
          "1": "PVNet [23] predicted the direction from each pixel to each key point, so the spatial probability distribution of two-dimensional key points can be obtained just like RANSAC."
        },
        "Casapose: Class-adaptive and semantic-aware multi-object pose estimation": {
          "authors": [
            "N Gard",
            "A Hilsmann",
            "P Eisert"
          ],
          "url": "https://arxiv.org/abs/2210.05318",
          "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR, 2019.",
          "ref_ids": [
            "30"
          ],
          "6": "The first estimates a segmentation mask that guides the second in estimating vectors pointing to 2D projections of predefined 3D object keypoints [30].",
          "7": "We use this property to avoid the non-differentiable RANSAC estimation, commonly used to get 2D points from vector fields [30].",
          "12": "It applies DKR on the largest connected component of each object class and clearly outperforms RANSAC voting (PVRANSAC) [30] used with the same trained model."
        },
        "Shape Enhanced Keypoints Learning with Geometric Prior for 6D Object Pose Tracking": {
          "authors": [
            "Mateusz Majcher",
            "Bogdan Kwolek"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.html",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR, pages 4556\u20134565, 2019. 2, 3",
          "ref_ids": [
            "20"
          ],
          "1": "To mitigate this effect, many top performing two-stage approaches are either based on pixel-vise voting [20] or on generating an ensemble of predictions from each image pixel or patch [17], and then aggregating them to improve final predictions.",
          "2": "Relevant Work Top-performing methods on existing benchmarks, rather than directly regressing the object pose, are based on twostage approaches [12, 16, 17, 20, 21, 24, 25, 31, 33], which first predict landmarks of the object (intermediate features) with established 2D-3D correspondences, and then utilize a PnP like algorithm to determine the pose.",
          "3": "To better cope with occluded objects, [20] proposed a neural network for pixel-wise voting for the 2D keypoints location.",
          "4": "While [21, 25] utilize bounding box corners as keypoints, more recent approaches [20] use designated surface keypoints."
        },
        "Towards two-view 6D object pose estimation: A comparative study on fusion strategy": {
          "authors": [
            "J Wu",
            "L Liu",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981630/",
          "ref_texts": "[18] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "To use more reliable correspondence, PVNet [18] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints then vote with confidence.",
          "10": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces, bizarre viewing angle, and some normal situations.",
          "11": "Also, our method beats both [18] and RGBD-based method [6] in occluded occasions.",
          "13": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces (e), bizarre viewing angle (c), and some normal situations (a)(b)(d).",
          "14": "Also, our methods beats [18] and RGBD-based method [6] in occluded occasions (f)(g)."
        },
        "Adversarial samples for deep monocular 6d object pose estimation": {
          "authors": [
            "J Zhang",
            "W Li",
            "S Liang",
            "H Wang",
            "J Zhu"
          ],
          "url": "https://arxiv.org/abs/2203.00302",
          "ref_texts": "33. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "33"
          ],
          "1": "In particular, recent deep learning based methods show that even using only a single RGB image without any additional depth data, 6D object pose can be predicted with high accuracy on large-scale public benchmarks [33,44,25].",
          "4": "PVNet [33] is built upon the idea of voting-based key-point localization.",
          "7": "Three SOTA models are selected as the target model, which are the GDRNet [44], PVNet [33], CDPN [25].",
          "9": "4 Transferability Results In this section, we perform U6DA to three mainstream RGB based deep 6D pose estimation networks, which are the direct methods GDRNet [44], the key-point based methods PVNet [33], the dense coordinate based methods CDPN [25]."
        },
        "Review on 6d object pose estimation with the focus on indoor scene understanding": {
          "authors": [
            "N Nejatishahidin",
            "P Fayyazsanavi"
          ],
          "url": "https://arxiv.org/abs/2212.01920",
          "ref_texts": "[58] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019.",
          "ref_ids": [
            "58"
          ],
          "2": "To address the occlusion problem for keypoint detection, PVNet [58] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
          "3": "3DPVNet [44], inspired from V oteNet [12] and pvnet [58], employed deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation.",
          "4": "The promise of votingbased techniques [58, 44] are being robust to these challenges."
        },
        "Real-time embedded reconstruction of dynamic objects for a 3D maritime situational awareness picture": {
          "authors": [
            "F Sattler",
            "S Barnes",
            "BJ Carrillo Perez"
          ],
          "url": "https://elib.dlr.de/193059/",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "The tasks of object detection, instance segmentation and pose estimation can be trained using a single back-end, as shown by [20]."
        },
        "CenDerNet: Center and Curvature Representations for Render-and-Compare 6D Pose Estimation": {
          "authors": [
            "P De Roovere",
            "R Daems",
            "J Croenen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25085-9_6",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "20"
          ],
          "3": "On DIMO, we show our method significantly outperforms PVNet [20], a strong single-view baseline.",
          "4": "PVNet [20] is based on estimating 2D keypoints followed by perspective n-point optimization."
        },
        "Augmented Reality Pilot Assistance System for Helicopter Shipboard Operations": {
          "authors": [
            "TO Mehling"
          ],
          "url": "https://mediatum.ub.tum.de/1655458",
          "ref_texts": "[114] Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X. (2018). PVNet: Pixel -wise Voting Network for 6DoF Pose Estimation. Retrieved from https://arxiv.org/pdf/1812.11788 ",
          "ref_ids": [
            "114"
          ],
          "1": "Therefore, well known approaches for real-time 6D pose estimation are analyzed within this work: Pose CNN [157], PV Net [114], Dense Fusion [148] and Single Shot 6D Pose [136]."
        },
        "Category-level 6d object pose estimation with flexible vector-based rotation representation": {
          "authors": [
            "W Chen",
            "X Jia",
            "Z Zhang",
            "HJ Chang",
            "L Shen"
          ],
          "url": "https://arxiv.org/abs/2212.04632",
          "ref_texts": "[23] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86."
        },
        "Weakly Supervised Learning of Keypoints for 6D Object Pose Estimation": {
          "authors": [
            "M Tian",
            "GH Lee"
          ],
          "url": "https://arxiv.org/abs/2203.03498",
          "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2, 4, 6, 7, 8",
          "ref_ids": [
            "26"
          ],
          "2": "PVNet [26] and its extensions [10, 32] predict the 2D projections of a sparse point set selected on the surface of the object model.",
          "7": "methods Relative Pose Synthetic Data RGB with Pose Annotations OK-POSE [42] Ours AAE [34] DPOD [41] NOL [25] PVNet [26] CDPN [20] DPOD [41] ape 35.",
          "8": "PVNet [26], CDPN [20], and DPOD [41]) trained on real images annotated with full 6D object poses.",
          "9": "Methods PVNet Pix2Pose HybridPose RLLG Ours[26] [24] [32] [5] ape 15.",
          "10": "3 summaries the comparison with PVNet [26], Pix2Pose [24], HybridPose [32], and RLLG [5] on the OCCLUSION dataset in terms of ADD(-S) metric."
        },
        "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices": {
          "authors": [
            "YC Lau",
            "KW Tseng",
            "IJ Hsieh",
            "HC Tseng"
          ],
          "url": "https://arxiv.org/abs/2210.12476",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "29"
          ],
          "2": "For example, PVNet [29] may be a good choice for general use."
        },
        "6d object pose estimation in cluttered scenes from RGB images": {
          "authors": [
            "XL Yang",
            "XH Jia",
            "Y Liang",
            "LB Fan"
          ],
          "url": "https://link.springer.com/article/10.1007/s11390-021-1311-2",
          "ref_texts": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "46"
          ],
          "1": "We compare our proposed approach with the following recent advanced methods: PoseCNN [11], Seg-Driven[14], Tekin [28], SilhoNet [34], Pix2Pose [35], BB8[44], Heatmaps [45], PVnet [46], CDPN [47], and our previous work (OCP) [17].",
          "2": "80 PVnet [46] 15."
        },
        "One-Shot General Object Localization": {
          "authors": [
            "Y You",
            "Z Miao",
            "K Xiong",
            "W Wang",
            "C Lu"
          ],
          "url": "https://arxiv.org/abs/2211.13392",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "15"
          ],
          "1": "During inference, inspired by PVNet [15], we randomly sample point pairs and cast a center vote for each pair."
        },
        "Advances in biplanar X-ray imaging: calibration and 2D/3D registration": {
          "authors": [
            "VTH Nguyen"
          ],
          "url": "https://repository.uantwerpen.be/link/irua/192253",
          "ref_texts": "[41] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , (Long Beach, CA, USA, USA), June 2019.",
          "ref_ids": [
            "41"
          ],
          "1": "PVNet [41] is another deep learning 67 CHAPTER 4.",
          "3": "Finally, BoneNet, inspired by PVNet [41], is trained to detect 2D landmarks in fluoroscopy images automatically.",
          "7": "[41] trained a deep neural network (PVNet) to automatically detect 2D landmarks in an optical image scene.",
          "16": "One of the most relevant models is PVNet [41], which was introduced to detect 2D landmarks in optical images."
        },
        "An intelligent robotic vision system with environment perception": {
          "authors": [
            "Y Jin"
          ],
          "url": "https://etheses.whiterose.ac.uk/31259/",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the 131 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "23"
          ],
          "3": "[23], Pixel-wise Voting Network (PVNet), which detect vectors between pixel and keypoints rather than directly regressing keypoints.",
          "4": "[9] proposed PVN3D, that is, an extension of PV-Net [23] in the 3D domain shown in the Figure 2."
        },
        "Simultaneous object detection and pose estimation under domain shift": {
          "authors": [
            "Stefan Thalhammer"
          ],
          "url": "https://repositum.tuwien.at/handle/20.500.12708/120374",
          "ref_texts": "[22] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570 (cit. on pp. 2, 6, 14, 18, 19, 31, 38, 40).",
          "ref_ids": [
            "22"
          ],
          "13": "Notably, the authors of [22] show that providing a large set of keypoint hypotheses also leads to good pose estimation results.",
          "14": "The best performing deep learning approaches for monocular 6D object pose estimation employ encoder-decoder architectures [22], [39], [40], [49], [53]."
        },
        "Perception Systems for Robust Autonomous Navigation in Natural Environments": {
          "authors": [
            "A Trabelsi"
          ],
          "url": "https://search.proquest.com/openview/70d1f296e7b8ad5411f271816f28bc54/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[32] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "32"
          ],
          "2": "Thanks to the robustness of CNNs in key-point localization, these methods managed to overcome the difficulty in handling texture-less objects and low-resolution images [32].",
          "6": "For instance, the end-to-end extension of PVNet ([32] + [40]) outperforms its two-stage version ([32]) by 6."
        },
        "QUANTIZED NEURAL NETWORKS FOR 6D POSE ESTIMATION": {
          "authors": [
            "Z Zhao"
          ],
          "url": "https://robertflame.github.io/Homepage/assets/docs/theses/Master%20Thesis.pdf",
          "ref_texts": "[18] S. Peng, Y . Liu, Q. Huang, X. Zhou and H. Bao, \u2018Pvnet: Pixel-wise voting network for 6dof pose estimation,\u2019 in CVPR, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "PVNet [18] regressed keypoint-pointing vectors for each pixel and voted for keypoint locations using these vectors."
        },
        "PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation\u2013Supplementary Material\u2013": {
          "authors": [
            "T Jantos",
            "MA Hamdad",
            "W Granig",
            "S Weiss"
          ],
          "url": "https://proceedings.mlr.press/v205/jantos23a/jantos23a-supp.pdf",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "PnP IR Method PoseCNN [2] PoET PoETgt Pix2Pose [19] PVNet [20] GDR-Net [16]DeepIM [11] PE 1 1 1 N N N 1 Data real + syn pbr pbr real real + syn real + pbr real + syn Ape 9."
        },
        "Supplementary Material for EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation": {
          "authors": [
            "H Chen",
            "P Wang",
            "F Wang",
            "W Tian",
            "L Xiong",
            "H Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/supplemental/Chen_EPro-PnP_Generalized_End-to-End_CVPR_2022_supplemental.pdf",
          "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 5",
          "ref_ids": [
            "13"
          ],
          "1": "9 ms overhead to the base pose estimator PVNet [13] at the same batch size, measured on RTX 2080 Super GPU, which is slower than ours."
        },
        "Monocular Markerless 6D Pose Estimation of ANYmal": {
          "authors": [
            "I Alberico",
            "K Shi"
          ],
          "url": "https://tenhearts.github.io/assets/pdf/plr_report.pdf",
          "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "Most recent state-of-the-art works with RGB images focus on first detecting 2D targets of the object in the given image and subsequently solving a Perspective-n-Point(PnP) problem with predicting 2D-3D correspondences for 6D poses[11, 12, 13, 14, 15, 16]."
        },
        "RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization": {
          "authors": [
            "YXKYL Guofeng",
            "ZXWH Li"
          ],
          "url": "https://decayale.github.io/publication/rnnpose/paper.pdf",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
          "ref_ids": [
            "34"
          ],
          "1": "These methods may estimate the object\u2019s bounding box corners [35, 45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
          "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit (estimated by any direct methods [34,53]).",
          "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
          "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
          "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
          "6": "1 cm following [34].",
          "7": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
          "8": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [59], DeepIM [23] and RePOSE [20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
          "9": "ObjectPoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51]DPOD [59] RePOSE [20]OursApe 9.",
          "10": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [59] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
          "11": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "A Smart Workcell for Automatic Pick and Sorting for Logistics": {
          "authors": [
            "N Castaman",
            "A Gottardi",
            "E Menegatti"
          ],
          "url": "https://www.research.unipd.it/handle/11577/3471171",
          "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, Pvnet: \u201cPixelwise voting network for 6DOF pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570",
          "ref_ids": [
            "4"
          ],
          "1": "In general, this problem relies on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [2], with an increasing trend toward data-driven approaches based on deep models [3], [4]."
        },
        "Supplementary Material: Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images": {
          "authors": [
            "Y Liu",
            "Y Wen",
            "S Peng",
            "C Lin",
            "X Long",
            "T Komura"
          ],
          "url": "https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920297-supp.pdf",
          "ref_texts": "5. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019)",
          "ref_ids": [
            "5"
          ],
          "1": "On the LINEMOD [2] dataset, we use the training set of previous instance-specific estimators [8,5] as reference images and the other images are selected as query images.",
          "2": "Note that reference images are used in inference of Gen6D but not in training the Gen6D estimator while instance-specific estimators like PVNet [5] actually use these reference images to train their models.",
          "3": "Note PVNet [5] is trained on the specific test object with both synthetic and real images while our Gen6D is not trained on the test object.",
          "4": "1d PVNet [5] Ours PVNet [5] Ours Eggbox 99."
        },
        "Direct pose estimation from RGB images using 3D objects 3 Boyutlu nesneleri kullanarak imgelerden poz kestirimi": {
          "authors": [
            "MA DEDE",
            "Y GEN\u00c7"
          ],
          "url": "https://jag.journalagent.com/z4/download_fulltext.asp?pdir=pajes&plng=tur&un=PAJES-08566",
          "ref_texts": "[15] Peng S, Liu, Y, Huang Q, Zhou X, Bao H. \u201cPVNet: Pixel-Wise voting network for 6DoF pose estimation\u201d. In the IEEE Conference on Computer Vision and Pattern Recognition ",
          "ref_ids": [
            "15"
          ],
          "5": "Peng et al [15] uses the Farthest Point Sampling algorithm (FPS) to select key-points on the target object."
        },
        "Supplementary Material for OnePose: One-Shot Object Pose Estimation without CAD Models": {
          "authors": [
            "J Sun",
            "Z Wang",
            "S Zhang",
            "X He",
            "H Zhao",
            "G Zhang"
          ],
          "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/OnePose/onepose_supp.pdf",
          "ref_texts": "[8] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3",
          "ref_ids": [
            "8"
          ],
          "1": "Implementation Details of the Evaluation of PVNet For the experiments of evaluating PVNet [8], we directly use the original implementation and training configurations provided by the authors at [1]."
        },
        "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation": {
          "authors": [
            "Gu Wang",
            "Fabian Manhardt",
            "Federico Tombari",
            "Xiangyang Ji"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.html",
          "ref_texts": "[40] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. InIEEE Conference on Computer V ision and P attern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "40"
          ],
          "4": "16617 Method w/o Refinement w/ Refinement PoseCNN [60] PVNet [40] Single-Stage [19] HybridPose [47] GDR-Net (Ours) DPOD [62] DeepIM [27] P .",
          "5": "0 PVNet [40] N 73."
        },
        "Ffb6d: A full flow bidirectional fusion network for 6d pose estimation": {
          "authors": [
            "Yisheng He",
            "Haibin Huang",
            "Haoqiang Fan",
            "Qifeng Chen",
            "Jian Sun"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/He_FFB6D_A_Full_Flow_Bidirectional_Fusion_Network_for_6D_Pose_CVPR_2021_paper.html",
          "ref_texts": "[46] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "46"
          ],
          "7": "Qualitative results are reported in 3008 RGB RGB-D PoseCNN DeepIM [68, 33] PVNet[46] CDPN[34] DPOD[70] PointFusion[69] DenseFusion[65] G2LNet[7] PVN3D[17] Our FFB6D MEAN 88.",
          "8": "[26] Pix2Pose [45] PVNet [46] ADD-0."
        },
        "DexYCB: A benchmark for capturing hand grasping of objects": {
          "authors": [
            "Wei Chao",
            "Wei Yang",
            "Yu Xiang",
            "Pavlo Molchanov",
            "Ankur Handa",
            "Jonathan Tremblay",
            "Yashraj S. Narang",
            "Karl Van",
            "Umar Iqbal",
            "Stan Birchfield",
            "Jan Kautz",
            "Dieter Fox"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "27"
          ],
          "1": "State-of-the-art approaches for both 3D object pose [37, 20, 34, 27, 40, 26, 19] and 3D hand pose estimation [49, 23, 18, 2, 9, 14, 31] rely on deep learning and thus require large datasets with labeled hand or object poses for training."
        },
        "Semi-supervised 3d hand-object poses estimation with interactions in time": {
          "authors": [
            "Shaowei Liu",
            "Hanwen Jiang",
            "Jiarui Xu",
            "Sifei Liu",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Liu_Semi-Supervised_3D_Hand-Object_Poses_Estimation_With_Interactions_in_Time_CVPR_2021_paper.html",
          "ref_texts": "[43] Sida Peng, Y uan Liu, Qi-Xing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CVPR, pages 4556\u20134565, 2019.",
          "ref_ids": [
            "43"
          ],
          "1": "There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [28, 67] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [45, 60, 43, 24]."
        },
        "H2o: Two hands manipulating objects for first person interaction recognition": {
          "authors": [
            "Taein Kwon",
            "Bugra Tekin",
            "Jan Stuhmer",
            "Federica Bogo",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.html",
          "ref_texts": "[59] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3",
          "ref_ids": [
            "59"
          ],
          "1": "While a significant amount of research has focused on predicting the pose of hands [27, 54, 56, 58, 70, 93, 94, 98] or objects [5, 48, 59, 78, 83, 90] in isolation, joint understanding of handobject interactions has received far less attention."
        },
        "Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism": {
          "authors": [
            "Wei Chen",
            "Xi Jia",
            "Hyung Jin",
            "Jinming Duan",
            "Linlin Shen",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chen_FS-Net_Fast_Shape-Based_Network_for_Category-Level_6D_Object_Pose_Estimation_CVPR_2021_paper.html",
          "ref_texts": "[23] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "23"
          ],
          "1": "Correspondences-based methods trained their model to establish 2D-3D correspondences [28, 29, 23] or 3D-3D correspondences [6, 5].",
          "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86."
        },
        "Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation": {
          "authors": [
            "Kai Chen",
            "Qi Dou"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.1, 2",
          "ref_ids": [
            "20"
          ],
          "1": "Different from conventional instance-level [12, 20, 30, 35] object pose estimation, which gives instance CAD models and predicts poses for the instances that have been seen during training, category-level task requires capturing the general properties while accounting for the large variation of differPrior Point Cloud Camera Instance I Camera Instance II w/o Prior adaptation w/ Prior adaptation Figure 1.",
          "2": "Methods [20, 25, 2, 13, 17, 16] mainly focus on learning a robust embedding that is conditioned on the object pose.",
          "3": "The second group of methods [20, 12, 25, 13] assume the object 3D CAD model is available."
        },
        "So-pose: Exploiting self-occlusion for direct 6d pose estimation": {
          "authors": [
            "Yan Di",
            "Fabian Manhardt",
            "Gu Wang",
            "Xiangyang Ji",
            "Nassir Navab",
            "Federico Tombari"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Di_SO-Pose_Exploiting_Self-Occlusion_for_Direct_6D_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019. 2, 6, 7",
          "ref_ids": [
            "28"
          ],
          "1": "[28] demonstrate that keypoints away from the object surface induce larger errors and, therefore, instead sample several keypoints on the object model based on farthest point sampling.",
          "2": "HybridPose [36] follows and develops [28] by introducing hybrid representations."
        },
        "Geometry-based distance decomposition for monocular 3d object detection": {
          "authors": [
            "Xuepeng Shi",
            "Qi Ye",
            "Xiaozhi Chen",
            "Chuangrong Chen",
            "Zhixiang Chen",
            "Kyun Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.html",
          "ref_texts": "[35] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "35"
          ],
          "2": "These works [35, 17, 23, 44] recover the pose or distance by several factors, such as 2D keypoints, 2D bounding boxes, and object physical size, which achieves interpretable and robust pose or distance estimation.",
          "3": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress 2D keypoints of objects, then optimize the estimation of the 6D pose by solving a Perspective-n-Point (PnP) problem."
        },
        "Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency": {
          "authors": [
            "Jiehong Lin",
            "Zewei Wei",
            "Zhihao Li",
            "Songcen Xu",
            "Kui Jia",
            "Yuanqing Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2",
          "ref_ids": [
            "20"
          ],
          "2": "More recent solutions build on the power of deep networks and can directly estimate object poses from RGB images alone [16, 32, 26, 20] or RGB-D ones [17, 29]."
        },
        "Category-level 6d object pose estimation via cascaded relation and recurrent reconstruction networks": {
          "authors": [
            "J Wang",
            "K Chen",
            "Q Dou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636212/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019, pp. 4561\u2013",
          "ref_ids": [
            "2"
          ],
          "2": "Instead of explicitly detecting and matching object keypoints, some methods [8] take corner points of 3D object bounding boxes as keypoints, or implicitly represent keypoints by a dense voting field [2]."
        },
        "Wide-depth-range 6d object pose estimation in space": {
          "authors": [
            "Yinlin Hu",
            "Sebastien Speierer",
            "Wenzel Jakob",
            "Pascal Fua",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Hu_Wide-Depth-Range_6D_Object_Pose_Estimation_in_Space_CVPR_2021_paper.html",
          "ref_texts": "[31] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InConference on Computer V ision and P attern Recognition, 2019.",
          "ref_ids": [
            "31"
          ],
          "1": "This network is usually trained to predict the image location of the 3D object bounding box corners, either in a single global fashion [18, 33, 37, 42], or by aggregating multiple local predictions to improve robustness to occlusions [29, 16, 11, 31, 43, 23]."
        },
        "Repose: Fast 6d object pose refinement via deep texture rendering": {
          "authors": [
            "Shun Iwase",
            "Xingyu Liu",
            "Rawal Khirodkar",
            "Rio Yokota",
            "Kris M. Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Iwase_RePOSE_Fast_6D_Object_Pose_Refinement_via_Deep_Texture_Rendering_ICCV_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Xiaowei Liu, and Hujun Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In CVPR, 2019. 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "1": "Related Work Two-stage pose estimation methods Recently, Oberweger [26], PVNet [27], DPOD [40], and HybridPose [33] have shown excellent performance on 6D object pose estimation using a two-stage pipeline to estimate a pose: (i) estimating a 2D representation (e.",
          "2": "Instead of regarding the corners as keypoints, PVNet [27] places the keypoints on the object surface via the farthest point sampling algorithm.",
          "3": "RePOSE adopts PVNet [27] as the initial pose estimator using the official implementation.",
          "4": "RePOSE then refines the initial pose estimate Pini = \u2126(I) where \u2126 is any pose estimation method like PVNet [27] and PoseCNN [39] in real time using differentiable Levenberg\u2013Marquardt (LM) optimization [24].",
          "5": "The pre-trained weights of PVNet [27] or PoseCNN [39] are used for the encoder and only the decoder is trained while training RePOSE.",
          "6": "We used pretrained PVNet [27] on the LineMOD and Occlusion LineMOD datasets, and PoseCNN [39] on the YCB-Video [39] dataset as the initial pose estimator \u2126.",
          "7": "Following [27], we also add 500 synthetic and fused images for LineMOD and 20K synthetic images for YCBVideo to avoid overfitting during training.",
          "8": "Metric PoseCNN [39] DeepIM [21] PVNet [27] CosyPose [19] RePOSE RePOSE w/ track AUC, ADD(-S) 61.",
          "9": "1 Refinement FPS 22 6 26 13 181 111 80 125 90 71 #Iterations 1 4 1 2 1 3 5 1 3 5 Table 2: Comparison of RePOSE on Linemod dataset with recent methods including PVNet [27], DPOD [40], HybridPose [33], and EfficientPose [9] using the ADD(-S) score.",
          "10": "As shown in Tables 2 and 3, RePOSE achieves Table 3: Comparison of RePOSE on Occlusion LineMOD dataset with recent methods including PVNet [27], DPOD [40], and HybridPose [33] using the ADD(-S) score.",
          "11": "In comparison to PVNet [27], RePOSE successfully refines the initial pose estimate in all the objects, achieving an improvement of 9.",
          "12": "The key difference is mainly on ape and duck where our initial pose estimator PVNet [27] performs poorly.",
          "13": "Object PVNet [27]RGB CNN w/ FW DPODOurs w/ FW Ours Ape 43.",
          "14": "Object PVNet [27]RGB CNN w/ FW DPODOur w/ FW Ours Ape 15.",
          "15": "Ablation Study All ablations for RePOSE are conducted on the LineMOD and Occlusion LineMOD datasets using PVNet [27] as an initial pose estimator.",
          "16": "2% absolute improvement from PVNet [27] on the LineMOD dataset [15].",
          "17": "In this experiment, we use the same initial pose estimator [27]."
        },
        "A vector-based representation to enhance head pose estimation": {
          "authors": [
            "Zongcheng Chu",
            "Dongfang Liu",
            "Yingjie Chen",
            "Zhiwen Cao"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Chu_A_Vector-Based_Representation_to_Enhance_Head_Pose_Estimation_WACV_2021_paper.html",
          "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "The approaches can be divided into two categories: [20, 33, 27] first estimate the object mask to determine its location in the image, then build the correspondence between the image pixels and the available 3D models."
        },
        "Occlusion-aware self-supervised monocular 6D object pose estimation": {
          "authors": [
            "G Wang",
            "F Manhardt",
            "X Liu",
            "X Ji"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9655492/",
          "ref_texts": "[14] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "14"
          ],
          "2": "Similarly, SegDriven [47] and PVNet [14] also regress 2D projections of associated sparse 3D keypoints, however, both employ segmentation paired with voting to improve reliability.",
          "5": "On the other hand, as for training with real pose labels, we outperform all other recently published methods including PVNet [14] and CDPN [50] reporting a mean average recall of 91."
        },
        "Dpodv2: Dense correspondence-based 6 dof pose estimation": {
          "authors": [
            "I Shugurov",
            "S Zakharov",
            "S Ilic"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9565319/",
          "ref_texts": "[34] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "34"
          ],
          "1": "Some of the most recent representatives include iPose [6], PVNet [34], DPOD [35], Pix2Pose [36], CDPN [37], and SDFlabel [38].",
          "2": "PVNet [34] takes a different approach and designs a network which for every pixel in the image regresses an offset to the predefined keypoints located on the object itself."
        },
        "Survey on localization systems and algorithms for unmanned systems": {
          "authors": [
            "S Yuan",
            "H Wang",
            "L Xie"
          ],
          "url": "https://www.worldscientific.com/doi/abs/10.1142/S230138502150014X",
          "ref_texts": "[321] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. \u201cPvnet: Pixel-wise voting network for 6dof pose estimation.\u201d In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4561-4570.",
          "ref_ids": [
            "321"
          ],
          "1": "PVNet [321] proposed a deep feature extraction model for solving challenging localization problems in a dynamic and occlusion scene."
        },
        "Vs-net: Voting with segmentation for visual localization": {
          "authors": [
            "Zhaoyang Huang",
            "Han Zhou",
            "Yijin Li",
            "Bangbang Yang",
            "Yan Xu",
            "Xiaowei Zhou",
            "Hujun Bao",
            "Guofeng Zhang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.html",
          "ref_texts": "[38] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "38"
          ],
          "2": "Recently , PVNet [38] significantly improves robustness and accuracy of object pose estimation by detecting keypoints with pixel-wise votes, inspired by which, we propose to detect scene-specific landmarks with pixelwise votes."
        },
        "Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency": {
          "authors": [
            "Zongxin Yang",
            "Xin Yu",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_DSC-PoseNet_Learning_6DoF_Object_Pose_Estimation_via_Dual-Scale_Consistency_CVPR_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019. 2, 4, 7",
          "ref_ids": [
            "29"
          ],
          "2": "Fully-supervised deep model based methods: Deep learning based methods have demonstrated promising pose estimation performance [29, 43, 47, 46, 19, 22].",
          "4": "CPDN [49], DPOD [47] and Pix2Pose [18] output the 2D UV coordinates or 3D coordinates of 3D object models from images, while PoseCNN [43] and PVNet [29] employ Hough voting to localize object keypoints from estimated vector fields.",
          "6": "PVNet [29] predicts a vector field for each keypoint and employs voting to determine keypoint locations."
        },
        "Stereobj-1m: Large-scale stereo image dataset for 6d object pose estimation": {
          "authors": [
            "Xingyu Liu",
            "Shun Iwase",
            "Kris M. Kitani"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_StereOBJ-1M_Large-Scale_Stereo_Image_Dataset_for_6D_Object_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 1, 2, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "3": "Specifically, we implement PVNet [27] and KeyPose [17], two classic keypoint-based 6D pose estimation frameworks that have achieved state-of-the-art performance on various datasets.",
          "5": "25 Table 4: The results of PVNet [27]on single-object pose estimation in terms of ADD(-S) AUCand ADD(-S) accuracyon StereOBJ-1M dataset."
        },
        "Keypoint-graph-driven learning framework for object pose estimation": {
          "authors": [
            "Shaobo Zhang",
            "Wanqing Zhao",
            "Ziyu Guan",
            "Xianlin Peng",
            "Jinye Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Keypoint-Graph-Driven_Learning_Framework_for_Object_Pose_Estimation_CVPR_2021_paper.html",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "Recently , deep learning approaches [13, 39, 4, 29, 34, 10, 26, 24, 25, 18, 3] have shown impressive results of pose estimation in RGB images.",
          "3": "Synthetic data generation Given 3D models of the objects, first, we define the keypoints on the surface of them as proposed in PVnet [26] where K keypoints are selected using the farthest point sampling (FPS) algorithm.",
          "4": "W e use blender [26] to render these 3D models from different camera viewpoints to sufficiently cover the objects and project the keypoints to images under the viewpoints.",
          "5": "T o evaluate the accuracy of the estimated pose, we use two standard metrics for LINEMOD used in other related paper [36, 41, 26] which are ADD and ADD-S (for symmetric objects).",
          "7": "W e compare our method with state-of-the-art 6D pose estimation methods (AAE [33], MHP [20], DPOD [41] Self6D [36]) that use the synthetic images generated by 3D CAD models and the methods (YOLD6D [34], DPOD [41], PVNet [26], CDPN[18]) using real images with manual 3D annotations for training.",
          "9": "W e use the model trained on the synthetic images for testing on the Occlusion dataset and compare our method with the three methods (DPOD [41], CDPN [18] and Self6D [36]) that do not require manual pose labels for training and three methods (YOLO6D [34], HMap [23] and PVNet [26]) using manual pose labels for training."
        },
        "Pr-gcn: A deep graph convolutional network with point refinement for 6d pose estimation": {
          "authors": [
            "Guangyuan Zhou",
            "Huiqun Wang",
            "Jiaxin Chen",
            "Di Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] proposes a deep offset prediction model to alleviate negative impacts of occlusions.",
          "3": "RGB based methods RGB-D based methods Object PoseCNN* PVNet CDPN DPOD DPVL PF* SSD6D\u2020 DF* PVN3D G2L* Ours* Ours [40, 18] [29] [19] [44] [42] [41] [14] [38] [9] [4] ape 77.",
          "4": "Object PoseCNN [40] DeepHeat [26] SS [12] Pix2pose [28] PVNet [29] HybridPose [34] PVN3D [9] Ours Ape 9.",
          "5": "We first compare PR-GCN to the state-of-the-art methods on Linemod, including the RGB based models: PoseCNN (+DeepIM) [40, 18], PVNet [29], CDPN [19], DPOD [44] and DPVL [42] and the RGB-D based ones: Point Fusion [41], SSD6D (+ICP) [14], Dense Fusion [38], PVN3D [9] and G2L[4]."
        },
        "Cloudaae: Learning 6d object pose regression with on-line data synthesis on point clouds": {
          "authors": [
            "G Gao",
            "M Lauri",
            "X Hu",
            "J Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561475/",
          "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "Depending on the applicability of the methods, for each dataset, we compare to a subset of state-of-the-art methods SSD-6D [29], EEPG-AAE [15], CloudPose [8], PVNet [30], PoseCNN [4], DenseFusion [12], PVN3D [13] and PointV oteNet [31]."
        },
        "A survey on deep learning based methods and datasets for monocular 3D object detection": {
          "authors": [
            "S Kim",
            "Y Hwang"
          ],
          "url": "https://www.mdpi.com/2079-9292/10/4/517",
          "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "72"
          ],
          "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
          "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
          "3": "Overview of the keypoint localization in PVNet [72].",
          "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "T6d-direct: Transformers for multi-object 6d pose direct regression": {
          "authors": [
            "A Amini",
            "AS Periyasamy",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-92659-5_34",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: CVPR, pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "20"
          ],
          "2": "We compare our results with PoseCNN [35], PVNet [20] and DeepIM [14]."
        },
        "Towards markerless surgical tool and hand pose estimation": {
          "authors": [
            "Jonas Hein"
          ],
          "url": "https://link.springer.com/article/10.1007/s11548-021-02369-2",
          "ref_texts": "26. Peng S, Liu Y , Huang Q, Zhou X, Bao H (2019) Pvnet: pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4561\u20134570",
          "ref_ids": [
            "26"
          ],
          "1": "One of the current state-of-the-art object tracking models, PVNet [26], utilizes this technique and performs well even under occlusions.",
          "2": "Baseline models PVNet We choose PVNet [26] as the first baseline since it is a state-of-the-art model for object-only pose estimation on single-shot RGB images.",
          "6": "Instead of directly regressing the 3D object pose via fully connected layers, such as employed in HandObjectNet, we propose to adopt the pose estimation method by using vector field encoded keypoints, similar to the method introduced for PVNet [26].",
          "8": "These results are in line with the results from the current state-of-the-art from computer vision applications, such as reported in [15,26]."
        },
        "A pose proposal and refinement network for better 6d object pose estimation": {
          "authors": [
            "Ameni Trabelsi",
            "Mohamed Chaabane",
            "Nathaniel Blanchard",
            "Ross Beveridge"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Trabelsi_A_Pose_Proposal_and_Refinement_Network_for_Better_6D_Object_WACV_2021_paper.html",
          "ref_texts": "[21] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "21"
          ],
          "3": "W e use a threshold of 2 cm for the ADD(-S) metric Methods HMap[20] PVNet[21] DeepIM \u2020 [15] OURS \u2020",
          "4": "W e report percentages of correctly estimated poses averaged over all object classes Method T ekin[28] PVNet[21] SSD6D\u2020 [13] DeepIM\u2020 [15] OURS\u2020 ADD(-S) 55.",
          "5": "W e report percentages of correctly estimated poses averaged over all object classes Method HMap[20] PVNet[21] BB8\u2020 [23] DeepIM\u2020 [15] OURS\u2020 ADD(-S) 30."
        },
        "Neural free-viewpoint performance rendering under complex human-object interactions": {
          "authors": [
            "G Sun",
            "X Chen",
            "Y Chen",
            "A Pang",
            "P Lin"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3475442",
          "ref_texts": "[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In CVPR.",
          "ref_ids": [
            "46"
          ],
          "1": "We believe the coarse-to-fine strategy [57] and the end-to-end 6DoF estimation [46] can accelerate human reconstruction and object tracking respectively."
        },
        "Self-supervised geometric perception": {
          "authors": [
            "Heng Yang",
            "Wei Dong",
            "Luca Carlone",
            "Vladlen Koltun"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_paper.html",
          "ref_texts": "[58] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conf. on Computer V ision and P attern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "58"
          ],
          "1": "Learned feature descriptors have been shown to consistently and significantly outperform their hand-crafted counterparts across applications such as relative camera pose estimation [69, 61], 3D point cloud registration [21, 32], and object detection and pose estimation [58, 86, 64, 72].",
          "2": "For example, ground-truth relative camera poses are needed for training image keypoint descriptors [69, 54, 27], pairwise rigid transformations are required for training point cloud descriptors [21, 32, 74, 85, 70], and object poses are used to train image keypoint predictors [58, 86].",
          "3": "For example, we also present the formulation for object detection and pose estimation [64, 58, 86, 15], and discuss the application of SGP in the Supplementary Material."
        },
        "Monocinis: Camera independent monocular 3d object detection using instance segmentation": {
          "authors": [
            "Jonas Heylen",
            "Mark De",
            "Bruno Dawagne",
            "Marc Proesmans",
            "Luc Van",
            "Wim Abbeloos",
            "Hazem Abdelkawy",
            "Daniel Olmeda"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.html",
          "ref_texts": "[65] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d inProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "65"
          ],
          "1": "Several existing methods can be used to map the 2D predicted RPs to a 3D object pose [60, 65, 66, 61, 62]."
        },
        "Roft: Real-time optical flow-aided 6d object pose and velocity tracking": {
          "authors": [
            "NA Piga",
            "Y Onyshchuk",
            "G Pasquale"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9568706/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVnet: Pixel-wise voting network for 6DoF pose estimation,\u201d in2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2019, pp.",
          "ref_ids": [
            "2"
          ],
          "1": "Several approaches have been proposed to tackle the problems of 6D object pose estimation [1], [2], [3], refinement [4], [5] and tracking [6], [7].",
          "2": "Mk and Tk can be obtained using either separate deep learning-based networks for segmentation and 6D object pose estimation, or a single network which performs both tasks jointly [1], [2]."
        },
        "Multi-view fusion for multi-level robotic scene understanding": {
          "authors": [
            "Y Lin",
            "J Tremblay",
            "S Tyree",
            "PA Vela"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9635994/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "6"
          ],
          "1": "Despite the tremendous progress made in the computer vision community on solving problems such as 3D reconstruction [1], [2], [3], [4] and object pose estimation [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], existing deployed robotic manipulators have limited, if any, perception of their surroundings."
        },
        "Visual identification of articulated object parts": {
          "authors": [
            "V Zeng",
            "TE Lee",
            "J Liang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636054/",
          "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "Of these works, PVNet [29] also regresses to a residual (pointing to object keypoints for pose estimation), whereas our motion residual is used to infer the kinematic constraint."
        },
        "Rede: End-to-end object 6d pose robust estimation using differentiable outliers elimination": {
          "authors": [
            "W Hua",
            "Z Zhou",
            "J Wu",
            "H Huang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9363576/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "Therefore, PVNet [13] employs the farthest point sampling (FPS) algorithm to select more representative keypoints.",
          "2": "As [13], we employ the farthest point sampling (FPS) algorithm to sample 3D keypoints {mk}K k=1 from the CAD model of each object.",
          "7": "With the same mask as PVNet [13].",
          "8": "With the same mask as PVNet [13] on Occlusion LineMOD dataset.",
          "9": "As for Occlusion LineMOD dataset with many hard cases, we use the same masks as PVNet [13].",
          "11": "During implementation, center point and 8 points selected by FPS algorithm are picked up as keypoints following PVNet [13].",
          "12": "10000 images using the \u201cCut and Paste\u201d strategy are further synthesized for training on LineMOD dataset as [13].",
          "13": "The second and third experiments extend PVNet [13] to 3D and the second experiment also employs SVD 3D-3D estimator."
        },
        "Sparse steerable convolutions: An efficient learning of se (3)-equivariant features for estimation and tracking of object poses in 3d space": {
          "authors": [
            "J Lin",
            "H Li",
            "K Chen",
            "J Lu",
            "K Jia"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/8c1b6fa97c4288a4514365198566c6fa-Abstract.html",
          "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "These works can be broadly categorized into three types: i) template matching [12] by constructing templates to search for the best matched poses; ii) 2D-3D correspondence methods [1, 14, 16, 19, 17], which establish 2D-3D correspondence via 2D keypoint detection [19, 17] or dense 3D coordinate predictions [1, 14, 16], followed by a PnP algorithm to obtain the target pose; iii) direct pose regression [26, 13, 23] via deep networks."
        },
        "6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping": {
          "authors": [
            "TT Le",
            "TS Le",
            "YR Chen",
            "J Vidal",
            "CY Lin"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0921889021000609",
          "ref_texts": "[44] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wisevotingnetworkfor6dofposeestimation, in: Proc. IEEE Conf. Comput. Vis. Pa\u0000ern Recognit., 2019: pp. 4561\u20134570.",
          "ref_ids": [
            "44"
          ],
          "1": "Some recent remarkable researches on deep learning-based 3D object recognition includePoseCNN[43]andPVNet[44]."
        },
        "Fast uncertainty quantification for deep object pose estimation": {
          "authors": [
            "G Shi",
            "Y Zhu",
            "J Tremblay",
            "S Birchfield"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561483/",
          "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "Recent leading methods rely on an approach similar to the one used in our work: A network is trained to predict object keypoints in the 2D image, followed by P nP [28] to estimate the pose of the object in the camera coordinate frame [2, 5, 24, 25, 27, 29]."
        },
        "Optimal pose and shape estimation for category-level 3d object perception": {
          "authors": [
            "J Shi",
            "H Yang",
            "L Carlone"
          ],
          "url": "https://arxiv.org/abs/2104.08383",
          "ref_texts": "[57] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 2",
          "ref_ids": [
            "57"
          ],
          "1": "Such approaches first recover the position of semantic keypoints [56] in the images with neural networks, and then recover the 3D pose of the object by solving a geometric optimization problem [31, 53, 56, 57, 64]."
        },
        "Multi-view object pose refinement with differentiable renderer": {
          "authors": [
            "I Shugurov",
            "I Pavlov",
            "S Zakharov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9363552/",
          "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "In contrast, PVNet [14] used per-pixel Hough voting to allow all object\u2019s pixels to vote for the few keypoints lying on the object."
        },
        "Pyrapose: Feature pyramids for fast and accurate object pose estimation under domain shift": {
          "authors": [
            "S Thalhammer",
            "M Leitner",
            "T Patten"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9562108/",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "5"
          ],
          "1": "The best performing deep-learning approaches for single-shot object pose estimation employ encoder-decoder architectures [2], [3], [4], [5], [6].",
          "2": "The dominant strategy is to employ encoder-decoder architectures for dense hypotheses generation and subsequent pose estimation using the PnP algorithm [13], [14], [3], [15], [4], [5], [16], [6].",
          "3": "Therefore, only one model needs to be trained per dataset in contrast to the majority of state-of-the-art approaches [30], [4], [5], [17], [24], [27], [16], [6].",
          "4": "The dominant approach to recover the most likely reference frame transformation that aligns a set of 2D points to a set of noisy 3D points [2], [4], [5], [16], [6] is PnP."
        },
        "3D object tracking with adaptively weighted local bundles": {
          "authors": [
            "JC Li",
            "F Zhong",
            "SH Xu",
            "XY Qin"
          ],
          "url": "https://link.springer.com/article/10.1007/s11390-021-1272-5",
          "ref_texts": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE Conference on Computer Vision and Pattern Recognition, June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "6"
          ],
          "1": "3 this is different from the 3D object detection and 6DOF pose estimation from a single image, which has been greatly advanced using learning-based approaches [6,7] ."
        },
        "Single-shot scene reconstruction": {
          "authors": [
            "Anonymous Submission"
          ],
          "url": "https://openreview.net/forum?id=CGn3XKSf7vf",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "7"
          ],
          "2": "The current state-of-the-art methods in object pose estimation almost exclusively belong to the latter group with such representatives as PVNet [7], CDPN [28], EPOS [27], Pix2Pose [4], GDR-Net [29] and DPOD [3, 6]."
        },
        "ARShoe: Real-time augmented reality shoe try-on system on smartphones": {
          "authors": [
            "S An",
            "G Che",
            "J Guo",
            "H Zhu",
            "J Ye",
            "F Zhou"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3481537",
          "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 4556\u2013",
          "ref_ids": [
            "23"
          ],
          "1": "Pixel-wise voting network (PVNet) [23] predicts pixel-wise vectors pointing to the object keypoints and uses these vectors to vote for keypoint locations."
        },
        "Kdfnet: Learning keypoint distance field for 6d object pose estimation": {
          "authors": [
            "X Liu",
            "S Iwase",
            "KM Kitani"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636489/",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d inCVPR, 2019. 1, 2, 4, 5, 6",
          "ref_ids": [
            "5"
          ],
          "1": "There are mainly two types of methods for localizing 2D keypoints: heatmap-based [3, 4] and voting-based [2, 5, 6].",
          "2": "In voting-based methods, the visible parts of the object hallucinate and vote for the 2D locations of the invisible keypoints [5].",
          "3": "every object pixel predicts the 2D direction to the keypoints and the keypoint hypotheses are the intersections of the direction votes [2, 5, 6].",
          "4": "3%, significantly outperforms related baselines such as [5] and the current state-of-the-art HybridPose [6].",
          "5": "Besides heatmap, 2D direction field representation has also been proposed for localizing keypoints [2, 5].",
          "6": "Specifically, direction-based voting methods have been adopted by previous works to robustly localize object centers [2] or object keypoints [5, 10] on the images where the voting scores are based on the number of direction inliers.",
          "7": "The intuition behind this method is that given an object, the location of the invisible keypoint can be inferred from the visible parts [2, 5, 6].",
          "8": "Inspired by recent works [4, 5], we first predict KDF to localize 2D keypoints through voting, then compute object 6D pose by solving a PnP problem.",
          "9": "method PVNet [5] KDFNet (ours) GT mask 93.",
          "10": "DISTANCE -BASED VOTING : A TOY EXPERIMENT The key difference between our method and previous works [2, 5, 6] is the predicted representation used in voting, i.",
          "11": "The baseline we compare our KDFNet against is PVNet [5], a direction-based keypoint method.",
          "12": "The data used to train our model are the same as [5]: real images from LINEMOD and synthetically rendered images using the scanned 3D object models.",
          "13": "We adopt the same object keypoint set as [5], which are generated by Farthest Point Sampling (FPS) of the object 3D point set.",
          "14": "We evaluate and compare our model against previous baselines [2, 5, 6, 17, 24] on Occlusion LINEMOD dataset.",
          "15": "Among these baselines, the most relevant is PVNet [5], a direction-based keypoint voting method which was also compared against in the toy experiment in Section IV.",
          "16": "methods PoseCNN [2] Oberweger [24] Pix2Pose [17] PVNet [5] HybridPose [6] KDFNet (ours) ape 9.",
          "17": "3 methods PoseCNN [2] Oberweger [24] PVNet [5] KDFNet (ours) ape 34.",
          "18": "5 particular, our method outperforms PVNet [5] by a margin of 9."
        },
        "6D object pose estimation using keypoints and part affinity fields": {
          "authors": [
            "M Zappel",
            "S Bultmann",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-98682-7_7",
          "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "15"
          ],
          "1": "In recent years, two-stage approaches, which first detect keypoints and then solve a Perspective-n-Point (PnP) problem to infer the object pose [14,15,16], have been shown to provide robust and accurate results.",
          "2": "PVNet [15] also defines keypoints on the object surface but infers them in a dense manner: Each pixel in the object segmentation mask predicts vectors that point to every keypoint.",
          "3": "The automatically defined set of keypoints is chosen with the farthest-point-algorithm, inspired by PVNet [15]: Starting with the object center, points on the object surface which are farthest from the already chosen points are added to the keypoint set."
        },
        "End-to-end learning improves static object geo-localization from video": {
          "authors": [
            "Mohamed Chaabane",
            "Lionel Gueguen",
            "Ameni Trabelsi",
            "Ross Beveridge",
            "Stephen O"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2021/html/Chaabane_End-to-End_Learning_Improves_Static_Object_Geo-Localization_From_Video_WACV_2021_paper.html",
          "ref_texts": "[24] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "5D Pose Estimation Many state-of-the-art methods for object pose estimation [15, 22, 24, 30, 35] use 3D models of the objects."
        },
        "L6dnet: Light 6 DoF network for robust and precise object pose estimation with small datasets": {
          "authors": [
            "M Gonzalez",
            "A Kacete",
            "A Murienne"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9364353/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF pose estimation,\u201d in IEEE Conf. on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "7"
          ],
          "4": "PVNet[7] proposes to apply an offset based approach to predict the 2D location of a set of keypoints on the object surface.",
          "5": "Inspired by [7], we select the keypoints using the farthest point sampling algorithm which allows us to get a good coverage of the object."
        },
        "ASM-Net: Category-level Pose and Shape Estimation Using Parametric Deformation.": {
          "authors": [
            "S Akizuki",
            "M Hashimoto"
          ],
          "url": "https://www.bmvc2021-virtualconference.com/assets/papers/1277.pdf",
          "ref_texts": "[19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise V oting Network for 6DoF Pose Estimation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "19"
          ],
          "2": "Recently, this process has been replaced by deep learning [10, 11, 19]."
        },
        "Fast-learning grasping and pre-grasping via clutter quantization and Q-map masking": {
          "authors": [
            "D Ren",
            "X Ren",
            "X Wang",
            "ST Digumarti"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636165/",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u2013",
          "ref_ids": [
            "20"
          ],
          "1": "We also extend the FCN by utilizing a combination of skip connections [20] and upsampling to improve learning efficiency.",
          "2": "In addition, the combination of skip connections [20] and bilinear upsampling in our FCN further boosts accuracy."
        },
        "Deepflux for skeleton detection in the wild": {
          "authors": [
            "Y Xu",
            "Y Wang",
            "S Tsogkas",
            "J Wan",
            "X Bai"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-021-01430-6",
          "ref_texts": "45. Peng, S., Liu, Y ., Huang, Q., Zhou, X., & Bao, H. (2019). PVNet: Pixel-wise voting network for 6dof pose estimation. InProceedings of IEEE international conference on computer vision and pattern recognition (pp. 4561\u20134570).",
          "ref_ids": [
            "45"
          ],
          "1": "O u rw o r ki sa l s or e l a t e dt o the approaches in [1,2,6,9,27,38,45,67]w h i c hl e a r nd i r e c tion cues for edge detection, instance segmentation, and pose estimation.",
          "2": "Finally, direction cues are also used to improve instance segmentation in [1]a n dd i r e c t i o nfi e l d sp o i n t i n g towards keypoints are used for pose estimation in [27,45]."
        },
        "Strumononet: Structure-aware monocular 3d prediction": {
          "authors": [
            "Zhenpei Yang",
            "Li Erran",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_StruMonoNet_Structure-Aware_Monocular_3D_Prediction_CVPR_2021_paper.html",
          "ref_texts": "[29] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer V ision and P attern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer V ision Foundation / IEEE, 2019.",
          "ref_ids": [
            "29"
          ],
          "1": "Examples include learning a machine translator between two minor languages by composing machine translators via a mother language [19], solving 6D object pose prediction via intermediate keypoint detections [1, 31, 28, 36, 27, 29, 34], and predicting 3D human poses through 2D keypoint predictions [44]."
        },
        "Dynamical pose estimation": {
          "authors": [
            "Heng Yang",
            "Chris Doran",
            "Jacques Slotine"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Yang_Dynamical_Pose_Estimation_ICCV_2021_paper.html",
          "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.2",
          "ref_ids": [
            "39"
          ],
          "1": "4 Problem (1), when specialized to the primitives 1-7, includes a broad class of fundamental perception problems concerning pose estimation from visual measurements, and finds extensive applications to object detection and localization [30, 39], motion estimation and 3D reconstruction [58, 56], and simultaneous localization and mapping [10, 52, 42]."
        },
        "DRNet: A depth-based regression network for 6D object pose estimation": {
          "authors": [
            "Lei Jin",
            "Xiaojuan Wang",
            "Mingshu He",
            "Jingyue Wang"
          ],
          "url": "https://www.mdpi.com/1424-8220/21/5/1692",
          "ref_texts": "28. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR, Long Beach, CA, USA, 15\u201321 June 2019; pp. 4561\u20134570. Sensors 2021, 21, 1692 14 of 14",
          "ref_ids": [
            "28"
          ],
          "1": "[28] proposed a Pixel-wise Voting Network (PVNet) to identify keypoints with the aid of RANSAC-based voting.",
          "2": "We render 10,000 images for each object in the Linemod dataset as [28].",
          "3": "In addition, we also compare our method with PVNet [28], which uses key points to figure out poses.",
          "4": "4 for the average ADD(S) [28], which calculates the ADD AUC for asymmetric objects and the ADD-S AUC for symmetric objects.",
          "5": "Methods BB8 [53] PoseCNN [20] Pix2Pose [54] PVNet [28] CDPN [51] Our Mean 43."
        },
        "Instancepose: Fast 6dof pose estimation for multiple objects from a single rgb image": {
          "authors": [
            "Lee Aing",
            "Nung Lie",
            "Chiu Chiang",
            "Shiang Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.html",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4556\u20134565, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "19"
          ],
          "1": "Introduction Many studies [8, 14, 9, 10, 27, 15, 1, 18, 19, 25, 17, 4, 13, 24, 11] involve 6DoF object pose estimation using a single RGB image.",
          "3": "Two studies, [9, 19] predict the unit-vector fields to estimate the pose.",
          "4": "The maximum processing speed for this method with an 2625 Metrics 2DPro ADD(S) Methods PoseCNN S-Driven PVNet S-Stage Ours PoseCNN S-Driven Pix2Pose PVNet S-Stage Ours[26] [10] [19] [9] [26] [10] [18] [19] [9] ape 34.",
          "5": "Datasets The datasets that are used for training come from the Normal LINEMOD dataset [7] and the rendered dataset [19].",
          "6": "However, for the object \u201dglue\u201d, 2626 Metrics 5CMD 2CMD 5CMD 10CMD Methods DeepIM PVNet Ours[13] [19] ape 51.",
          "7": "Metrics Time consumption (ms) Methods [10] [18] [19] [9] Ours Data loading 10."
        },
        "MBAPose: Mask and bounding-box aware pose estimation of surgical instruments with photorealistic domain randomization": {
          "authors": [
            "M Yoshimura",
            "MM Marinho",
            "K Harada"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636404/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "1": "They estimate 2D information such as key points [23], or projected corner points of 3D bounding boxes [24], in the first stage and subsequently use an optimization algorithm in the second stage using the information of the first stage to obtain the instrument\u2019s pose."
        },
        "Pose estimation from RGB images of highly symmetric objects using a novel multi-pose loss and differential rendering": {
          "authors": [
            "SH Bengtson",
            "H \u00c5str\u00f6m",
            "TB Moeslund"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636839/",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, June 2019.",
          "ref_ids": [
            "9"
          ],
          "1": "Lately, many of these methods have started to be replaced or complemented by machine learning methods [1], [9], [10], [11], [12]."
        },
        "Investigations on output parameterizations of neural networks for single shot 6d object pose estimation": {
          "authors": [
            "K Kleeberger",
            "M V\u00f6lk",
            "R Bormann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561712/",
          "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "28"
          ],
          "1": "Other single shot approaches to object pose estimation output the 2D projections of the 3D bounding box and use a P nP algorithm [27] to compute the 6D pose [17], [18], [28], [29]."
        },
        "A 3d keypoints voting network for 6dof pose estimation in indoor scene": {
          "authors": [
            "Huikai Liu",
            "Gaorui Liu",
            "Yue Zhang",
            "Linjian Lei",
            "Hui Xie",
            "Yan Li",
            "Shengli Sun"
          ],
          "url": "https://www.mdpi.com/2075-1702/9/10/230",
          "ref_texts": "7. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "7"
          ],
          "1": "CNN is also used in pose estimation, PVNet [7] regress the 2d keypoint through the end-to-end network, and then use the PnP algorithm, estimate the 6d pose by calculating the 2d-3d correspondence relationship of the object.",
          "2": "PVNet [7] first votes the keypoints through RANSAC, then utilizes the 2D-3D correspondence to calculate the 6D pose.",
          "3": "We compare our method with the RGB based methods PoseCNN [9], PVNet [7] and RGDB based methods PointFusion [49], Densefusion [12], PVN3D [15]."
        },
        "A high-resolution network-based approach for 6D pose estimation of industrial parts": {
          "authors": [
            "J Fan",
            "S Li",
            "P Zheng",
            "CKM Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9551495/",
          "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "23"
          ],
          "1": "To further improve the pose estimation accuracy, some studies decided to borrow the idea from feature points-based methods and predict the correspondence of key points via CNN models [23], [24], while others added an extra refinement stage upon the CNN-based 6D pose estimation model [25], [26]."
        },
        "From IR images to point clouds to pose: point cloud-based AR glasses pose estimation": {
          "authors": [
            "Ahmet Firintepe",
            "Carolin Vey",
            "Stylianos Asteriadis",
            "Alain Pagani",
            "Didier Stricker"
          ],
          "url": "https://www.mdpi.com/2313-433X/7/5/80",
          "ref_texts": "1. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
          "ref_ids": [
            "1"
          ],
          "1": "Given that depth information usually necessitates the use of dedicated hardware, image-only approaches for pose estimation have received significant attention over the last years [1,3,4,6,8].",
          "2": "Because handcrafting features is time-consuming and prone to errors, Deep Learning-based approaches have gained popularity and outperform traditional approaches [1,3,4,8].",
          "3": "Recent feature-based Deep Learning methods use Deep Neural Networks to estimate the objects\u2019 keypoints and combine them with PnP, partly relying on traditional methods [1,2,11].",
          "4": "[1] provide a state-of-the-art approach based on keypoint regression and further PnP execution.",
          "5": "Especially the definition of keypoints benefits pose estimation when dealing with occlusions and truncation [1]."
        },
        "Soft-jig-driven assembly operations": {
          "authors": [
            "T Kiyokawa",
            "T Sakuma",
            "J Takamatsu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9562008/",
          "ref_texts": "[36] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR, 2019, pp.",
          "ref_ids": [
            "36"
          ],
          "1": "To confirm the 6D pose estimation task for fixed parts, we apply PVNet [36], one of deep learning-based algorithms [37], [38]."
        },
        "Self-guided instance-aware network for depth completion and enhancement": {
          "authors": [
            "Z Luo",
            "F Zhang",
            "G Fu",
            "J Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561523/",
          "ref_texts": "[12] S. Y . Q. H.Bao and X.Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d IEEE Conference on Computer Vision and Pattern Recognition, pp. 4556\u20134565, 2018.",
          "ref_ids": [
            "12"
          ],
          "1": "In contrast, as convolutional neural network (CNN) have been successful in learning effective representations in object detection [9]\u2013[11], and pose estimation [12], [13]."
        },
        "Bridging the reality gap for pose estimation networks using sensor-based domain randomization": {
          "authors": [
            "Frederik Hagelskjaer",
            "Anders Glent"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.html",
          "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 2, 7",
          "ref_ids": [
            "24"
          ],
          "1": "In PVNet [24], the network instead locates keypoints by first segmenting the object and then letting all remaining pixels vote for keypoint locations.",
          "3": "The competing methods are DPOD [35], SSD-6D [19] (obtained from [32]), PVNet [24], DenseFusion [32], PointVoteNet [7] and PVN3D [10]."
        },
        "6D object pose estimation with pairwise compatible geometric features": {
          "authors": [
            "M Lin",
            "V Murali",
            "S Karaman"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561404/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "27"
          ],
          "1": "Recent work such as [27] and [25] predicts either a direction vector pointing to keypoints or offsets to the center of target objects for each point and achieves satisfactory accuracy.",
          "5": "The first observation we draw from the row of the mean recall is that, with a more consistent geometric representation, we can eliminate the gap with methods using RGB [15], [27] even without ICP refinement."
        },
        "Deep quaternion pose proposals for 6D object pose tracking": {
          "authors": [
            "Mateusz Majcher",
            "Bogdan Kwolek"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021W/DSC/html/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.html",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. CVPR, pages 4556\u20134565, 2019. 1, 4",
          "ref_ids": [
            "21"
          ],
          "2": "In [21], a Pixel-wise V oting Network (PVNet) to regress pixelwise unit vectors pointing to the keypoints and then using these vectors to vote for keypoint locations via RANSAC has been proposed."
        },
        "Attention voting network with prior distance augmented loss for 6DoF pose estimation": {
          "authors": [
            "Y He",
            "J Li",
            "X Zhou",
            "Z Chen",
            "X Liu"
          ],
          "url": "https://search.ieice.org/bin/summary.php?id=e104-d_7_1039",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp.4561\u20134570, 2019.",
          "ref_ids": [
            "7"
          ],
          "2": "Unit direction vector-field representation and Hough voting scheme were proposed for robust 2D keypoint localization and demonstrated its superiority in [6], [7], [17].",
          "7": "[7] achieve state-of-the-art performance via their unit direction vector-field representation and Hough voting scheme.",
          "21": "Comparing our methods with PVNet [7], both PDAL and AFAM have significant improvements on most objects, especially on ape, cat, duck, etc."
        },
        "Iterative coarse-to-fine 6D-pose estimation using back-propagation": {
          "authors": [
            "R Araki",
            "K Mano",
            "T Hirano",
            "T Hirakawa"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9636098/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d inProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "21"
          ],
          "1": "PVNet [21] estimates pose in two stages: feature point extraction and Perspective-n-Point."
        },
        "Robust 2d/3d vehicle parsing in cvis": {
          "authors": [
            "H Miao",
            "F Lu",
            "Z Liu",
            "L Zhang",
            "D Manocha"
          ],
          "url": "https://arxiv.org/abs/2103.06432",
          "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "34"
          ],
          "1": ", AM3D [29], DPOD [58], PV-Net [34], D4LCN [9])."
        },
        "Pose estimation and image matching for tidy-up task using a robot arm": {
          "authors": [
            "J Piao",
            "HJ Jo",
            "JB Song"
          ],
          "url": "https://koreascience.kr/article/JAKO202109065713449.page",
          "ref_texts": "[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 2019, DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "3"
          ],
          "1": "\uae30\uc874\uc758 \uc790\uc138\ucd94\uc815 \uc54c\uace0\ub9ac\uc998 \uc911\uc5d0\uc11c \uc2ec\uce35\ud559\uc2b5\uc5d0 \uae30\ubc18\ud55c \uc5f0\uad6c \ub4e4[2,3]\uc774 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\uba74\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4."
        },
        "Iterative optimisation with an innovation CNN for pose refinement": {
          "authors": [
            "G Kennedy",
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "https://arxiv.org/abs/2101.08895",
          "ref_texts": "[PLH+19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "PLH\\+19"
          ],
          "4": "We choose PVNet [PLH+19] to be the baseline 3 PREPRINT object pose estimation network.",
          "6": "We use a pretrained PVNet [PLH+19] to provide the initial estimate Xk ij(0) for each object.",
          "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [PLH+19].",
          "8": "7 PREPRINT Methods CDPN[lWJ19b] YOLO6D[RDGF16] PVNet[PLH+19] OURS ape 92.",
          "9": "Methods DPOD[ZSI19] Pix2Pose[PPV19] PVNet[PLH+19] OURS ape 22.",
          "10": "Methods DPOD[ZSI19] CDPN[lWJ19b] PVNet[PLH+19] OURS ape 53."
        },
        "End-to-end multi-instance robotic reaching from monocular vision": {
          "authors": [
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9561518/",
          "ref_texts": "[3] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "3"
          ],
          "1": "Algorithms that first regress an intermediate representation, such as image keypoints, and then compute object pose by solving a PnP problem [3], [4], [5], have achieved impressive performance on popular monocular pose estimation datasets including LINEMOD [6] and Occlusion LINEMOD [7]."
        },
        "Experimental Evaluation of Affordance Detection Applied to 6-DoF Pose Estimation for Intelligent Robotic Grasping of Household Objects": {
          "authors": [
            "Aidan Keaveny"
          ],
          "url": "https://uwspace.uwaterloo.ca/handle/10012/17716",
          "ref_texts": "[19] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNET: Pixel-wise Voting Network for 6DoF Pose Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 4556\u20134565, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "19"
          ],
          "4": "An alternative to learning sparse keypoints is offered by dense methods in Pixel-wise Voting Network (PVNet) [19], which predict unit vectors pointing to keypoints for each pixel.",
          "5": "The latter has been more effective in estimating pose under heavy occlusion [19].",
          "6": "Thus, the P nP algorithm is still widely used in many deep learning frameworks for 6-DoF pose estimation today and such frameworks can be deployed in real-time [18, 19]."
        },
        "Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation": {
          "authors": [
            "J Wu",
            "L Liu",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://arxiv.org/abs/2109.12266",
          "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "To use more reliable correspondence, PVNet [14] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints.",
          "2": "Since feature extraction is not the focus of this paper, we follow [14] to build a multi-scale convolutional neural network, to extract features at multiple resolutions.",
          "3": "Many recent works adopt RANSAC(RANdom SAmple Consensus) algorithm [24] or its variants to pick inliers [14] [16].",
          "4": "Implementation Details In inplementation, we follow [14] to select 9 keypoints for every object, and perform the same data augmentation.",
          "6": "For fair comparison, we take the 2D predicted keypoints from PVNet [14], and triangulate the two keypoints to 3D space by classic method [29].",
          "7": "It can be observed that compared with [14] and late-fusion approach, our method can accurately estimate the pose of objects, especially in some hard cases with occlusion."
        },
        "Learning Robot Skills from Few Demonstrations": {
          "authors": [
            "S Stev\u0161i\u0107"
          ],
          "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/478140/3/Stefan_Thesis.pdf",
          "ref_texts": "[131] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation\u201d, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u2013",
          "ref_ids": [
            "131"
          ],
          "1": "Contrary, papers for initial pose estimation use advanced techniques, such as RANSAC for keypoint selection [131] or masking for removing occlusions [132].",
          "2": "Inspired by this scheme, SoA methods for one-shot pose estimation rely on RANSAC to extract the most reliable points for prediction [122, 131].",
          "3": "1 Neural Network Model At the initial step, we obtain the object pose from an existing one-shot pose estimation algorithm, such as PVNet [131].",
          "4": "2 Evaluation Metric For evaluation, we use the standard ADD(-S) evaluation metric as is done in most 6D pose estimation papers [120\u2013122, 126, 131, 154].",
          "5": "The red object outlines show the initial pose obtained from PVNet [131].",
          "6": "We use the SoA in the one-shot setting, PVNet [131], for initialization."
        },
        "DEEP LEARNING FOR OBJECT DETECTION": {
          "authors": [
            "Akber Khan"
          ],
          "url": "https://trepo.tuni.fi/bitstream/handle/10024/135807/KhanAkberAli.pdf?sequence=4",
          "ref_texts": "[29] S. Peng, Y. Liu, Q. Huang, X. Zhou and H. Bao, \"PVNET: Pixel-wise voting network for 6dof pose estimation,\" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019 06-01, vol. 2019, pp. 4556\u20134565, 2019. ",
          "ref_ids": [
            "29"
          ],
          "1": "PVNet [29] is an example of an indirect voting-based technique and outperforms some of the earlier methods."
        },
        "A Survey on Deep Learning Based Methods and Datasets for Monocular 3D Object Detection. Electronics 2021, 10, 517": {
          "authors": [
            "SH Kim",
            "Y Hwang"
          ],
          "url": "https://pdfs.semanticscholar.org/076b/052fe9aa43e1f8619cc9e8aab29966a32f6d.pdf",
          "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
          "ref_ids": [
            "72"
          ],
          "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
          "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
          "3": "Overview of the keypoint localization in PVNet [72]."
        },
        "Learning Innovations for State Estimation": {
          "authors": [
            "G Kennedy",
            "J Gao",
            "Z Zhuang",
            "X Yu",
            "R Mahony"
          ],
          "url": "http://www.gerard-kennedy.com/files/iros-2021.pdf",
          "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "31"
          ],
          "1": "A recent popular approach is to first regress to an intermediate representation such as keypoints, from which pose can be obtained via 2D-3D correspondences and a PnP algorithm [36], [32], [31], [40], [41], [28], [17].",
          "2": "Of these approaches, some, including [36], [32] regress to a set of bounding box corners, while others regress to vector fields [31], [34], [40], or dense correspondences.",
          "3": "State Estimation for Object Pose and Depth Refinement For object pose estimation we choose PVNet [31] to be the baseline network that provides \u02c6X(0).",
          "4": "(9) The function h maps this vector field representation of keypoints to object pose via a RANSAC and uncertaintydriven PnP framework (EPnP), as discussed in [31].",
          "5": "For object pose estimation the baseline network is PVNet [31].",
          "6": "For object pose estimation we use a pretrained PVNet [31] to provide the initial estimate for each object.",
          "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [31]."
        },
        "Supplementary Material-FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation": {
          "authors": [
            "Y He",
            "H Huang",
            "H Fan",
            "Q Chen",
            "J Sun"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/He_FFB6D_A_Full_CVPR_2021_supplemental.pdf",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 3",
          "ref_ids": [
            "15"
          ],
          "1": "RGB RGB-D PoseCNN DeepIM [18, 10] PVNet[15] CDPN[11] DPOD[20] PointFusion[19] DenseFusion[17] G2LNet[1] PVN3D[5] Our FFB6D ape 77.",
          "2": "[9] Pix2Pose [14] PVNet [15] DPOD [20] Hu et al."
        },
        "Self-supervised Geometric Perception": {
          "authors": [
            "CMU RI"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_supplemental.pdf",
          "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019. 3",
          "ref_ids": [
            "12"
          ],
          "1": "Recent works such as YOLO6D [14], PVNet [12], and DPOD [16] can all serve as the student network, despite using different methodologies.",
          "2": ", by rendering synthetic projections of the 3D models under different simulated poses, which is common in [16, 12, 14, 2].",
          "3": "2There are many different ways to establish 2D-3D correspondences, see PVNet [12], YOLO6D [14] and references therein."
        },
        "\ub85c\ubd07 \ud314\uc744 \ud65c\uc6a9\ud55c \uc815\ub9ac\uc791\uc5c5\uc744 \uc704\ud55c \ubb3c\uccb4 \uc790\uc138\ucd94\uc815 \ubc0f \uc774\ubbf8\uc9c0 \ub9e4\uce6d": {
          "authors": [
            "Media Contents"
          ],
          "url": "https://jkros.org/xml/31147/31147.pdf",
          "ref_texts": "[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 2019, DOI: 10.1109/CVPR.2019.00469.",
          "ref_ids": [
            "3"
          ],
          "1": "\uae30\uc874\uc758 \uc790\uc138\ucd94\uc815 \uc54c\uace0\ub9ac\uc998 \uc911\uc5d0\uc11c \uc2ec\uce35\ud559\uc2b5\uc5d0 \uae30\ubc18\ud55c \uc5f0\uad6c\ub4e4[2,3]\uc774 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\uba74\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4."
        },
        "A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators": {
          "authors": [
            "C Sahin",
            "G Garcia-Hernando",
            "J Sock",
            "TK Kim"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885620300305",
          "ref_texts": "[160] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation , In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4561-4570), 2019.",
          "ref_ids": [
            "160"
          ],
          "4": "PVNet [160] estimates full 6D poses of the objects of interest under severe occlusion or truncation."
        },
        "Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation": {
          "authors": [
            "Yisheng He",
            "Wei Sun",
            "Haibin Huang",
            "Jianran Liu",
            "Haoqiang Fan",
            "Jian Sun"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.html",
          "ref_texts": "[37] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer V ision and P attern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "37"
          ],
          "3": "T o better deal with truncated and occluded scenes, [37] proposes a pixel-wise voting network to vote for the 2D keypoints location.",
          "4": "PVNet [37] uses per-pixel voting for 2D Keypoints to combine the advantages of Dense methods and keypoint-based methods.",
          "5": "Therefore, we follow [37] and use the farthest point sampling (FPS) algorithm to select keypoints on the mesh.",
          "6": "Also, we follow [37] and add synthesis images into our training set."
        },
        "6D pose estimation of objects: Recent technologies and challenges": {
          "authors": [
            "Z He",
            "W Feng",
            "X Zhao",
            "Y Lv"
          ],
          "url": "https://www.mdpi.com/2076-3417/11/1/228",
          "ref_texts": "32. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201323 June 2018.",
          "ref_ids": [
            "32"
          ],
          "1": "To solve this problem, Hu [32] et al."
        },
        "Honnotate: A method for 3d annotation of hand and object poses": {
          "authors": [
            "Shreyas Hampali",
            "Mahdi Rad",
            "Markus Oberweger",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.html",
          "ref_texts": "[41] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InThe IEEE Conference on Computer V ision and P attern Recognition (CVPR) , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "41"
          ],
          "1": "Some methods are now robust to partial occlusions [20, 36, 41], but many works rely on RGB-D data to handle this problem [5, 8, 22, 30], by fitting the 3D object model to depth data."
        },
        "Deep snake for real-time instance segmentation": {
          "authors": [
            "Sida Peng",
            "Wen Jiang",
            "Huaijin Pi",
            "Xiuli Li",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Peng_Deep_Snake_for_Real-Time_Instance_Segmentation_CVPR_2020_paper.html",
          "ref_texts": "[33] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "33"
          ],
          "1": "An alternative method is to use standard CNNs to regress a pixel-wise vector field from the input image to guide the evolution of the initial contour [37, 33, 40]."
        },
        "Epos: Estimating 6d pose of objects with symmetries": {
          "authors": [
            "Tomas Hodan",
            "Daniel Barath",
            "Jiri Matas"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Hodan_EPOS_Estimating_6D_Pose_of_Objects_With_Symmetries_CVPR_2020_paper.html",
          "ref_texts": "[50] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation.CVPR, 2019. 1, 2, 3",
          "ref_ids": [
            "50"
          ],
          "1": "Recent methods, which are mostly based on convolutional neural networks, produce dense correspondences [4, 48, 69] or predict 2D image locations of pre-selected 3D keypoints [52, 61, 50].",
          "2": "A popular approach is to establish 2D-3D correspondences by predicting the 2D projections of a fixed set of 3D keypoints, which are pre-selected for each object model, and solve for the object pose using PnP-RANSAC [52, 49, 47, 61, 65, 15, 29, 50]."
        },
        "Hybridpose: 6d object pose estimation under hybrid representations": {
          "authors": [
            "Chen Song",
            "Jiaru Song",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.html",
          "ref_texts": "[34] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation.CoRR, abs/1812.11788, 2018.",
          "ref_ids": [
            "34"
          ],
          "1": "While early works typically formulate pose estimation as end-to-end pose classification [39] or pose regression [16, 42], recent pose estimation methods usually leverage keypoints as an intermediate representation [38, 34], and align predicted 2D keypoints with ground-truth 3D keypoints.",
          "2": "To express the geometric information in an RGB image, a prevalent intermediate representation is keypoints, which achieves state-of-the-art performance [34, 32, 36].",
          "4": "To mitigate pose error, several works assign different weights to different predicted elements in the 2D-3D alignment stage [34, 32].",
          "6": "In our experiments, HybridPose incorporates an off-the-shelf architecture called PVNet [34], which is the state-of-the-art keypoint-based pose estimator that employs a voting scheme to predict both visible and invisible keypoints.",
          "10": "HybridPose outperforms PVNet [34], the backbone model we use to predict keypoints."
        },
        "H3dnet: 3d object detection using hybrid geometric primitives": {
          "authors": [
            "Z Zhang",
            "B Sun",
            "H Yang",
            "Q Huang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58610-2_19",
          "ref_texts": "26. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 4561\u20134570",
          "ref_ids": [
            "26"
          ],
          "1": "This regression methodology, which is motivated from the recent success of keypoint-based pose regression for 6D object pose estimation [19, 25, 11, 21, 26, 36], displays two appealing advantages for 3D object detection."
        },
        "Satellite pose estimation challenge: Dataset, competition design, and results": {
          "authors": [
            "M Kisantal",
            "S Sharma",
            "TH Park",
            "D Izzo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9076337/",
          "ref_texts": "[40] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral , 2019.",
          "ref_ids": [
            "40"
          ],
          "1": "While various DNN-based approaches have been proposed to perform pose estimation [30]\u2013[40], current state-of-the-art methods employ Convolutional Neural Networks (CNN) that either directly predict the 6D pose or an intermediate information that can be used to compute the 6D pose, notably a set of keypoints defined a priori.",
          "2": "Most recently, architectures like KPD [39] and PVNet [40] have been proposed to predict the locations of the 2D keypoints on the target\u2019s surface."
        },
        "Single-stage 6d object pose estimation": {
          "authors": [
            "Yinlin Hu",
            "Pascal Fua",
            "Wei Wang",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[36] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. InConference on Computer V ision and P attern Recognition , 2019.",
          "ref_ids": [
            "36"
          ],
          "2": "W e then demonstrate the generality of this network by combining it with two state-of-the-art correspondenceextraction frameworks [13, 36].",
          "3": "W e show that these single-stage frameworks systematically outperform the original twostage ones [13, 36], in terms of both accuracy and runtime.",
          "5": "2(b), our formalism can handle 3D point to 2D vector correspondences, which have been shown to be better-suited to use in conjunction with a deep network [36].",
          "6": "After establishing 3D-to-2D correspondences by some segmentation-driven CNN for 6D pose [13, 36], we use three main modules to infer the pose from these correspondence clusters directly: a local feature extraction module with shared network parameters, a feature aggregation module operating within the different clusters, and a global inference module consisting of simple fully-connected layers to estimate the final pose as a quaternion and a translation.",
          "7": "T o implement f, we use the recent encoder-decoder architecture of either [13] or [36].",
          "9": "W e take Ls to be the Focal Loss of [25], and Lk to be the regression term of either [13] or [36] depending on which of the two architectures we use.",
          "10": "Experiments W e compare our single-stage approach to more traditional but state-of-the-art two-stage frameworks [13, 36], first on synthetic data and then on real data from the challenging Occluded-LINEMOD [19] and YCB-V ideo [50] datasets.",
          "11": "Since one of the best current techniques [36] uses directions instead and infers poses from those using a voting-based PnP scheme, we feed the same 3D point to 2D vector correspondences to our own network.",
          "12": "3 noise level \u03c3 (outliers=30%) pose error Voting-based PnP Ours Figure 8: Comparison with PVNet\u2019s voting-based PnP [36].",
          "13": "For Occluded-LINEMOD, as in [41, 13, 36], we first use the Cut-and-Paste synthetic technique [6] to generate 20K images from LINEMOD data and random background data [51], with 4 to 10 different instances for each image.",
          "14": "Then, we generate 10K rendering images for each object type from the textured 3D mesh, as in [36].",
          "16": "[13] [13] + Ours [36] [36] + Ours Ape 12.",
          "17": "W e evaluate two state-ofthe-art correspondence-extraction networks: SegDriven [13] and PVNet [36], by replacing their original RANSAC-based post processing with our small network.",
          "18": "For both datasets, we use an input image resolution of 640 \u00d7 480 for both training and testing, as in [36].",
          "19": "1 Occluded-LINEMOD Results As discussed before, to demonstrate that our method is generic, we test it in conjunction with two correspondenceextraction networks SegDriven [13] and PVNet [36].",
          "20": "W e compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of both ADD-0.",
          "21": "W e compare the running times (in milliseconds) of PoseCNN [50], SegDriven [13], PVNet [36] and our method on a modern GPU (GTX1080 Ti).",
          "22": "W e compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of ADD-0.",
          "23": "In T able 2, we shown that our single-stage network outperform the state-of-the-art methods, PoseCNN [50], SegDriven [13] and PVNet [36].",
          "24": "2 YCB-Video Results T able 4 summarizes the results comparing against PoseCNN [50], SegDriven [13], and PVNet [36]."
        },
        "Learning canonical shape space for category-level 6d object pose and size estimation": {
          "authors": [
            "Dengsheng Chen",
            "Jun Li",
            "Zheng Wang",
            "Kai Xu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.html",
          "ref_texts": "[17] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InProc. CVPR , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "3": "PVNet [17] is a unique approach of feature point detection using CNNs: A vector field is estimated for the input RGB image based on which the feature points are voted."
        },
        "Self6d: Self-supervised monocular 6d object pose estimation": {
          "authors": [
            "G Wang",
            "F Manhardt",
            "J Shao",
            "X Ji",
            "N Navab"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58452-8_7",
          "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "44"
          ],
          "3": "On the other hand, as for training with real pose labels, we are again on par with other recently published methods such as PVNet [44] and CDPN [30] reporting a mean average recall of 86."
        },
        "G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features": {
          "authors": [
            "Wei Chen",
            "Xi Jia",
            "Hyung Jin",
            "Jinming Duan",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[29] Sida Peng, Y uan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation.arXiv preprint arXiv:1812.11788 , 2018.",
          "ref_ids": [
            "29"
          ],
          "7": "In experiments, we have found that our proposed method can make faster and more accurate predictions than the methods [29, 42, 4].",
          "9": "Method PVNet [29] PoseCNN + DeepIM [42, 21] DPOD [45] Frustum-P [30] Hinterstoisser [13] DenseFusion [40] Ours Input RGB RGB RGB RGB+Depth Depth RGB+Depth RGB+Depth Refinement \u00d7 \u2713 \u2713 (\u00d7) \u00d7 \u2713 \u2713 (\u00d7) \u00d7 Ape 43."
        },
        "End-to-end learnable geometric vision by backpropagating pnp optimization": {
          "authors": [
            "Bo Chen",
            "Alvaro Parra",
            "Jiewei Cao",
            "Nan Li",
            "Jun Chin"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.html",
          "ref_texts": "[36] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "36"
          ],
          "3": "W e provide the result of the current state-of-the-art PVNet [36] as a reference."
        },
        "Robust 6d object pose estimation by learning rgb-d features": {
          "authors": [
            "M Tian",
            "L Pan",
            "MH Ang",
            "GH Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9197555/",
          "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.",
          "ref_ids": [
            "7"
          ],
          "3": "Among them, [7] and [8] achieve top performances.",
          "7": "Of all those methods without pose refinement, PVNet [7] and Per-Pixel DF [13] are the state of the art on LINEMOD dataset."
        },
        "PointPoseNet: Point pose network for robust 6D object pose estimation": {
          "authors": [
            "Wei Chen",
            "Jinming Duan",
            "Hector Basevi",
            "Hyung Jin",
            "Ales Leonardis"
          ],
          "url": "http://openaccess.thecvf.com/content_WACV_2020/html/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.html",
          "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
          "ref_ids": [
            "22"
          ],
          "10": "From T able 2, we can see that our method outperforms its 2D counterpart PVNet [22], the baseline and other state-of-the-art methods, which shows that our method can better utilize 3D information from depth image."
        },
        "Monocular localization with vector HD map (MLVHM): A low-cost method for commercial IVs": {
          "authors": [
            "Z Xiao",
            "D Yang",
            "T Wen",
            "K Jiang",
            "R Yan"
          ],
          "url": "https://www.mdpi.com/1424-8220/20/7/1870",
          "ref_texts": "31. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019.",
          "ref_ids": [
            "31"
          ],
          "1": "In recent years, with the development of deep learning and the improvement of on-board computing ability, the information contained in an image can be interpreted down to pixel-level resolution [30], thereby enabling more complex image feature recognition such as semantic information extraction [31,32]."
        },
        "Pointvotenet: Accurate object detection and 6 dof pose estimation in point clouds": {
          "authors": [
            "F Hagelskj\u00e6r",
            "AG Buch"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9191119/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "6"
          ],
          "1": "The method in [6] uses a compromising approach where a semi-dense set of keypoints are predicted over the image.",
          "2": "[2,3,6], the training examples are gathered from real scenes, each annotated with one or more ground truth poses of objects.",
          "6": "The competing methods are SSD-6D [8], BB8 [2], PVNet [6], and DenseFusion [10]."
        },
        "Reconstruct locally, localize globally: A model free method for object pose estimation": {
          "authors": [
            "Ming Cai",
            "Ian Reid"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.html",
          "ref_texts": "[40] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR, 2019.",
          "ref_ids": [
            "40"
          ],
          "3": "PVNet [40] proposes a method that automatically discovers a set of keypoints on the 3D object surface based on the physical structure, to ensure that their 2D projection are all within the silhouette.",
          "6": "Our method outperforms more than half of the learning-based methods and achieves comparable result with the state-of-the-art method, which use a large amount of synthetic training images from new viewpoints [40] and/or 3D model for refinement [52, 27]."
        },
        "6dof object pose estimation via differentiable proxy voting loss": {
          "authors": [
            "X Yu",
            "Z Zhuang",
            "P Koniusz",
            "H Li"
          ],
          "url": "https://arxiv.org/abs/2002.03923",
          "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4561\u20134570, 2019.",
          "ref_ids": [
            "21"
          ],
          "5": "Rather than only estimating a centroid, PVNet [21] votes several features of interest, while the work [12] votes the corners of a 3D boundingbox from each segmentation grid.",
          "6": "Since voting based methods [21, 30] have demonstrated their robustness to occlusions and view changes, we therefore follow the voting based pose estimation pipeline.",
          "8": "3 Network architecture and training strategy To demonstrate the effectiveness of our proposed loss, we adopt the same architecture as PVNet [21], as illustrated in Fig."
        },
        "3d object detection and pose estimation of unseen objects in color images with local surface embeddings": {
          "authors": [
            "Giorgia Pitteri",
            "Aurelie Bugeau",
            "Slobodan Ilic",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content/ACCV2020/html/Pitteri_3D_Object_Detection_and_Pose_Estimation_of_Unseen_Objects_in_ACCV_2020_paper.html",
          "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pix el-Wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision a nd Pattern Recognition. (2019) 4561\u20134570",
          "ref_ids": [
            "20"
          ],
          "1": "Another approach, aiming to be more robust to occlusions, is to predict for each pixel offsets to the reprojections of 3D points related to the object [7,19,20]."
        },
        "TANet: towards fully automatic tooth arrangement": {
          "authors": [
            "G Wei",
            "Z Cui",
            "Y Liu",
            "N Chen",
            "R Chen",
            "G Li"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58555-6_29",
          "ref_texts": "25. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "25"
          ],
          "1": "It aims to infer the three-dimensional pose, which has six degrees of freedom, of an object present in an RGB image, [3, 7, 45, 5, 33, 34, 18, 25], RGB-D image [39, 40, 35], or point cloud data [26, 44, 29, 30]."
        },
        "Occlusion-aware region-based 3D pose tracking of objects with temporally consistent polar-based local partitioning": {
          "authors": [
            "L Zhong",
            "X Zhao",
            "Y Zhang",
            "S Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9003503/",
          "ref_texts": "[46] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u2013",
          "ref_ids": [
            "46"
          ],
          "1": "Apart from template matching, a lot of deep learning-based 3D object detection methods have been proposed recently [41]\u2013[46].",
          "2": "These methods train deep neural networks either to directly predict the pose parameters [41]\u2013[43], or to first predict the keypoint locations and then estimate the 6-DOF pose via the PnP algorithm [44]\u2013[46].",
          "3": "When GPU is available, deep learning-based methods (such as [44], [46]) could be incorporated for better performance."
        },
        "Super-BPD: Super boundary-to-pixel direction for fast image segmentation": {
          "authors": [
            "Jianqiang Wan",
            "Yang Liu",
            "Donglai Wei",
            "Xiang Bai",
            "Yongchao Xu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Wan_Super-BPD_Super_Boundary-to-Pixel_Direction_for_Fast_Image_Segmentation_CVPR_2020_paper.html",
          "ref_texts": "[32] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. InProc. of CVPR , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "32"
          ],
          "1": "PifPaf [24] and PVNet [32] leverage direction cue for 2D human pose estimation and 6 DoF pose estimation, respectively."
        },
        "Deepurl: Deep pose estimation framework for underwater relative localization": {
          "authors": [
            "B Joshi",
            "M Modasshir",
            "T Manderson"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9341201/",
          "ref_texts": "[32] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proc. CVPR, 2019.",
          "ref_ids": [
            "32"
          ],
          "1": "[32] used a pixel-wise voting network to regress pixel-wise unit vectors pointing to the keypoints and used these vectors to vote for keypoint locations using RANSAC.",
          "3": "[15] and PVNet [32] trained on a synthetic dataset and tested on a real pool dataset.",
          "4": "[15] and PVNet [32] in terms of rotation and translation errors along with REP-10px and ADD-0.",
          "5": "Moreover, the runtime performance is realtime, outperforming PVNet [32] and only slightly slower than that of Tekin et al.",
          "7": "We also present a detection bounding box based keypoint sampling strategy that is more robust to related work [15], [32] which leads to a better estimate of the pose of the observed robot, up to an order of magnitude is some cases; see Table I."
        },
        "MaskedFusion: Mask-based 6D object pose estimation": {
          "authors": [
            "N Pereira",
            "LA Alexandre"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9356139/",
          "ref_texts": "17. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "17"
          ],
          "3": "One of the most accurate method in 6D pose using RGB images is PVNet [17]."
        },
        "Edge enhanced implicit orientation learning with geometric prior for 6D pose estimation": {
          "authors": [
            "Y Wen",
            "H Pan",
            "L Yang",
            "W Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9126189/",
          "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "27"
          ],
          "1": "While [28] and [33] use bounding box corners as keypoints, a recent work [27] explores using designated surface keypoints for more robust 2D keypoint localization."
        },
        "How to track your dragon: A multi-attentional framework for real-time rgb-d 6-dof object pose tracking": {
          "authors": [
            "I Marougkas",
            "P Koutras",
            "N Kardaris",
            "G Retsinas"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_45",
          "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "31"
          ],
          "1": "In PVNet [31], Peng et al."
        },
        "Lit: Light-field inference of transparency for refractive object localization": {
          "authors": [
            "Z Zhou",
            "X Chen",
            "OC Jenkins"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9113653/",
          "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "Other end-to-end method methods have explored using synthetic data in training [3], [13], pixel-wise voting over keypoints [14], [15], and residual networks to iteratively refine object poses [5], [2].",
          "2": "In addition, the center point estimation branch does not regress multiple keypoints which is common in texture-rich object pose estimation networks [14], [15]."
        },
        "Neural object learning for 6d pose estimation using a few cluttered images": {
          "authors": [
            "K Park",
            "T Patten",
            "M Vincze"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58548-8_38",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 2, 3",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23,24,28].",
          "2": "To overcome this limitation, both real images and synthetic images are used for training [23,24,28,45], which currently achieves state-of-the-art performance."
        },
        "6 dof pose estimation of textureless objects from multiple rgb frames": {
          "authors": [
            "R Kaskman",
            "I Shugurov",
            "S Zakharov",
            "S Ilic"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_41",
          "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "40"
          ],
          "1": "In Pixel-wise Voting Network (PVNet) [40].",
          "2": "The network outputs per-seed point classification labels as well as estimation of the keypoint direction vectors, which are further used for the RANSAC-based voting for the object keypoint locations, similar to PVNet [40]."
        },
        "Spatial attention improves iterative 6D object pose estimation": {
          "authors": [
            "S Stev\u0161i\u010d",
            "O Hilliges"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9320380/",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2, 3, 5, 6, 16, 17",
          "ref_ids": [
            "21"
          ],
          "1": "To solve this challenging task, a variety of methods such as BB8 [22], PoseCNN [28], PVNet [21] and DPOD [29] have been proposed recently.",
          "2": "The 6D pose estimation task is typically separated into two subtasks: initial detection and pose estimation of objects from raw images [21, 10, 12, 28, 22] and subsequent pose refinement [20, 29].",
          "3": "With modern CNN architectures, the one-shot pose estimation sub-task has saturated somewhat [22, 28, 21, 29, 12] but such methods have not yet been able to precisely align spatial details, leaving much room for improvement via iterative pose refinement, which is the focus of our work.",
          "4": "Inspired by this scheme, SoA methods for one-shot pose estimation rely on RANSAC to extract the most reliable points for prediction [21, 29].",
          "5": "Thus, the accuracy of estimation on occlusion data is far below the non-occluded case [21, 29].",
          "6": "State-of-the-art methods [29, 21] do not use hand-designed keypoints.",
          "7": "Contrary, papers for initial pose estimation use advanced techniques, such as RANSAC for keypoint selection [21] or masking for removing occlusions [12].",
          "8": "Neural Network Model At the initial step, we obtain the object pose from an existing one-shot pose estimation algorithm, such as PVNet [21].",
          "9": "Evaluation Metric For evaluation, we use the standard ADD(-S) evaluation metric as is done in most 6D pose estimation papers [21, 20, 29, 17, 11, 1].",
          "10": "The red object outlines show the initial pose obtained from PVNet [21].",
          "11": "We use the SoA in the one-shot setting, PVNet [21], for initialization.",
          "12": "Note that here the accuracy of the initialization method is significantly lower than PVNet [21] which implies that some initial estimates maybe outside of the distribution used during training of our method.",
          "13": "1, 3, 4, 5, 6 [21] S.",
          "14": "For initialization, we use PVNet [21].",
          "15": "For initialization, we use PVNet [21]."
        },
        "Dronepose: photorealistic uav-assistant dataset synthesis for 3d pose estimation via a smooth silhouette loss": {
          "authors": [
            "G Albanis",
            "N Zioulis",
            "A Dimou",
            "D Zarpalas"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-66096-3_44",
          "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "48"
          ],
          "1": "PVNet [48] votes for keypoints, which are then used to estimate the object\u2019s 6DOF pose using PnP."
        },
        "Prima6d: Rotational primitive reconstruction for enhanced and robust 6d pose estimation": {
          "authors": [
            "MH Jeon",
            "A Kim"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9123683/",
          "ref_texts": "[1] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d 12 2018.",
          "ref_ids": [
            "1"
          ],
          "2": "The latter approaches reported current state-of-the-art (SOTA) performance by leveraging a two-step approach for pose estimation [1, 7, 8].",
          "3": "To alleviate this issue, PVNet [1] predicted the unit vectors that point to predefined keypoints for each pixel in an object.",
          "4": "Evaluation Metric (i) 2D Projection error metric To evaluate the pose estimation in terms of the 2D projection error we also use the same metric as in [1] and measure the mean pixel distance between projection of the 3D model and the image pixel points.",
          "5": "We chose the average distance metric (ADD) [28] as the evaluation metric, which computes the distance of transformed 3D model points methods Holistic Approach PnP based Approach w/o refinement w/ refinement PoseCNN [19] Deep-6D Pose [16]BB8 [11] Tekin [12] Pix2Pose [15] DPOD [26] PVNet [1] PrimA6D-S PrimA6D-SRBB8 Tekin HybridPose [27] ape\u2020 38."
        },
        "Robust rgb-based 6-dof pose estimation without real pose annotations": {
          "authors": [
            "Z Li",
            "Y Hu",
            "M Salzmann",
            "X Ji"
          ],
          "url": "https://arxiv.org/abs/2008.08391",
          "ref_texts": "23. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
          "ref_ids": [
            "23"
          ],
          "5": "4 PVNet[23] 43.",
          "6": "4 PVNet[23] 15."
        },
        "Multi-view shape estimation of transparent containers": {
          "authors": [
            "A Xompero",
            "R Sanchez-Matilla",
            "A Modas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9054112/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d inProc. IEEE Conf. Comput. Vis. Pattern Recognit., Long Beach, CA, USA, 16\u201320 June 2019.",
          "ref_ids": [
            "6"
          ],
          "4": "Pixel-wise V oting Network (PVNet) [6] estimates the pose of occluded or truncated objects with an uncertainty-driven PnP, learning a vector-field representation to localise a sparse set of 2D keypoints and their spatial uncertainty."
        },
        "Towards an egocentric framework for rigid and articulated object tracking in virtual reality": {
          "authors": [
            "C Taylor",
            "R McNicholas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9090593/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Comp. Vision and Pattern Recognition, 2019.",
          "ref_ids": [
            "24"
          ],
          "1": "Works such as PoseCNN [39] and PVNet [24] demonstrate accurate 6DoF pose predictions from RGB images, even in complex, uncontrolled environments [1, 21, 38, 40]."
        },
        "3d-aware ellipse prediction for object-based camera pose estimation": {
          "authors": [
            "M Zins",
            "G Simon",
            "MO Berger"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9320405/",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.",
          "ref_ids": [
            "24"
          ],
          "1": "Many works exist on this subjects [12, 18, 24, 25, 30, 31, 36]."
        },
        "Pixel-pair occlusion relationship map (p2orm): formulation, inference and application": {
          "authors": [
            "X Qiu",
            "Y Xiao",
            "C Wang",
            "R Marlet"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58548-8_40",
          "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019) P2ORM: Formulation, Inference & Application 17",
          "ref_ids": [
            "36"
          ],
          "1": "Besides the joint treatment of occlusion when developing techniques for specific tasks [40,19,54,36,35,37,18], task-independent occlusion reasoning [42,24,49,53,51,30] offers valuable occlusion-related features for high-level scene understanding tasks."
        },
        "Fully convolutional geometric features for category-level object alignment": {
          "authors": [
            "Qiaojun Feng",
            "Nikolay Atanasov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9341550/",
          "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation,\u201d inIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019, pp.",
          "ref_ids": [
            "13"
          ],
          "1": "PVNet [13] estimates sparse object keypoints on the RGB image via voting mechanism."
        },
        "Deep soft procrustes for markerless volumetric sensor alignment": {
          "authors": [
            "V Sterzentsenko",
            "A Doumanoglou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9089541/",
          "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR, pp. 4561\u20134570, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "PVNet [30] densely regresses vectors pointing at the keypoints to improve robustness to occlusions."
        },
        "End-to-end differentiable 6DoF object pose estimation with local and global constraints": {
          "authors": [
            "A Gupta",
            "J Medhi",
            "A Chattopadhyay"
          ],
          "url": "https://arxiv.org/abs/2011.11078",
          "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "9"
          ],
          "2": "Their end-to-end trainable model showed improved results compared to the two stage approach as validated with two state of the art correspondence estimators [8] [9].",
          "3": "We follow up on their model with [9] as the correspondence estimator as it shows superior performance and refer to it as SSPE."
        },
        "Pose proposal critic: Robust pose refinement by learning reprojection errors": {
          "authors": [
            "L Brynte",
            "F Kahl"
          ],
          "url": "https://arxiv.org/abs/2005.06262",
          "ref_texts": "[18] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
          "ref_ids": [
            "18"
          ],
          "2": "[18] does indeed prove robust to partial occlusions, and yields accurate estimates of rotation as well as lateral translation on the Occlusion LINEMOD benchmark.",
          "6": "On the Occlusion LINEMOD benchmark, we initialize our method with pose proposals from PVNet [18], yielding state-of-the-art results for two out of three metrics on this competitive benchmark, while performing on-par with previous methods for the third metric.",
          "7": "Despite the sub-optimal pose proposals from PVNet [18], the poses are accurately recovered."
        },
        "3D point-to-keypoint voting network for 6D pose estimation": {
          "authors": [
            "W Hua",
            "J Guo",
            "Y Wang",
            "R Xiong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9305322/",
          "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
          "ref_ids": [
            "6"
          ],
          "3": "To overcome this problem, PVNet [6] selects kepoints using the farthest point sampling (FPS) algorithm.",
          "4": "Inspired by recent 2D methods [5], [6], we estimate pose by 3D keypoints instead of regression directly.",
          "5": "Before training, FPS algorithm is employed to sample K 3D keypoints on CAD model surface like PVNet [6]."
        },
        "End-to-end learning improves static object geo-localization in monocular video": {
          "authors": [
            "M Chaabane",
            "L Gueguen",
            "A Trabelsi"
          ],
          "url": "https://arxiv.org/abs/2004.05232",
          "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "32"
          ],
          "1": "3 5D Pose Estimation Many state-of-the-art methods for object pose estimation [30, 31, 32, 33] use 3D models of the objects."
        },
        "Pam: Point-wise attention module for 6d object pose estimation": {
          "authors": [
            "M Song",
            "J Lee",
            "D Kim"
          ],
          "url": "https://arxiv.org/abs/2008.05242",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
          "ref_ids": [
            "20"
          ],
          "1": "In [16], [17], [18], [19], [20], [21], [22] regressed the coordinates of sparse 2D keypoints by learning features from RGB using CNN structure.",
          "2": "[20] to compensate for the occlusion and truncation, which are the problems covered by the sparse keypoints and heatmap method above, an attempt was made to find the keypoints by representing the direction toward the keypoints in pixel-wise."
        },
        "Pose estimation of specular and symmetrical objects": {
          "authors": [
            "J Hu",
            "H Ling",
            "P Parashar",
            "A Naik"
          ],
          "url": "https://arxiv.org/abs/2011.00372",
          "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "17"
          ],
          "1": "Deep learning method Recently, deep learning methods[17][9][7][18] have been used to detect robust keypoint."
        },
        "Kosnet: A unified keypoint, orientation and scale network for probabilistic 6d pose estimation": {
          "authors": [
            "K Hashimoto",
            "DN Ta",
            "E Cousineau",
            "R Tedrake"
          ],
          "url": "https://groups.csail.mit.edu/robotics-center/public_papers/Hashimoto20.pdf",
          "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "19"
          ],
          "1": "More recent works focus on fixing this problem by using local patches to reduce the effect of occlusion [17] or adding a segmentation head to aggregate information only from pixels in the object regions [18], [19].",
          "2": "Several other works [15], [19] realize the benefits of heat maps of keypoints in enabling probabilistic fusion."
        },
        "3DPVNet: Patch-level 3D Hough voting network for 6D pose estimation": {
          "authors": [
            "Y Liu",
            "J Zhou",
            "Y Zhang",
            "C Ding",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2009.06887",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4561\u20134570, 2019. 1, 2",
          "ref_ids": [
            "26"
          ],
          "1": "5D images, such as PVNet [26].",
          "2": "Existing methods [26, 36, 11] employ point/pixel-wise voting, which account for a large amount of computation redundancy.",
          "3": "PVNet [26] predicts vectors pointing to 2D keypoints, forming dense correspondences between 2D and 3D data.",
          "4": "Our work is inspired by PVNet [26] and V oteNet [27], and employs deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation."
        },
        "Model-free Bin-Picking: Food Processing and Parcel Processing Use Cases": {
          "authors": [
            "N Castaman",
            "A Cenzato",
            "S Tonello",
            "E Menegatti"
          ],
          "url": "https://i-rim.it/wp-content/uploads/2020/12/I-RIM_2020_paper_153.pdf",
          "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "5"
          ],
          "1": "This problem, often referred to as random bin picking, rely on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [6], with an increasingly trend towards datadriven approaches based on deep models [3], [5]."
        },
        "Instance-specific 6-dof object pose estimation from minimal annotations": {
          "authors": [
            "RP Singh",
            "I Kumagai",
            "A Gabas"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9026239/",
          "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "2"
          ],
          "3": "The more recent PVNet method [2] regresses to vector fields for each of the pre-defined object keypoints and uses these vectors to vote for keypoint locations using RANSAC."
        },
        "Automatic 3D Object Recognition and Localization for Robotic Grasping": {
          "authors": [
            "BMSE Santo"
          ],
          "url": "https://search.proquest.com/openview/a645f0a6e44b8f38da9372b7d267a074/1?pq-origsite=gscholar&cbl=2026366&diss=y",
          "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019. URL: http://dx.doi.org/10.1109/CVPR.",
          "ref_ids": [
            "29"
          ],
          "3": "It is concluded that PVNet achieves the best results in terms of the ADD(-S) metric when compared to other approaches (Table 3, 5, and 7 in [29])."
        },
        "Fall and activity detection framework on a robotic platform for older person care": {
          "authors": [
            "Frederico Belmonte"
          ],
          "url": "https://plymouth.researchcommons.org/cgi/viewcontent.cgi?article=1100&context=secam-theses",
          "ref_texts": "[123] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. arXiv:1812.11788 [cs], December 2018.",
          "ref_ids": [
            "123",
            "cs"
          ],
          "1": "Additional useful information may be available by considering object 6D information [71, 185, 123] and even concurrently with CNN skeleton pose data [169]."
        },
        "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images": {
          "authors": [
            "M Vincze"
          ],
          "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490630.pdf",
          "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4556\u20134565 (June 2019)",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23, 24, 28].",
          "2": "To overcome this limitation, both real images and synthetic images are used for training [23, 24, 28, 45], which currently achieves state-of-the-art performance."
        },
        "Supplement to \u201cPose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors\u201d": {
          "authors": [
            "L Brynte",
            "F Kahl"
          ],
          "url": "https://research.chalmers.se/publication/538336/file/538336_AdditionalFile_dad0f2b5.pdf",
          "ref_texts": "[9] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 9",
          "ref_ids": [
            "9"
          ],
          "1": "Despite the sub-optimal pose proposals from PVNet [9], the poses are accurately recovered.",
          "5": "1 Negative Depth Correction of Pose Proposals We observed that the pose proposals from PVNet [9] sometimes have negative depth, and in this case we switched sign for the object center position (in the camera frame), and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera).",
          "6": "This correction is done both when reporting the results of [9], and when reporting the results of our refinement."
        },
        "Review on 6D Object Pose Estimation With the Focus on Indoor Scene Understanding. 2022\u037e 2 (4): 41": {
          "authors": [
            "N Nejatishahidin",
            "P Fayyazsanavi"
          ],
          "url": "https://www.oajaiml.com/uploads/archivepdf/24821141.pdf",
          "ref_texts": "[41] Peng S, Liu Y , Huang Q, Zhou X, Bao H. Pvnet: Pixel-Wise V oting Network for 6DOF Pose Estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019:4561-4570.",
          "ref_ids": [
            "41"
          ],
          "2": "To address the occlusion problem for keypoint detection, PVNet [41] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
          "4": "The promise of voting-based techniques [41, 72] are being robust to these challenges."
        },
        "Learning to Estimate 3D Object Pose from Synthetic Data": {
          "authors": [
            "Sergey Zakharov"
          ],
          "url": "https://mediatum.ub.tum.de/1550255",
          "ref_texts": "[93] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "93"
          ],
          "5": "If trained on real data, our method is the second best after [93].",
          "7": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [40, 101], by a large margin and performs similarly to [93]."
        },
        "Dpod: 6d pose object detector and refiner": {
          "authors": [
            "Sergey Zakharov",
            "Ivan Shugurov",
            "Slobodan Ilic"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html",
          "ref_texts": "[25] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 4561\u20134570, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "Recent deep learning-based approaches, such as SSD6D [15], YOLO6D [33], AAE [31], PoseCNN [34] and PVNet [25], are the current top performers for this task in RGB images.",
          "4": "Among the methods that are specifically designed to be robust to occlusions we would like to highlight iPose [14], PoseCNN [34], and PVNet [25].",
          "9": "If trained on real data, our method is the second best after [25].",
          "11": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [33, 34], by a large margin and performs similarly to [25]."
        },
        "Satellite pose estimation with deep landmark regression and nonlinear pose refinement": {
          "authors": [
            "Bo Chen",
            "Jiewei Cao",
            "Alvaro Parra",
            "Jun Chin"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.html",
          "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019.",
          "ref_ids": [
            "26"
          ],
          "1": "Inspired by works that combine the strength of deep neural networks and geometric optimisation [26, 25, 35], our approach contains three main components: 1.",
          "2": "While the keypoint matching problem can be solved using machine learning, deep CNN-based feature learning methods typically fix the 2D-3D keypoint associations and learn to predict the image locations of each corresponding 3D keypoint such as [26, 25, 35]."
        },
        "Towards robust learning-based pose estimation of noncooperative spacecraft": {
          "authors": [
            "TH Park",
            "S Sharma",
            "S D'Amico"
          ],
          "url": "https://arxiv.org/abs/1909.00392",
          "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201dThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral, 2019.",
          "ref_ids": [
            "21"
          ],
          "1": "[21] S."
        },
        "On object symmetries and 6d pose estimation from images": {
          "authors": [
            "G Pitteri",
            "M Ramamonjisoa",
            "S Ilic"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/8885689/",
          "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 1, 2, 6",
          "ref_ids": [
            "22"
          ],
          "1": "Introduction 3D object detection and pose estimation are of primary importance for tasks such as robotic manipulation, virtual and augmented reality and they have been the focus of intense research in recent years, mostly due to the advent of Deep Learning based approaches and the possibility of using large datasets for training such methods [12, 7, 17, 23, 27, 16, 29, 22].",
          "2": "[21, 22] predict the 2D projections of 3D points from image patches or local features, to avoid the effects of occluders when performing the prediction.",
          "3": "We chose to predict the objects\u2019 6D poses in the form of the 2D reprojections of the 8 corners of the 3D bounding boxes, as in [23, 27, 28, 22] for simplicity."
        },
        "ConvPoseCNN: Dense convolutional 6D object pose estimation": {
          "authors": [
            "C Capellen",
            "M Schwarz",
            "S Behnke"
          ],
          "url": "https://arxiv.org/abs/1912.07333",
          "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
          "ref_ids": [
            "22"
          ],
          "1": "[22] also removed the RoIpooled orientation prediction branch, but with a different method: Here, 2D directions to a fixed number of keypoints are densely predicted."
        },
        "Cullnet: Calibrated and pose aware confidence scores for object pose estimation": {
          "authors": [
            "Kartik Gupta",
            "Lars Petersson",
            "Richard Hartley"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.html",
          "ref_texts": "[17] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition, pages 4561\u20134570, 2019.",
          "ref_ids": [
            "17"
          ],
          "1": "Recently proposed PV -Net [17] tries to address the problem of partial occlusion in RGB based object pose estimation by regressing for dense pixel-wise unit vectors pointing to the keypoints, which are combined together using RANSAC like voting scheme."
        },
        "CorNet: generic 3D corners for 6D pose estimation of new objects without retraining": {
          "authors": [
            "Giorgia Pitteri",
            "Slobodan Ilic",
            "Vincent Lepetit"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCVW_2019/html/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.html",
          "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Figure 9: Some qualitative results on Object #20 in Scene #13 of the T -LESS dataset. Figure 10: Some qualitative results on Object #20 in Scene #14 of the T -LESS dataset. Figure 11: Some qualitative results on Object #26 and Object #29 in Scene #15 of the T -LESS dataset. Pixel-Wise V oting Network for 6DoF Pose Estimation. CoRR, abs/1812.11788, 2018.",
          "ref_ids": [
            "24"
          ],
          "1": "It is therefore often desirable to rely on color images, and many methods to do so have been proposed recently [19, 25, 31, 18, 35, 24].",
          "2": "Also focusing on occlusion handling, PVNet [24] proposed a network that for each pixel regresses an offset to predefined keypoints.",
          "3": "Somewhat related to our approach, [18, 4, 37, 24] first predict the 3D coordinates of the image locations lying on the objects, in the object coordinate system, and predict the 3D object pose through hypotheses sampling with preemptive RANSAC."
        },
        "W-posenet: Dense correspondence regularized pixel pair pose regression": {
          "authors": [
            "Z Xu",
            "K Chen",
            "K Jia"
          ],
          "url": "https://arxiv.org/abs/1912.11888",
          "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "Intuitively, compared to sparse semantic keypoints pre-defined in keypoint-based methods [16], [17], [18], [19], [20], dense correspondence mapping in our scheme treats each pixel as a keypoint to regress its corresponding 3D coordinate in object model space, which makes each pixel-wise feature more discriminative, and thus our pixel-wise feature encoding is more robust to occlusion.",
          "2": "Recently, PVNet [20] is proposed to detect keypoints via voting on pixel-wise predictions of the directional vector that points to keypoints and is robust to truncation and occlusion."
        },
        "Car Pose in Context: Accurate Pose Estimation with Ground Plane Constraints": {
          "authors": [
            "P Li",
            "W Qiu",
            "M Peven",
            "GD Hager",
            "AL Yuille"
          ],
          "url": "https://arxiv.org/abs/1912.04363",
          "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 2",
          "ref_ids": [
            "36"
          ],
          "1": "More recent methods, including SOTA method [36] in 6-DoF pose estimation, adopts a two-stage method."
        },
        "Evaluating Computer Vision Methods for Detection and Pose Estimation of Textureless Objects": {
          "authors": [
            "Name Last"
          ],
          "url": "https://uis.brage.unit.no/uis-xmlui/handle/11250/2620242",
          "ref_texts": "[10] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixelwise voting network for 6dof pose estimation.CoRR, abs/1812.11788, 2018. URL http://arxiv.org/abs/1812.11788.",
          "ref_ids": [
            "10"
          ],
          "1": "2 Pose estimation techniques DeepIM [8], Keypoint detector localization [9] and PVNet [10] are examples of the current cutting edge pose estimation methods."
        }
      }
    },
    {
      "title": "loftr: detector-free local feature matching with transformers",
      "id": 1,
      "valid_pdf_number": "422/525",
      "matched_pdf_number": "346/422",
      "matched_rate": 0.8199052132701422,
      "citations": {
        "Matrix3D: Large Photogrammetry Model All-in-One": {
          "authors": [
            "Y Lu",
            "J Zhang",
            "T Fang",
            "JD Nahmias",
            "Y Tsin"
          ],
          "url": "https://arxiv.org/abs/2502.07685",
          "ref_texts": "[96] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "96"
          ],
          "1": "Recent advances have focused on enhancing the robustness of SfM through learning-based feature extractors [21, 24, 65, 80], improved image-matching techniques [12, 56, 86, 96], and neural bundle adjustment [53, 55, 108, 117]."
        },
        "R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization": {
          "authors": [
            "X Jiang",
            "F Wang",
            "S Galliani",
            "C Vogel"
          ],
          "url": "https://arxiv.org/abs/2501.01421",
          "ref_texts": "[69] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 4, 5, 7, 12, 14",
          "ref_ids": [
            "69"
          ],
          "2": "We investigate pretrained feature extractors for both dense and sparse matching methods, such as LoFTR [69] and Dedode [25].",
          "8": "4, for large scale indoor scenes with small illumination changes, alternative off-the-shelf local feature extractors [25, 69] achieve similar or even superior performance compared to the original ACE [9].",
          "10": "6, reducing the feature dimensionality to 128 dimensions preserves over 90% of the variance for different local encoders [9, 25, 69] on various datasets [37, 60, 63]."
        },
        "DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection": {
          "authors": [
            "J Edstedt",
            "G B\u00f6kman",
            "M Wadenb\u00e4ck"
          ],
          "url": "https://arxiv.org/abs/2503.07347",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 1, 7",
          "ref_ids": [
            "37"
          ],
          "2": "MegaDepth1500: MegaDepth1500 is a standard benchmark for image matching introduced in LoFTR [37]."
        },
        "Fmrt: Learning accurate feature matching with reconciliatory transformer": {
          "authors": [
            "L Wang",
            "X Zhang",
            "Z Jiang",
            "K Dai",
            "T Xie"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10872979/",
          "ref_texts": "[36] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "36"
          ],
          "1": "Recently, Transformer [41] has achieved excellent performance in several computer vision tasks due to its strong capability to model long-range dependencies, which inspires the researchers to leverage Transformer to boost matching accuracy [36]\u2013[39].",
          "2": "As a representative work, LoFTR [36] leverages the self and cross-attention mechanism in Linear Transformer [42] to enhance the visual descriptors of keypoints with manageable computational cost.",
          "4": "Recently, after witnessing the success of SuperGlue, the cutting-edge detector-free methods [36]\u2013[39], [56], [57], [59] focuses on realizing global consensus with the help of Transformer.",
          "5": "As a representative work, LoFTR [36] utilizes Linear Transformer to model long-range global dependencies and update features of all keypoints, thereby achieving outstanding performance and ensuring manageable computation costs.",
          "6": "Current detectorfree approaches [36], [38] utilize absolute sinusoidal positional encoding to incorporate the positional information into the visual descriptors of keypoints.",
          "8": "Coarse Matches Block (CMB) Given L \u00afFseq A and L \u00afFseq B , we utilize the inner product to obtain score matrix S \u2208 RN\u00d7N , which are processed by dualsoftmax [36] to derive confidence matrix G.",
          "10": "Following [36], we resize the resolution of the images to 840 \u00d7 840.",
          "11": "Following [36], we resolution the AUC values of the pose errors under three thresholds (5\u25e6, 10\u25e6, 20\u25e6).",
          "15": "54 Follow [36], [37], we utilize the DUC1 and DUC2 that report the percentage of images with localization errors less than thresholds as metrics.",
          "16": "Methods AUC@5\u25e6 AUC@10\u25e6 AUC@20\u25e6 Positional Encoding in [36]56.",
          "17": "To verify the proposed Axis-Wise Position Encoder (AWPE), we compared it with other positional encoding methods proposed in [36] and [19]."
        },
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views": {
          "authors": [
            "S Zhang",
            "J Wang",
            "Y Xu",
            "N Xue",
            "C Rupprecht"
          ],
          "url": "https://arxiv.org/abs/2502.12138",
          "ref_texts": "[67] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "67"
          ],
          "1": "These improvements focus on three main areas: developing learning-based feature descriptors [12, 13, 96], learning more accurate matching algorithms [54, 67], and implementing differentiable bundle adjustment [36, 75]."
        },
        "XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications": {
          "authors": [
            "S Zhai",
            "N Wang",
            "X Wang",
            "D Chen",
            "W Xie",
            "H Bao"
          ],
          "url": "https://arxiv.org/abs/2502.01297",
          "ref_texts": "[44] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8922\u20138931, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "Additionally, other approaches have leveraged deep learning techniques, such as LoFTR [44] and SuperGlue [40], to learn feature representations and matching strategies that are robust to textureless regions or repetitive patterns."
        },
        "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360 Cameras": {
          "authors": [
            "D Jung",
            "J Choi",
            "Y Lee",
            "D Manocha"
          ],
          "url": "https://arxiv.org/abs/2502.12545",
          "ref_texts": "[53] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "53"
          ],
          "1": "Recently, detector-free or dense matchers [14, 15, 42, 53, 56, 57] have emerged as a promising alternative to detector-based approaches [12, 48, 58], particularly in handling repetitive or indiscriminate regions where keypoint detection performance tends to degrade.",
          "2": "Detectorfree or dense matching methods [14, 15, 53] is proposed to solve this issue by estimating dense feature matches at pixel level.",
          "3": "DetectorFreeSfM [24] builds these detectorfree matches [53] and refines the tracks and geometry of the coarse SfM model by enforcing multi-view consistency."
        },
        "A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization": {
          "authors": [
            "Y Zhang",
            "S Wang",
            "J Kannala"
          ],
          "url": "https://arxiv.org/abs/2502.20036",
          "ref_texts": "[65] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "65"
          ],
          "1": "In addition, detector-free methods [12, 65, 73] input images directly for matching and achieve significant improvements in accuracy but encounter challenges in heavy computation."
        },
        "Dense-SfM: Structure from Motion with Dense Consistent Matching": {
          "authors": [
            "JM Lee",
            "S Yoo"
          ],
          "url": "https://arxiv.org/abs/2501.14277",
          "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 7, 8",
          "ref_ids": [
            "57"
          ],
          "1": "With the advent of deep learning and attention mechanism [64], semi-dense and dense matching methods [5, 13, 14, 57, 68] have been introduced, enabling the Figure 1.",
          "2": "To address these limitations, more recent methods bypass the need for keypoint detection and instead directly perform semi-dense [5, 57, 68] or dense [13, 14] matching between image pairs.",
          "3": "28 Semi-DenseMatching LoFTR [57] + PixSfM 74.",
          "4": "55 Semi-Dense Matching LoFTR [57] + PixSfM 54."
        },
        "CNSv2: Probabilistic Correspondence Encoded Neural Image Servo": {
          "authors": [
            "A Chen",
            "H Yu",
            "S Li",
            "Y Chen",
            "Z Zhou",
            "W Sun"
          ],
          "url": "https://arxiv.org/abs/2503.00132",
          "ref_texts": "[7] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "7"
          ],
          "1": "Recently, detector-free image matching approaches [7], [8], [9] have shown robust matching in challenging conditions.",
          "2": "LoFTR [7] is the first to utilize the Transformer for detector-free matching, effectively capturing long-range dependencies.",
          "3": "Recent detector free matching methods [7], [8], [9], [22] estimate dense or semi-dense matches according to the correlations of image features.",
          "4": "5 [25] with ViT-B [31] structure of patch size 16 to extract coarse feature maps F \u2208 RH16\u00d7W16\u00d7768 of images I \u2208 RH1\u00d7W1\u00d73 (H16 = H1/16, W16 = W1/16) : Fc = ViT(Ic), Fd = ViT(Id) (12) Following the practices of LoFTR [7] and GMFlow [32], we add a transformer with several self/cross attention layers to make features more distinctive for matching: {\u02dcFc, \u02dcFd} = Transformer(Fc, Fd) (13) We use 2D axial [33] RoPE [34] for positional encoding to better generalization to different image resolution."
        },
        "MambaGlue: Fast and Robust Local Feature Matching With Mamba": {
          "authors": [
            "K Ryoo",
            "H Lim",
            "H Myung"
          ],
          "url": "https://arxiv.org/abs/2502.00462",
          "ref_texts": "[16] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "16"
          ],
          "1": "One of them is LoFTR [16], which is a detector-free dense local feature matching model.",
          "2": "Extractor + Matcher PR LO-RANSAC AUC DLT AUC @1 px @5 px @1 px @5 px Dense LoFTR [16] 92."
        },
        "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching": {
          "authors": [
            "D Jung",
            "J Choi",
            "Y Lee",
            "S Jeong",
            "T Lee"
          ],
          "url": "https://arxiv.org/abs/2502.20685",
          "ref_texts": "[55] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 2",
          "ref_ids": [
            "55"
          ],
          "1": "Recent advancements have introduced semi-dense or dense approaches for feature matching such as LoFTR [55] and DKM [15], which demonstrate superior performance in repetitive or textureless environments compared to keypoint-based methods [13, 28, 36, 46, 47]."
        },
        "Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions": {
          "authors": [
            "D Pisanti",
            "R Hewitt",
            "R Brockers",
            "G Georgakis"
          ],
          "url": "https://arxiv.org/abs/2502.09795",
          "ref_texts": "[32] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "32"
          ],
          "3": "LoFTR [32] uses Transformers [35] in a detectorfree manner, RoMa [12] leverages features from the vision foundation model of DINOv2 [27], and DKM [11] estimates a dense warp to provide a match for every pixel.",
          "4": "The produced features then follow the coarseto-fine approach proposed in LoFTR [32].",
          "5": "Inspired by these recent developments, we aim to adapt the state-of-the-art method of LoFTR [32] for MbL on Mars and extend its architecture to leverage geometric context.",
          "6": "III-A), based on the state-of-the-art method of LoFTR [32].",
          "8": "While LoFTR [32] has shown sufficient invariance to viewpoint changes in in-the-wild datasets [19], it is still not clear whether this would transfer LMST: 05:30 06:00 08:00 11:29 15:00 17:00 Fig."
        },
        "PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching": {
          "authors": [
            "H Nie",
            "B Luo",
            "J Liu",
            "Z Fu",
            "H Zhou",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2502.18104",
          "ref_texts": "[16] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "16"
          ],
          "2": "LoFTR [16] further introduces a Transformer-based detectorless approach that performs well in weakly textured regions, while RoMa [42] develops a robust and dense matching model that achieves state-of-theart results on real-world data."
        },
        "EDM: Efficient Deep Feature Matching": {
          "authors": [
            "X Li",
            "T Rao",
            "C Pan"
          ],
          "url": "https://arxiv.org/abs/2503.05122",
          "ref_texts": "[56] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "56"
          ],
          "1": "Given the powerful capability of modelling long-range global context information, some studies [8, 56, 62] have started using Transformer [60] to establish precise correspondences.",
          "2": "Benefiting from the long-range modeling capability of the Transformer [60], LoFTR [56] and its follow-ups [8, 58, 61] apply the Transformer to enhance local features.",
          "15": "Our approach is able to extract more adequate and accurate matches compared to LoFTR [56] and ELoFTR [62], even in challenging scenes characterized by wide viewpoints, repetitive patterns and textureless regions.",
          "18": "Compared with LoFTR [56] and EfficientLoFTR [62], our method is more robust in scenarios with large viewpoint changes and repetitive semantics."
        },
        "SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting": {
          "authors": [
            "L Yang",
            "X Zhao",
            "Q Sun",
            "K Wang",
            "A Chen"
          ],
          "url": "https://arxiv.org/abs/2503.05174",
          "ref_texts": "[33] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "33"
          ],
          "1": "The refinement process begins with the extraction and matching of 2D feature points using LoFTR [33], a transformer-based feature matching method."
        },
        "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba": {
          "authors": [
            "X Lu",
            "S Du"
          ],
          "url": "https://arxiv.org/abs/2503.03437",
          "ref_texts": "[36] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the CVPR, pages 8922\u20138931, 2021. 1, 2, 6, 7, 8, 3",
          "ref_ids": [
            "36"
          ],
          "2": "Semi-dense matching methods are pioneered by LoFTR [36], which proposes a coarse-to-fine paradigm to establish coarse matches between grids and then adjust the matching points with fine features.",
          "5": "LoFTR [36], we compute the mean reprojection error of corner points, and report the AUC of the corner error up to threshold values of 3, 5, and 10 pixels, respectively."
        },
        "Transferring between sparse and dense matching via probabilistic reweighting": {
          "authors": [
            "Y Fan",
            "R Lang"
          ],
          "url": "https://arxiv.org/abs/2503.01472",
          "ref_texts": "[32] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching with Transformers. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8918\u20138927, 2021. 1, 2",
          "ref_ids": [
            "32"
          ],
          "1": "Recent advancements in learning-based feature matching have led to the emergence of two paradigms: detector-based matching [18, 27] and detector-free matching [12, 32].",
          "2": "LoFTR [32] and followups [5, 22, 37, 38, 40] leverage Transformers to aggregate image features."
        },
        "Global Map Optimization based Real-time UAV Geolocation without GNSS": {
          "authors": [
            "W Xu",
            "J Liu",
            "D Yang",
            "Y Li",
            "M Zhou"
          ],
          "url": "https://www.preprints.org/frontend/manuscript/3f6a230051cca04b9eb19473c2f84339/download_pub",
          "ref_texts": "28. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 20-25 June 2021. Preprints.org (www.preprints.org) | NOT PEER-REVIEWED | Posted: 9 January 2025 doi:10.20944/preprints202501.0678.v1",
          "ref_ids": [
            "28"
          ],
          "1": "With the development of deep learning technology, state-of-the-art image matching networks [28] have emerged as effective solutions to tackle challenges arising from significant variations in appearance caused by seasonal changes, lighting conditions, and other factors between UAV and satellite images."
        },
        "Rekonstrukce trojrozm\u011brn\u00e9ho modelu muzejn\u00edho expon\u00e1tu z obraz\u016f": {
          "authors": [
            "F Tom\u00e1\u0161"
          ],
          "url": "https://dspace.cvut.cz/handle/10467/120384",
          "ref_texts": "[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "27"
          ],
          "1": "More recently, machine learning-based approaches, such as SuperPoint [26] and LoFTR [27], have gained attention due to their superior performance."
        },
        "Dust3r: Geometric 3d vision made easy": {
          "authors": [
            "Shuzhe Wang",
            "Vincent Leroy",
            "Yohann Cabon",
            "Boris Chidlovskii",
            "Jerome Revaud"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.html",
          "ref_texts": "[92] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 6, 7",
          "ref_ids": [
            "92"
          ],
          "2": "These improvements encompass advanced feature description [23, 26, 77, 101, 127], more accurate image matching [3, 15, 27, 28, 51, 65, 81, 92, 96, 107], featuremetric refinement [50], and neural bundle adjustment [49, 116].",
          "3": "To find the metric scale of the scene, we leverage metric depth from an off-the-shelf DPT-KITTI again using the provided code, similarly to most other methods like RoMa [28], LoFTR [92] and SuperPoint-SuperGlue [23, 81]."
        },
        "Foundationpose: Unified 6d pose estimation and tracking of novel objects": {
          "authors": [
            "Bowen Wen",
            "Wei Yang",
            "Jan Kautz",
            "Stan Birchfield"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.html",
          "ref_texts": "[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 6",
          "ref_ids": [
            "52"
          ],
          "1": "For the model-free setup, a number of reference images capturing the novel object PREDATOR [27]LoFTR [52]FS6D-DPM [21]Ours Ref.",
          "2": "Table 1 presents the comparison results against the state-of-art RGBD methods [21, 27, 52] on YCB-Video dataset."
        },
        "Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving": {
          "authors": [
            "Yuqi Wang",
            "Jiawei He",
            "Lue Fan",
            "Hongxin Li",
            "Yuntao Chen",
            "Zhaoxiang Zhang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Driving_into_the_Future_Multiview_Visual_Forecasting_and_Planning_with_CVPR_2024_paper.html",
          "ref_texts": "[56] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InCVPR, pages 8922\u20138931, 2021.6, 20, 21",
          "ref_ids": [
            "56"
          ],
          "1": "This metric utilizes a pre-trained matching model [56] to calculate the average number of matching key points, thereby quantifying the KPM score."
        },
        "Efficient LoFTR: Semi-dense local feature matching with sparse-like speed": {
          "authors": [
            "Yifan Wang",
            "Xingyi He",
            "Sida Peng",
            "Dongli Tan",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_Efficient_LoFTR_Semi-Dense_Local_Feature_Matching_with_Sparse-Like_Speed_CVPR_2024_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 1, 2, 4, 5, 6, 7, 8",
          "ref_ids": [
            "46"
          ],
          "1": "Recently, LoFTR [46] introduces a detector-free matching paradigm with transformer to directly establish semi-dense correspondences between two images without detecting keypoints.",
          "2": "LoFTR [46] first employs the Transformer in detectorfree matching to model the long-range dependencies.",
          "3": "We feed feature of one image as query and feature of the other image as key and value into cross-attention, similar to SG [39] and LoFTR [46].",
          "4": "The softmax operator on both S dimensions (referred to as dualsoftmax) is then applied to obtain the probability of mutual nearest matching, which is commonly used in [35, 46, 53].",
          "5": "For efficiency, our key idea here is to re-leverage the previously transformed coarse features \u02dcF t A, \u02dcF t B to obtain cross-view attended discriminative fine features, instead of introducing additional feature transform networks as in LoFTR [46].",
          "6": "To refine a coarse match, a commonly used strategy [6, 15, 46] is to select the center-patch feature of IA as a fixed reference point, and perform feature correlation and expectation on the entire corresponding feature patch for its fine match.",
          "7": "The coarse ground truth matches {Mc}gt with a total number of N are built by warping grid-level points from IA to IB via depth maps and image poses following previous methods [39, 46].",
          "8": "The test scenes are separated from training data following [46].",
          "9": "We follow the test split of the previous method [46] that uses 1500 sampled pairs from scenes \u201cSacre Coeur\u201d and \u201cSt.",
          "10": "2) semi-dense matchers, including DRC-Net [23], LoFTR [46], QuadTree Attention [48], MatchFormer [55], AspanFormer [6], TopicFM [15], and 3) state-of-the-art dense matcher ROMA [14] that predict matches for each pixel.",
          "11": "As for semi-dense methods, we compare with Sparse-NCNet [36], DRC-Net [23], and LoFTR [46].",
          "12": "Following SuperGlue [39] and LoFTR [46], we resize all images for matching so that their smallest edge equals 480 pixels.",
          "13": "Following [6, 46], the open-sourced localization framework HLoc [38] is utilized.",
          "14": "detector-free methods including LoFTR [46], TopicFM [15], PATS [32] and Aspanformer [6]."
        },
        "RoMa: Robust dense feature matching": {
          "authors": [
            "Johan Edstedt",
            "Qiyu Sun",
            "Georg Bokman",
            "Marten Wadenback",
            "Michael Felsberg"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Edstedt_RoMa_Robust_Dense_Feature_Matching_CVPR_2024_paper.html",
          "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 1, 3, 7, 8",
          "ref_ids": [
            "48"
          ],
          "5": "4 LoFTR [48] CVPR\u201921 78.",
          "6": "4 LoFTR [48] CVPR\u201921 55.",
          "7": "SotA comparison on MegaDepth-1500 [30, 48] .",
          "8": "7 LoFTR [48] CVPR\u201921 52.",
          "9": "8 LoFTR [48] CVPR\u201921 22."
        },
        "Grounding image matching in 3d with mast3r": {
          "authors": [
            "V Leroy",
            "Y Cabon",
            "J Revaud"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73220-1_5",
          "ref_texts": "[82] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers.CVPR, 2021. 2, 3, 7, 8, 10",
          "ref_ids": [
            "82"
          ],
          "1": "Such approaches, like LoFTR [82], thus consider images as a whole and the resulting set of correspondences is dense and more robust to repetitive patterns and low-texture areas [43,68,69,82].",
          "4": "Dense matching has proven effective in scenarios where detailed spatial relationships and textures are critical for understanding scene geometry, leading to top performance on many benchmarks [4,5,6,59,72,82] that are especially challenging for keypoints due to extreme changesinviewpointorillumination.",
          "5": "This is a 30% absolute improvement compared to the second best published method, LoFTR+KBR [81, 82], that get 63."
        },
        "Optimal transport aggregation for visual place recognition": {
          "authors": [
            "Sergio Izquierdo",
            "Javier Civera"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Izquierdo_Optimal_Transport_Aggregation_for_Visual_Place_Recognition_CVPR_2024_paper.html",
          "ref_texts": "[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
          "ref_ids": [
            "52"
          ],
          "2": "We also trained a model using a dual-softmax [46] to solve the optimal transport assignment, following LoFTR and Gluestick [42, 52]."
        },
        "Xfeat: Accelerated features for lightweight image matching": {
          "authors": [
            "Guilherme Potje",
            "Felipe Cadar",
            "Andre Araujo",
            "Renato Martins",
            "Erickson R. Nascimento"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Potje_XFeat_Accelerated_Features_for_Lightweight_Image_Matching_CVPR_2024_paper.html",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 1, 2, 4, 5, 6",
          "ref_ids": [
            "40"
          ],
          "2": "This versatility brings the best of both worlds: keypoint-based methods are more suitable to efficient visual localization based on Structure-from-Motion (SfM) maps [35], while dense feature matching can be more effective for relative camera pose estimation in poorly textured scenes [5, 40].",
          "3": "Our new strategy does not require high resolution features besides the local descriptors themselves as opposed to existing techniques [5, 40], greatly reducing compute and achieving high accuracy and matching density shown in Fig.",
          "4": "More recently, middle-end approaches, known as learned matchers [14, 20, 36], and also end-toend semi-dense [5, 40] and dense [8, 41] methods, demonstrated remarkable improvements in robustness and accuracy for matching wide-baseline image pairs, especially with the recent advances introduced by the transformer architecture [43].",
          "5": "Recent research [5, 40] has demonstrated the benefits of dense image region matching, improving coverage and robustness."
        },
        "Omniglue: Generalizable feature matching with foundation model guidance": {
          "authors": [
            "Hanwen Jiang",
            "Arjun Karpur",
            "Bingyi Cao",
            "Qixing Huang",
            "Andre Araujo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jiang_OmniGlue_Generalizable_Feature_Matching_with_Foundation_Model_Guidance_CVPR_2024_paper.html",
          "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918\u20138927, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "44"
          ],
          "5": "For example, the semidense method LoFTR introduces a coarse-to-fine correspondence prediction paradigm [44].",
          "9": "\u2022 (Semi-)Dense matchers: LoFTR[44] and PDCNet [46] are used as reference dense matching techniques, to contextualize our sparse matching performance with respect to other types of approaches."
        },
        "Sam-6d: Segment anything model meets zero-shot 6d object pose estimation": {
          "authors": [
            "Jiehong Lin",
            "Lihua Liu",
            "Dekun Lu",
            "Kui Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lin_SAM-6D_Segment_Anything_Model_Meets_Zero-Shot_6D_Object_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3",
          "ref_ids": [
            "52"
          ],
          "1": "Methods Based on Feature MatchingMethods within this group [5, 10, 11, 17, 20, 53] align the 2D pixels or 3D points of the proposals with the object surface in the feature space [21, 52], thereby building correspondence to compute object poses."
        },
        "Panacea: Panoramic and controllable video generation for autonomous driving": {
          "authors": [
            "Yuqing Wen",
            "Yucheng Zhao",
            "Yingfei Liu",
            "Fan Jia",
            "Yanhui Wang",
            "Chong Luo",
            "Chi Zhang",
            "Tiancai Wang",
            "Xiaoyan Sun",
            "Xiangyu Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wen_Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_CVPR_2024_paper.html",
          "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 1925, 2021, pages 8922\u20138931. Computer Vision Foundation / IEEE, 2021. 6",
          "ref_ids": [
            "39"
          ],
          "1": "We also use a matching-based [39] consistency score, View-Matching-Score (VMS), to measure the cross-view consistency of generated videos."
        },
        "Sea-raft: Simple, efficient, accurate raft for optical flow": {
          "authors": [
            "Y Wang",
            "L Lipson",
            "J Deng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72667-5_3",
          "ref_texts": "47. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021) 3, 4, 5, 7",
          "ref_ids": [
            "47"
          ],
          "3": "For example, LoFTR [47] filters out uncertain matching pairs."
        },
        "Matching 2d images in 3d: Metric relative pose from metric correspondences": {
          "authors": [
            "Axel Barroso",
            "Sowmya Munukutla",
            "Victor Adrian",
            "Eric Brachmann"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Barroso-Laguna_Matching_2D_Images_in_3D_Metric_Relative_Pose_from_Metric_CVPR_2024_paper.html",
          "ref_texts": "[67] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "67"
          ],
          "1": "Learned matchers [46, 63], detector-free algorithms [15, 23, 36, 67, 74], outlier rejection methods [14, 68, 82, 84, 85], or better robust estimators [7, 10, 18, 72, 73] can improve further the quality of the estimated essential matrices.",
          "4": "9 Dense Features LoFTR [67] 0.",
          "6": "06 LoFTR [67] 0.",
          "7": "4 LoFTR [67] DPT [59] 0."
        },
        "Detector-free structure from motion": {
          "authors": [
            "Xingyi He",
            "Jiaming Sun",
            "Yifan Wang",
            "Sida Peng",
            "Qixing Huang",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/He_Detector-Free_Structure_from_Motion_CVPR_2024_paper.html",
          "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3, 6, 8",
          "ref_ids": [
            "44"
          ],
          "1": "Recently, detector-free matchers [8, 44, 51] have achieved state-of-the-art performance on the image matching task.",
          "4": "With the help of Transformer [49], some semi-dense matching methods [8, 44, 51] achieve higher accuracy compared to detector-based baselines and show strong capabilities in building correspondences in low-textured regions.",
          "5": "Note that the coarse-level correspondences output by some detector-free [8, 44, 51] matchers are typically at 1/8 image resolution, which can be directly used as quantized matches.",
          "6": "Our method with detector-free matcher LoFTR [44] is qualitatively compared with the detector-based baseline SP + SG + PixSfM on multiple scenes.",
          "7": "2) Detector-free SfM baseline LoFTR [44] matches with PixSfM [26] and OnePose++ [19], where these methods are fed with LoFTR\u2019s quantized matches, same as our pipeline.",
          "8": "Implementation Details Our detector-free SfM framework is implemented with multiple detector-free matchers, including LoFTR [44], MatchFormer [51] and AspanTransformer [8], to demonstrate the compatibility of our pipeline.",
          "9": "We compare our method with PixSfM that uses the same LoFTR [44] coarse matches as ours, where its cost map approximation is used to reduce the memory footprint and improve efficiency."
        },
        "Foundpose: Unseen object pose estimation with foundation features": {
          "authors": [
            "EP \u00d6rnek",
            "Y Labb\u00e9",
            "B Tekin",
            "L Ma",
            "C Keskin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73347-5_10",
          "ref_texts": "78. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 3, 13",
          "ref_ids": [
            "78"
          ],
          "1": "We show that the intermediate DINOv2 descriptors are in fact the key enabler of FoundPose, yielding significantly higher accuracy also compared to descriptors extracted with SAM [40], CLIP [70], LoFTR [78], S2DNet [23], and dense SIFT [49]."
        },
        "Nerfiller: Completing scenes via generative 3d inpainting": {
          "authors": [
            "Ethan Weber",
            "Aleksander Holynski",
            "Varun Jampani",
            "Saurabh Saxena",
            "Noah Snavely",
            "Abhishek Kar",
            "Angjoo Kanazawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Weber_NeRFiller_Completing_Scenes_via_Generative_3D_Inpainting_CVPR_2024_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 7",
          "ref_ids": [
            "46"
          ],
          "1": "For geometry, we report the number of high-quality LoFTR [46] correspondences between 100 randomly sampled pairs of frames."
        },
        "Ifvit: Interpretable fixed-length representation for fingerprint matching via vision transformer": {
          "authors": [
            "Y Qiu",
            "H Chen",
            "X Dong",
            "Z Lin",
            "IY Liao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10806774/",
          "ref_texts": "[32] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "32"
          ],
          "3": "The pre-trained ViT model for feature matching in LoFTR [32] is employed to accelerate the convergence for training."
        },
        "Map-relative pose regression for visual re-localization": {
          "authors": [
            "Shuai Chen",
            "Tommaso Cavallari",
            "Victor Adrian",
            "Eric Brachmann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_Map-Relative_Pose_Regression_for_Visual_Re-Localization_CVPR_2024_paper.html",
          "ref_texts": "[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 4, 6",
          "ref_ids": [
            "50"
          ],
          "1": "1 Dynamic Positional Encoding Unlike many vision transformers (ViTs) for high-level tasks [9, 15, 50], where the transformer is conditioned to operate directly upon input RGB images (or higherdimensional features), our transformer is designed to interpret accurate 3D geometric information strongly connected to real-world physics.",
          "2": "Camera-Aware 2D Positional Embedding We draw inspiration from LoFTR\u2019s [50] positional embedding, but integrate information from the camera\u2019s intrinsics to generate the high-frequency components that are fed to the network.",
          "3": "Specifically, for each pixel coordinate (u, v) in the input image, we first compute the (x, y) components of the 3D ray originating in the camera center and passing through the pixel (ignoring the z component); then apply the positional embedding from [50] on the (now camerainvariant) ray\u2019s directional components.",
          "4": "The map-relative pose regressor M is built upon a cascade of linear-attention transformer blocks [50] with dmodel = 256and h = 8parallel attention layers."
        },
        "Large spatial model: End-to-end unposed images to semantic 3d": {
          "authors": [
            "Z Fan",
            "J Zhang",
            "W Cong",
            "P Wang",
            "R Li"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/46fd43174b82660f24e3ba11cf5e1340-Abstract-Conference.html",
          "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "To directly regress the pixel-aligned point maps from the unposed images for view v \u2208 {1, 2}, cross-view attention is also employed, enhancing the architecture\u2019s capacity to infer spatial relationships and propagate information between views\u2014an approach that has proven effective in prior research [51, 20, 52]."
        },
        "Deepmatcher: a deep transformer-based network for robust and accurate local feature matching": {
          "authors": [
            "T Xie",
            "K Dai",
            "K Wang",
            "R Li",
            "L Zhao"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0957417423018638",
          "ref_texts": "[2] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "2"
          ],
          "1": "Parallel to the detector-based matching, another stream of research seeks to establish correspondences directly from original images by extracting visual descriptors on dense grids across an image, thus assuring that substantial repeating keypoints could well be captured [2], [21]\u2013[28].",
          "2": "On the basis of this insight, various studies base their modelling of long-range relationships on Transformer backbone [2], [26]\u2013[28].",
          "3": "As a representative work, LoFTR [2] updates features by repeatedly interleaving the selfand cross-attention layers, and replace vanilla Transformer with linear Transformer [30] to achieve manageable computation cost.",
          "4": "To handle this issue, LoFTR [2], the pioneering detector-free GNN method, utlizes Transformer to realize global context information exchange and extracts matches in a coarse-to-fine manner.",
          "5": "Previous works [2], [3], [16], [26] attach a distinctive absolute positional embedding to each keypoint, thus alleviating such ambiguity.",
          "6": "Compared to sinusoidal encoding [2], [3], [26], rotary positional embedding has two advantages: (i) \u0398(\u00b7) is an orthogonal function, the encoding only changes the feature\u2019s direction but not the feature\u2019s length, which could stabilize the learning process.",
          "7": "Following [2], we calculate the index Egt of the ground truth matches, which are utilized in conjunction with soft assignment matrix G to calculate matching loss Lm defined as focal loss [50].",
          "8": "23 Detector-free Methods \u2014\u2014 LoFTR [2] 22.",
          "9": "In accordance with [2], [3], we report the area under the cumulative curve (AUC) of pose errors at the thresholds (5\u25e6,10\u25e6,20\u25e6), where pose errors are defined as the maximum of translational and rotational errors between ground-truth poses and predicted poses by DeepMatcher.",
          "10": "31 LoFTR [2] 52.",
          "11": "Following [2], [14], we select 100 image pairs each scene for training and 1500 image pairs for testing.",
          "12": "76LoFTR [2]0.",
          "13": "MethodsParams GFLOPs Runtime TCLoFTR [2]11."
        },
        "Local All-Pair Correspondence for Point Tracking": {
          "authors": [
            "S Cho",
            "J Huang",
            "J Nam",
            "H An",
            "S Kim",
            "JY Lee"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72684-2_18",
          "ref_texts": "52. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "52"
          ],
          "1": "This is often achieved by matching a hand-designed descriptors [1,31] or, more recently, learnable deep features [10,22,32,52]."
        },
        "Street-view image generation from a bird's-eye view layout": {
          "authors": [
            "A Swerdlow",
            "R Xu",
            "B Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10443014/",
          "ref_texts": "[29] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-Free Local Feature Matching With Transformers,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual, June 19-25, 2021, 2021, pp. 8922\u2013",
          "ref_ids": [
            "29"
          ],
          "1": "We then attempt to find keypoint correspondences between these image patches by performing feature matching with [29]."
        },
        "NTIRE 2024 challenge on HR depth from images of specular and transparent surfaces": {
          "authors": [
            "Pierluigi Zama",
            "Fabio Tosi",
            "Luigi Di",
            "Radu Timofte",
            "Alex Costanzino",
            "Matteo Poggi",
            "Samuele Salti",
            "Stefano Mattoccia",
            "Yangyang Zhang",
            "Cailin Wu",
            "Zhuangda He",
            "Shuangshuang Yin",
            "Jiaxu Dong",
            "Yangchenxu Liu",
            "Hao Jiang",
            "Jun Shi",
            "Yong A",
            "Yixiang Jin",
            "Dingzhe Li",
            "Bingxin Ke",
            "Anton Obukhov",
            "Tinafu Wang",
            "Nando Metzger",
            "Shengyu Huang",
            "Konrad Schindler",
            "Yachuan Huang",
            "Jiaqi Li",
            "Junrui Zhang",
            "Yiran Wang",
            "Zihao Huang",
            "Tianqi Liu",
            "Zhiguo Cao",
            "Pengzhi Li",
            "Lin Wang",
            "Wenjie Zhu",
            "Hui Geng",
            "Yuxin Zhang",
            "Long Lan",
            "Kele Xu",
            "Tao Sun",
            "Qisheng Xu",
            "Sourav Saini",
            "Aashray Gupta",
            "Sahaj K. Mistry",
            "Aryan Shukla",
            "Vinit Jakhetiya",
            "Sunil Jaiswal",
            "Yuejin Sun",
            "Zhuofan Zheng",
            "Yi Ning",
            "Hao Cheng",
            "I Liu",
            "Wei Huang",
            "Yen Yang",
            "Zhongyu Jiang",
            "Hao Peng",
            "Aishi Huang",
            "Neng Hwang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/html/Ramirez_NTIRE_2024_Challenge_on_HR_Depth_from_Images_of_Specular_CVPRW_2024_paper.html",
          "ref_texts": "[73] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers.CVPR, 2021.",
          "ref_ids": [
            "73"
          ],
          "1": "This approach employs an adaptive group local correlation layer that uses cross and self attention [73] to aggregate global context information, a 2D-1D alternate local strategy to handle imperfect epipolar images, a deformable search window to reduce matching ambiguity, and feature map grouping [22] to improve performance."
        },
        "DeDoDe: Detect, don't describe\u2014Describe, don't detect for local feature matching": {
          "authors": [
            "J Edstedt",
            "G B\u00f6kman",
            "M Wadenb\u00e4ck"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550550/",
          "ref_texts": "[29] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 6, 8, 11",
          "ref_ids": [
            "29"
          ],
          "2": "SotA Comparison MegaDepth-1500 Relative Pose: MegaDepth-1500 is a relative pose benchmark proposed in LoFTR [29], and consists of 1500 pairs of images in two scenes of the MegaDepth dataset, which are non-overlapping with our training set.",
          "3": "Method \u2193 AUC \u2192 @5\u25e6 @10\u25e6 @20\u25e6 LoFTR [29] CVPR\u201921 52.",
          "4": "Method \u2193 mAA \u2192 @10 LoFTR [29] CVPR\u201921 78."
        },
        "Local feature matching using deep learning: A survey": {
          "authors": [
            "S Xu",
            "S Chen",
            "R Xu",
            "C Wang",
            "P Lu",
            "L Guo"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1566253524001222",
          "ref_texts": "[72] J. Sun, Z. Shen, Y . Wang, H. Bao, X. Zhou, Loftr: Detector-free local feature matching with transformers, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 8922\u2013",
          "ref_ids": [
            "72"
          ],
          "1": "In 2021, approaches such as LoFTR [72] and Aspanformer [73] successfully incorporated Transformer or Attention mechanisms into the Detector-free matching process.",
          "2": "LoFTR [72] is groundbreaking because it creates a GNN with keypoints as nodes, utilizing self-attention layers and mutual attention layers to obtain feature descriptors for two images and generating dense matches in regions with low texture.",
          "15": "Techniques such as SuperGlue [69], LoFTR [72], and DISK [112] have been identified as particularly effective, surpassing classical methods by achieving higher robustness against severe illumination and viewpoint shifts."
        },
        "Xoftr: Cross-modal feature matching transformer": {
          "authors": [
            "Onder Tuzcuoglu",
            "Aybora Koksal",
            "Bugra Sofu",
            "Sinan Kalkan",
            "Aydin Alatan"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/IMW/html/Tuzcuoglu_XoFTR_Cross-modal_Feature_Matching_Transformer_CVPRW_2024_paper.html",
          "ref_texts": "[69] Jiaming Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
          "ref_ids": [
            "69"
          ],
          "1": "Our XoFTR provides significant improvements over LoFTR [69] on visible and thermal image pairs.",
          "3": "T o address this gap, our study endeavors to extend the methods for visible image matching advancements to the visible-TIR matching problem, choosing the LoFTR network [69] as our baseline model.",
          "5": "The use of Transformer in detector-free matching provided state-of-the-art results [14, 38, 69, 80, 89].",
          "6": "A prominent example, LoFTR [69], utilizes a Transformer architecture for local image feature matching, generating matches from coarse to fine, especially in low-texture regions.",
          "7": "LoFTR Module:W e directly adopted the LoFTR module [69], which consists of linear selfand cross-attention [41] blocks, to correlate the feature maps F A 1/ 8 and F B 1/ 8, providing refined feature maps denoted as \u02c6F A 1/ 8 and \u02c6F B 1/ 8.",
          "8": "Coarse-Level Matching Loss:T o supervise the matching probability matrices P 0 and P 1, we apply the Focal Loss (FL) [48] following LoFTR [69] and AdaMatcher [38]: Lc = (F L(P 0, \u02c6P ) + F L(P 1, \u02c6P ) ), (9) where \u02c6P is coarse-level ground-truth matching matrix.",
          "9": "W e obtain ground-truth coarse matches similar to LoFTR [69] but without a mutual nearest neighbor constraint.",
          "11": "Experiment 1: Relative Pose Estimation Evaluation protocol:T o evaluate our method with the METU-V isTIR, following [69], we assess pose error using area under curve (AUC) at 5\u00b0, 10\u00b0, and 20\u00b0 thresholds, defined as the maximum angular deviation from GT Category Method Pose estimation AUC @5\u00b0 @10\u00b0 @20\u00b0 Detector-based D2-Net [25]+NN 1.",
          "13": "Compared methods:W e compared our XoFTR with the following publicly available methods: (1) Detector-based methods including D2-Net [25], SuperGlue [63], LightGlue [49] and ReDFeat [18] , and (2) detector-free matchers including LoFTR [69], LoFTR-MTV [52], ASpanFormer [14] and DKM [26].",
          "15": "The qualitative homography estimation results for XoFTR and LoFTR [69]."
        },
        "Matchu: Matching unseen objects for 6d pose estimation from rgb-d images": {
          "authors": [
            "Junwen Huang",
            "Hao Yu",
            "Ting Yu",
            "Nassir Navab",
            "Slobodan Ilic",
            "Benjamin Busam"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Huang_MatchU_Matching_Unseen_Objects_for_6D_Pose_Estimation_from_RGB-D_CVPR_2024_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 3, 4",
          "ref_ids": [
            "46"
          ],
          "2": "Following LoFTR [46], we adopt a modified encoder of FPN [35] as our CNN backbone."
        },
        "FAR: Flexible Accurate and Robust 6DoF Relative Camera Pose Estimation": {
          "authors": [
            "Chris Rockwell",
            "Nilesh Kulkarni",
            "Linyi Jin",
            "Jeong Joon",
            "Justin Johnson",
            "David F. Fouhey"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Rockwell_FAR_Flexible_Accurate_and_Robust_6DoF_Relative_Camera_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[67] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "67"
          ],
          "3": "We use recent SOTA methods LoFTR [67] and SuperPoint+SuperGlue [15, 63], but note FAR can readily adapt to alternative estimators.",
          "5": "It consists first of one LoFTR [67] self-attention and crossattention layer followed by an 8-Point ViT cross-attention layer [16, 44, 60]; both networks are aimed at producing good features for pose estimation.",
          "7": "For solver-based methods, we choose the popular LoFTR [67] and SuperGlue [63].",
          "8": "LoFTR [67] perform best in rotation."
        },
        "Splatpose & detect: Pose-agnostic 3d anomaly detection": {
          "authors": [
            "Mathis Kruse",
            "Marco Rudolph",
            "Dominik Woiwode",
            "Bodo Rosenhahn"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/VAND/html/Kruse_SplatPose__Detect_Pose-Agnostic_3D_Anomaly_Detection_CVPRW_2024_paper.html",
          "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 1925, 2021, pages 8922\u20138931. Computer Vision Foundation / IEEE, 2021. 3, 6",
          "ref_ids": [
            "38"
          ],
          "1": "In line with the author\u2019s reference implementation, the \u201cLocal Feature Matching with Transformers (LoFTR)\u201d framework [38] is used to collect feature matches between the test sample and every training sample.",
          "2": "As for the modules replicated from OmniAD, coarse pose estimation is done using a pre-trained LoFTR [38] with a ResNet [12] backbone, while the image feature matching is done using a pre-trained EfficientNet-b4 [39]."
        },
        "Object detection in remote sensing images based on adaptive multi-scale feature fusion method": {
          "authors": [
            "Chun Liu",
            "Sixuan Zhang",
            "Mengjie Hu",
            "Qing Song"
          ],
          "url": "https://www.mdpi.com/2072-4292/16/5/907",
          "ref_texts": "15. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 8922\u20138931.",
          "ref_ids": [
            "15"
          ],
          "1": "Motivations: We were inspired by the coarse and fine matching processes in image matching [15], which provided insights into handling features within network hierarchies."
        },
        "Learning to estimate 6dof pose from limited data: A few-shot, generalizable approach using rgb images": {
          "authors": [
            "P Pan",
            "Z Fan",
            "BY Feng",
            "P Wang",
            "C Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550594/",
          "ref_texts": "[91] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3, 4",
          "ref_ids": [
            "91"
          ],
          "1": "For 6D pose estimation, the few-shot pose estimator usually employs local image feature matching [59], establishing correspondences between two images in a detector-based [61, 62, 82, 83] or detector-free [49, 56, 81, 91] fashion.",
          "2": "Both W and \u2113 can be applied to either key points or whole images [91]."
        },
        "Sparse global matching for video frame interpolation with large motion": {
          "authors": [
            "Chunxu Liu",
            "Guozhen Zhang",
            "Rui Zhao",
            "Limin Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_Sparse_Global_Matching_for_Video_Frame_Interpolation_with_Large_Motion_CVPR_2024_paper.html",
          "ref_texts": "[31] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.3",
          "ref_ids": [
            "31"
          ],
          "1": "LoFTR [31] replaces the traditional dense matching method, using cost volume to search for correspondence, with self and cross attention layers in Transformer."
        },
        "The nerfect match: Exploring nerf features for visual localization": {
          "authors": [
            "Q Zhou",
            "M Maximov",
            "O Litany",
            "L Leal-Taix\u00e9"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72691-0_7",
          "ref_texts": "59. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "59"
          ],
          "1": "Visual features [14,20,22,52,59,67,78] are often extracted from a database of scene images to represent 3D features and matched against the query image features extracted using the same algorithm to obtain 2D-3D matches.",
          "2": "The full version is notably more expressive, incorporating powerful attention modules [14,52,59] and follows a coarse-to-fine matching paradigm [14,59,78].",
          "3": "After the feature extraction, NeRFMatch-Mini directly matches the NeRF feature mapFs against the coarse-level image feature map Fc m with a non-learnable dual-softmax matching function adopted from [59].",
          "4": "3, NeRFMatch shares the same feature extraction processes as NeRFMatch-Mini, but learns an attention-based matching module which follows the coarse-to-fine paradigm of image-to-image feature matching [14,59,78]: (i) firstly, we identify 3D-to-image patch matches using a coarse matching module, (ii) secondly, we refine to pixel accuracy with a fine matching module.",
          "5": "Equipped with coarse image featuresFc m that represent image local patches and raw NeRF featuresFs that represent individual 3D scene points, we first apply 2D positional encoding [13,59] to equip the image features with positional information.",
          "6": "This alignment [59] produces a heatmap, which depicts the likelihoodofthe3Dpoint j matchingwitheachpixelinthevicinityofimagepixel NeRFMatch 9 i.",
          "7": "To supervise the coarse matching, we apply the log loss [59] to increase the dual-softmax probability at the ground-truth matching locations inMgt.",
          "8": "Following [59, 67], we compute the total variance \u03c32(j) of the corresponding heatmap and minimize the weighted loss function: Lf = 1 Mf X (i,j)\u2208Mf 1 \u03c32(i)||\u02dcxj \u2212 xj||2."
        },
        "Mesa: Matching everything by segmenting anything": {
          "authors": [
            "Yesheng Zhang",
            "Xu Zhao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_MESA_Matching_Everything_by_Segmenting_Anything_CVPR_2024_paper.html",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers.CVPR, 2021.1, 2, 5, 6, 7, 8, 3, 4",
          "ref_ids": [
            "40"
          ],
          "1": "Especially the semi-dense [8, 40] and dense methods [17], which search for matches densely by deep feature comparison, obtain an impressive precision gap over sparse methods [14].",
          "3": "Thus, we turn to the learning-based framework, inspired by recent successes of learning models in point matching [8, 21, 40].",
          "4": "LoFTR [40], ASpan [8], QuadT [43] and DKM [17], to demonstrate our performance and compare with recent methods [6, 19, 21].",
          "8": "Our method is combined with four baselines [8, 17, 40, 43] to demonstrate its effectiveness.",
          "9": "4 shows that our method achieves prominent precision improvement for DKM [17], ASpan [8] and LoFTR [40] in all sequences."
        },
        "Ditto: Demonstration imitation by trajectory transformation": {
          "authors": [
            "N Heppert",
            "M Argus",
            "T Welschehold"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801982/",
          "ref_texts": "[8] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d Proc. IEEE Conf. Comput. Vis. Pattern Recog., pp. 8918\u20138927, 2021.",
          "ref_ids": [
            "8"
          ],
          "2": "These methods can either be based on analytical features such as SIFT [42] and ORB [43] or learned features such as SuperGlue [44] and LoFTR [8].",
          "3": "II-D, we also propose to replace the inherently local flow estimation with a semidense, global method, LoFTR [8] which does not require an additional detection step."
        },
        "Dvmnet: Computing relative pose for unseen objects beyond hypotheses": {
          "authors": [
            "Chen Zhao",
            "Tong Zhang",
            "Zheng Dang",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DVMNet_Computing_Relative_Pose_for_Unseen_Objects_Beyond_Hypotheses_CVPR_2024_paper.html",
          "ref_texts": "[31] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021.2, 3, 6, 7, 8",
          "ref_ids": [
            "31"
          ],
          "3": "We compare our approach with state-of-the-art techniques including image-matching methods, SuperGlue [26], LoFTR [31], and ZSP [12], hypothesis-based methods, RelPose [47], RelPose++ [18], and 3DAHV [52], and a direct regression method implemented in [18].",
          "4": "47 LoFTR [31] 82."
        },
        "Gaustudio: A modular framework for 3d gaussian splatting and beyond": {
          "authors": [
            "C Ye",
            "Y Nie",
            "J Chang",
            "Y Chen",
            "Y Zhi",
            "X Han"
          ],
          "url": "https://arxiv.org/abs/2403.19632",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 4",
          "ref_ids": [
            "42"
          ],
          "1": "Semi-Dense Point Cloud Initialization Inspired by recent advances in semi-dense feature matching [8, 42] and point tracking [19, 50], we propose initializing the point cloud using dense feature matches instead of traditional detector-based matches."
        },
        "Nerfdeformer: Nerf transformation from a single view via 3d scene flows": {
          "authors": [
            "Zhenggang Tang",
            "Zhongzheng Ren",
            "Xiaoming Zhao",
            "Bowen Wen",
            "Jonathan Tremblay",
            "Stan Birchfield",
            "Alexander Schwing"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Tang_NeRFDeformer_NeRF_Transformation_from_a_Single_View_via_3D_Scene_CVPR_2024_paper.html",
          "ref_texts": "[38] Jiaming Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "38"
          ],
          "1": "SuperGlue [34] matches keypoints detected from SuperPoint [6], and LoFTR [38] as well as ASpanFormer [4] match pixels using a downsampled image pair."
        },
        "DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector": {
          "authors": [
            "Johan Edstedt",
            "Georg Bokman",
            "Zhenjun Zhao"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/IMW/html/Edstedt_DeDoDe_v2_Analyzing_and_Improving_the_DeDoDe_Keypoint_Detector_CVPRW_2024_paper.html",
          "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 2, 5",
          "ref_ids": [
            "25"
          ],
          "1": "Furthermore, improving on the detector-descriptor pipeline, graph neural network-based descriptor matching such as SuperGlue and follow-up work [18, 22, 24] as well as end-to-end semidense methods starting with LoFTR [6, 8, 25, 26, 32] and dense image matchers like GLU-Net [11, 13, 27\u201329] have started to see increasing use, however at non-negligible computational expense.",
          "2": "SotA Comparison MegaDepth-1500 Relative Pose: MegaDepth-1500 is a relative pose benchmark proposed in LoFTR [25] and consists of 1500 pairs of images in two scenes of the MegaDepth dataset, which are non-overlapping with our training set."
        },
        "The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement": {
          "authors": [
            "Gabriele Trivigno",
            "Carlo Masone",
            "Barbara Caputo",
            "Torsten Sattler"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Trivigno_The_Unreasonable_Effectiveness_of_Pre-Trained_Features_for_Camera_Pose_Refinement_CVPR_2024_paper.html",
          "ref_texts": "[105] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 1, 7",
          "ref_ids": [
            "105"
          ],
          "2": "1 Textured Mesh LoFTR [105] 50 73.",
          "3": "8 LoFTR [105] 20 71.",
          "4": "8 LoFTR [105] 10 70.",
          "5": "8 (ours)+ LoFTR [105] 20 74.",
          "6": "5 (ours)+ LoFTR [105] 10 73."
        },
        "Global and hierarchical geometry consistency priors for few-shot nerfs in indoor scenes": {
          "authors": [
            "Xiaotian Sun",
            "Qingshan Xu",
            "Xinjie Yang",
            "Yu Zang",
            "Cheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Sun_Global_and_Hierarchical_Geometry_Consistency_Priors_for_Few-shot_NeRFs_in_CVPR_2024_paper.html",
          "ref_texts": "[31] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 4",
          "ref_ids": [
            "31"
          ],
          "1": "Then LoFTR [31] is used to match them to generate sparse correspondences."
        },
        "Track everything everywhere fast and robustly": {
          "authors": [
            "Y Song",
            "J Lei",
            "Z Wang",
            "L Liu",
            "K Daniilidis"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72646-0_20",
          "ref_texts": "32. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "32"
          ],
          "1": "Detector-free methods [32] learn to match between all pairs of image locations without running feature detectors while having a global field of view."
        },
        "Learning structure-from-motion with graph attention networks": {
          "authors": [
            "Lucas Brynte",
            "Jose Pedro",
            "Carl Olsson",
            "Fredrik Kahl"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Brynte_Learning_Structure-from-Motion_with_Graph_Attention_Networks_CVPR_2024_paper.html",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8922\u20138931, 2021. 8",
          "ref_ids": [
            "40"
          ],
          "1": "Other possible research directions include unrolling the architecture to multiple learned iterations / refinement steps, extensions to non-rigid Structure-fromMotion, and incorporating modern learned image matching pipelines such as [5, 6, 40] in an end-to-end fashion."
        },
        "Colorpcr: Color point cloud registration with multi-stage geometric-color fusion": {
          "authors": [
            "Juncheng Mu",
            "Lin Bie",
            "Shaoyi Du",
            "Yue Gao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Mu_ColorPCR_Color_Point_Cloud_Registration_with_Multi-Stage_Geometric-Color_Fusion_CVPR_2024_paper.html",
          "ref_texts": "[23] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8922\u20138931, 2021. 1",
          "ref_ids": [
            "23"
          ],
          "1": "Recent methods in the image matching domain have underscored the importance of local neighborhood information [23, 40], inspiring approaches in point cloud registration [21, 34]."
        },
        "Unifying correspondence pose and nerf for generalized pose-free novel view synthesis": {
          "authors": [
            "Sunghwan Hong",
            "Jaewoo Jung",
            "Heeseong Shin",
            "Jiaolong Yang",
            "Seungryong Kim",
            "Chong Luo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hong_Unifying_Correspondence_Pose_and_NeRF_for_Generalized_Pose-Free_Novel_View_CVPR_2024_paper.html",
          "ref_texts": "[54] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3",
          "ref_ids": [
            "54"
          ],
          "1": "Recent progress in image correspondence has demonstrated the value of self and cross-attention mechanisms in capturing global context within images and enhancing inter-image feature extraction, vital for understanding multi-view geometry [20, 54, 62].",
          "2": "Unlike the standard practice of using a cross-attention map from two feature maps [13, 54, 62], we introduce a simple and more effective adaptation by employing the refined cost volume from the aggregation block, rather than computing a separate cross-attention map."
        },
        "Earthmatch: Iterative coregistration for fine-grained localization of astronaut photography": {
          "authors": [
            "Gabriele Berton",
            "Gabriele Goletto",
            "Gabriele Trivigno",
            "Alex Stoken",
            "Barbara Caputo",
            "Carlo Masone"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/IMW/html/Berton_EarthMatch_Iterative_Coregistration_for_Fine-grained_Localization_of_Astronaut_Photography_CVPRW_2024_paper.html",
          "ref_texts": "[54] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3, 7",
          "ref_ids": [
            "54"
          ],
          "1": "The introduction of LoFTR [54] represented a departure from discrete feature detection towards a detector-free, semi-dense matching paradigm, using transformers to coarsely match features.",
          "2": "In addition to NN matching, we combine them with LightGlue [31] where possible; \u2022 Detector-free: among these, LoFTR [54] is the most popular."
        },
        "Learning representations from foundation models for domain generalized stereo matching": {
          "authors": [
            "Y Zhang",
            "L Wang",
            "K Li",
            "Y Wang",
            "Y Guo"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72946-1_9",
          "ref_texts": "35. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "[58] adopt the linear attention module LoFTR [35] to correlate two-frame features in geometric matching."
        },
        "Neural refinement for absolute pose regression with feature synthesis": {
          "authors": [
            "Shuai Chen",
            "Yash Bhalgat",
            "Xinghui Li",
            "Wang Bian",
            "Kejie Li",
            "Zirui Wang",
            "Victor Adrian"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_Neural_Refinement_for_Absolute_Pose_Regression_with_Feature_Synthesis_CVPR_2024_paper.html",
          "ref_texts": "[54] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3",
          "ref_ids": [
            "54"
          ],
          "1": "Notably, classical geometry-based techniques [4\u20136, 28, 41, 43\u201345] that require explicit feature correspondence search [16, 17, 26, 42, 54, 55] also employ test-time refinement to improve localization accuracy."
        },
        "Zero123-6d: Zero-shot novel view synthesis for rgb category-level 6d pose estimation": {
          "authors": [
            "F Di Felice",
            "A Remus",
            "S Gasperini"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10802157/",
          "ref_texts": "[35] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "35"
          ],
          "1": "POPE [34] represents an alternative approach, which employs off-the-shelf methods [35], [11] to retrieve the camera pose between different views."
        },
        "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces": {
          "authors": [
            "Linyi Jin",
            "Nilesh Kulkarni",
            "David F. Fouhey"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jin_3DFIRES_Few_Image_3D_REconstruction_for_Scenes_with_Hidden_Surfaces_CVPR_2024_paper.html",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers.CVPR, 2021.",
          "ref_ids": [
            "37"
          ],
          "1": "Finally , 3DFIRES can reconstruct when given LoFTR [37] estimated poses with known translation scale.",
          "2": "W e use LoFTR [37] to estimate the camera rotation and translation angle and evaluate the reconstruction within all the camera frustums."
        },
        "Evit: Privacy-preserving image retrieval via encrypted vision transformer in cloud computing": {
          "authors": [
            "Q Feng",
            "P Li",
            "Z Lu",
            "C Li",
            "Z Wang",
            "Z Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10445482/",
          "ref_texts": "[71] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-Free Local Feature Matching With Transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021 . Computer Vision Foundation / IEEE, 8922\u20138931.",
          "ref_ids": [
            "71"
          ],
          "1": "Then many researchers explore ViT and find ViT can obtain excellent performance in many computer vision tasks [16, 41, 44, 48, 64, 71], even surpass the CNN model in performance [4, 31, 51, 78, 94].",
          "2": "In this paper, we use ViT as backbone in our retrieval model, and the reasons are as follows: a) ViT is popular and makes excellent performance in recent many computer vision tasks [4, 31, 41, 44, 48, 51, 64, 71, 78, 94]."
        },
        "Oamatcher: An overlapping areas-based network with label credibility for robust and accurate feature matching": {
          "authors": [
            "K Dai",
            "T Xie",
            "K Wang",
            "Z Jiang",
            "R Li",
            "L Zhao"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0031320323007914",
          "ref_texts": "[31] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "31"
          ],
          "1": "Concurrent with the detector-based matching methods, the detector-free approaches [25]\u2013[30], [30]\u2013[35] focus on extracting visual descriptors on dense grids across images directly, therefore, the repeatable keypoints in image pairs can be captured [31].",
          "2": "Recently, Transformer [36] has been successfully applied for establishing precise matches due to its powerful capability of modelling long-range global context information [14], [31].",
          "3": "As a pioneering work, LoFTR [31] views keypoints as nodes to construct a graphy neural network (GNN), in which the Linear Transformer [37] are leveraged to aggregate global message intra/inter images.",
          "5": "Upon witnessing the great success of SuperGlue [14], LoFTR [31] innovatively design a Transformer-based detectorfree architecture, achieving outstanding matching performance.",
          "6": "For LAL, we follow LoFTR [31] and utilize self/cross linear attention [37] to perform long-range global context aggregation intra/inter images.",
          "11": "Following [14], [31], we report the area under the cumulative curve (AUC) of the pose errors at thresholds (5\u25e6,10\u25e6,20\u25e6).",
          "14": "Following [13], [31], we select 100 image pairs each scene for training and 1500 image pairs for testing.",
          "15": "Following [31], we use the same evaluation metrics AUC@(5 \u25e6, 10 \u25e6, 20 \u25e6) as the indoor pose estimation task."
        },
        "STHN: Deep homography estimation for UAV thermal geo-localization with satellite imagery": {
          "authors": [
            "J Xiao",
            "N Zhang",
            "D Tortei"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10643637/",
          "ref_texts": "[46] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d CVPR, 2021.",
          "ref_ids": [
            "46"
          ],
          "1": "We also evaluate learned keypoint methods including R2D2 [43] trained on our dataset and LoFTR [46] with pretrained weights."
        },
        "Steerers: A framework for rotation equivariant keypoint descriptors": {
          "authors": [
            "Georg Bokman",
            "Johan Edstedt",
            "Michael Felsberg",
            "Fredrik Kahl"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Bokman_Steerers_A_Framework_for_Rotation_Equivariant_Keypoint_Descriptors_CVPR_2024_paper.html",
          "ref_texts": "[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021.2, 3, 7",
          "ref_ids": [
            "50"
          ],
          "1": "At the same time, we are with the same models able to perform on par with or even outperform existing non-invariant methods on upright images on the competitive MegaDepth-1500 benchmark [33, 50]."
        },
        "Fusing personal and environmental cues for identification and segmentation of first-person camera wearers in third-person views": {
          "authors": [
            "Ziwei Zhao",
            "Yuchen Wang",
            "Chuhua Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Fusing_Personal_and_Environmental_Cues_for_Identification_and_Segmentation_of_CVPR_2024_paper",
          "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "LoFTR [44] utilizes both self-attention and cross-attention to update cross-view features, with additional improvements introduced by [9]."
        },
        "Efficient 3D instance mapping and localization with neural fields": {
          "authors": [
            "G Tang",
            "KM Jatavallabhula"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611715/",
          "ref_texts": "[17] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931. 2, 3",
          "ref_ids": [
            "17"
          ],
          "1": "Specifically, we use visual descriptors produced by NetVLAD for matching image viewpoints as a first step, then perform keypoint extraction and matching using LoFTR [17] as a preliminary for mask association on the matched image viewpoints.",
          "2": "set of keypoints in both images and a correspondence mapping from one set to the other) using LoFTR [17] Since LoFTR computes keypoints independently per image pair, we aggregated keypoints from all pairs."
        },
        "A survey on monocular re-localization: From the perspective of scene map representation": {
          "authors": [
            "J Miao",
            "K Jiang",
            "T Wen",
            "Y Wang",
            "P Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10475537/",
          "ref_texts": "[161] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 8918\u20138927, 2021.",
          "ref_ids": [
            "161"
          ],
          "1": "LoFTR [161] proposes to perform dense matching in a coarse-to-fine framework, that it firstly obtains confidence matrices for patch-level coarse matching and then refines matched points positions at pixel level."
        },
        "SOS-Match: segmentation for open-set robust correspondence search and robot localization in unstructured environments": {
          "authors": [
            "A Thomas",
            "J Kinnari",
            "PC Lusk"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801471/",
          "ref_texts": "[35] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d CVPR, 2021.",
          "ref_ids": [
            "35"
          ],
          "1": "To compare against state-of-the-art learned detector and descriptor methods, we evaluate against LoFTR [35] with pretrained outdoor weights and the SuperPoint detector [11] using SuperGlue with pretrained outdoor weights from [10] for correspondence search."
        },
        "Raising the ceiling: Conflict-free local feature matching with dynamic view switching": {
          "authors": [
            "X Lu",
            "S Du"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72946-1_15",
          "ref_texts": "39. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-Free Local Feature Matching With Transformers. In: Proceedings of the CVPR. pp. 8922\u20138931",
          "ref_ids": [
            "39"
          ],
          "1": "1: Comparison among RCM, RCMLite, SuperGlue [36], LoFTR [39] and MatchFormer [42].",
          "2": "In contrast, dense methods [20,33,39,47] undertake a direct pursuit of matches between the dense features.",
          "3": "For the semi-sparse [16,17] and dense [39] matching paradigms, this challenge only arises in the source image as they perform a dense search in the target image.",
          "4": "We adopt the semi-sparse paradigm and integrate it seamlessly with the coarse-to-fine architecture [39], allowing RCM to maintain efficiency while globally searching for sub-pixel matches in the target image.",
          "5": "Dense matching methods [6,39,42] eschew the detector and instead establish dense correspondences between images, resulting in a significant improvement over sparse methods.",
          "6": "In our approach, we integrate the semi-sparse matching paradigm with coarse-to-fine matching [39] to maintain high efficiency.",
          "7": "In the first case, both sparse [5,36] and dense [39,42] methods face a reduction in matchable points in the source image, which is solved by the proposed dynamic view switcher.",
          "8": "Finally the fine matching module crops out the fine features in the coarse matching positions and refines coarse matches through the correlation-based approach [39].",
          "9": "(6) We compute the correlation map between the centroid of the source feature window and all the features in the target feature window, which represents the matching probability, and then get the fine matching position by computing the expectation of the probability distribution, as introduced in [39].",
          "10": "(8) The fine matching loss Lf is computed by the L2-distance between each refined position and the ground-truth position, as introduced in [39].",
          "14": "5:Qualitative comparison of SuperGlue [36], LoFTR [39] and proposed RCM on MegaDepth and ScanNet.",
          "15": "5m,5\u25e6) /(1m,10\u25e6) Localization with matching pairs generated by HLoc LoFTR [39] 88."
        },
        "Lightglue: Local feature matching at light speed": {
          "authors": [
            "Philipp Lindenberger",
            "Edouard Sarlin",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.html",
          "ref_texts": "[65] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with Transformers. CVPR, 2021. 2, 6, 7",
          "ref_ids": [
            "65"
          ],
          "1": "Conversely, dense matchers like LoFTR [65] and followups [8, 73] match points distributed on dense grids rather than sparse locations.",
          "5": "For reference, we also evaluate the dense matchers LoFTR [65], MatchFormer [73] features + matcher R P AUC RANSAC AUC DLT @1px @5px @1px @5px dense LoFTR 92.",
          "8": "We also evaluate the recent, dense deep matchers LoFTR [65], MatchFormer [73], and ASpanFormer [8].",
          "9": "Furthermore, we compare our best scoring method on IMC 2021, DISK+LightGlue, with tuned versions of DISK [69], SuperPoint+SuperGlue [15, 54] as well as the SfM implementation of the dense matcher LoFTR [65]."
        },
        "One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization": {
          "authors": [
            "M Liu",
            "C Xu",
            "H Jin",
            "L Chen"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/4683beb6bab325650db13afd05d1a14a-Abstract-Conference.html",
          "ref_texts": "[74] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "74"
          ],
          "1": "Given a set of four predicted nearby views, we perform feature matching to identify corresponding keypoints across each pair of images (a total of six pairs) using an off-the-shelf module LoFTR [74]."
        },
        "Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects": {
          "authors": [
            "Bowen Wen",
            "Jonathan Tremblay",
            "Valts Blukis",
            "Stephen Tyree",
            "Thomas Muller",
            "Alex Evans",
            "Dieter Fox",
            "Jan Kautz",
            "Stan Birchfield"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.html",
          "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "57"
          ],
          "1": "Feature correspondences in RGB between Ft and Ft\u22121 are established via a transformer-based feature matching network [57], which was pretrained on a large collection 607 Coarse Pose Initialization (Sec."
        },
        "Hierarchical dense correlation distillation for few-shot segmentation": {
          "authors": [
            "Bohao Peng",
            "Zhuotao Tian",
            "Xiaoyang Wu",
            "Chengyao Wang",
            "Shu Liu",
            "Jingyong Su",
            "Jiaya Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.html",
          "ref_texts": "[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 3",
          "ref_ids": [
            "35"
          ],
          "1": "Previous transformer-based methods [35, 49] adopt the self-attention layer to parse features and then feed query and support features to the cross-attention layer for pattern matching, as illustrated in Fig.",
          "2": "Recent work explores combining few-shot semantic segmentation and transformer architecture [19,35].",
          "3": "In previous matching-based methods with transformer architecture, the self-attention and crossattention layers are interleaved for multiple times for feature parsing and pattern matching respectively [35, 49] as shown in Fig."
        },
        "Eigenplaces: Training viewpoint robust models for visual place recognition": {
          "authors": [
            "Gabriele Berton",
            "Gabriele Trivigno",
            "Barbara Caputo",
            "Carlo Masone"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3",
          "ref_ids": [
            "46"
          ],
          "1": "Popular examples are SuperGlue [44], DELG [12], LoFTR [46], Patch-NetVLAD [22], GeoWarp [9], and others [55, 48]."
        },
        "Geotransformer: Fast and robust point cloud registration with geometric transformer": {
          "authors": [
            "Z Qin",
            "H Yu",
            "C Wang",
            "Y Guo",
            "Y Peng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10076895/",
          "ref_texts": "[10] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in CVPR, 2021, pp.",
          "ref_ids": [
            "10"
          ],
          "1": "Inspired by the recent advances in image matching [8], [9], [10], keypoint-free methods [6] downsample the input point clouds into superpoints and then match them through examining whether their local neighborhood (patch) overlaps.",
          "2": "Global context has proven critical in many computer vision tasks [6], [10], [45].",
          "3": "Besides our powerful hybrid features, we also perform a dual-normalization operation [8], [10] on S to further suppress ambiguous matches, leading to \u00afS with \u00afsi,j = si,j P| \u02c6Q| k=1 si,k \u00b7 si,j P| \u02c6P| k=1 sk,j .",
          "4": "Existing methods [6], [10] usually formulate superpoint matching as a multi-label classification problem and adopt a cross-entropy loss with dualsoftmax [10] or optimal transport [6], [15]."
        },
        "Silk: Simple learned keypoints": {
          "authors": [
            "Pierre Gleize",
            "Weiyao Wang",
            "Matt Feiszli"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Gleize_SiLK_Simple_Learned_Keypoints_ICCV_2023_paper.html",
          "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "47"
          ],
          "3": "Silk: Simple learned keypoints",
          "8": "Silk: Simple learned keypoints",
          "11": "Silk: Simple learned keypoints"
        },
        "Gluestick: Robust image matching by sticking points and lines together": {
          "authors": [
            "Remi Pautrat",
            "Iago Suarez",
            "Yifan Yu",
            "Marc Pollefeys",
            "Viktor Larsson"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Pautrat_GlueStick_Robust_Image_Matching_by_Sticking_Points_and_Lines_Together_ICCV_2023_paper.html",
          "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2021. 1, 2, 3, 4, 6, 7, 8",
          "ref_ids": [
            "57"
          ],
          "5": "Gluestick: Robust image matching by sticking points and lines together"
        },
        "RayMVSNet++: learning ray-based 1D implicit fields for accurate multi-view stereo": {
          "authors": [
            "Y Shi",
            "J Xi",
            "D Hu",
            "Z Cai",
            "K Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10185080/",
          "ref_texts": "[54] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16 recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "54"
          ],
          "1": "RayMVSNet++: learning ray-based 1D implicit fields for accurate multi-view stereo"
        },
        "PEAL: Prior-embedded explicit attention learning for low-overlap point cloud registration": {
          "authors": [
            "Junle Yu",
            "Luwei Ren",
            "Yu Zhang",
            "Wenhui Zhou",
            "Lili Lin",
            "Guojun Dai"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.html",
          "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "30"
          ],
          "2": "PEAL: Prior-embedded explicit attention learning for low-overlap point cloud registration"
        },
        "DKM: Dense kernelized feature matching for geometry estimation": {
          "authors": [
            "Johan Edstedt",
            "Ioannis Athanasiadis",
            "Marten Wadenback",
            "Michael Felsberg"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "41"
          ],
          "1": "To tackle this issue, semisparse or detector-free methods such as LoFTR [41] and Patch2Pix [53] were introduced.",
          "2": "This has the benefit of avoiding the detection problem [41].",
          "4": "[41] use transformers, with additional improvements by later work [7, 44, 50].",
          "5": "This approach has similarities to the approach in LoFTR [41], but they indirectly apply the constraint by finding mutual nearest neighbours.",
          "7": "Specifically, for the warp loss we use the \u21132 distance between the predicted and ground truth warp, as in [41].",
          "9": "State-of-the-Art Comparison Similarly to previous approaches [7, 36, 41, 44], we train and evaluate our approach separately on outdoor and indoor geometry estimation.",
          "10": "Outdoor Training: We train on the real world dataset MegaDepth [22], using the same training and test split as in previous work [7, 41].",
          "11": "Indoor Training: For indoor two-view pose estimation we additionally train on the ScanNet [9] dataset in a similar fashion as previous work [36, 41] and use a resolution of 480 \u00d7 640.",
          "12": "We follow the evaluation protocol proposed LoFTR [41], resizing the shorter side of the images to 480.",
          "13": "MegaDepth-1500 Pose Estimation: We use the MegaDepth-1500 test set [41] which consists of 1500 pairs from scene 0015 (St.",
          "14": "We follow the protocol in [7, 41] and use a RANSAC threshold of 0."
        },
        "Sfd2: Semantic-guided feature detection and description": {
          "authors": [
            "Fei Xue",
            "Ignas Budvytis",
            "Roberto Cipolla"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xue_SFD2_Semantic-Guided_Feature_Detection_and_Description_CVPR_2023_paper.html",
          "ref_texts": "[67] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 3, 6",
          "ref_ids": [
            "67"
          ],
          "1": "Recently, advanced matchers based on sparse keypoints [8, 55, 65] or dense pixels [9, 18, 19, 33, 40, 47, 67, 74] are proposed to enhance keypoint/pixel-wise matching and have obtained remarkable accuracy.",
          "2": "As NN matching is unable to incorporate spatial connections of keypoints for matching, advanced matchers are proposed to enhance the accuracy by leveraging the spatial context of a set of keyppoints [8, 55, 65] or an image patch [9, 18, 33, 52, 67, 84]."
        },
        "Rethinking optical flow from geometric matching consistent perspective": {
          "authors": [
            "Qiaole Dong",
            "Chenjie Cao",
            "Yanwei Fu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Dong_Rethinking_Optical_Flow_From_Geometric_Matching_Consistent_Perspective_CVPR_2023_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 3, 4",
          "ref_ids": [
            "46"
          ],
          "2": "Among recent detector-free matching methods [11, 12, 37, 46, 49], Tanget al."
        },
        "Pats: Patch area transportation with subdivision for local feature matching": {
          "authors": [
            "Junjie Ni",
            "Yijin Li",
            "Zhaoyang Huang",
            "Hongsheng Li",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Guofeng Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.html",
          "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "49"
          ],
          "2": "By removing the information bottleneck caused by detectors, LoFTR [49] produces better feature matches.",
          "4": "[49] propose encoding features from both images based on the Transformer [18, 26, 46, 59], which better model long-range dependencies and achieve satisfying performance."
        },
        "Pmatch: Paired masked image modeling for dense geometric matching": {
          "authors": [
            "Shengjie Zhu",
            "Xiaoming Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_PMatch_Paired_Masked_Image_Modeling_for_Dense_Geometric_Matching_CVPR_2023_paper.html",
          "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 4, 5, 6, 7, 8",
          "ref_ids": [
            "48"
          ],
          "4": "LoFTR [48] and ASpanFormer [9] operate all-to-all matching on coarse-scale discrete grid locations.",
          "5": "Then, we follow LoFTR [48] in using linear transformer blocks to correlate the source and support frame feature: {\u03c6s=8 1 , \u03c6s=8 2 } = L\u03b8(\u03c6s=8 1 \u2032 , \u03c6s=8 2 \u2032 ).",
          "6": "6 with: Ts=8 \u2217 , Ps=8 \u2217 = D\u03b8 eC \u00d7 M(X) , (7) where M(X) is cosine positional embeddings with learnable tokens [20, 48], projecting the 2D pixel locations to a high dimensional space to avoid ambiguity when multiple similar patches exist.",
          "7": "(14) Global Matching Loss Following [48], we minimize a binary cross-entropy loss over the correlation volume C after a dual-softmax operation: ]Cijkl \u2032 = softmax(Cij) \u00b7 softmax(Ckl), (15) where Cij and Ckl are (H/8)(W/8) \u00d7 1 vectors.",
          "8": "Then, to comprehensively reflect the contributions from both the density and accuracy of geometric matching, we follow [20, 48] in using the two-view relative camera pose estimation performance as the metric.",
          "9": "We follow [48] in sampling the paired images, weighted by the sequence length and overlap ratio.",
          "11": "Two-View Camera Pose Estimation Evaluation Protocol In the MegaDepth, ScanNet, and Hpatches datasets, we follow the evaluation protocol of [20, 44, 48] in reporting the pose accuracy AUC curve thresholded at 5, 10, and 20 degrees.",
          "15": "Generalization to HPatches Following LoFTR [48], we test the MegaDepth dataset trained model on HPatches.",
          "17": "We conduct the visual comparison against the SoTA dense [20] and sparse [48] methods on the MegaDepth and the ScanNet datasets.",
          "20": "In Row 1, (c), and (e), compared to LoFTR [48], multi-scale dense refinement improves fine-scale correspondence accuracy.",
          "21": "Running Time Evaluated on an RTX 2080 Ti GPU, we run 160 ms for an image of 480 \u00d7 640 while LoFTR [48] runs 116 ms and DKM [20] runs 148 ms."
        },
        "Navi: Category-agnostic image collections with high-quality 3d shape and pose annotations": {
          "authors": [
            "V Jampani",
            "KK Maninis",
            "A Engelhardt"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/efc90033e6e1b05485312dd09fe302b8-Abstract-Datasets_and_Benchmarks.html",
          "ref_texts": "[57] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021.",
          "ref_ids": [
            "57"
          ],
          "2": "We evaluate the following 4 types of correspondence estimation methods: SIFT + MNN/NN-Ratio [37] that use traditional keypoint detection with heuristic traditional matching; SuperPoint + MNN/NN-Ratio [17] that use learned keypoint detection with traditional matching; SuperPoint + SuperGlue [51] that use both learned keypoint detection and learned matching and; LoFTR [57] that proposes dense learnable matching.",
          "4": "\u2022 LoFTR [57]."
        },
        "Fine-grained cross-view geo-localization using a correlation-aware homography estimator": {
          "authors": [
            "X Wang",
            "R Xu",
            "Z Cui",
            "Z Wan"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/112d8e0c7563de6e3408b49a09b4d8a3-Abstract-Conference.html",
          "ref_texts": "[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "28"
          ],
          "3": "We replace the homography estimation module with feature-based methods SuperGlue [22] and LoFTR [28], utilizing RANSAC-based functions from OpenCV to compute homography matrix from matched feature points."
        },
        "Lu-nerf: Scene and pose estimation by synchronizing local unposed nerfs": {
          "authors": [
            "Zezhou Cheng",
            "Carlos Esteves",
            "Varun Jampani",
            "Abhishek Kar",
            "Subhransu Maji",
            "Ameesh Makadia"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cheng_LU-NeRF_Scene_and_Pose_Estimation_by_Synchronizing_Local_Unposed_NeRFs_ICCV_2023_paper.html",
          "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 2, 7",
          "ref_ids": [
            "51"
          ],
          "2": "COLMAP-LoFTR improves COLMAP with LoFTR [51], a detector-free feature matcher."
        },
        "Generating aligned pseudo-supervision from non-aligned data for image restoration in under-display camera": {
          "authors": [
            "Ruicheng Feng",
            "Chongyi Li",
            "Huaijin Chen",
            "Shuai Li",
            "Jinwei Gu",
            "Chen Change"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Feng_Generating_Aligned_Pseudo-Supervision_From_Non-Aligned_Data_for_Image_Restoration_in_CVPR_2023_paper.html",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, 2021. 8",
          "ref_ids": [
            "37"
          ],
          "1": "Thus, we indirectly measure the displacement error with LoFTR [37] serving as a keypoint matcher."
        },
        "Generalized differentiable RANSAC": {
          "authors": [
            "Tong Wei",
            "Yash Patel",
            "Alexander Shekhovtsov",
            "Jiri Matas",
            "Daniel Barath"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wei_Generalized_Differentiable_RANSAC_ICCV_2023_paper.html",
          "ref_texts": "[66] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 6, 9",
          "ref_ids": [
            "66"
          ],
          "2": "\u2022 To demonstrate its potential to unlock the end-to-end training of geometric pipelines, \u2207-RANSAC is incorporated into an end-to-end feature matcher, LoFTR [66], to improve the predicted matches and confidence.",
          "3": ", LoFTR [66], to improve matching prediction with reliable confidence scores (Section 4.",
          "4": "Learning Feature Matching with\u2207-RANSAC In this section, we tune an end-to-end feature matcher, LoFTR [66], on the epipolar error using \u2207-RANSAC.",
          "5": "This observation indicates the robustness of \u2207-RANSAC as a pre-trained model Inference Protocol LoFTR [66] F1 score (%)\u2191avg.",
          "6": "To demonstrate its potential in unlocking the training of geometric pipelines, we train \u2207-RANSAC together with a recent detector-free feature matcher, LoFTR [66], with which we achieve improved confidence prediction and accurate robust estimation."
        },
        "Dynamicstereo: Consistent dynamic depth from stereo videos": {
          "authors": [
            "Nikita Karaev",
            "Ignacio Rocco",
            "Benjamin Graham",
            "Natalia Neverova",
            "Andrea Vedaldi",
            "Christian Rupprecht"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3, 4, 7",
          "ref_ids": [
            "42"
          ],
          "1": "Transformer architectures have shown that attention can be a powerful and flexible method for pooling information over a range of contexts [6, 8, 9, 42].",
          "2": "LoFTR [42] and SuperGlue [36] use combinations of self and cross-attention layers for sparse feature matching."
        },
        "Recurrent homography estimation using homography-guided image warping and focus transformer": {
          "authors": [
            "Yuan Cao",
            "Runmin Zhang",
            "Lun Luo",
            "Beinan Yu",
            "Zehua Sheng",
            "Junwei Li",
            "Liang Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.html",
          "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 4",
          "ref_ids": [
            "36"
          ],
          "1": "Following previous works [23, 34, 36, 40], we interleave the self-attention layer and the cross-attention layer, which is shown in Fig."
        },
        "Kick back & relax: Learning to reconstruct the world by watching slowtv": {
          "authors": [
            "Jaime Spencer",
            "Chris Russell",
            "Simon Hadfield",
            "Richard Bowden"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Spencer_Kick_Back__Relax_Learning_to_Reconstruct_the_World_by_ICCV_2023_paper.html",
          "ref_texts": "[58] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 8",
          "ref_ids": [
            "58"
          ],
          "1": "The feature-matching baseline [2] consists of LoFTR [58] correspondences, a PnP solver and DPT [46] fine-tuned on either Kitti or NYUD-v2."
        },
        "Adaptive assignment for geometry aware local feature matching": {
          "authors": [
            "Dihe Huang",
            "Ying Chen",
            "Yong Liu",
            "Jianlin Liu",
            "Shang Xu",
            "Wenlong Wu",
            "Yikang Ding",
            "Fan Tang",
            "Chengjie Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Huang_Adaptive_Assignment_for_Geometry_Aware_Local_Feature_Matching_CVPR_2023_paper.html",
          "ref_texts": "[29] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InCVPR, pages 8922\u20138931, 2021. 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "29"
          ],
          "3": "Benefiting from the global receptive field and long-range dependencies from Transformers, LoFTR [29] and its variants [6, 30] extend neighborhood consensus to the whole image, setting the SOTA performance for dense feature matching approaches.",
          "4": "The proposed feature interaction structure can be directly applied to LoFTR [29], QuadTree [30] and ASpanFormer [6],i.",
          "5": "For the loss function of the adaptive matching probability matrix P, we use the same Focal Loss [16] as in LoFTR [29]: LM = F L(P, eP), (6) where eP is the ground-truth labels of the adaptive matching probability matrix calculated from the camera poses and depth maps, andP is the predicted matching probability matrix.",
          "6": "Inspired by LoFTR [29], we use the same loss Lrefine = 1 |Mk gt| P i,j\u2032 1 \u03c3(i)2 j\u2032 \u2212 j\u2032 gt function for the final predicted matches, where k is the index calculated above.",
          "9": "We apply AdaMatcher to LoFTR [29] and its variants (QuadTree Attention [30] and ASpanFormer [6]), named AdaMatcherLoFTR, AdaMatcher-Quad and AdaMatcher-ASpan, respectively.",
          "10": "The image feature extractor is a standard ResNet-FPN [10, 15] architecture, which is identical to LoFTR [29].",
          "14": "We compare AdaMatcher with traditional and current SOTA methods: 1) detectorbased methods including SIFT [19]+HardNet [21], KeyNet+HardNet [21], R2D2 [23], ASLFeat [20], Disk [32], SuperGlue(SG) [28] with SuperPoint(SP) [8] or Disk [32] detector and SuperGlue [28] with OETR [7] for pre-processing, 2) detector-free methods including PDC-Net [31], LoFTR [29], QuadTree Attention [30] and ASpanFormer [6].",
          "17": "It can be seen that our proposed method achieves significant performance gains when applied on LoFTR [29], ASpanFormer [6] and QuadTree Attention [30]."
        },
        "A unified transformer based tracker for anti-uav tracking": {
          "authors": [
            "Qianjin Yu",
            "Yinchao Ma",
            "Jianfeng He",
            "Dawei Yang",
            "Tianzhu Zhang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.html",
          "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 5, 6",
          "ref_ids": [
            "49"
          ],
          "1": "As shown in Figure 2, the dense matching algorithm, LoFTR [49], is first applied to find key point pairs.",
          "2": "More details of the dense matching algorithm can refer to LoFTR [49].",
          "3": "We try different models and eventually choose LoFTR [49] model as our dense matching algorithm."
        },
        "Nerf-loc: Visual localization with conditional neural radiance field": {
          "authors": [
            "J Liu",
            "Q Nie",
            "Y Liu",
            "C Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161420/",
          "ref_texts": "[29] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "To handle texture-less scene, detector-free 2D matching methods [29][11][6] has been proposed, which shows promising results.",
          "2": "Inspired by LoFTR[29], a coarse-to-fine matching scheme is adopted to directly estimate 3D-2D correspondences."
        },
        "Visual localization using imperfect 3d models from the internet": {
          "authors": [
            "Vojtech Panek",
            "Zuzana Kukelova",
            "Torsten Sattler"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Panek_Visual_Localization_Using_Imperfect_3D_Models_From_the_Internet_CVPR_2023_paper.html",
          "ref_texts": "[91] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 3, 7, 8",
          "ref_ids": [
            "91"
          ],
          "1": "As shown in [4, 63, 85, 88, 109], modern local features such as [21,23,91,111] are capable of matching real images with renderings of 3D models, even though they were never explicitly trained for this task.",
          "2": "We are motivated by the observation that modern local features, such as the ones used in state-of-the-art localization pipelines [63,73,75,91], can establish correspondences between a real image and renderings of 3D models [15, 63, 109].",
          "3": "Vitus Cathedral scene, obtained with LoFTR [91].",
          "4": "We evaluate different features and matchers inside MeshLoc: LoFTR [91], SuperGlue [75], and the combination of Patch2Pix [111] and SuperGlue [75]."
        },
        "Long-term visual localization with mobile sensors": {
          "authors": [
            "Shen Yan",
            "Yu Liu",
            "Long Wang",
            "Zehong Shen",
            "Zhen Peng",
            "Haomin Liu",
            "Maojun Zhang",
            "Guofeng Zhang",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yan_Long-Term_Visual_Localization_With_Mobile_Sensors_CVPR_2023_paper.html",
          "ref_texts": "[60] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 2, 4, 5, 8",
          "ref_ids": [
            "60"
          ],
          "2": "Recently, many traditional hand-crafted steps have been substituted by learning-based counterparts [16,17,51,60,68].",
          "5": "The backbone of our model is fixed with the LoFTR-DS [60] outdoor model, while the other parts are randomly initialized and backpropagated.",
          "6": "We compare our approach with the following baselines in two categories: 1) Feature matching pipelines HLoc [50], using different keypoint descriptors (SIFT [40], SuperPoint [16] and D2-Net [17]), and matchers (Nearest Neighbour and learned SuperGlue [51]), as well as the detector-free matcher LoFTR [60]."
        },
        "Are local features all you need for cross-domain visual place recognition?": {
          "authors": [
            "Giovanni Barbarani",
            "Mohamad Mostafa",
            "Hajali Bayramov",
            "Gabriele Trivigno",
            "Gabriele Berton",
            "Carlo Masone",
            "Barbara Caputo"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Barbarani_Are_Local_Features_All_You_Need_for_Cross-Domain_Visual_Place_CVPRW_2023_paper.html",
          "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 3, 4, 5",
          "ref_ids": [
            "49"
          ],
          "4": "This approach has been subsequently generalized by LoFTR [49], which removes the dependence from an underlying detector exploiting cross-attention transformers for directly selecting keypoints matches among an image pair.",
          "6": "Specifically, we use SuperGlue [43] (which uses SuperPoint [12] local features), D2-Net [15], R2D2 [42], DELG [9], Reranking Transformers (RRT) [50] (which uses DELG local features), Patch-NetVLAD [18] with both its RANSAC and Rapid Scoring implementation, TransVPR [55], LoFTR [49] and CVNet [24]."
        },
        "Etr: An efficient transformer for re-ranking in visual place recognition": {
          "authors": [
            "Hao Zhang",
            "Xin Chen",
            "Heming Jing",
            "Yingbin Zheng",
            "Yuan Wu",
            "Cheng Jin"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.html",
          "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "Inspired by the success of Transformers [39] and seminal work such as SuperGlue [31] and LoFTR [33], we use Transformers to process local descriptors extracted from pre-trained CNN models.",
          "2": "Several algorithms such as SuperGlue [31] and LoFTR [33] have been developed to optimize this process."
        },
        "Single-stage visual query localization in egocentric videos": {
          "authors": [
            "H Jiang",
            "SK Ramakrishnan"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/4bfe7af38d4e5cd85ae0da639a933652-Abstract-Conference.html",
          "ref_texts": "[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918\u20138927, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "The advancement of visual correspondence models has significantly contributed to the success of various computer vision tasks, including image matching and retrieval [45, 42], tracking and flow estimation [2, 16], 3D vision [29, 21], and representation learning [51, 49]."
        },
        "Target-referenced reactive grasping for dynamic objects": {
          "authors": [
            "Jirong Liu",
            "Ruo Zhang",
            "Shu Fang",
            "Minghao Gou",
            "Hongjie Fang",
            "Chenxi Wang",
            "Sheng Xu",
            "Hengxu Yan",
            "Cewu Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.html",
          "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3",
          "ref_ids": [
            "33"
          ],
          "1": "Different from detector-based methods that extract sparse local features, detector-free methods establish pixel-wise or pointwise dense features [3,13,16,31,33]."
        },
        "The change you want to see (now in 3d)": {
          "authors": [
            "Ragav Sachdeva",
            "Andrew Zisserman"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Sachdeva_The_Change_You_Want_to_See_Now_in_3D_ICCVW_2023_paper.html",
          "ref_texts": "[30] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021. 5",
          "ref_ids": [
            "30"
          ],
          "1": "For correspondence extractor C(\u00b7), we experimented with [11, 30] but found SuperGlue [29] to work the best at generating high quality dense correspondences and at high inference speed."
        },
        "Featurebooster: Boosting feature descriptors with a lightweight neural network": {
          "authors": [
            "Xinjiang Wang",
            "Zeyu Liu",
            "Yu Hu",
            "Wei Xi",
            "Wenxian Yu",
            "Danping Zou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_FeatureBooster_Boosting_Feature_Descriptors_With_a_Lightweight_Neural_Network_CVPR_2023_paper.html",
          "ref_texts": "[41] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. InCVPR, pages 8922\u20138931, 2021.2",
          "ref_ids": [
            "41"
          ],
          "1": "The core idea of our approach, motivated by recent work [25, 36, 41], is integrating the visual and geometric information of all the keypoints into individual descriptors by a Transformer."
        },
        "3d video object detection with learnable object-centric global optimization": {
          "authors": [
            "Jiawei He",
            "Yuntao Chen",
            "Naiyan Wang",
            "Zhaoxiang Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/He_3D_Video_Object_Detection_With_Learnable_Object-Centric_Global_Optimization_CVPR_2023_paper.html",
          "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 3",
          "ref_ids": [
            "39"
          ],
          "1": "Besides, feature correspondence learning [36, 39] has received extensive attention in recent years."
        },
        "Doppelgangers: Learning to disambiguate images of similar structures": {
          "authors": [
            "Ruojin Cai",
            "Joseph Tung",
            "Qianqian Wang",
            "Hadar Averbuch",
            "Bharath Hariharan",
            "Noah Snavely"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cai_Doppelgangers_Learning_to_Disambiguate_Images_of_Similar_Structures_ICCV_2023_paper.html",
          "ref_texts": "[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 5, 6",
          "ref_ids": [
            "34"
          ],
          "1": "After resizing and padding each image to 1024 \u00d7 1024 resolution, we compute matches using LoFTR [34], a detector-free, learned local feature matching method.",
          "2": "38 SIFT [23]+RANSAC [11] LoFTR [34] DINO [2]-ViT Ours#matches %matches #matches %matches Latent code Feature map Average precision 83.",
          "3": "We evaluate two feature matching baselines, one based on SIFT [23] and one on LoFTR [34]."
        },
        "Joint appearance and motion learning for efficient rolling shutter correction": {
          "authors": [
            "Bin Fan",
            "Yuxin Mao",
            "Yuchao Dai",
            "Zhexiong Wan",
            "Qi Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2023_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021. 3",
          "ref_ids": [
            "46"
          ],
          "1": "As the improvement of vision transformer, there are many works using a transformer, especially cross attention [23, 46, 53] for cross-view modeling."
        },
        "Supervised homography learning with realistic dataset generation": {
          "authors": [
            "Hai Jiang",
            "Haipeng Li",
            "Songchen Han",
            "Haoqiang Fan",
            "Bing Zeng",
            "Shuaicheng Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Supervised_Homography_Learning_with_Realistic_Dataset_Generation_ICCV_2023_paper.html",
          "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proc. CVPR, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "36"
          ],
          "2": "We compare our method with three categories of existing homography estimation methods: 1) feature-based methods including SIFT [26], ORB [33], SuperPoint [7] with SuperGlue [34] (SPSG), and LoFTR [36], 2) supervised methods including DHN [6], LocalTrans [35], and IHN [4], 3) unsupervised methods including CAHomo [42], BasesHomo [41], and HomoGAN [12].",
          "3": "81%) 9) LoFTR [36]+RANSAC [9]1.",
          "4": "32%) 10) LoFTR [36]+MAGSAC [2]1.",
          "5": "196) LoFTR [36]+MAGSAC [2]2."
        },
        "Gmsf: Global matching scene flow": {
          "authors": [
            "Y Zhang",
            "J Edstedt",
            "B Wandt"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/cb1c4782f159b55380b4584671c4fd88-Abstract-Conference.html",
          "ref_texts": "[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Each of the blocks includes self-attention followed by cross-attention [38, 43, 47, 56]."
        },
        "Jigsaw: Learning to assemble multiple fractured objects": {
          "authors": [
            "J Lu",
            "Y Sun",
            "Q Huang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/30ae2af8612ac74357363e8ae877d80c-Abstract-Conference.html",
          "ref_texts": "[20] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "20"
          ],
          "1": "Methods utilizing CNN, GNN, and attentions have found successful applications in various domains, such as image registration [19, 20] and graph or multi-graph matching on images [21, 22, 23, 24, 25]."
        },
        "Geometrized transformer for self-supervised homography estimation": {
          "authors": [
            "Jiazhen Liu",
            "Xirong Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Geometrized_Transformer_for_Self-Supervised_Homography_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[26] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 3, 6, 7",
          "ref_ids": [
            "26"
          ],
          "1": "A novel trend is to develop detector-free feature matching methods, see NCNet [23], LoFTR [26], ASpanFormer [3] and DKM [6].",
          "5": "LoFTR [26] improves over SuperGlue with Transformers to exploit self-/intercorrelations among all dense-positioned local features."
        },
        "2d3d-matr: 2d-3d matching transformer for detection-free registration between images and point clouds": {
          "authors": [
            "Minhao Li",
            "Zheng Qin",
            "Zhirui Gao",
            "Renjiao Yi",
            "Chenyang Zhu",
            "Yulan Guo",
            "Kai Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 2, 4",
          "ref_ids": [
            "46"
          ],
          "1": "Recently, detection-free approach has received increasing attention in both stereo matching [41, 27, 54, 46] and point cloud registration [53, 38].",
          "2": "For this reason, [27, 54, 46] further propose to adopt a coarse-to-fine matching framework, which achieves accurate and efficient image matching."
        },
        "OFVL-MS: Once for visual localization across multiple indoor scenes": {
          "authors": [
            "Tao Xie",
            "Kun Dai",
            "Siyi Lu",
            "Ke Wang",
            "Zhiqiang Jiang",
            "Jinghan Gao",
            "Dedong Liu",
            "Jie Xu",
            "Lijun Zhao",
            "Ruifeng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xie_OFVL-MS_Once_for_Visual_Localization_across_Multiple_Indoor_Scenes_ICCV_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "42"
          ],
          "1": "However, as opposed to directly matching within an exhaustive 3D map as in [36], current state-of-the-art methods [34, 59, 58] employ image retrieval [2] to narrow down the searching space and utilize advanced feature matching techniques such as Patch2pix [62], SuperGlue [35], LoFTR [42], MatchFormer [50], OAMatcher [9], and DeepMatcher [54] to generate precise 2D-2D correspondences, which are subsequently elevated to 2D-3D matches."
        },
        "Two-view geometry scoring without correspondences": {
          "authors": [
            "Axel Barroso",
            "Eric Brachmann",
            "Victor Adrian",
            "Gabriel J. Brostow",
            "Daniyar Turmukhambetov"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.html",
          "ref_texts": "[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 1, 2, 3, 4, 5, 6, 8",
          "ref_ids": [
            "50"
          ],
          "2": "Inspired by the success of Vision Transformers [61], and detector-free matchers [31, 50], we define an architecture that incorporates the epipolar geometry into an attention layer, and hence the quality of the fundamental matrix hypothesis conditions the coherence of the computed features.",
          "5": "Transformers are getting attention in the vision community, and have been applied successfully to image matching [31, 50], multi-view stereo [20, 29, 63], or depth estimation [27, 34], among others [21, 44]."
        },
        "Posematcher: One-shot 6d object pose estimation by deep feature matching": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Castro_PoseMatcher_One-Shot_6D_Object_Pose_Estimation_by_Deep_Feature_Matching_ICCVW_2023_paper.html",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3, 4, 5, 6",
          "ref_ids": [
            "37"
          ],
          "1": "The closest work to our own is OnePose++, an improved version of OnePose[38], where the feature extraction is replaced by the detector free feature matcher LoFTR [37].",
          "2": "In order to optimize through the matching task we apply the differentiable dual-softmax operator as proposed by LoFTR [37] and subsequently used by both OnePose and OnePose++ [38, 12].",
          "4": "(3) The coarse loss Lc used to optimize C uses focal loss [24] as suggested by LoFTR [37].",
          "5": "IO-Layer: An object-image attention layer Image to image feature matching has seen its effectiveness increase with the introduction of self and cross attention mechanisms [44, 34, 37, 26].",
          "7": "Pose Refinement We add a fine level 2D-refinement post-processing step as described by LoFTR [37] and OnePose++[12].",
          "9": "Moreover, in order to supervise the refinement step, LoFTR [37] proposes computing the total variance of the matching heatmap in order to penalize more confident but erroneous estimations."
        },
        "Autorecon: Automated 3d object discovery and reconstruction": {
          "authors": [
            "Yuang Wang",
            "Xingyi He",
            "Sida Peng",
            "Haotong Lin",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_AutoRecon_Automated_3D_Object_Discovery_and_Reconstruction_CVPR_2023_paper.html",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 3",
          "ref_ids": [
            "37"
          ],
          "1": "Specifically, we use the recent semidense image matcher LoFTR [37] for SfM to reconstruct 21384"
        },
        "Graph-covis: Gnn-based multi-view panorama global pose estimation": {
          "authors": [
            "Negar Nejatishahidin",
            "Will Hutchcroft",
            "Manjunath Narayana",
            "Ivaylo Boyadzhiev",
            "Yuguang Li",
            "Naji Khosravan",
            "Jana Kosecka",
            "Sing Bing"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Nejatishahidin_Graph-CoVis_GNN-Based_Multi-View_Panorama_Global_Pose_Estimation_CVPRW_2023_paper.html",
          "ref_texts": "[24] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918\u2013",
          "ref_ids": [
            "24"
          ],
          "2": "Lofter [24] achieves detector-free matching across images by learning feature descriptors starting from a dense pixelwise sampling and refining them for high quality fine-level matching."
        },
        "Neumap: Neural coordinate mapping by auto-transdecoder for camera localization": {
          "authors": [
            "Shitao Tang",
            "Sicong Tang",
            "Andrea Tagliasacchi",
            "Ping Tan",
            "Yasutaka Furukawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 5",
          "ref_ids": [
            "42"
          ],
          "1": "This approach exploits learning-based feature extraction and correspondence matching methods [13, 35, 42, 44] to achieve robust localization.",
          "2": "FMbased localization has achieved state-of-the-art performance [13,14,32,34,35,37,42,49].",
          "3": "This pipeline has demonstrated significant improvements, with most followup works focusing on enhancing feature matching capability [35, 42, 49, 50].",
          "4": "LoFTR [42] and its variants [9,44] propose dense matching frameworks without key-points.",
          "5": "For Aachen, with images that have significant view and illumination differences, we use a feature extractor from LoFTR [42], pretrained with images containing such variations."
        },
        "End2end multi-view feature matching with differentiable pose optimization": {
          "authors": [
            "Barbara Roessle",
            "Matthias Niessner"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.html",
          "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918\u2013",
          "ref_ids": [
            "48"
          ],
          "1": "Recent learning-based approaches [45, 48, 26, 35] instead leverage the greater image context to improve the matching, e.",
          "2": "LoFTR [48] and COTR [26] recently proposed detector-free methods that operate on RGB images directly.",
          "3": "Hence, we focus on comparisons to recent matching networks: SuperGlue [45], LoFTR [48], COTR [26], and 3DG-STFM [35].",
          "4": "Two-View Pose Estimation Following prior work [45, 48, 35], we evaluate on the same 1500 image pairs of ScanNet and MegaDepth and 481 Pose est."
        },
        "D2Former: Jointly learning hierarchical detectors and contextual descriptors via agent-based transformers": {
          "authors": [
            "Jianfeng He",
            "Yuan Gao",
            "Tianzhu Zhang",
            "Zhe Zhang",
            "Feng Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
          "ref_ids": [
            "42"
          ],
          "2": "Recently, attention mechanisms have been also introduced to the image matching task, where LoFTR [42] and ASpanFormer [7] are representative works.",
          "10": "We compare our model with previous state-of-the-art image matching methods [9, 12, 16, 31, 32, 34, 42, 47].",
          "11": "Compared with LoFTR [42], our D 2Former improves by 5.",
          "12": "Specifically, compared with ASpanFormer [42], our method improves by 5."
        },
        "S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction": {
          "authors": [
            "Emanuele Santellani",
            "Christian Sormann",
            "Mattia Rossi",
            "Andreas Kuhn",
            "Friedrich Fraundorfer"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Santellani_S-TREK_Sequential_Translation_and_Rotation_Equivariant_Keypoints_for_Local_Feature_ICCV_2023_paper.html",
          "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Recent studies have also explored new research directions, utilizing deep matching architectures like SuperGlue [32], or developing methods able to find pointwise correspondences directly from image pairs such as LoFTR [38]."
        },
        "Improving transformer-based image matching by cascaded capturing spatially informative keypoints": {
          "authors": [
            "Chenjie Cao",
            "Yanwei Fu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cao_Improving_Transformer-based_Image_Matching_by_Cascaded_Capturing_Spatially_Informative_Keypoints_ICCV_2023_paper.html",
          "ref_texts": "[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021. 1, 2, 3, 4, 5, 6, 7",
          "ref_ids": [
            "43"
          ],
          "2": "To solve this issue, transformer-based detector-free methods have emerged as more robust alternatives, demonstrating impressive matching abilities in texture-less regions [43, 18, 47, 57, 4].",
          "9": "Some transformer-based manners [43, 47, 57, 4], led by LoFTR [43], largely enlarge the receptive fields with interlacing self/cross attention modules, and enjoy better performance in texture-less regions.",
          "10": "We briefly review the transformer-based matching baseline in the example of LoFTR [43].",
          "14": "Local Regressive Refinement The patch-wise refinement module in LoFTR [43] is also incorporated in our work for sub-pixel matching.",
          "15": "Confidence based Detection with NMS Different from detector-based methods [11, 9, 38], latest attention based methods [43, 47, 4, 57] achieve good results even without detector.",
          "19": "CasMTR can outperform all competitors especially in AUC5\u25e6 and AUC10\u25e6, which include both transformerbased [43, 47, 57, 4] and CNN-based [12] manners."
        },
        "Corresnerf: Image correspondence priors for neural radiance fields": {
          "authors": [
            "Y Lao",
            "X Xu",
            "X Liu",
            "H Zhao"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7f77492bb8070a5c825a87c0c5181da2-Abstract-Conference.html",
          "ref_texts": "[23] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 2, 3, 4",
          "ref_ids": [
            "23"
          ],
          "1": "Additionally, the acquisition of image correspondence is inexpensive, as they can be directly computed using pre-trained off-the-shelf methods [22, 23, 24].",
          "2": "The image correspondences obtained from modern image matchers [24, 23] are typically denser than the depth maps generated from traditional SfM system [19], when given sparse input views, since the matching is completed according to various characteristics in addition to geometrical information.",
          "3": "Recently, detector free methods [23], transformer-based [42, 43], and dense geometric matching [44, 45, 24] have been proposed to further improve the performance."
        },
        "Pixel-perfect structure-from-motion with featuremetric refinement": {
          "authors": [
            "Philipp Lindenberger",
            "Edouard Sarlin",
            "Viktor Larsson",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Lindenberger_Pixel-Perfect_Structure-From-Motion_With_Featuremetric_Refinement_ICCV_2021_paper.html",
          "ref_texts": "[74] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with Transformers. CVPR, 2021. 2",
          "ref_ids": [
            "74"
          ],
          "1": "Differently, dense matching [13, 46, 58, 71, 74, 78, 80] considers all pixels in each image, resulting in denser and more accurate correspondences."
        },
        "Multi-sensor data fusion for 3d reconstruction of complex structures: A case study on a real high formwork project": {
          "authors": [
            "Linlin Zhao",
            "Huirong Zhang",
            "Jasper Mbachu"
          ],
          "url": "https://www.mdpi.com/2072-4292/15/5/1264",
          "ref_texts": "92. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021.",
          "ref_ids": [
            "92"
          ],
          "1": "Image Feature Matching Motivated by the above statement, this study uses local feature matching with transformers (LoFTR) proposed by the study [92], a novel detector-free method for local feature matching.",
          "2": "Image Feature Matching Motivated by the above statement, this study uses local feature matching with transformers (LoFTR) proposed by the study [92], a novel detector-free method for local feature matching."
        },
        "RobustMat: Neural diffusion for street landmark patch matching under challenging environments": {
          "authors": [
            "R She",
            "Q Kang",
            "S Wang",
            "YR Y\u00e1ng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10268340/",
          "ref_texts": "[15] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "15"
          ],
          "2": "Moreover, LoFTR [15] introduced selfand cross-attention layers into the transformers to obtain semi-dense matching.",
          "3": "In addition, to improve image matching performance, spatial information is also considered in [14], [15], [39].",
          "4": "Comparison with keypoint-level matching: We compare RobustMat with keypoint-level matching methods, including D2-Net [21], SuperGlue [14] and LoFTR [15].",
          "6": "To further improve the accuracy of odometry estimation, it is promising to combine pixelor point-level matching methods [14], [15], [21], [22] with our patch-level matching."
        },
        "Nope-sac: Neural one-plane ransac for sparse-view planar 3d reconstruction": {
          "authors": [
            "B Tan",
            "N Xue",
            "T Wu",
            "GS Xia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10247631/",
          "ref_texts": "[36] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 8922\u20138931. 3, 8, 9, 10, 12, 13, 14",
          "ref_ids": [
            "36"
          ],
          "1": "Following this paradigm, there have been tremendous efforts to improve the performance of keypoint detection and description [31], [32], [33], [34] and matching by neural networks [9], [35], [36], [37], [38].",
          "2": "4 Baseline Configurations We compare our method with the state-of-the-art learning solutions, including SparsePlanes [7] and PlaneFormers [8] for plane matching, pose estimation, and planar 3D reconstruction, as well as two keypoint-based solutions SuperGlue [9] and LoFTR [36].",
          "10": "2% and LoFTR [36] for pose initialization and refine them with the pose embeddings achieved from our AIM.",
          "14": "We use the official models of SuperGlue [9] and LoFTR [36]."
        },
        "Omnimatterf: Robust omnimatte with 3d background modeling": {
          "authors": [
            "Geng Lin",
            "Chen Gao",
            "Bin Huang",
            "Changil Kim",
            "Yipeng Wang",
            "Matthias Zwicker",
            "Ayush Saraf"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lin_OmnimatteRF_Robust_Omnimatte_with_3D_Background_Modeling_ICCV_2023_paper.html",
          "ref_texts": "[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 6",
          "ref_ids": [
            "28"
          ],
          "1": "For all videos, we also estimate homographies with LoFTR [28] and OpenCV to enable Omnimatte processing."
        },
        "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching": {
          "authors": [
            "Y Sun",
            "X Wang",
            "Y Zhang",
            "J Zhang",
            "C Jiang"
          ],
          "url": "https://arxiv.org/abs/2312.09031",
          "ref_texts": "33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. pp. 8922\u20138931 (2021) 4, 7, 8, 12",
          "ref_ids": [
            "33"
          ],
          "1": "Architectures such as SuperGlue [30], LoFTR [33], and MatchFormer [37] leverage distinct Transformer structures, meticulously considering the global information of images and the potential correspondences between image pairs, resulting in significant matching outcomes.",
          "2": "iComMa 7 When obtaining the image rendered via 3D Gaussian splatting at the current optimized pose, we employ LoFTR [33], a detector-free local feature matching method, to identify the corresponding feature points between the rendered and query images.",
          "3": "3, we quantitatively compare our proposed method with several matching-based pose estimation methods, namely LightGlue [25], MatchFormer [37], and LoFTR [33].",
          "5": "3 Relative Pose Estimation Experimental Setting: In this study, we quantitatively compared our proposed method with several matching-based pose estimation methods, namely, LightGlue [25], MatchFormer [37], and LoFTR [33]."
        },
        "Image patch-matching with graph-based learning in street scenes": {
          "authors": [
            "R She",
            "Q Kang",
            "S Wang",
            "WP Tay"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10144565/",
          "ref_texts": "[50] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. , 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "50"
          ],
          "1": "LoFTR [50] achieves accurate semidense matches with Transformers including self and crossattention layers.",
          "2": "To improve image matching performance, spatial information is used in [33], [50], [51] through spatial verification, graph learning, and cross attention.",
          "3": "We compare VGIDM with several baseline methods, including MatchNet [24], SiameseNet [47], HardNet [27], SOSNet [45], D2-Net [31], ASLFeat [32], SuperGlue [33] and LoFTR [50].",
          "8": "8680 We compare VGIDM with MatchNet [24], NetVLAD [72], SuperGlue [33], and LoFTR [50] under the place recognition task with around 600 pairs of place images from the KITTI dataset."
        },
        "Context-PIPs: persistent independent particles demands spatial context features": {
          "authors": [
            "W Bian",
            "Z Huang",
            "X Shi",
            "Y Dong"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad2fa437f7c23e4e9875599c6065d18a-Abstract-Conference.html",
          "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "36"
          ],
          "1": "Since FlowNet [6, 20], learning optical flow with neural networks presents superior performance over traditional optimization-based methods and is fast progressing with more training data obtained by the renderer and better network architecture [6, 20, 28, 35, 36, 18, 19, 45]."
        },
        "Privacy-preserving representations are not enough: Recovering scene content from camera poses": {
          "authors": [
            "Kunal Chelani",
            "Torsten Sattler",
            "Fredrik Kahl",
            "Zuzana Kukelova"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chelani_Privacy-Preserving_Representations_Are_Not_Enough_Recovering_Scene_Content_From_Camera_CVPR_2023_paper.html",
          "ref_texts": "[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2",
          "ref_ids": [
            "43"
          ],
          "2": "The robustness is a direct consequence of recent advances in local features [10, 13, 30] and effective feature matchers [32,43,48,53]."
        },
        "Mipi 2023 challenge on rgb+ tof depth completion: Methods and results": {
          "authors": [
            "Qingpeng Zhu",
            "Wenxiu Sun",
            "Yuekun Dai",
            "Chongyi Li",
            "Shangchen Zhou",
            "Ruicheng Feng",
            "Qianhui Sun",
            "Chen Change",
            "Jinwei Gu",
            "Yi Yu",
            "Yangke Huang",
            "Kang Zhang",
            "Meiya Chen",
            "Yu Wang",
            "Yongchao Li",
            "Hao Jiang",
            "Amrit Kumar",
            "Vikash Kumar",
            "Kunal Swami",
            "Pankaj Kumar",
            "Yunchao Ma",
            "Jiajun Xiao",
            "Zhi Ling"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/html/Zhu_MIPI_2023_Challenge_on_RGBToF_Depth_Completion_Methods_and_Results_CVPRW_2023_paper.html",
          "ref_texts": "[21] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021. 4",
          "ref_ids": [
            "21"
          ],
          "1": "The self-attention module of light LoFTR[21] is introduced to aggregate more sparse depth points."
        },
        "Learning probabilistic coordinate fields for robust correspondences": {
          "authors": [
            "W Zhao",
            "H Lu",
            "X Ye",
            "Z Cao",
            "X Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10147320/",
          "ref_texts": "[18] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "18"
          ],
          "3": "We have evaluated our approach on indoor (SUN3D [24] and ScanNet [25]) and outdoor (YFCC100M [26], PhotoTourism [27], and MegaDepth [28]) datasets on top of various state-of-the-art matching approaches (SuperGlue [14], LoFTR [18], and OANet [2]).",
          "7": "We choose the state-of-the-art method LoFTR [18] as our baseline.",
          "10": "Particularly, SuperGlue with PCFs and the SuperPoint descriptor achieves competitive results with the detector-free method LoFTR [18].",
          "11": "We choose the state-of-the-art detector-free network LoFTR [18] as our baseline.",
          "12": "To further isolate the contribution of barycentric coordinates and reliable region prediction, we conduct the experiment on the MegaDepth dataset using LoFTR-OT [18] as a baseline."
        },
        "Neural Refinement for Absolute Pose Regression with Feature Synthesis": {
          "authors": [
            "S Chen",
            "Y Bhalgat",
            "X Li",
            "J Bian",
            "K Li",
            "Z Wang"
          ],
          "url": "https://arxiv.org/abs/2303.10087",
          "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3",
          "ref_ids": [
            "57"
          ],
          "1": "Notably, classical geometry-based techniques [4\u20136, 28, 44, 46\u201348] that require explicit feature correspondence search [16, 17, 26, 45, 57, 58] also employ test-time refinement to improve localization accuracy."
        },
        "Find my astronaut photo: Automated localization and georectification of astronaut photography": {
          "authors": [
            "Alex Stoken",
            "Kenton Fisher"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/html/Stoken_Find_My_Astronaut_Photo_Automated_Localization_and_Georectification_of_Astronaut_CVPRW_2023_paper.html",
          "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3, 6",
          "ref_ids": [
            "33"
          ],
          "1": "These matchers follow the style of the Local Feature TRansformer (LoFTR) [33], leveraging the benefits of self and cross attention between the images themselves or their features [5,9,17,34,35].",
          "2": "We divide the models into four categories (1) detector-based local feature matchers (SIFT [22], SuperPoint+SuperGlue [29], D2-Net [13]) (2) detector-free local feature matchers (LoFTR [33], SE2-LoFTR [5], Aspanformer [9], Matchformer [35], Patch2Pix [39]) (3) pretrained global embeddings (NetVLAD [2], GeM-Net [24, 25], SwA V [7], DINO [8], Barlow Twins [38]) and (4) dense similarity/overlap predictors (DKM [14])."
        },
        "Intertrack: Interaction transformer for 3d multi-object tracking": {
          "authors": [
            "J Willes",
            "C Reading"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10229847/",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "37"
          ],
          "2": "SuperGlue [33] and LoFTR [37] introduce a GNN and Transformer module respectively, as a means to incorporate both local and global image information during feature extraction.",
          "3": "We follow the Transformer design of LoFTR [37] to incorporate global object information in our feature extraction.",
          "4": "We adopt the transformer module from LoFTR [37] and interleave the self and cross attention blocks Nc times."
        },
        "Visualizing Skiers' Trajectories in Monocular Videos": {
          "authors": [
            "Matteo Dunnhofer",
            "Luca Sordi",
            "Christian Micheloni"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Dunnhofer_Visualizing_Skiers_Trajectories_in_Monocular_Videos_CVPRW_2023_paper.html",
          "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3, 5, 6, 14",
          "ref_ids": [
            "57"
          ],
          "2": "We executed the state-of-the-art LOFTR image matching algorithm [57] to detect and match key-points of consecutive frames of SkiTB\u2019s monocular test videos."
        },
        "Guiding local feature matching with surface curvature": {
          "authors": [
            "Shuzhe Wang",
            "Juho Kannala",
            "Marc Pollefeys",
            "Daniel Barath"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Guiding_Local_Feature_Matching_with_Surface_Curvature_ICCV_2023_paper.html",
          "ref_texts": "[55] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1, 2, 3, 4, 5, 6, 7",
          "ref_ids": [
            "55"
          ],
          "1": "LoFTR [55] and its recent variants [58, 8, 65] leverage both the global and local context from raw images by jointly learning the feature extraction and matching in a single network.",
          "2": "More recently, transformer-based detectorfree matchers [55, 58, 65, 8, 24, 57] have received attention due to their strong performance on local feature matching.",
          "3": "The most representative work, LoFTR [55], inherits the advantages of graph matching from SuperGlue and leverages a linear transformer [64] for efficient dense matching.",
          "4": "The overview of our method coupled with LoFTR [55] is shown in Fig.",
          "5": "For a pair of images, it follows the LoFTR [55] matching pipeline to extract the coarse and fine level image features and generate a coarse matching confidence map Pc.",
          "6": "Curvature-Guided Match Selection The proposed curvature similarity extractor (CSE) is a depth-based plug-in component that can be combined with both detector-based [32, 49] and detector-free [55, 58, 8] matchers.",
          "7": "The new confidence matrix P is minimized with the negative loglikelihood, similarly as in [49, 55], where the loss is L = \u2212 1 |Mgt c | X (i,j)\u2208Mgt c log P(i, j), (12) and Mgt c is the ground truth matches defined at the coarselevel in [55].",
          "9": "We add the curvature similarity extractor module to LoFTR [55] and QuadTree [58] and compare the two approaches in both indoor and outdoor scenes.",
          "10": "The matches are coloured by the epipolar error threshold in [55].",
          "11": "For detectorbased approaches [55, 58], we extract the curvature similarity at keypoint locations and use multi-scales {7 \u2217 7, 9 \u2217 9, 11 \u2217 11}.",
          "12": "For the depth predictor training, following the default setting in LoFTR [55], we fine-tune the predictor with the outdoor matching model on MegaDepth [27].",
          "13": "We fine-tune the outdoor model on MegaDepth with the longest dimension of the images re17986 sized to 840 and evaluate our method on MegaDepth-1500 from [55] and YFCC100M-4000 from [49].",
          "14": "Similar to the previous methods [49, 55], we report the Area Under the recall Curve (AUC) of the translation and rotation errors with multiple thresholds (5 \u25e6, 10\u25e6, 20\u25e6)."
        },
        "Fusing visual appearance and geometry for multi-modality 6dof object tracking": {
          "authors": [
            "M Stoiber",
            "M Elsayed",
            "AE Reichert"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10341961/",
          "ref_texts": "[41] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "41"
          ],
          "1": "Prominent examples are SuperPoint [39], D2-Net [40], or LoFTR [41]."
        },
        "Affineglue: Joint matching and robust estimation": {
          "authors": [
            "D Barath",
            "D Mishkin",
            "L Cavalli",
            "PE Sarlin"
          ],
          "url": "https://arxiv.org/abs/2307.15381",
          "ref_texts": "[88] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. InComputer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 1, 2, 6, 7, 8",
          "ref_ids": [
            "88"
          ],
          "4": "Also, we show the results of LoFTR [88].",
          "6": "It makes SuperPoint+SuperGlue comparable to the detector-less LoFTR [88] with achieving even smaller avg."
        },
        "Objectmatch: Robust registration using canonical object correspondences": {
          "authors": [
            "Can Gumeli",
            "Angela Dai",
            "Matthias Niessner"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Methods such as LoFTR [42] introduce more dense and accurate sub-pixel level matching."
        },
        "Flowformer: A transformer architecture and its masked cost volume autoencoding for optical flow": {
          "authors": [
            "Z Huang",
            "X Shi",
            "C Zhang",
            "Q Wang",
            "Y Li",
            "H Qin"
          ],
          "url": "https://arxiv.org/abs/2306.05442",
          "ref_texts": "[54] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "54"
          ],
          "2": "LoFTR [54] utilized transformer to remove feature detector and further improved performance."
        },
        "Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy": {
          "authors": [
            "OL Barbed",
            "JMM Montiel",
            "P Fua",
            "AC Murillo"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-43907-0_56",
          "ref_texts": "23. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. IEEE (2021)",
          "ref_ids": [
            "23"
          ],
          "1": "Other recent works have extended the networks to take advantage of the advances in attention for the matching task, as in SuperGlue [20] and LoFTR [23]."
        },
        "A light touch approach to teaching transformers multi-view geometry": {
          "authors": [
            "Yash Bhalgat",
            "Joao F. Henriques",
            "Andrew Zisserman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.html",
          "ref_texts": "[72] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 6, 7, 8",
          "ref_ids": [
            "72"
          ],
          "1": "These features along with learning based local matching methods [69, 72, 86] and robust optimization methods [6, 9, 25] form a powerful toolbox for relative geometry estimation.",
          "2": "We use a combination of LoFTR [72] and MAGSAC++ [6] to generate pseudo-geometry information in one of our compared methods.",
          "3": "cal image feature matching method, LoFTR [72], to extract high-quality semi-dense matches between the image pair.",
          "4": "2, we obtain the pseudo groundtruth geometry information using LoFTR [72] for matchTable 3.",
          "5": "We thank the authors of [6, 72, 74] for open-sourcing their code."
        },
        "Improving the matching of deformable objects by learning to detect keypoints": {
          "authors": [
            "F Cadar",
            "W Melo",
            "V Kanagasabapathi",
            "G Potje"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0167865523002325",
          "ref_texts": "[12] J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, LoFTR: detectorfree local feature matching with transformers, in: CVPR, 2021.",
          "ref_ids": [
            "12"
          ],
          "1": "Some methods, like LoFTR [12], rely on fewer confident matches at a lower resolution and propagate this more robust information of matching for neighboring pixels to higher resoluarXiv:2309."
        },
        "Occ^ 2Net: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions": {
          "authors": [
            "Miao Fan",
            "Mingrui Chen",
            "Chen Hu",
            "Shuchang Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.html",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "40"
          ],
          "2": "LoFTR [40] obtains confidence matrices for coarse matching, and refines matching points positions at fine level, both steps using self and cross attention layers in Transformer.",
          "3": "QuadTree LoFTR [41] is a follow-up method on the LoFTR [40] framework, which uses a tree data structure, in which each internal node has exactly four children, to improve matching accuracy.",
          "4": "For points in visible patches, we calculate the offset from the center of the matched patch using the heatmap produced by a small LoFTR module [40].",
          "5": "We use dual-softmax [34] to compute the matching probability for matches, following LoFTR [40].",
          "6": "(8) We minimize the sum of loss over the three cases: Lc =\u03b3(M(v,v) c\u2212gt, \u02c6PAB) +\u03bb1\u03b3(M(v,o) c\u2212gt, \u02c6PAB)+ \u03bb1\u03b3(M(o,v) c\u2212gt, \u02c6PAB), (9) where \u03b3 is the negative log-likelihood loss used in LoFTR [40]: \u03b3(M,P)= \u2212 1\u23d0\u23d0M \u23d0\u23d0 \u2211 (i,j)\u2208M logP(i,j).",
          "7": "Fine Matching Loss The target is to optimize the weighted loss function proposed by LoFTR [40]: Lf = L(v,v) f +\u03bb2L(v,o) f +\u03bb2L(o,v) f , (12) which is calculated by reprojection error."
        },
        "Photomatch: An open-source tool for multi-view and multi-modal feature-based image matching": {
          "authors": [
            "E Ruiz de O\u00f1a",
            "I Barbero-Garc\u00eda"
          ],
          "url": "https://www.mdpi.com/2076-3417/13/9/5467",
          "ref_texts": "38. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp.",
          "ref_ids": [
            "38"
          ],
          "1": "For instance, in the last Image Matching Challenge (IMC) (Image Matching Challenge\u20142022 edition) [34], the best-performing algorithms were ASpanFormer [35], and combinations of SuperGlue [36], SuperPoint [37], LoFTR [38], DKM [39] and DISK [40]."
        },
        "Self-supervised correspondence estimation via multiview registration": {
          "authors": [
            "Mohamed El",
            "Ignacio Rocco",
            "David Novotny",
            "Andrea Vedaldi",
            "Natalia Neverova",
            "Justin Johnson",
            "Ben Graham"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Banani_Self-Supervised_Correspondence_Estimation_via_Multiview_Registration_WACV_2023_paper.html",
          "ref_texts": "[62] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 3, 6, 7",
          "ref_ids": [
            "62"
          ],
          "3": "We consider three end-to-end approaches: SuperGlue [57], LoFTR [62], and BYOC [21].",
          "6": "2 LoFTR [62] + RANSAC 75.",
          "7": "2 LoFTR [62] +Ours 84."
        },
        "Uncertainty-driven dense two-view structure from motion": {
          "authors": [
            "W Chen",
            "S Kumar",
            "F Yu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10036010/",
          "ref_texts": "[17] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in CVPR, 2021, pp. 8922\u2013",
          "ref_ids": [
            "17"
          ],
          "7": ", SuperPoint [40] + SuperGlue [39] and LoFTR [17], which have been commonly used by many recent 3D reconstruction and localization systems [4] [56]."
        },
        "DualRC: A Dual-Resolution Learning Framework With Neighbourhood Consensus for Visual Correspondences": {
          "authors": [
            "X Li",
            "K Han",
            "S Li",
            "V Prisacariu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10255317/",
          "ref_texts": "[46] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d Proceedings of IEEE Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "46"
          ],
          "2": "Particularly, for the geometric task, LoFTR [46], a transformer based method sharing a similar dual-resolution design as our DualRC, achieves the current state-of-the-art performance."
        },
        "Searching from area to point: A hierarchical framework for semantic-geometric combined feature matching": {
          "authors": [
            "Y Zhang",
            "X Zhao",
            "D Qian"
          ],
          "url": "https://arxiv.org/abs/2305.00194",
          "ref_texts": "[9] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d CVPR, 2021.",
          "ref_ids": [
            "9"
          ],
          "3": "To alleviate the high computational cost, semi-dense methods such as LoFTR [9] and its variants [10], [39] suggest selecting sparse patch matches after dense computation to refine point matches.",
          "4": "In semi-dense methods [9], [10], [39], patch-level matches are established through feature attention of the coarse level matching, which serve as the search space for the fine level matching.",
          "7": "Particularly, we combine SGAM with SOTA sparse method (SGAM SP+SG [7], [29]), semidense methods (SGAM ASpan [10], SGAM QuadT [39] and SGAM LoFTR [9]) and dense methods (SGAM COTR [35]), to demonstrate the improvement we achieved."
        },
        "Panopoint: Self-supervised feature points detection and description for 360deg panorama": {
          "authors": [
            "Hengzhi Zhang",
            "Hong Yi",
            "Haijing Jia",
            "Wei Wang",
            "Makoto Odamaki"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Zhang_PanoPoint_Self-Supervised_Feature_Points_Detection_and_Description_for_360deg_Panorama_CVPRW_2023_paper.html",
          "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 7",
          "ref_ids": [
            "47"
          ],
          "1": "In addition to the methods mentioned in the previous section, we add two transformer-based matchers [47, 51]: the detect-based method SuperGlue and the detect-free method LoFTR."
        },
        "A deep learning-based visual map generation for mobile robot navigation": {
          "authors": [
            "CA Garc\u00eda-Pintos",
            "NG Aldana-Murillo"
          ],
          "url": "https://www.mdpi.com/2673-4117/4/2/92",
          "ref_texts": "20. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 15\u201320 June 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "20"
          ],
          "1": "In such a way, from the existing local detectors/descriptors, we found two outstanding methods: Oriented FAST and Rotated BRIEF (ORB) [19], and LOFTR (local feature transformer) [20].",
          "2": "On the other hand, LoFTR (local feature transformer) [20] is a detector-free deep neural network architecture that performs local feature matching end-to-end."
        },
        "Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning": {
          "authors": [
            "L Wang",
            "J Wu",
            "X Fang",
            "Z Liu",
            "C Cao",
            "Y Fu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3612458",
          "ref_texts": "[18] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8922\u20138931.",
          "ref_ids": [
            "18"
          ],
          "1": "There are also studies adopting Transformer to better model the process of correspondence prediction [10, 18]."
        },
        "Simsc: A simple framework for semantic correspondence with temperature learning": {
          "authors": [
            "X Li",
            "K Han",
            "X Wan",
            "VA Prisacariu"
          ],
          "url": "https://arxiv.org/abs/2305.02385",
          "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 2",
          "ref_ids": [
            "38"
          ],
          "1": "Recently, dense feature maps extracted by deep neural networks [15, 37] have shown great success in many computer vision tasks [8,10,26,36,38,39]."
        },
        "CoFiI2P: Coarse-to-fine correspondences for image-to-point cloud registration": {
          "authors": [
            "S Kang",
            "Y Liao",
            "J Li",
            "F Liang",
            "Y Li",
            "X Zou",
            "F Li"
          ],
          "url": "https://arxiv.org/abs/2309.14660",
          "ref_texts": "[10] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021, pp.",
          "ref_ids": [
            "10"
          ],
          "1": "Inspired by recent coarse-to-fine matching schedules and transformers in image-to-image (I2I) registration [10], [11] and point cloud-to-point cloud (P2P) registration approaches [12], this paper proposes the Coarse-to-Fine Image-to-Point cloud (CoFiI2P) network for I2P registration.",
          "2": "CoFiNet [22] followed LoFTR\u2019s [10] design and proposed the coarse-to-fine correspondences for registration, which computed coarse matches with descriptors strengthened by the transformer and refined coarse matches through density adaptive matching module."
        },
        "RGM: A Robust Generalizable Matching Model": {
          "authors": [
            "S Zhang",
            "X Sun",
            "H Chen",
            "B Li",
            "C Shen"
          ],
          "url": "https://arxiv.org/abs/2310.11755",
          "ref_texts": "37. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp.",
          "ref_ids": [
            "37"
          ],
          "2": "Since the introduction of LoFTR [37], detector-free local feature matching methods, which discard the feature detector stage, have attracted great research attention [5,38,44].",
          "3": "LoFTR [37] takes a coarse-to-fine strategy by first establishing a dense matching correspondence and removing the unreliable matches at the refinement stage."
        },
        "Render-and-compare: Cross-view 6-DoF localization from noisy prior": {
          "authors": [
            "S Yan",
            "X Cheng",
            "Y Liu",
            "J Zhu",
            "R Wu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10219638/",
          "ref_texts": "[4] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition(CVPR2021), virtual, June 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "4"
          ],
          "4": "For our Render-and-Compare framework, SuperPoint(SPP) [2]+SuperGlue(SPG) [3] and LoFTR [4] are adopted to match query images and virtual renderings.",
          "8": "The right column illuminates that LoFTR [4] is good at matching real-and-synthetic views with similar poses."
        },
        "End-to-end evaluation of practical video analytics systems for face detection and recognition": {
          "authors": [
            "P Singh",
            "EJ Delp",
            "AR Reibman"
          ],
          "url": "https://arxiv.org/abs/2310.06945",
          "ref_texts": "[17] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free Local Feature Matching with Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021.",
          "ref_ids": [
            "17"
          ],
          "1": "To fix the annotations, we first eliminate the offset between the RGB and IR cameras using a popular transformer-based feature-matching algorithm, LoFTR [17], that finds keypoints between the same scene captured in the different camera modalities."
        },
        "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers": {
          "authors": [
            "Xuyang Bai",
            "Zeyu Hu",
            "Xinge Zhu",
            "Qingqiu Huang",
            "Yilun Chen",
            "Hongbo Fu",
            "Lan Tai"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.html",
          "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 4",
          "ref_ids": [
            "41"
          ],
          "1": "Multi-head attention is a popular mechanism to perform information exchange and build a soft association between two sets of inputs, and it has been widely used for the feature matching task [34, 41]."
        },
        "Geometric transformer for fast and robust point cloud registration": {
          "authors": [
            "Zheng Qin",
            "Hao Yu",
            "Changjian Wang",
            "Yulan Guo",
            "Yuxing Peng",
            "Kai Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.html",
          "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 1, 3, 4, 5",
          "ref_ids": [
            "25"
          ],
          "1": "Inspired by the recent advances in image matching [22, 25, 39], keypoint-free methods [36] downsample the input point clouds into superpoints and then match them through examining whether their local neighborhood (patch) overlaps.",
          "2": "Global context has proven critical in many computer vision tasks [10, 25, 36].",
          "3": "Besides our powerful hybrid features, we also perform a dual-normalization operation [22,25] on S to further suppress ambiguous matches, leading to \u00afS with \u00afsi,j = si,j \u2211|\u02c6Q| k=1 si,k \u00b7 si,j \u2211|\u02c6P| k=1 sk,j .",
          "4": "Existing methods [25,36] usually formulate superpoint matching as a multi-label classification problem and adopt a cross-entropy loss with dualsoftmax [25] or optimal transport [23, 36]."
        },
        "Gmflow: Learning optical flow via global matching": {
          "authors": [
            "Haofei Xu",
            "Jing Zhang",
            "Jianfei Cai",
            "Hamid Rezatofighi",
            "Dacheng Tao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.html",
          "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 1, 2, 3, 4",
          "ref_ids": [
            "38"
          ],
          "2": ", SuperGlue [34] and LoFTR [38]) adopt Transformers [41] to reason about the mutual relationship between feature descriptors, and the correspondences are extracted with an explicit matching layer (e.",
          "4": "LoFTR [38] further improves its performance by removing the feature detection step in the typical pipelines.",
          "5": "Such design philosophies have enabled great achievement in sparse feature matching frameworks [34, 38].",
          "6": "To further consider their mutual dependencies, a natural choice is Transformer [41], which is particularly suitable for modeling the mutual relationship between two sets with the attention mechanism, as demonstrated in sparse matching methods [34,38]."
        },
        "Flowformer: A transformer architecture for optical flow": {
          "authors": [
            "Z Huang",
            "X Shi",
            "C Zhang",
            "Q Wang",
            "KC Cheung"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_40",
          "ref_texts": "44. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "44"
          ],
          "2": "Recently, transformers also lead a trend in such tasks [39,44,7,27], which is more related to ours."
        },
        "Practical stereo matching via cascaded recurrent network with adaptive correlation": {
          "authors": [
            "Jiankun Li",
            "Peisen Wang",
            "Pengfei Xiong",
            "Tao Cai",
            "Ziwei Yan",
            "Lei Yang",
            "Jiangyu Liu",
            "Haoqiang Fan",
            "Shuaicheng Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.html",
          "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proc. CVPR, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "41"
          ],
          "1": "In light of LoFTR [41] for sparse feature matching, we add an attention module before correlation computation in the first stage of cascades in order to aggregate global context information in single or cross feature maps.",
          "2": "Following [41], we add positional encoding to the backbone output, which enhances positional dependence of the feature maps."
        },
        "Aspanformer: Detector-free image matching with adaptive span transformer": {
          "authors": [
            "H Chen",
            "Z Luo",
            "L Zhou",
            "Y Tian",
            "M Zhen",
            "T Fang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_2",
          "ref_texts": "13. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. (2021)",
          "ref_ids": [
            "13"
          ],
          "2": "Recently, some works [13,14,21] base their methods on Transformer [23,24] backbone for better modeling of long-range dependencies.",
          "3": "Recently, with the help of deep neural network, possibility is explored to build high performance detector-free matching frameworks based on deep features, which can roughly be classified into two categories: cost volume-based methods [16,18,15,22,17,37] and Transformer-based methods [13,14,21].",
          "5": "As validated in Transformer networks [13,30,14], positional encoding is critical in maintaining spatial information for the flattened tokens.",
          "6": "Following the same formulation in LoFTR [13], 2D sinusoidal signals in different frequencies are used to encode position information and are added to initial features.",
          "7": "5 Matches Determination We inherit the scheme in LoFTR [13] to generate final correspondences, including a coarse matching stage and a sub-pixel refinement stage.",
          "8": "The coarse matches Mc are further fed into a correlation-based refinement block, which is the same with LoFTR [13], to obtain the final matching results.",
          "9": "For fair comparison, we follow the same training and testing protocols used by SuperGlue [30] and LoFTR [13], where 230M and 1.",
          "10": "Following previous works [30,13], we train and evaluate our method separately on the two datasets.",
          "11": "We compare the proposed method with 1) detectorbased approaches, including SuperGlue [30] and SGMNet [31] that are equipped with SuperPoint(SP) [9] as local feature extractor, 2) detector-free approaches, including DRC-Net [17], PDC-Net [15,50], LoFTR [13], QuadTree Attention [21], MatchFormer [51] and DKM [52].",
          "14": "For both datasets, we use pretrained HLoc [57] to retrieve candidate pairs, and recover camera poses with the model trained on MegaDepth dataset following SuperGlue [30] and LoFTR [13].",
          "17": "We evaluate the runtime of proposed method and compare it with LoFTR [13] where both methods apply Transformer backend."
        },
        "Onepose: One-shot object pose estimation without cad models": {
          "authors": [
            "Jiaming Sun",
            "Zihao Wang",
            "Siyu Zhang",
            "Xingyi He",
            "Hongcheng Zhao",
            "Guofeng Zhang",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.html",
          "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3, 4, 5, 6",
          "ref_ids": [
            "36"
          ],
          "2": "Recently, learning-based local feature detection, description [10\u201312, 39] and matching [32, 36] surpass these handcrafted methods and have substituted the traditional counterparts in the localization pipeline.",
          "4": "Inspired by [32, 36], we further use selfand crossattention layers following the aggregation-attention layers to process and transform the aggregated 3D descriptors and query 2D descriptors.",
          "5": "We follow [36] to use the dual-softmax operator to differentiablly extract match confidence scores P3D.",
          "6": "Linear Attention [14, 41] is used in all the attention layers following [36]."
        },
        "Matchformer: Interleaving attention in transformers for feature matching": {
          "authors": [
            "Qing Wang",
            "Jiaming Zhang",
            "Kailun Yang",
            "Kunyu Peng",
            "Rainer Stiefelhagen"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_MatchFormer_Interleaving_Attention_in_Transformers_for_Feature_Matching_ACCV_2022_paper.html",
          "ref_texts": "36. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 2, 3, 4, 7, 8, 9, 10, 11, 12, 13",
          "ref_ids": [
            "36"
          ],
          "2": "For instance, while COTR [14] feeds CNN-extracted features into a transformer-based decoder, SuperGlue [32] and LoFTR [36] only apply attention modules atop the decoder.",
          "6": "For example, SuperGlue [32] and LoFTR [36] applied selfand cross-attention to process the features which were extracted from CNNs.",
          "7": "Finally, the coarse and fine features are passed to perform the coarse-to-fine matching, as introduced in LoFTR [36].",
          "8": "We use38,300 image pairs from368 scenarios for training, and the same1,500 testing pairs from [36] for evaluation.",
          "11": "Tocompare LoFTRandMatchFormer at different data scales on outdoor pose estimation task, both use 8 A100 GPUs, otherwise use 64 A100 GPUs following LoFTR [36].",
          "12": "Qualitative visualization of MatchFormer and LoFTR [36]."
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
          "authors": [
            "X He",
            "J Sun",
            "Y Wang",
            "D Huang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/e43f900f571de6c96a70d5724a0fb565-Abstract-Conference.html",
          "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 4, 6, 7, 8",
          "ref_ids": [
            "47"
          ],
          "1": "Built upon the detector-free feature matching method LoFTR [47], we devise a new keypoint-free SfM method to reconstruct a semi-dense point-cloud model for the object.",
          "2": "The keypoint-free semi-dense feature matching method LoFTR [47] achieves outstanding performance on matching image pairs and shows strong capabilities for finding correspondences in low-textured regions.",
          "3": "More specifically, to better adapt LoFTR [47] for SfM, we design a coarse-to-fine scheme for accurate and complete semi-dense object reconstruction.",
          "4": "Note that there are also methods proposed by keypoint-free matchers [47, 62] to adapt themselves for SfM.",
          "5": "They either round matches to grid level [47] or merge matches within a grid to the average location [62] to obtain repeatable \u201ckeypoints\u201d for SfM.",
          "6": "Since our method is highly related to the keypoint-free matching method LoFTR [47], we give it a short overview in Section 3.",
          "7": "1 Background Keypoint-Free Feature Matching Method LoFTR [47].",
          "8": "Inspired by [47], we first extract hierarchical feature maps of Iq and then perform matching in a coarse-to-fine manner for efficiency, as illustrated in Fig.",
          "9": "Linear Attention [19] is used in our model to reduce the computational complexity, following [47].",
          "10": "Similar to [47], we crop a local window W with a size of w \u00d7 w around \u02dcuq in the fine feature map \u02c6F2D \u2208 R H 2 \u00d7W 2 \u00d7Cf .",
          "11": "We jointly train the coarse and fine modules of our 2D-3D matching framework with different supervisions, following [47].",
          "12": "We use the LoFTR [47] outdoor model pre-trained on MegaDepth [28].",
          "13": "To be specific, we compare with HLoc combined with different feature matching methods including SuperGlue [41] and LoFTR [47]."
        },
        "Global matching with overlapping attention for optical flow estimation": {
          "authors": [
            "Shiyu Zhao",
            "Long Zhao",
            "Zhixing Zhang",
            "Enyu Zhou",
            "Dimitris Metaxas"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Global_Matching_With_Overlapping_Attention_for_Optical_Flow_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "40"
          ],
          "1": "Furthermore, LoFTR [40] adopted the selfand crossattention to extract better descriptors for feature matching."
        },
        "Clustergnn: Cluster-based coarse-to-fine graph neural network for efficient feature matching": {
          "authors": [
            "Yan Shi",
            "Xiong Cai",
            "Yoli Shavit",
            "Jiang Mu",
            "Wensen Feng",
            "Kai Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Shi_ClusterGNN_Cluster-Based_Coarse-To-Fine_Graph_Neural_Network_for_Efficient_Feature_Matching_CVPR_2022_paper.html",
          "ref_texts": "[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 1, 3, 4, 5",
          "ref_ids": [
            "35"
          ],
          "1": "Recent works [5, 30, 35] have proposed to learn the task of feature matching using graph neural networks (GNNs)",
          "2": "Motivation Existing works [30, 35] which learn the feature matching task with attention-based GNNs, use densely connected graphs.",
          "3": "3), based on the dot product of updated feature representations and the Dual-Softmax operator [26, 35].",
          "4": "The hierarchical clustering enables coarseto-fine grouping Dual-softmax operator [26,35] for computing the matching probability matrix P, by applying the log-softmax operator on both the row and column dimensions of eC, as follows: Pi,j = logSoftMax (eCi,\u00b7)j + logSoftMax (eC\u00b7,j)i."
        },
        "Lamar: Benchmarking localization and mapping for augmented reality": {
          "authors": [
            "PE Sarlin",
            "M Dusmanu",
            "JL Sch\u00f6nberger"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_40",
          "ref_texts": "75. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 13",
          "ref_ids": [
            "75"
          ],
          "1": "We thus compare 1) and 2) with SuperGlue to 2) with LoFTR [75], a state-of-the-art dense matcher."
        },
        "Fs6d: Few-shot 6d pose estimation of novel objects": {
          "authors": [
            "Yisheng He",
            "Yao Wang",
            "Haoqiang Fan",
            "Jian Sun",
            "Qifeng Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.html",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 4, 6, 7",
          "ref_ids": [
            "46"
          ],
          "2": "(1) Gifted with the capability of capturing long-term dependency, the Transformer networks [51, 56] have been successfully applied to aggregate contextual information in the local feature matching [43, 46] and point cloud registration [23] field.",
          "3": "LoFTR [46] is a detector-free deep learning architecture for local image feature matching.",
          "4": "We visualize the results of PREDATOR [23], LoFTR [46] and the proposed FS6D-DPM."
        },
        "MS2DG-Net: Progressive correspondence learning via multiple sparse semantics dynamic graph": {
          "authors": [
            "Luanyuan Dai",
            "Yizhang Liu",
            "Jiayi Ma",
            "Lifang Wei",
            "Taotao Lai",
            "Changcai Yang",
            "Riqing Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dai_MS2DG-Net_Progressive_Correspondence_Learning_via_Multiple_Sparse_Semantics_Dynamic_Graph_CVPR_2022_paper.html",
          "ref_texts": "[32] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "COTR [10] and LoFTR [32] introduce the idea of Transformer [37] to improve the performance of networks."
        },
        "Map-free visual relocalization: Metric pose relative to a single image": {
          "authors": [
            "E Arnold",
            "J Wynn",
            "S Vicente"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_40",
          "ref_texts": "66. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 3, 4, 6, 8",
          "ref_ids": [
            "66"
          ],
          "1": "We show that a combination of deep feature matching [54,66] and deep single-image depth prediction [48,40] currently achieves highest relative pose accuracy.",
          "2": "This basic formula has been improved by learning better features [41,52,21,74,8], better matching [54,66] and better robust estimators [47,85,49,13,5,6,67], and progress has been measured in wide-baseline feature matching challenges [35] and small overlap regimes.",
          "3": "LoFTR [66].",
          "4": ", for learning-based 2D correspondence methods such as SuperGlue [54] and LoFTR [66]."
        },
        "Meshloc: Mesh-based visual localization": {
          "authors": [
            "V Panek",
            "Z Kukelova",
            "T Sattler"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_34",
          "ref_texts": "80. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)",
          "ref_ids": [
            "80"
          ],
          "1": "It is common to use state-of-the-art learned local features [22, 24, 57, 62, 80, 100].",
          "2": "We employ state-of-the-art learned local features [22,24,57,62,80,100] and matching strategies [62].",
          "3": "Using the original database images, we evaluate the approach using multiple learned local features [57,62,80,95,100] and 3D models of different levels of detail.",
          "4": "While SG is based on explicitly detecting local features, LoFTR [80] and Patch2Pix [100] densely match descriptors between pairs of images and extract matches from the resulting correlation volumes.",
          "5": "3D C PA (SG) [62] [80] [100] + SG [100] 800 I 72.",
          "6": "For the retrieval stage, we follow the literature [60, 62, 80, 100] and use the top50 retrieved database images / renderings based on NetVLAD [2] descriptors extracted from the real database and query images.",
          "7": "5 LoFTR [80] 77.",
          "8": "5m and 5\u25e6 / 5m and 10 \u25e6 of the ground truth pose 2DSuperGlue LoFTR Patch2Pix P2P R2D2 CAP+SP t 3D C PA (SG) [62] [80] (P2P) [100] + SG [100] [57] [96] 6.",
          "9": "7 LoFTR [80] 6.",
          "10": "8 LoFTR [80] 6.",
          "11": "2 LoFTR [80] 6.",
          "12": "1 LoFTR [80] 6."
        },
        "Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild": {
          "authors": [
            "W Zhao",
            "S Liu",
            "H Guo",
            "W Wang",
            "YJ Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_31",
          "ref_texts": "67. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "67"
          ],
          "1": "While the de-facto methods for localization and mapping operate on sparse feature points, dense pixel correspondences [23,59,67] have shown great potential especially on videos, thanks to the rapid developments of optical flow predictors."
        },
        "Dynast: Dynamic sparse transformer for exemplar-guided image generation": {
          "authors": [
            "S Liu",
            "J Ye",
            "S Ren",
            "X Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_5",
          "ref_texts": "43. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "43"
          ],
          "1": "2 Image-wise Matching Given a pair of images, image matching such as [19,27,31,41,49,17,43] aims to find pixel-wise correspondence leveraging local features, which is a fundamental problem in computer vision and is one related field to exemplar-guided image generation in this paper."
        },
        "One-inlier is first: Towards efficient position encoding for point cloud registration": {
          "authors": [
            "F Yang",
            "L Guo",
            "Z Chen",
            "W Tao"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/2e163450c1ae3167832971e6da29f38d-Abstract-Conference.html",
          "ref_texts": "[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "35"
          ],
          "1": "One of the most important strategies of Transformer is position encoding, which has been proven crucial when applying Transformer to many computer vision tasks [22, 34, 35].",
          "2": "LoFTR [35] uses the 2D extension of the position encoding to produce position-dependent features for image matching.",
          "3": "Following [35, 48, 36], our method employs a coarse-to-fine manner to find correspondences."
        },
        "A case for using rotation invariant features in state of the art feature matchers": {
          "authors": [
            "Georg Bokman",
            "Fredrik Kahl"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.html",
          "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "39"
          ],
          "1": "Left: LoFTR [39] finds good matches between images under illumination and small viewpoint changes, but the performance deteriorates completely under large rotation changes.",
          "2": "This approach has recently been used with great success in [16, 39, 43].",
          "3": "We will base our experiments on LoFTR [39], which uses a CNN for dense feature description and transformer layers for further feature processing and matching.",
          "4": "Models The base model for our experiments is LoFTR [39].",
          "6": "We use the dual-softmax version of the LoFTR matching in all experiments as this version was best performing in [39].",
          "7": "Following LoFTR [39] and DISK [44], validation is done on theSacre Coeur and St Peter\u2019s Squaresets.",
          "8": "1 Metrics The performance on MegaDepth is measured as in [38, 39] in terms of the area under curve (AUC) of the pose accuracy up to a specific threshold.",
          "9": "2 Metrics The AUC@x metric is used as in [39].",
          "10": "All models perform better than DRC-Net [28] and SuperPoint+SuperGlue [13, 38] which are the comparisons in the LoFTR paper [39, Table 3]."
        },
        "Transformatcher: Match-to-match attention for semantic correspondence": {
          "authors": [
            "Seungwook Kim",
            "Juhong Min",
            "Minsu Cho"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Kim_TransforMatcher_Match-to-Match_Attention_for_Semantic_Correspondence_CVPR_2022_paper.html",
          "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "49"
          ],
          "2": "LoFTR [49] extends this idea to dense 2D feature maps of the images to match, leveraging selfand cross-attention layers between the feature maps to generate strong features for matching."
        },
        "The 8-point algorithm as an inductive bias for relative pose prediction by vits": {
          "authors": [
            "C Rockwell",
            "J Johnson"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044394/",
          "ref_texts": "[60] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 1, 2, 4, 6",
          "ref_ids": [
            "60"
          ],
          "7": "8 LoFTR [60] 0.",
          "8": "In addition, we compare to LoFTR [60].",
          "9": "Methods that are most competitive (LoFTR [60], SuperGlue [55], [22]) require depth supervision in addition to pose, while the best rotation results ([60], [55]) are produced by correspondence-based methods not predicting translation scale."
        },
        "Semi-supervised keypoint detector and descriptor for retinal image matching": {
          "authors": [
            "J Liu",
            "X Li",
            "Q Wei",
            "J Xu",
            "D Ding"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19803-8_35",
          "ref_texts": "29. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 4",
          "ref_ids": [
            "29"
          ],
          "1": "In contrast to RIM, a number of end-to-end methods exist for natural image matching, including SuperPoint [7], R2D2 [21], SuperGlue [25], NCNet [23], LoFTR [29], COTR [10], PDC-Net [32], etc."
        },
        "tsf: Transformer-based semantic filter for few-shot learning": {
          "authors": [
            "J Lai",
            "S Yang",
            "W Liu",
            "Y Zeng",
            "Z Huang",
            "W Wu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20044-1_1",
          "ref_texts": "49. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
          "ref_ids": [
            "49"
          ],
          "1": "Due to its power in learning representation, it has been introduced in many computer vision tasks, such as image classification [8,58,53], detection [76,4,82], segmentation [78,22,66], image matching [49,43] and few-shot learning [6,70,28]."
        },
        "Cats++: Boosting cost aggregation with convolutions and transformers": {
          "authors": [
            "S Cho",
            "S Hong",
            "S Kim"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9933865/",
          "ref_texts": "[27] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "27"
          ],
          "2": "Several works [21], [22], [25], [27], [31], [32] focused on the feature extraction stage, as it has been demonstrated that the more powerful feature representation the model learns, the more robust matching is obtained [27], [31], [32].",
          "4": "3 Transformers in Vision Transformers [41], the de facto standard for Natural Language Processing (NLP) tasks, has recently imposed significant impact on various tasks in Computer Vision fields such as image classification [42], [66], object detection [67], [68], tracking and matching [27], [69].",
          "5": "For those works addressing visual correspondence, LoFTR [27] uses a cross and self-attention module to refine the feature maps conditioned on both input images, and formulate the hand-crafted aggregation layer with dual-softmax [19], [70], and Optimal Transport [25] to infer correspondences.",
          "6": "4 Transformer Aggregator Several works [27], [42], [67], [68] have shown that given images or features as input, transformers [41] integrate the global context information by learning to find the attention scores for all pairs of tokens.",
          "7": "5 Effects of varying the number of the encoders As done in numerous works [27], [39], [42], [67] that utilize transformers, we can also stack more encoders to increase their capacity and validate the effectiveness by varying the number of encoders."
        },
        "ECO-TR: Efficient correspondences finding via coarse-to-fine refinement": {
          "authors": [
            "D Tan",
            "JJ Liu",
            "X Chen",
            "C Chen",
            "R Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20080-9_19",
          "ref_texts": "39. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "39"
          ],
          "1": "LoFTR [39] establishes accurate semi-dense matches with linear transformers in a coarse-to-fine manner."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
          "authors": [
            "Chiu Ma",
            "Anqi Joyce",
            "Shenlong Wang",
            "Raquel Urtasun",
            "Antonio Torralba"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.html",
          "ref_texts": "[74] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InCVPR, 2021.2, 6, 8",
          "ref_ids": [
            "74"
          ],
          "4": "We also compare with LoFTR [74].",
          "5": "We initialize BARF with the poses recovered by our method and LoFTR [74] respectively."
        },
        "Planeformers: From sparse view planes to 3d reconstruction": {
          "authors": [
            "S Agarwala",
            "L Jin",
            "C Rockwell",
            "DF Fouhey"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_12",
          "ref_texts": "49. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 3, 10, 11",
          "ref_ids": [
            "49"
          ],
          "1": "Additionally, we use self-attention, a powerful concept that has been successfully used in several vision tasks [45,49,31,3,69].",
          "2": "Our approach of using self-attention through transformers [54] is similar to SuperGlue [45] and LoFTR [49] in that it permits joint reasoning over the set of potential correspondences.",
          "8": "Our approach is competitive with SuperGlue [45] while LoFTR [49] outperforms competing systems in rotation estimation."
        },
        "3dg-stfm: 3d geometric guided student-teacher feature matching": {
          "authors": [
            "R Mao",
            "C Bai",
            "Y An",
            "F Zhu",
            "C Lu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19815-1_8",
          "ref_texts": "44. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition pp. 8922\u20138931 (2021) 1, 2, 4, 6, 7, 9, 10, 11, 12, 13",
          "ref_ids": [
            "44"
          ],
          "5": "Recently, LoFTR [44] was proposed to learn global consensus between image correspondences by leveraging Transformers.",
          "6": "Our method is based on the matching strategies mentioned in LoFTR [44] due to their high performances.",
          "7": "2, two transformer-based matching modules, inspired by [44], are adopted in both teacher and student branches of our 3DG-STFM system.",
          "10": "It is worth mentioning that our method is based on LoFTR [44], which provides two version implementations for the outdoor dataset in their official code."
        },
        "Input-level inductive biases for 3d reconstruction": {
          "authors": [
            "Wang Yifan",
            "Carl Doersch",
            "Relja Arandjelovic",
            "Joao Carreira",
            "Andrew Zisserman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yifan_Input-Level_Inductive_Biases_for_3D_Reconstruction_CVPR_2022_paper.html",
          "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "51"
          ],
          "1": "Transformers have also contributed to improvements in more general scene correspondence [26,51,58], and even using learned correspondence to improve few-shot learning [9], though these transformers are still applied on feature grids with relatively complex mechanisms to represent correspondence explicitly."
        },
        "Fvor: Robust joint shape and pose optimization for few-view object reconstruction": {
          "authors": [
            "Zhenpei Yang",
            "Zhile Ren",
            "Miguel Angel",
            "Zaiwei Zhang",
            "Qi Shan",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yang_FvOR_Robust_Joint_Shape_and_Pose_Optimization_for_Few-View_Object_CVPR_2022_paper.html",
          "ref_texts": "[51] Jiaming Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition (CVPR) , pages 8922\u20138931, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "Inspired by [31, 51], the multiimage attention module is composed by alternating between self-attention and cross-attention blocks."
        },
        "Global multi-modal 2D/3D registration via local descriptors learning": {
          "authors": [
            "V Markova",
            "M Ronchetti",
            "W Wein",
            "O Zettinig"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-16446-0_26",
          "ref_texts": "23. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
          "ref_ids": [
            "23"
          ],
          "1": "In recent years learned keypoint detectors and descriptors [23,3,2,22] have been successfully applied to computer vision problems involving natural images, replacing hand-crafted approaches such as SIFT [10] and ORB [18].",
          "2": "1 Challenges of local feature extraction for medical images Our method is based on the LoFTR algorithm proposed in [23].",
          "3": "Similarly to [20,23], we define the similarity between two descriptors as their dot product, which allows the networks to encode the quality of a keypoint in the norm of its descriptor."
        },
        "Semi-supervised learning of semantic correspondence with pseudo-labels": {
          "authors": [
            "Jiwon Kim",
            "Kwangrok Ryoo",
            "Junyoung Seo",
            "Gyuseong Lee",
            "Daehwan Kim",
            "Hansang Cho",
            "Seungryong Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Kim_Semi-Supervised_Learning_of_Semantic_Correspondence_With_Pseudo-Labels_CVPR_2022_paper.html",
          "ref_texts": "[59] Jiaming Sun, Zehong Shen, Y uang W ang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 8922\u20138931, 2021.",
          "ref_ids": [
            "59"
          ],
          "1": "Although formulated in various ways, most recent approaches [9, 36, 40, 41, 43, 45, 48, 49, 51, 54, 63, 64] addressed these challenges by carefully designing deep neural networks, such as CNNs [36, 40, 43, 45, 48, 49, 51, 54, 63, 64] or Transformers [9, 59], based models."
        },
        "A transformer-based coarse-to-fine wide-swath SAR image registration method under weak texture conditions": {
          "authors": [
            "Yibo Fan",
            "Feng Wang",
            "Haipeng Wang"
          ],
          "url": "https://www.mdpi.com/2072-4292/14/5/1175",
          "ref_texts": "37. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Montreal, QC, Canada, 11\u201317 October 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "37"
          ],
          "1": "LoFTR (Local Feature TRansformer) [37] has been proposed as a coarse-to-fine image matching method based on Transformers.",
          "2": "Inspired by [37], in this article we use Transformer and CNN to improve the performance of SAR image registration.",
          "3": "Loss Function L = Lc+Lf= \u2212 1\u23d0\u23d0\u23d0Mgt c \u23d0\u23d0\u23d0 \u2211 (\u02dci,\u02dcj)\u2208Mgt C logPc ( \u02dci,\u02dcj ) + 1 |Mf| \u2211 (\u02c6i,\u02c6j\u2032)\u2208Mf 1 \u03c32 ( \u02c6i )\u2225\u02c6j \u2032 \u2212\u02c6j \u2032 gt\u22252 (17) As in [37], this article uses a similar loss function configuration."
        },
        "Pump: Pyramidal and uniqueness matching priors for unsupervised learning of local descriptors": {
          "authors": [
            "Jerome Revaud",
            "Vincent Leroy",
            "Philippe Weinzaepfel",
            "Boris Chidlovskii"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Revaud_PUMP_Pyramidal_and_Uniqueness_Matching_Priors_for_Unsupervised_Learning_of_CVPR_2022_paper.html",
          "ref_texts": "[59] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR, 2021. 3",
          "ref_ids": [
            "59"
          ],
          "1": "To increase the receptive field during feature extraction, LoFTR [59] proposes detector-free local features matching with transformers."
        },
        "CoVisPose: Co-visibility Pose Transformer for Wide-Baseline Relative Pose Estimation in 360 Indoor Panoramas": {
          "authors": [
            "W Hutchcroft",
            "Y Li",
            "I Boyadzhiev",
            "Z Wan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_36",
          "ref_texts": "47. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8922\u20138931 (June 2021)",
          "ref_ids": [
            "47"
          ],
          "2": "LoFTR [47] learns to perform both steps in a detector-free approach and directly outputs dense correspondences.",
          "6": "We use the LoFTR [47] feature matcher as trained in the original paper, exhaustively run on combinations of crops from a panorama pair, and project putative feature matches back to spherical space.",
          "11": ", classic methods like SIFT [31] and learned ones like LoFTR [47], are competitive inthehigh-overlapregime,wherepointfeaturescanbematchedrobustly."
        },
        "Rethinking low-level features for interest point detection and description": {
          "authors": [
            "Changhao Wang",
            "Guanwen Zhang",
            "Zhengyun Cheng",
            "Wei Zhou"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Rethinking_Low-level_Features_for_Interest_Point_Detection_and_Description_ACCV_2022_paper.html",
          "ref_texts": "33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
          "ref_ids": [
            "33"
          ],
          "2": "Inspired by SuperGlue, LoFTR [33], a detector-free approach, proposes to obtain high-quality matches with transformers [39].",
          "3": "In recent years, several studies calculate the similarity matrix of descriptors between input pairs and use a differentiable matching layer to learn good correspondences [25,27,33]."
        },
        "DenseGAP: Graph-structured dense correspondence learning with anchor points": {
          "authors": [
            "Z Kuang",
            "J Li",
            "M He",
            "T Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9956472/",
          "ref_texts": "[4] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "4"
          ],
          "2": "Recently, the concurrent works [4], [7] involve global context between matches by using transformers [30] which achieve great success in many NLP and vision tasks [31], [32], [33] using the attention mechanism.",
          "3": "We follow the attentionbased mechanism of Transformer [30] to implement messagepassing layers in the graph network, while Transformer [30] is also used by recent works [4], [7] in a different way.",
          "5": "66 CAPS [14], SuperGlue [5], LofTR [4] and DualRC-Net [12] and show that our model achieves the best overall performance with a large number of correspondences in Fig."
        },
        "A multi-view thermal\u2013visible image dataset for cross-spectral matching": {
          "authors": [
            "Yuxiang Liu",
            "Yu Liu",
            "Shen Yan",
            "Chen Chen",
            "Jikun Zhong",
            "Yang Peng",
            "Maojun Zhang"
          ],
          "url": "https://www.mdpi.com/2072-4292/15/1/174",
          "ref_texts": "33. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "33"
          ],
          "1": "Instead of performing image feature detection, description, and matching sequentially, LoFTR [33] presents a novel coarse-to-refine method to establish pixel-wise dense matches.",
          "4": "Local feature-free matching methods such as LoFTR [33] and QuadTreeAttention [34] achieve a relatively decent performance."
        },
        "Learning soft estimator of keypoint scale and orientation with probabilistic covariant loss": {
          "authors": [
            "Pei Yan",
            "Yihua Tan",
            "Shengzhou Xiong",
            "Yuan Tai",
            "Yansheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Soft_Estimator_of_Keypoint_Scale_and_Orientation_With_Probabilistic_CVPR_2022_paper.html",
          "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
          "ref_ids": [
            "37"
          ],
          "2": "5, three methods, LIFT [40], D2Net [12] and LoFTR [37] 9, are added as comparison methods."
        },
        "Learning a task-specific descriptor for robust matching of 3D point clouds": {
          "authors": [
            "Z Zhang",
            "Y Dai",
            "B Fan",
            "J Sun"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9847261/",
          "ref_texts": "[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "28"
          ],
          "1": "It not only learns the local geometry of each point in the current point cloud by convolution but also exploits the repetitive structure in the paired point cloud by using the Transformer [26]\u2013[28]."
        },
        "Psmnet: Position-aware stereo merging network for room layout estimation": {
          "authors": [
            "Haiyan Wang",
            "Will Hutchcroft",
            "Yuguang Li",
            "Zhiqiang Wan",
            "Ivaylo Boyadzhiev",
            "Yingli Tian",
            "Sing Bing"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_PSMNet_Position-Aware_Stereo_Merging_Network_for_Room_Layout_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 5",
          "ref_ids": [
            "33"
          ],
          "1": "The transformer (inspired by [33]) consists of self-attention and crossattention layers."
        },
        "Data association between event streams and intensity frames under diverse baselines": {
          "authors": [
            "D Zhang",
            "Q Ding",
            "P Duan",
            "C Zhou",
            "B Shi"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_5",
          "ref_texts": "52. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "52"
          ],
          "1": "2 top), similar to the design of methods [50, 52], as Transformer module can enlarge each feature\u2019s receptive field and thereby include long-range association during matching.",
          "2": "Following DETR [4] and LoFTR [52], we apply the 2D extension of positional encoding in our Transformer module."
        },
        "Pose refinement with joint optimization of visual points and lines": {
          "authors": [
            "S Gao",
            "J Wan",
            "Y Ping",
            "X Zhang",
            "S Dong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981420/",
          "ref_texts": "[7] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d CoRR, vol. abs/2104.00680, 2021.",
          "ref_ids": [
            "7"
          ],
          "1": "A large number of visual point extractors [4] [5], matching methods [6] [7] and point-based 3D mapping approaches [8] [9] have been proposed.",
          "2": "2 LoFTR [7] 47."
        },
        "Superpoint features in endoscopy": {
          "authors": [
            "OL Barbed",
            "F Chadebecq",
            "J Morlana"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-21083-9_5",
          "ref_texts": "[28] Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. IEEE (2021)",
          "ref_ids": [
            "28"
          ],
          "1": "Recent image matching trends propose dense matching as an intermediate step to local matching [34] and incorporating attention for the matching stages [23, 9, 28]."
        },
        "Learning co-segmentation by segment swapping for retrieval and discovery": {
          "authors": [
            "Xi Shen",
            "Alexei A. Efros",
            "Armand Joulin",
            "Mathieu Aubry"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Shen_Learning_Co-Segmentation_by_Segment_Swapping_for_Retrieval_and_Discovery_CVPRW_2022_paper.html",
          "ref_texts": "[58] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv, 2021. 2",
          "ref_ids": [
            "58"
          ],
          "1": "Finally, LoFTR [58] adopts a coarse-to-fine approach to matching with a transformer encoder."
        },
        "OpenGlue: Open source graph neural net based pipeline for image matching": {
          "authors": [
            "O Viniavskyi",
            "M Dobko",
            "D Mishkin"
          ],
          "url": "https://arxiv.org/abs/2204.08870",
          "ref_texts": "[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 6",
          "ref_ids": [
            "45"
          ],
          "1": "We compare OpenGlue against Mutual Nearest Neighbors matcher with second nearest check (SMNN [20]), SuperGlue [40] and LoFTR [45]."
        },
        "Improving feature-based visual localization by geometry-aided matching": {
          "authors": [
            "H Yu",
            "Y Feng",
            "W Ye",
            "M Jiang",
            "H Bao"
          ],
          "url": "https://arxiv.org/abs/2211.08712",
          "ref_texts": "[21] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "21"
          ],
          "1": "Recently, some works adopt graph neural networks [20], [36]\u2013[38] or a detect-free manner [21], [39] to improve 2D-2D feature matching."
        },
        "Recursive deformable image registration network with mutual attention": {
          "authors": [
            "JQ Zheng",
            "Z Wang",
            "B Huang",
            "T Vincent",
            "NH Lim"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-12053-4_6",
          "ref_texts": "19. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "19"
          ],
          "1": "Local feature matching can also benefit from self and cross attention, because transformer networks are proved to obtain feature descriptors that are conditioned on both images [19].",
          "2": "3 Mutual Attention Similar to the idea from [13,19,7,26], Mutual Attention (MA) mechanism [21] is used in the RMAn to obtain the global receptive field and use so-called indicator matrices to quantify the relationship between each pair of pixels from two images, and the usage of multiple indicator matrices is called multi-head."
        },
        "A large-scale invariant matching method based on DeepSpace-ScaleNet for small celestial body exploration": {
          "authors": [
            "Mingrui Fan",
            "Wenlong Lu",
            "Wenlong Niu",
            "Xiaodong Peng",
            "Zhen Yang"
          ],
          "url": "https://www.mdpi.com/2072-4292/14/24/6339",
          "ref_texts": "30. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 21\u201324 June 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "30"
          ],
          "1": "LOFTR proposes a coarse-to-fine local dense feature matching method that achieves better results in weakly textured regions [30].",
          "2": "The AUC of the pose error is the maximum of the angular errors in rotation and translation, which is often used in existing positional error estimates [13,30]."
        },
        "C-3PO: Towards rotation equivariant feature detection and description": {
          "authors": [
            "P Bagad",
            "F Eijkelboom",
            "M Fokkema"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25069-9_44",
          "ref_texts": "26. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021) 4",
          "ref_ids": [
            "26"
          ],
          "1": "Several recent approaches also leverage transformer-based attention models for feature matching [12,22,26].",
          "2": "Notably, [5] replaces the CNN backbone of LoFTR [26] with steerable CNNs based on discrete groups."
        },
        "Check and link: Pairwise lesion correspondence guides mammogram mass detection": {
          "authors": [
            "Z Zhao",
            "D Wang",
            "Y Chen",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19803-8_23",
          "ref_texts": "32. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 5",
          "ref_ids": [
            "32"
          ],
          "1": "3 Learnable Image Matching The well-known image matching in computer vision aims to establish dense correspondences across images for camera pose recovery and scene structure estimation in geometric vision tasks, such as Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) [8,10,23,38,26,2,30,32]."
        },
        "Salve: Semantic alignment verification for floorplan reconstruction from sparse panoramas": {
          "authors": [
            "J Lambert",
            "Y Li",
            "I Boyadzhiev",
            "L Wixson"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19821-2_37",
          "ref_texts": "57. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
          "ref_ids": [
            "57"
          ],
          "1": "Recently, deep learning with graph-based attention [49] or transformers [57] for deep, differentiable key point matching has been exploited to learn and match features from data."
        },
        "Global-aware registration of less-overlap rgb-d scans": {
          "authors": [
            "Che Sun",
            "Yunde Jia",
            "Yi Guo",
            "Yuwei Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_Global-Aware_Registration_of_Less-Overlap_RGB-D_Scans_CVPR_2022_paper.html",
          "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "The local neighborhood information is often similar with less discriminative [14, 25], especially in blurred and textureless regions."
        },
        "Scalenet: A shallow architecture for scale estimation": {
          "authors": [
            "Axel Barroso",
            "Yurun Tian",
            "Krystian Mikolajczyk"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Barroso-Laguna_ScaleNet_A_Shallow_Architecture_for_Scale_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 2, 6",
          "ref_ids": [
            "45"
          ],
          "1": "Besides the added complexity, a multi-scale pyramid is not always a straightforward solution to incorporate in some methods, such as dense correspondence networks [18,45,53].",
          "2": ", that use two input images at the same time to establish the local or dense correspondences [14, 18, 25, 35, 45, 53, 55], but, in that scenario, there is no 12809",
          "3": "We use Lowe\u2019s ratio test [23] and MAGSAC [2] to compute camera poses, and, as in [40,45], report the AUC of the pose errors at 5\u25e6, 10\u25e6, and 20\u25e6, where the error is calculated as the maximum of the rotation and translation angular errors.",
          "4": "the AUC of the camera pose error at 5\u25e6, 10\u25e6, and 20\u25e6 as in [40, 45].",
          "5": "Note that ScaleNet can also be combined with other recent methods [45, 51, 52]."
        },
        "Level set-based camera pose estimation from multiple 2D/3D ellipse-ellipsoid correspondences": {
          "authors": [
            "M Zins",
            "G Simon",
            "MO Berger"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9981161/",
          "ref_texts": "[9] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in CVPR, 2021.",
          "ref_ids": [
            "9"
          ],
          "1": "Recently, detector-free matching, which directly produces dense matches without the detection phase, have also shown promising results [9].",
          "2": "[9] J."
        },
        "Multiview image matching of optical satellite and UAV based on a joint description neural network": {
          "authors": [
            "Chuan Xu",
            "Chang Liu",
            "Hongli Li",
            "Zhiwei Ye",
            "Haigang Sui",
            "Wei Yang"
          ],
          "url": "https://www.mdpi.com/2072-4292/14/4/838",
          "ref_texts": "33. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 19\u201325 June 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "33"
          ],
          "1": "[33] proposed a method of local image feature matching based on the Transformer model, which operates under the idea that intensive pixel-level matching should be established at the coarse level first, and then fine matching should be refined at the fine level, rather than executing image feature detection, description, and matching first."
        },
        "Camera pose estimation and localization with active audio sensing": {
          "authors": [
            "K Yang",
            "M Firman",
            "E Brachmann",
            "C Godard"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19836-6_16",
          "ref_texts": "86. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
          "ref_ids": [
            "86"
          ],
          "1": "The most prevalent methods are feature-matching methods that use a pose solver integrated within a RANSAC framework [69], with state-of-the-art approaches using learned methods for feature detection [27, 71, 96, 6], matching [74, 86] and robust model fitting [108, 70, 11, 87]."
        },
        "Rendernet: Visual relocalization using virtual viewpoints in large-scale indoor environments": {
          "authors": [
            "J Zhang",
            "S Tang",
            "K Qiu",
            "R Huang",
            "C Fang",
            "L Cui"
          ],
          "url": "https://arxiv.org/abs/2207.12579",
          "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "30"
          ],
          "3": "Compared with very recent state-of-the-art LoFTR [30] which is a transformer-based dense image matching method, we achieve improvements on most metrics, which are 11."
        },
        "Integrative feature and cost aggregation with transformers for dense correspondence": {
          "authors": [
            "S Hong",
            "S Cho",
            "S Kim",
            "S Lin"
          ],
          "url": "https://arxiv.org/abs/2209.08742",
          "ref_texts": "[68] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "68"
          ],
          "2": "The advantages of feature aggregation are particularly evident in attention and Transformer-based methods [74, 60, 68, 32, 77] thanks to their attention layers with global receptive fields and adaptability to input tokens, which previous works with convolutions [59, 36, 31, 25, 46] lack.",
          "5": "In accordance with this, recent matching networks [13, 60, 48, 36, 25, 50, 32, 68, 77] proposed effective means for feature aggregation.",
          "10": "However, we observe that combining selfand cross-attention, which is highly similar to LoFTR [68], for feature aggregation helps to boost the performance by conditioning features on both images."
        },
        "Agenti2p: Optimizing image-to-point cloud registration via behaviour cloning and reinforcement learning": {
          "authors": [
            "Shen Yan",
            "Maojun Zhang",
            "Yang Peng",
            "Yu Liu",
            "Hanlin Tan"
          ],
          "url": "https://www.mdpi.com/2072-4292/14/24/6301",
          "ref_texts": "19. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 8922\u20138931. Remote Sens.2022, 14, 6301 17 of 18",
          "ref_ids": [
            "19"
          ],
          "1": "As demonstrated in recent research, Transformers have brought about significant performance boosts for image-to-image matching [18,19] and point-to-point correspondence [67]."
        },
        "Sim2e: Benchmarking the group equivariant capability of correspondence matching algorithms": {
          "authors": [
            "S Su",
            "Z Zhao",
            "Y Fei",
            "S Li",
            "Q Chen",
            "R Fan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25056-9_47",
          "ref_texts": "25. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "LoFTR [25] is a detector-free and end-to-end architecture.",
          "2": "\u2013 LoFTR [25] is trained with the same experimental setup as SuperGlue."
        },
        "RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training": {
          "authors": [
            "PK Rajendran",
            "S Mishra",
            "LF Vecchietti"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25075-0_18",
          "ref_texts": "33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "33"
          ],
          "1": "Detector-free local feature matching with transformers (LoFTR) employs a cross attention layer to obtain feature descriptors conditioned on both images to obtain dense matches in low texture areas where traditional methods struggle to produce repeatable points [33]."
        },
        "A method for detecting feature-sparse regions and matching enhancement": {
          "authors": [
            "Longhao Wang",
            "Chaozhen Lan",
            "Beibei Wu",
            "Tian Gao",
            "Zijun Wei",
            "Fushan Yao"
          ],
          "url": "https://www.mdpi.com/2072-4292/14/24/6214",
          "ref_texts": "21. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Montreal, QC, Canada, 11\u201318 October 2021; pp. 8922\u20138931.",
          "ref_ids": [
            "21"
          ],
          "1": "SuperGlue [20] and LoFTR [21] are recent graph neural network (GNN) matching algorithms whose image matching operations are based on the learning of affine relationships between 3D image pairs, i."
        },
        "Danish airs and grounds: A dataset for aerial-to-street-level place recognition and localization": {
          "authors": [
            "A Vallone",
            "F Warburg",
            "H Hansen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9811266/",
          "ref_texts": "[26] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d CVPR, 2021.",
          "ref_ids": [
            "26"
          ],
          "1": "LOFTR [26], on the other hand, takes a pair of images as input and via a ViT [27]-based transformer architecture estimates both keypoints and matches simultaneously."
        },
        "Lightweight monocular depth estimation with an edge guided network": {
          "authors": [
            "X Dong",
            "MA Garratt",
            "SG Anavatti"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10004313/",
          "ref_texts": "[25] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in CVPR, 2021, pp.",
          "ref_ids": [
            "25"
          ],
          "1": "Inspired by [25], we adopt the linear transformer encoder layer to capture the long-range dependencies (or global context) between the edge and context features through crossattention in two directions."
        },
        "Object-guided day-night visual localization in urban scenes": {
          "authors": [
            "A Benbihi",
            "C Pradalier",
            "O Chum"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9955638/",
          "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922\u20138931, 2021. 2, 4, 6",
          "ref_ids": [
            "46"
          ],
          "1": "Recently, Patch2Pix [60] learns to regress a single pixel correspondence, while Loftr [46] learns a dense matching of the pixels between corresponding receptive fields.",
          "2": "1 Experimental Setup OGuL is evaluated against other feature matching methods: the default Nearest-Neighbor (NN) approach, the coarse-to-fine Patch2Pix [60] and LOFTR [46], the graph-based approach SuperGlue [39], and the filtering method AdaLAM [7]."
        },
        "A hybrid deep feature-based deformable image registration method for pathology images": {
          "authors": [
            "C Zhang",
            "Y Jiang",
            "N Li",
            "Z Zhang",
            "MT Islam"
          ],
          "url": "https://arxiv.org/abs/2208.07655",
          "ref_texts": "[19] Sun, J., Shen, Z., Wang, Y., Bao, H. & Zhou, X. LoFTR: Detector-free local feature matching with transformers. Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition. pp. 8922-8931",
          "ref_ids": [
            "19"
          ],
          "2": "Detector-free techniques, such as correspondence transformer-based image matching network [22], and LOFTR [19], employ the designed network for end-to-end matching without requiring a dedicated detector to identify interest sites."
        },
        "Revisiting the receptive field of conv-gru in droid-slam": {
          "authors": [
            "Antyanta Bangunharcana",
            "Soohyun Kim",
            "Soo Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/html/Bangunharcana_Revisiting_the_Receptive_Field_of_Conv-GRU_in_DROID-SLAM_CVPRW_2022_paper.html",
          "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "The recent surge in the adoption of Transformers [50] based methods towards vision tasks [6,12,27] have also sparked recent research which uses Graph Neural Networks [39] and Transformers [44, 56] for correspondence search.",
          "2": "Self-attention Conv-GRU Inspired by the success of recent correspondence search works by using Transformers to attend to global features [44, 56], we investigate attention-based updates of the ConvGRU."
        },
        "Coarse TRVO: A robust visual odometry with detector-free local feature": {
          "authors": [
            "Y Gao",
            "L Zhao"
          ],
          "url": "https://www.jstage.jst.go.jp/article/jaciii/26/5/26_731/_article/-char/ja/",
          "ref_texts": "[5] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 8922-8931, 2021.",
          "ref_ids": [
            "5"
          ],
          "1": "Therefore, the developer of LoFTR [5] integrated transformer [6] into feature matching and proposed a novel local feature matching method without detection process."
        },
        "A real-time fusion framework for long-term visual localization": {
          "authors": [
            "Y Yang",
            "X Zhang",
            "S Gao",
            "J Wan",
            "Y Ping",
            "Y Liu"
          ],
          "url": "https://arxiv.org/abs/2210.09757",
          "ref_texts": "[14] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021.",
          "ref_ids": [
            "14"
          ],
          "1": "[14] depends on Transformer networks to exceed the limits of local features in order to improve the accuracy of 2D-3D correspondences and localization performance."
        },
        "Learning geometric feature embedding with transformers for image matching": {
          "authors": [
            "Xiaohu Nan",
            "Lei Ding"
          ],
          "url": "https://www.mdpi.com/1424-8220/22/24/9882",
          "ref_texts": "27. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. In Proceedings of the CVPR, Nashville, TN, USA, 19\u201321 June 2021.",
          "ref_ids": [
            "27"
          ],
          "1": "Some of the subsequent methods [27] also continue the idea of using the Transformer to learn the overall correspondence.",
          "2": "From the Loftr [27] we know that dual-softmax (DS) is used to compute the assignment matrix with the same effect as optimal transport, and we choose the easier one to implement a dual-softmax operation to compute the assignment matrix.",
          "3": "It can be demonstrated that the additional information of geometry is a crucial factor in enhancing the descriptors, and the performance is better than the other baseline methods in all cases except at night when the performance is slightly weaker than that of the Loftr [27] in the 0.",
          "4": "Method Day Night Loftr [27] 88."
        },
        "D-inloc++: Indoor localization in dynamic environments": {
          "authors": [
            "M Dubenova",
            "A Zderadickova",
            "O Kafka",
            "T Pajdla"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-16788-1_16",
          "ref_texts": "49. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "49"
          ],
          "1": "\u2013 Relative posestep finds local correspondences [31,17,18,40,49,27] between the query image and the kmost similar images selected in the image retrieval step.",
          "2": ", SuperPoint [17], D2-Net [18], R2D2 [40], LoFTR [49] or a Key."
        },
        "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization": {
          "authors": [
            "X Xu",
            "L Guan",
            "E Dunn",
            "H Li",
            "G Hua"
          ],
          "url": "https://arxiv.org/abs/2212.04575",
          "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
          "ref_ids": [
            "30"
          ],
          "1": "Conversely, learned dense feature descriptors [15, 22, 14, 30, 34] defined over the whole image instead of only local visual information and their corresponding feature matching methods [25, 24, 23, 12, 39] estimate the best correspondences based on these dense features."
        },
        "Nonlinear intensity sonar image matching based on deep convolution features": {
          "authors": [
            "X Zhou",
            "C Yu",
            "X Yuan",
            "Y Wu",
            "H Feng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9775321/",
          "ref_texts": "[20] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \"LoF TR: Detector-Free Local Feature Matching with Transformers,\" 2021. ",
          "ref_ids": [
            "20"
          ],
          "1": "Comparative approaches In the subsequent experiments, we introduced image matching approaches SIFT, ORB, BRISK [18], SuperPoint [19] and LoFTR [20] for comparison."
        },
        "Sa-dnet: A on-demand semantic object registration network adapting to non-rigid deformation": {
          "authors": [
            "H Xie",
            "J Qiu",
            "Y Dai",
            "Y Yang",
            "C Xiang"
          ],
          "url": "https://arxiv.org/abs/2210.09900",
          "ref_texts": "[18] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "18"
          ],
          "1": "[17], LoFTR [18], MatchFormer [9], have shown high generalizability for infrared and visible feature matching.",
          "2": "In addition, many methods such as D2Net [26], R2D2 [27], COTR [28], and LoFTR [18] have used SfM datasets to construct dense correspondence supervised training in recent years, bringing a new level of effectiveness for feature matching.",
          "4": "MatchFormer [9], LoFTR [18], and other methods [36] [17] can rely on their powerful matching ability to get many corresponding feature points in the natural scenes.",
          "7": "LoFTR [18] introduces Transformer [29] into feature matching, which improves the matching effect of detector-free based methods to an unprecedented level."
        },
        "A Lightweight Domain Adaptive Absolute Pose Regressor Using Barlow Twins Objective": {
          "authors": [
            "PK Rajendran",
            "QV Lai-Dang",
            "LF Vecchietti"
          ],
          "url": "https://arxiv.org/abs/2211.10963",
          "ref_texts": "[59] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021. 16",
          "ref_ids": [
            "59"
          ],
          "1": "Transformer-based methods like LoFTR [59] and TransforMatcher [27] carry out feature matching as well as semantic correspondence matching."
        },
        "SuperGF: Unifying local and global features for visual localization": {
          "authors": [
            "W Song",
            "R Yan",
            "B Lei",
            "T Okatani"
          ],
          "url": "https://arxiv.org/abs/2212.13105",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3",
          "ref_ids": [
            "40"
          ],
          "1": "image retrieval [26, 47, 49] and image matching [34, 40]."
        },
        "Homography augmented momentum contrastive learning for SAR image retrieval": {
          "authors": [
            "S Park",
            "M Rysz",
            "KM Dipple",
            "PM Pardalos"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-21225-3_3",
          "ref_texts": "[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers.arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "34"
          ],
          "1": "Thus, replacing these matching techniques with scalable DNN-based methods is in an active research area where many approaches such as SuperGlue [33] and LoFTR [34] have been proposed."
        },
        "Real-time local feature with global visual information enhancement": {
          "authors": [
            "J Miao",
            "H Yue",
            "Z Liu",
            "X Wu",
            "Z Fang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10006314/",
          "ref_texts": "[24] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "24"
          ],
          "1": "Thus, we adopt the 2-dimensional extension of absolute sinusoidal positional embedding [24] before Non-local block [23]: PEi x,y := ..."
        },
        "U (1) Symmetry-breaking Observed in Generic CNN Bottleneck Layers": {
          "authors": [
            "LF Bouchard",
            "MB Lazreg",
            "M Toews"
          ],
          "url": "https://arxiv.org/abs/2206.02220",
          "ref_texts": "[73] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "73"
          ],
          "1": "The mechanism of spatially localized activations (as opposed to global descriptors) is closely linked to the attention mechanisms [33], including non-local networks [83], squeeze-and-excitation networks [31], transformer architectures [81, 11, 27] including hierarchically shifted windows [45], thin bottleneck layers [65], self-attention mechanisms considering locations and channels [88], intra-kernel correlations [26], multi-layer perceptrons incorporating Euler\u2019s angle [78], correspondence-based transformers [34] and detectors [73], and geometrical embedding of spatial information via graphs [39, 29].",
          "2": "g PCA [22]) or specialized architectures [73, 34]."
        },
        "Deep Dense Local Feature Matching and Vehicle Removal for Indoor Visual Localization": {
          "authors": [
            "KH Park"
          ],
          "url": "https://arxiv.org/abs/2205.12544",
          "ref_texts": "[15] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8922\u20138931.",
          "ref_ids": [
            "15"
          ],
          "1": "On the other hand, LoFTR [15] produces dense matches even in the low-textured areas, thanks to positional encoding.",
          "2": "Hence, we employ deep dense local feature matching [15] which does not require detecting interest points.",
          "3": "It is due to the coarse-to-fine nature of LoFTR [15]."
        },
        "Leveraging Image Matching Toward End-to-End Relative Camera Pose Regression": {
          "authors": [
            "F Khatib",
            "Y Margalit",
            "M Galun",
            "R Basri"
          ],
          "url": "https://arxiv.org/abs/2211.14950",
          "ref_texts": "[53] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1, 2, 3, 4, 6, 7, 8, 12",
          "ref_ids": [
            "53"
          ],
          "1": "For the first step we compute for each image a semi-dense feature map using the pre-trained LoFTR architecture [53].",
          "3": "The recent LoFTR [53] and COTR [28] utilize self and cross-attention layers along with multiscale analysis to produce semi-dense, near-pixel-wise correspondences.",
          "4": "Our architecture improves over previous work by relying on the powerful LoFTR IM architecture [53] to produce a semi-dense feature map, warping the feature maps by utilizing hard matches between the produced features, followed by a camera motion regressor, which is trained by a loss in which translation direction and scale are separated into different terms.",
          "5": "Feature extraction For feature extraction we use the coarse module of LoFTR [53], which is pre-trained for matching on the Scannet dataset [18], depicting indoor scenes.",
          "8": "Nevertheless, while our method achieves significantly better performance than SIFT [36], its performance is still surpassed by learnable feature matching methods [50, 53] combined with a depth estimation method [35], for obtaining the metric scale from the depth as explained in [3].",
          "9": "As in the previous experiment, our method surpasses SIFT [36], but achieves inferior results compared to [50, 53] combined with a depth estimation method [43]."
        },
        "Visual SLAM in changing environments": {
          "authors": [
            "E Sinisalo"
          ],
          "url": "https://helda.helsinki.fi/server/api/core/bitstreams/a49ef563-dbfe-477c-a43c-ba95da487364/content",
          "ref_texts": "[115] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matchingwithtransformers. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8918\u20138927, 2021.",
          "ref_ids": [
            "115"
          ],
          "1": "Deep learning solutions for feature matching such as Learned Graph Neural Network feature/keypoint matchers such as LoFTR[115] and Superglue [105] have also appeared in the literature."
        },
        "Monocular 3D Object Detection and 3D Multi-Object Tracking for Autonomous Vehicles": {
          "authors": [
            "CA Reading"
          ],
          "url": "https://search.proquest.com/openview/fd74ea9328cd730911edca5f3b3c7e6a/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[94] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
          "ref_ids": [
            "94"
          ],
          "2": "LoFTR [94] is capable of finding correspondences on the texture-less wall and table.",
          "3": "Recently, Transformers have seen use in computer vision tasks such as image classification [26], object detection [8], semantic segmentation [98], and feature matching [94].",
          "5": "To mitigate the receptive field limitation, SuperGlue [85] and LoFTR [94] introduce a graph neural-network (GNN) and transformer module into their respective pipelines to allow for aggregation of both local and global information during feature extraction.",
          "6": "To extract track and detection features, we follow the Transformer design of LoFTR [94] to incorporate global object information.",
          "7": "We adopt the transformer module from LoFTR [94] and interleave the self and cross attention blocks Nc times."
        },
        "Multi-Modal Retinal Image Registration via Deep Neural Networks": {
          "authors": [
            "J Zhang"
          ],
          "url": "https://search.proquest.com/openview/0f437e88a479a17411e00ef2eb8ea3ea/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[93] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 8918\u20138927.",
          "ref_ids": [
            "93"
          ],
          "1": "First, in recent advancements of general image registration, transformers [93] have been proved effective in image matching in various scenarios, and thus could be introduced into the multi-modal retinal image registration task to help in estimating more accurate correspondence."
        },
        "Visual place recognition for unmanned vehicles in city-scale challenging environments": {
          "authors": [
            "G Peng"
          ],
          "url": "https://dr.ntu.edu.sg/handle/10356/165647",
          "ref_texts": "[46] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "46"
          ],
          "2": "Recently, LoFTR [46] propose a local feature transformer which excels in producing dense matches in low-texture areas.",
          "5": "To mine potential correspondences from the cross-matching, the latest SOTAs [89, 46, 27] is followed to use dual-softmax (FDS) and mutual nearest neighbor search (FMNN) strategies.",
          "6": "The latest SOTAs [43, 45, 46] for feature matching excel at finding dense correspondences.",
          "7": "To enable CAHIR to selectively learn more potential matches, the pre-trained LoFTR [46] is introduced as a teacher model for distillation.",
          "8": "As for local representations, the SOTA local feature matchers, SuperGlue [45] and LoFTR [46], are adapted to the VPR task.",
          "9": "2 LoFTR [46] \u221a 88."
        },
        "Detector-Free Dense Feature Matching for Fetoscopic Mosaicking": {
          "authors": [
            "S Bano",
            "F Vasconcelos",
            "A David",
            "J Deprest"
          ],
          "url": "https://discovery.ucl.ac.uk/id/eprint/10157679/",
          "ref_texts": "[4] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d inConference on Computer Vision and Pattern Recognition, 2021, pp.",
          "ref_ids": [
            "4"
          ],
          "1": "In the paper, we propose the use of transformer-based detector-free local feature matching (LoFTR) method [4] as a dense feature matching technique for creating reliable mosaics with minimal drifting error.",
          "2": "Detector-Free Feature Representation The recently proposed LoFTR [4] method first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level.",
          "3": "For more detail, please refer to [4], in which it is shown that LoFTR produces high-quality matches even in regions having low-textures and are affected by motion blur or repetitive patterns; making it an ideal matching module for fetoscopic mosaicking.",
          "4": "The LoFTR matching model, pretrained on the ScanNet dataset [4], is used for obtaining the fine-level matches between two consecutive frames.",
          "5": "CONCLUSIONS We propose a fetoscopic video mosaicking method that benefited from the detector-free feature matching with transformers (LoFTR) [4] method, resulting in generating reliable virtual expanded field-of-view image of the intraoperative fetoscopic environment."
        },
        "System for Detection and Tracking of Windows in Urban Environment": {
          "authors": [
            "J Bedkowski"
          ],
          "url": "https://wydawnictwo.umg.edu.pl/pp-rai2022/pdfs/31_pp-rai-2022-081.pdf",
          "ref_texts": "13. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
          "ref_ids": [
            "13"
          ],
          "1": "Recently, a detector-free method was introduced to solve image matching LoFTR [13] which uses Transformer architecture with global receptive field to provide dense matches at coarse level and refine only good ones.",
          "2": "The system output can be improved when use LoFTR image matching [13]."
        },
        "Deep vit features as dense visual descriptors": {
          "authors": [
            "S Amir",
            "Y Gandelsman",
            "S Bagon"
          ],
          "url": "https://dino-vit-features.github.io/paper.pdf",
          "ref_texts": "50. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
          "ref_ids": [
            "50"
          ],
          "1": "Recent supervised methods employ transformers for dense correspondence in images from the same scene [50, 25]."
        },
        "Cofinet: Reliable coarse-to-fine correspondences for robust pointcloud registration": {
          "authors": [
            "H Yu",
            "F Li",
            "M Saleh",
            "B Busam"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/c85b2ea9a678e74fdc8bafe5d0707c31-Abstract.html",
          "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "25"
          ],
          "1": "Recently, a coarse-to-fine mechanism is leveraged by our 2D counterparts [23, 24, 25] to avoid direct keypoint detection, which shows superiority over the state-of-the-art detection-based method [26].",
          "2": "As witnessed in 2D image matching, many recent works [23, 24, 25] leverage a coarse-to-fine mechanism to eliminate the inherent repeatability problem in keypoint detection and thus boost the performance.",
          "3": "In a similar coarse-to-fine manner with Patch2Pixel, LoFTR [25] leverages Transformers [33], together with an optimal transport matching layer [26], to match mutual-nearest patches on the coarse level, and then refines the corresponding pixel of the patch center on the finer level."
        },
        "Cats: Cost aggregation transformers for visual correspondence": {
          "authors": [
            "S Cho",
            "S Hong",
            "S Jeon",
            "Y Lee"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html",
          "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680, 2021.",
          "ref_ids": [
            "51"
          ],
          "2": "Several works [24, 9, 37, 39, 47, 51] focused on the feature extraction stage, as it has been proven that the more powerful feature representation the model learns, the more robust matching is obtained [24, 9, 51].",
          "4": "Transformer [61], the de facto standard for Natural Language Processing (NLP) tasks, has recently imposed significant impact on various tasks in Computer Vision fields such as image classification [10, 55], object detection [3, 62], tracking and matching [52, 51].",
          "5": "For visual correspondence, LoFTR [51] uses cross and self-attention module to refine the feature maps conditioned on both input images, and formulate the hand-crafted aggregation layer with dual-softmax [45, 60] and optimal transport [47] to infer correspondences.",
          "6": "Several works [10, 3, 62, 51] have shown that given images or features as input, Transformers [61] integrate the global information in a flexible manner by learning to find the attention scores for all pairs of tokens."
        },
        "Attention meets geometry: Geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation": {
          "authors": [
            "P Ruhkamp",
            "D Gao",
            "H Chen",
            "N Navab"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665911/",
          "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "40"
          ],
          "1": "Recent works [40] have shown that transformer models with self and cross-attention can outperform fully convolution networks [27] for the task of finding dense correspondences between image pairs.",
          "2": "Temporal-Attention Layer Inspired by the correlation layer in optical flow [21] and recent dense matching pipelines [40], we formulate a novel temporal attention across frames by exploiting the temporal image sequence input of the self-supervised training scheme."
        },
        "Multi-view stereo with transformer": {
          "authors": [
            "J Zhu",
            "B Peng",
            "W Li",
            "H Shen",
            "Z Zhang",
            "J Lei"
          ],
          "url": "https://arxiv.org/abs/2112.00336",
          "ref_texts": "[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "27"
          ],
          "1": "Besides, Transformers are utilized for homography estimation, relative pose estimation, and visual localization in [27]."
        },
        "Effect of parameter optimization on classical and learning-based image matching methods": {
          "authors": [
            "Ufuk Efe",
            "Kutalmis Gokalp",
            "Aydin Alatan"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021W/TradiCV/html/Efe_Effect_of_Parameter_Optimization_on_Classical_and_Learning-Based_Image_Matching_ICCVW_2021_paper.html",
          "ref_texts": "[33] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "2 Homography Estimation Accuracy (HEA) Homography Estimation Accuracy (HEA) is another widely used metric for image matching evaluation and used in [11, 35, 33, 14] as a performance metric."
        },
        "Digging into self-supervised learning of feature descriptors": {
          "authors": [
            "I Melekhov",
            "Z Laskar",
            "X Li",
            "S Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665840/",
          "ref_texts": "[66] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and ZhouXiaowei. LoFTR:Detector-freelocalfeaturematching with transformers. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021. 2",
          "ref_ids": [
            "66"
          ],
          "1": "Although, those approaches have demonstrated improved performance over classical hand-crafted methods on challenging benchmarks such as vision localization and image matching, most of them require ground-truth pixel correspondencesbetweentwoviews[20,30,57,66]."
        },
        "Tech details for loftr in the imw challenge": {
          "authors": [
            "X He",
            "Y Wang",
            "J Sun",
            "Z Shen",
            "H Bao",
            "X Zhou"
          ],
          "url": "https://zju3dv.github.io/loftr/files/LoFTR_IMC21.pdf",
          "ref_texts": "[6] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR, 2021.",
          "ref_ids": [
            "6"
          ],
          "1": "Method and Technical Details Our method is based on LoFTR [6], a detector-free local feature matching method with Transformers [8].",
          "2": "Dataset and Pre-trained Models We use the MegaDepth [4] dataset to train our models, following the same setup as in [6]."
        },
        "Performance analysis of interest point detection/matching on shiny and non-textured surfaces": {
          "authors": [
            "R Huizer",
            "J van Gemert",
            "B Yildiz"
          ],
          "url": "https://repository.tudelft.nl/file/File_09991e55-9531-4521-806c-f46fa8baa09e?preview=1",
          "ref_texts": "[14] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.",
          "ref_ids": [
            "14"
          ],
          "1": "This also applies to many other proposed neural network based algorithms such as LoFTR [14], the work by Ignacio Rocco, Relja Arandjelovic, and Josef Sivic [10] and DRC-Net [5].",
          "2": "The best performing algorithms are algorithms that do not just consider the local context of an image but instead also consider the global context, such as LoFTR [14].",
          "3": "Two more recent interest point matching algorithms that will be compared are SuperGlue [11] and LoFTR [14].",
          "4": "The results also show that learned descriptor representations outperform the hand-tuned descriptors and that the detector-free approach of LoFTR can find many correspondences in regions with low texture, confirming that the results found in [1] and [14] also hold in the context of aircraft engine borescope inspection videos."
        }
      }
    },
    {
      "title": "streetcrafter: street view synthesis with controllable video diffusion models",
      "id": 57,
      "valid_pdf_number": "1/1",
      "matched_pdf_number": "1/1",
      "matched_rate": 1.0,
      "citations": {
        "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models": {
          "authors": [
            "M YU",
            "W Hu",
            "J Xing",
            "Y Shan"
          ],
          "url": "https://arxiv.org/abs/2503.05638",
          "ref_texts": "[86] Yunzhi Yan, Zhen Xu, Haotong Lin, Haian Jin, Haoyu Guo, Yida Wang, Kun Zhan, Xianpeng Lang, Hujun Bao, Xiaowei Zhou, et al. Streetcrafter: Street view synthesis with controllable video diffusion models. In CVPR, 2025. 3",
          "ref_ids": [
            "86"
          ],
          "1": "Additionally, stereo video generation [102] and driving scene novel view synthesis [18, 79, 86] are also receiving significant attention."
        }
      }
    },
    {
      "title": "perceiving unseen 3d objects by poking the objects",
      "id": 38,
      "valid_pdf_number": "8/9",
      "matched_pdf_number": "7/8",
      "matched_rate": 0.875,
      "citations": {
        "Benchmarking neural radiance fields for autonomous robots: An overview": {
          "authors": [
            "Y Ming",
            "X Yang",
            "W Wang",
            "Z Chen",
            "J Feng"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0952197624018438",
          "ref_texts": "[265] L. Chen, Y. Song, H. Bao, X. Zhou, Perceiving Unseen 3D Objects by Poking the Objects, 2023. doi:10.48550/arXiv.2302.13375, arXiv:2302.13375 [cs].",
          "ref_ids": [
            "265",
            "cs"
          ],
          "1": "Some use a poking strategy to generate 3D reconstructions for unknown objects [265]."
        },
        "Nerf in robotics: A survey": {
          "authors": [
            "G Wang",
            "L Pan",
            "S Peng",
            "S Liu",
            "C Xu",
            "Y Miao"
          ],
          "url": "https://arxiv.org/abs/2405.01333",
          "ref_texts": "[139] L. Chen, Y . Song, H. Bao, and X. Zhou, \u201cPerceiving unseen 3d objects by poking the objects,\u201d ICRA, 2023.",
          "ref_ids": [
            "139"
          ],
          "1": "(b) Operation: The 3D structural bias of NeRF contains richer scene information compared to 2D perception methods and can be directly used for specific operational tasks when combined with some operation planning methods [11], [139]\u2013[152].",
          "2": "[139] propose to continuously poke the detected object with a robotic arm to obtain the complete visual perception for modelling an unknown target object."
        },
        "Easyhec: Accurate and automatic hand-eye calibration via differentiable rendering and space exploration": {
          "authors": [
            "L Chen",
            "Y Qin",
            "X Zhou",
            "H Su"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10251600/",
          "ref_texts": "[30] L. Chen, Y . Song, H. Bao, and X. Zhou, \u201cPerceiving unseen 3d objects by poking the objects,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 4834\u20134841.",
          "ref_ids": [
            "30"
          ],
          "1": "Real-world applications EasyHeC is useful in a variety of real-world applications [29, 30]."
        },
        "AIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model": {
          "authors": [
            "Z Qi",
            "S Yuan",
            "F Liu",
            "H Cao",
            "T Deng",
            "J Yang"
          ],
          "url": "https://arxiv.org/abs/2409.16019",
          "ref_texts": "[20] L. Chen, Y . Song, H. Bao, and X. Zhou, \u201cPerceiving unseen 3d objects by poking the objects,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2023, pp. 4834\u20134841.",
          "ref_ids": [
            "20"
          ],
          "1": "Vision Tasks Enhanced by Embodiment With a physical body to control, embodied AI can significantly enhance many robotics tasks such as perception [16] [17], tracking [18], and reconstruction [19] [20]."
        },
        "EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models": {
          "authors": [
            "Z Hong",
            "K Zheng",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801359/",
          "ref_texts": "[2] L. Chen, Y . Song, H. Bao, and X. Zhou, \u201cPerceiving unseen 3d objects by poking the objects,\u201d in ICRA, 2023, pp. 4834\u20134841.",
          "ref_ids": [
            "2"
          ],
          "1": "This is crucial for many robotic applications, such as robotic grasping [1], [2], robotic manipulation [3], [4], and robotic assembly [5].",
          "2": "This work opens up more possibilities for lab and household applications [1], [2], [3], [4], [5], [43], [44] that require hand-eye calibration to reduce the sim-to-real gap, such as robot manipulation and grasping."
        }
      }
    },
    {
      "title": "acquisition through my eyes and steps: a joint predictive agent model in egocentric worlds",
      "id": 53,
      "valid_pdf_number": "1/1",
      "matched_pdf_number": "1/1",
      "matched_rate": 1.0,
      "citations": {
        "EgoLife: Towards Egocentric Life Assistant": {
          "authors": [
            "J Yang",
            "S Liu",
            "H Guo",
            "Y Dong",
            "X Zhang"
          ],
          "url": "https://arxiv.org/abs/2503.03803",
          "ref_texts": "[165] Chen Lu, Wang Yizhou, Tang Shixiang, Ma Qianhong, He Tong, Ouyang Wanli, Zhou Xiaowei, Bao Hujun, and Peng Sida. Acquisition through my eyes and steps: A joint predictive agent model in egocentric worlds. arXiv preprint arXiv:2502.05857, 2025. 26",
          "ref_ids": [
            "165"
          ],
          "1": "Chen Lu, Wang Yizhou, Tang Shixiang, Ma Qianhong, He Tong, Ouyang Wanli, Zhou Xiaowei, Bao Hujun, and Peng Sida."
        }
      }
    },
    {
      "title": "the present and future of mixed reality in china",
      "id": 46,
      "valid_pdf_number": "1/2",
      "matched_pdf_number": "1/1",
      "matched_rate": 1.0,
      "citations": {
        "Aspek Teknis Pengembangan Karya Arsitektural di Metaverse": {
          "authors": [
            "F Purwanto"
          ],
          "url": "https://pdfs.semanticscholar.org/b442/7f76b95b7fdc80717be87de771013668f6a1.pdf",
          "ref_texts": "[11] G. Zhang, X. Zhou, F. Tian, H. Zha, Y. Wang, and H. Bao, \u201cThe present and future of mixed reality in China,\u201d Commun. ACM, vol. 64, no. ",
          "ref_ids": [
            "11"
          ],
          "1": "Huawei memiliki istilah Cyberverse [10], [11] dengan menggunakan Augmented Reality seperti halnya MagicLeap melalui tampila n layar di dalam ponsel cerdas.",
          "2": "[11] G."
        }
      }
    },
    {
      "title": "reconstructing 3d human pose by watching humans in the mirror",
      "id": 16,
      "valid_pdf_number": "52/56",
      "matched_pdf_number": "42/52",
      "matched_rate": 0.8076923076923077,
      "citations": {
        "Recovering 3d human mesh from monocular images: A survey": {
          "authors": [
            "Y Tian",
            "H Zhang",
            "Y Liu",
            "L Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10195242/",
          "ref_texts": "[306] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou, \u201cReconstructing 3D human pose by watching humans in the mirror,\u201d in CVPR, 2021, pp. 12 814\u201312 823.",
          "ref_ids": [
            "306"
          ],
          "1": "3M 1 8 1 SMPL [44] MuCo-3DHP [303] 200K 1 8 1 \u223c 4 MuPoTs-3D [303] > 8K 20 8 3 \u2713 MannequinChallenge [304] 24,428 567 742 5 \u2713 SMPL [305] 3DOH50K [141] 51,600 1 1 SMPL [141] Mirrored-Human [306] 1.",
          "2": "8M > 200 > 200 \u2265 1 \u2713 SMPL [306] MTC [105] 834K 1 40 1 EHF [22] 100 1 1 1 SMPL-X [22] HUMBI [307] 17.",
          "3": "Mirrored-Human [306] consists of videos from the Internet, in which we can see a person and the person\u2019s image in a mirror."
        },
        "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans": {
          "authors": [
            "Sida Peng",
            "Yuanqing Zhang",
            "Yinghao Xu",
            "Qianqian Wang",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Peng_Neural_Body_Implicit_Neural_Representations_With_Structured_Latent_Codes_for_CVPR_2021_paper.html",
          "ref_texts": "[15] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. InCVPR, 2021.",
          "ref_ids": [
            "15"
          ],
          "1": "T o obtain the 3D representation at a frame, we first transform the code locations based on the human pose, which can be reliably estimated from sparse camera views [3, 13, 15]."
        },
        "Cliff: Carrying location information in full frames into human pose and shape estimation": {
          "authors": [
            "Z Li",
            "J Liu",
            "Z Zhang",
            "S Xu",
            "Y Yan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20065-6_34",
          "ref_texts": "11. Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3d human pose by watching humans in the mirror. In: CVPR (2021)",
          "ref_ids": [
            "11"
          ],
          "1": "Optimization-based methods [4,11] are first proposed to iteratively fit the SMPL model to 2D evidences, while regression-based ones [18,62] make the predictions in a straightforward way that may support real time applications."
        },
        "Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction": {
          "authors": [
            "Wenjia Wang",
            "Yongtao Ge",
            "Haiyi Mei",
            "Zhongang Cai",
            "Qingping Sun",
            "Yanjun Wang",
            "Chunhua Shen",
            "Lei Yang",
            "Taku Komura"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[14] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 3",
          "ref_ids": [
            "14"
          ],
          "1": "Optimization-based methods [5, 14] directly fit the body model parameters to 2D evidence via gradient backpropagation in an iterative manner."
        },
        "Learning to estimate robust 3d human mesh from in-the-wild crowded scenes": {
          "authors": [
            "Hongsuk Choi",
            "Gyeongsik Moon",
            "Kyu Park",
            "Kyoung Mu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Choi_Learning_To_Estimate_Robust_3D_Human_Mesh_From_In-the-Wild_Crowded_CVPR_2022_paper.html",
          "ref_texts": "[7] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3D human pose by watching humans in the mirror. In CVPR, 2021. 8",
          "ref_ids": [
            "7"
          ],
          "1": "[7] 85."
        },
        "Learning analytical posterior probability for human mesh recovery": {
          "authors": [
            "Qi Fang",
            "Kang Chen",
            "Yinghui Fan",
            "Qing Shuai",
            "Jiefeng Li",
            "Weidong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.html",
          "ref_texts": "[9] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, pages 12814\u201312823, 2021. 2",
          "ref_ids": [
            "9"
          ],
          "1": "Leveraging the parametric human model [41, 54], optimization-based approaches [2, 9, 15, 51] fit the parameters via iteration while learning-based approaches regress the parameters with neural networks."
        },
        "Pina: Learning a personalized implicit neural avatar from a single rgb-d video sequence": {
          "authors": [
            "Zijian Dong",
            "Chen Guo",
            "Jie Song",
            "Xu Chen",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.html",
          "ref_texts": "[18] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12814\u2013",
          "ref_ids": [
            "18"
          ],
          "1": "These works typically leverage parametric models for minimally clothed human bodies [9, 17, 18, 29, 46, 55] (e."
        },
        "Probabilistic human mesh recovery in 3d scenes from egocentric views": {
          "authors": [
            "Siwei Zhang",
            "Qianli Ma",
            "Yan Zhang",
            "Sadegh Aliakbarian",
            "Darren Cosker",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Probabilistic_Human_Mesh_Recovery_in_3D_Scenes_from_Egocentric_Views_ICCV_2023_paper.html",
          "ref_texts": "[10] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3D human pose by watching humans in the mirror. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12814\u2013",
          "ref_ids": [
            "10"
          ],
          "1": "Given a single RGB image, the task of deterministic 3D human mesh recovery has been widely studied in the literature, with regression-based methods [7, 8, 11, 24, 27, 28, 30, 31, 35\u201337, 45, 49, 62, 71, 76, 83], optimization-based methods [4, 10, 33, 50, 69] or hybrid methods [23, 29, 58], mostly adopting parametric 3D body models [4, 70] to represent the 3D human mesh."
        },
        "Pliks: A pseudo-linear inverse kinematic solver for 3d human body estimation": {
          "authors": [
            "Karthik Shetty",
            "Annette Birkhold",
            "Srikrishna Jaganathan",
            "Norbert Strobel",
            "Markus Kowarschik",
            "Andreas Maier",
            "Bernhard Egger"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Shetty_PLIKS_A_Pseudo-Linear_Inverse_Kinematic_Solver_for_3D_Human_Body_CVPR_2023_paper.html",
          "ref_texts": "[6] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12814\u2013",
          "ref_ids": [
            "6"
          ],
          "1": "Optimization-based approaches [3, 6] make use of 2D keypoints estimated by a Deep Neural Network (DNN) which are iteratively fit with the SMPL model."
        },
        "Npc: Neural point characters from video": {
          "authors": [
            "Yang Su",
            "Timur Bagautdinov",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html",
          "ref_texts": "[8] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021. 6, 8",
          "ref_ids": [
            "8"
          ],
          "1": "We additionally evaluate NPC on one subject with loose clothing from ZJU-Mocap [8, 35], and use challenging motion sequences, including dancing and gymnastic from poses AIST++ [20] and SURREAL+CMU-Mocap [6, 48] dataset, for animating our learned characters.",
          "2": "NPC Deformation on Loose Clothing To provide a further comparison on pose-dependent deformation, we tested on subject 387 of ZJU-Mocap [8, 35], which includes loose clothing and long-range deformation dependency."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[19] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021.",
          "ref_ids": [
            "19"
          ],
          "1": "Based on the statistical human model, most works [8, 25, 26, 18, 17, 19] reconstruct the naked body mesh from various inputs and some works further add surface deformation to capture more details [63, 27, 48, 11, 61]."
        },
        "Human mesh recovery from multiple shots": {
          "authors": [
            "Georgios Pavlakos",
            "Jitendra Malik",
            "Angjoo Kanazawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Pavlakos_Human_Mesh_Recovery_From_Multiple_Shots_CVPR_2022_paper.html",
          "ref_texts": "[12] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3D human pose by watching humans in the mirror. In CVPR, 2021. 4",
          "ref_ids": [
            "12"
          ],
          "1": "[12] use mirror reflections as an additional view for resolving the depth ambiguity."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
          "authors": [
            "Chiu Ma",
            "Anqi Joyce",
            "Shenlong Wang",
            "Raquel Urtasun",
            "Antonio Torralba"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.html",
          "ref_texts": "[25] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. InCVPR, 2021.3",
          "ref_ids": [
            "25"
          ],
          "1": "With the flourishing of deep learning, these methods have made tremendous progress, either from a single image [41, 44, 47] or multi-view images [21,22,25,61]."
        },
        "Rohm: Robust human motion reconstruction via diffusion": {
          "authors": [
            "Siwei Zhang",
            "Bharat Lal",
            "Yuanlu Xu",
            "Alexander Winkler",
            "Petr Kadlecek",
            "Siyu Tang",
            "Federica Bogo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_RoHM_Robust_Human_Motion_Reconstruction_via_Diffusion_CVPR_2024_paper.html",
          "ref_texts": "[14] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3D human pose by watching humans in the mirror. In CVPR, 2021. 2",
          "ref_ids": [
            "14"
          ],
          "1": "Many approaches in the literature focus on 3D human shape and pose reconstruction from a single image [9, 14, 15, 33, 41\u201346, 49\u201351, 75, 86, 90, 97], recently also considering robustness to occlusions [38, 40, 48, 53, 69, 102, 104]."
        },
        "3D real-time human reconstruction with a single RGBD camera": {
          "authors": [
            "Yang Lu"
          ],
          "url": "https://link.springer.com/article/10.1007/s10489-022-03969-4",
          "ref_texts": "8. Fang Q, Shuai Q, Dong J, Bao H, Zhou X (2021) Reconstructing 3d human pose by watching humans in the mirror. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 12814\u201312823",
          "ref_ids": [
            "8"
          ],
          "1": "One is the non-parametric reconstruction methods [6, 7, 11, 31] based on multi-camera calibration and point cloud fusion, while the other is the parametric reconstruction methods [3, 8, 14, 16] based on the deformation of 3D human model template."
        },
        "Flexnerf: Photorealistic free-viewpoint rendering of moving humans from sparse views": {
          "authors": [
            "Vinoj Jayasundara",
            "Amit Agrawal",
            "Nicolas Heron",
            "Abhinav Shrivastava",
            "Larry S. Davis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.html",
          "ref_texts": "[7] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021. 6, 7",
          "ref_ids": [
            "7"
          ],
          "2": "Benchmark Datasets and Metrics We evaluate the proposed method on two public datasets: ZJU-MoCap [7, 28] and People Snapshot [2], and one SelfCaptured Fashion (SCF) dataset."
        },
        "Body size and depth disambiguation in multi-person reconstruction from single images": {
          "authors": [
            "N Ugrinovic",
            "A Ruiz",
            "A Agudo",
            "A Sanfeliu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665960/",
          "ref_texts": "[9] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021. 4322",
          "ref_ids": [
            "9"
          ],
          "1": "Although it is very difficult to annotate large-scale datasets with these models, there are optimization-based methods such as [7, 41, 9] that allow to fit the parameters very well using only 2D and 3D joint annotations.",
          "2": "The most relevant works in this area are [60, 61, 18, 63, 9, 52]."
        },
        "Hdg-ode: A hierarchical continuous-time model for human pose forecasting": {
          "authors": [
            "Yucheng Xing",
            "Xin Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xing_HDG-ODE_A_Hierarchical_Continuous-Time_Model_for_Human_Pose_Forecasting_ICCV_2023_paper.html",
          "ref_texts": "[25] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021.",
          "ref_ids": [
            "25"
          ],
          "1": "Although some schemes relying on multi-view images [50, 60, 94, 80, 72, 10, 70, 28, 40, 25, 11] or other sensors [54, 31, 27, 94] can ease this problem to some degree, we have to admit that paired images of different views or complementary sensors are not always available in most scenarios.",
          "2": "Similar to single-person case, multi-view sources were also used in [25, 10, 70, 40, 28]."
        },
        "Morf: Mobile realistic fullbody avatars from a monocular video": {
          "authors": [
            "Renat Bashirov",
            "Alexey Larionov",
            "Evgeniya Ustinova",
            "Mikhail Sidorenko",
            "David Svitov",
            "Ilya Zakharkin",
            "Victor Lempitsky"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Bashirov_MoRF_Mobile_Realistic_Fullbody_Avatars_From_a_Monocular_Video_WACV_2024_paper.html",
          "ref_texts": "[13] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. InCVPR, 2021. 2",
          "ref_ids": [
            "13"
          ],
          "1": "We evaluate our approach on 3 datasets: self-captured, ZJU-MoCap [13, 49] and People Snapshot [3]."
        },
        "DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image": {
          "authors": [
            "Q Wu",
            "Z Dou",
            "S Xu",
            "S Shimada",
            "C Wang",
            "Z Yu"
          ],
          "url": "https://arxiv.org/abs/2406.17988",
          "ref_texts": "[22] Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3d human pose by watching humans in the mirror. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 12814\u201312823 (2021) 3",
          "ref_ids": [
            "22"
          ],
          "1": "Conversely, regression-based methods [5, 22, 23, 25, 43, 45, 49, 53, 62] leverage deep neural networks to directly infer the pose and shape parameters of the SMPL model."
        },
        "Freeman: Towards benchmarking 3d human pose estimation under real-world conditions": {
          "authors": [
            "Jiong Wang",
            "Fengyu Yang",
            "Bingliang Li",
            "Wenbo Gou",
            "Danqi Yan",
            "Ailing Zeng",
            "Yijun Gao",
            "Junle Wang",
            "Yanqing Jing",
            "Ruimao Zhang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wang_FreeMan_Towards_Benchmarking_3D_Human_Pose_Estimation_under_Real-World_Conditions_CVPR_2024_paper.html",
          "ref_texts": "[13] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021. 2",
          "ref_ids": [
            "13"
          ],
          "1": "3M 14 303DPW[55] Real Scene7 47 4 60 51K 1 30Mirrored Human[13]Laboratory1."
        },
        "Learning dense uv completion for human mesh recovery": {
          "authors": [
            "Y Wang",
            "Q Sun",
            "W Wang",
            "J Ling",
            "Z Cai",
            "R Xie"
          ],
          "url": "https://arxiv.org/abs/2307.11074",
          "ref_texts": "[14] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2",
          "ref_ids": [
            "14"
          ],
          "1": "Optimization-based methods [19, 31, 15, 3, 14] work by fitting a statistical human body model to the 2D cues extracted from the input image, such as 2D keypoints [3] or silhouette [31]."
        },
        "Physically Plausible Color Correction for Neural Radiance Fields": {
          "authors": [
            "Q Zhang",
            "Y Feng",
            "H Li"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72784-9_10",
          "ref_texts": "22. Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3d human pose by watching humans in the mirror. In: CVPR (2021)",
          "ref_ids": [
            "22"
          ],
          "1": "With the increasing prevalence of multi-camera systems [18,22,31,34,53, 59,73], NeRF and its variants [4,5,50] face challenges in achieving photorealistic novel view synthesis with color consistency."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[105] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou, \u201cReconstructing 3d human pose by watching humans in the mirror,\u201d in CVPR, 2021.",
          "ref_ids": [
            "105"
          ],
          "1": "1 Evaluation datasets To validate the effectiveness of our proposed methods, we use a synthetic dataset (RANA [92]) and two real-world datasets (PeopleSnapshot [104], ZJU-MoCap [38], [105]) for performance evaluation.",
          "2": "ZJU-MoCap [38], [105] In ZJU-MoCap, the subjects captured in the video may conduct very complex motions."
        },
        "Fusion of short-term and long-term attention for video mirror detection": {
          "authors": [
            "M Xu",
            "J Wu",
            "Y Lai",
            "Z Ji"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10688367/",
          "ref_texts": "[2] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou, \u201cReconstructing 3d human pose by watching humans in the mirr or, \u201d in CVPR, 2021.",
          "ref_ids": [
            "2"
          ],
          "1": "The reflection of the mirror can provide hints for locating objects [1] with 3D information [2].",
          "2": "reconstructing human pose [2], and reconstructing scenes [3]."
        },
        "Decanus to Legatus: Synthetic training for 2D-3D human pose lifting": {
          "authors": [
            "Yue Zhu",
            "David Picard"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Zhu_Decanus_to_Legatus_Synthetic_training_for_2D-3D_human_pose_lifting_ACCV_2022_paper.html",
          "ref_texts": "13. Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3d human pose by watching humans in the mirror. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 3",
          "ref_ids": [
            "13"
          ],
          "1": "On the other hand, a difficulty that the discriminative models have is that depth information is hard to infer from a single image when it is not explicitly modeled, and thus additional bias must be learned using 3D supervision [25, 26], multiview spatial consistency [13, 45, 48] or temporal consistency [1, 9, 23].",
          "2": "[13] propose a virtual mirror so that the estimated 3D poses, after being symmetrically projected into the other side of the mirror, should also look correctly, thus simulating another way of \u2018multiview\u2019 consistency."
        },
        "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans": {
          "authors": [
            "A Chatziagapi",
            "B Chaudhuri",
            "A Kumar"
          ],
          "url": "https://arxiv.org/abs/2409.16666",
          "ref_texts": "13. Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3D Human Pose by Watching Humans in the Mirror. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)",
          "ref_ids": [
            "13"
          ],
          "1": "Our training data consists of monocular frontal-only videos of talking humans, whereas existing approaches leverage information from side and back views [11,13,32,42,51,68,71].",
          "2": "Compared to prior work [64,68], our videos are more challenging for the following reasons: (a) they only have frontal views, whereas previous methods use videos with side and back views as well, (b) the variation in limb articulation is more limited, following a long-tailed distribution (our subjects are mostly standing and talking, with mainly arm and hand motion, in contrast to free-movement videos [13,32,51]), and (c) we include facial expression and hand articulation, compared to only body pose considered in prior work."
        },
        "Structure from duplicates: neural inverse graphics from a pile of objects": {
          "authors": [
            "T Cheng",
            "WC Ma",
            "K Guan",
            "A Torralba"
          ],
          "url": "https://arxiv.org/abs/2401.05236",
          "ref_texts": "[16] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12814\u201312823, 2021.",
          "ref_ids": [
            "16"
          ],
          "1": "By sharing or regularizing their underlying representation, one can more effectively constrain and reconstruct their 3D geometry [25, 16, 9], as well as enable various powerful image/shape manipulation operations [34, 59]."
        },
        "Mirror-aware neural humans": {
          "authors": [
            "D Ajisafe",
            "J Tang",
            "SY Su",
            "B Wandt"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550614/",
          "ref_texts": "[7] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Supplementary Material: Reconstructing 3D Human Pose by Watching Humans in the Mirror. https:// zju3dv.github.io/Mirrored-Human/images/ mirror_supp.pdf. Accessed: 2023-08-03. 2",
          "ref_ids": [
            "7"
          ],
          "1": "Moreover, their best results are attained using manually annotated vanishing lines on the mirror boundary [7]."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "142. Fang, Q., Shuai, Q., Dong, J., Bao, H., Zhou, X.: Reconstructing 3D human pose by watching humans in the mirror. In: CVPR, pp. 12814\u201312823 (2021).https://doi.org/10.1109/CVPR46437.",
          "ref_ids": [
            "142"
          ],
          "2": "Multi-view Neural Human Rendering (NHR) [142], ZJUMoCap [142, 143], and Neural Actors [163] datasets are representative of a common multi-view human movement dataset in which actors perform complex motions with daily clothing."
        },
        "PersonaCraft: Personalized Full-Body Image Synthesis for Multiple Identities from Single References Using 3D-Model-Conditioned Diffusion": {
          "authors": [
            "G Kim",
            "SY Jeon",
            "S Lee",
            "SY Chun"
          ],
          "url": "https://arxiv.org/abs/2411.18068",
          "ref_texts": "[21] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021. 2",
          "ref_ids": [
            "21"
          ],
          "1": "The approach includes three core mechanisms: 1) Face and body identity extraction : We extract body shape parameters via SMPLx fitting [2, 13, 21] and obtain face ID embeddings from reference images using InsightFace [1]."
        },
        "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene": {
          "authors": [
            "S Biswas",
            "Q Wu",
            "B Banerjee",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2409.17459",
          "ref_texts": "[46] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021.",
          "ref_ids": [
            "46"
          ],
          "1": "71 Table 5: Reconstruction on ZJU-Mocap (upper) [9, 46] and synthetic animal dataset [28] (lower)."
        },
        "Key points trajectory and multi-level depth distinction based refinement for video mirror and glass segmentation": {
          "authors": [
            "Z Wang",
            "Y Liu",
            "X Cheng",
            "T Ikenaga"
          ],
          "url": "https://link.springer.com/article/10.1007/s11042-024-19627-5",
          "ref_texts": "9. Fang Q, Shuai Q, Dong J, Bao H, Zhou X (2021) Reconstructing 3d human pose by watching humans in the mirror. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp 12814\u201312823",
          "ref_ids": [
            "9"
          ],
          "1": "In pose estimation [7, 8], the reflection of the mirror provides an additional view of the human body and solves the depth ambiguity problem of the monocular camera [9]."
        },
        "ProPLIKS: Probablistic 3D human body pose estimation": {
          "authors": [
            "K Shetty",
            "A Birkhold",
            "B Egger",
            "S Jaganathan"
          ],
          "url": "https://arxiv.org/abs/2412.04665",
          "ref_texts": "[11] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou, \u201cReconstructing 3d human pose by watching humans in the mirror,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 12 814\u201312 823. 2",
          "ref_ids": [
            "11"
          ],
          "1": "Optimization methods, as exemplified in [10, 11], typically employ 2D keypoints derived from Deep Neural Networks (DNNs) and iteratively fit these keypoints to the SMPL model, though they are often subject to sensitivity regarding initial conditions and potential local optima."
        },
        "Multimodal Active Measurement for Human Mesh Recovery in Close Proximity": {
          "authors": [
            "T Maeda",
            "K Takeshita",
            "N Ukita"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10685076/",
          "ref_texts": "[8] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2021.",
          "ref_ids": [
            "8"
          ],
          "1": "EasyMocap utilizes multiple calibrated RGB environmental cameras to satisfy enough accuracy to create the human motion dataset ZJU-Mocap [8].",
          "2": "Multiple Calibrated Environmental cameras may estimate accurate poses as EasyMocap [1] and ZJU-Mocap dataset [8]."
        },
        "MirrorCalib: Utilizing Human Pose Information for Mirror-based Virtual Camera Calibration": {
          "authors": [
            "L Liao",
            "R Zheng",
            "A Mitchell"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10672615/",
          "ref_texts": "[23] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou, \u201cReconstructing 3D human pose by watching humans in the mirror,\u201d in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2021. doi: 10.1109/CVPR46437.2021.01262.",
          "ref_ids": [
            "23"
          ],
          "1": "Alternatively, they can be estimated using vanishing points [23] under the assumption that the camera has zero skew, square pixels, and the principal point is located at the image center."
        },
        "M-NeRF: model-based human reconstruction from scratch with mirror-aware neural radiance fields": {
          "authors": [
            "DA Ajisafe"
          ],
          "url": "https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0423218",
          "ref_texts": "[14] Q. Fang, Q. Shuai, J. Dong, H. Bao, and X. Zhou. Reconstructing 3D human pose by watching humans in the mirror. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12814\u201312823, 2021. \u2192 pages 2, 3, 7, 8, 26, 27, 28, 30, 31, 32, 34, 41",
          "ref_ids": [
            "14"
          ],
          "1": "In this line of research, previous work [14, 41] has leveraged reflections in mirrors for better human pose reconstruction.",
          "20": "MirrorHuman [14] achieves the lowest error, however, they use 2D ground truth, manual annotation of the mirror position, and require a good 3D initialization.",
          "22": "[14], outperforms the supervised approach SPIN [37] and single-view optimization SMPLify-X [57], and is comparable to their combination, as these single-view approaches do not fare well under occlusion and are prone to depth ambiguity."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
          "authors": [
            "JA Rahim"
          ],
          "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
          "ref_texts": "[11] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. InCVPR, 2021.",
          "ref_ids": [
            "11"
          ],
          "1": "We perform our training and testing on the ZJUMoCap dataset [11, 30]."
        }
      }
    },
    {
      "title": "relightable and animatable neural avatar from sparse-view video",
      "id": 31,
      "valid_pdf_number": "12/13",
      "matched_pdf_number": "12/12",
      "matched_rate": 1.0,
      "citations": {
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[36] Z. Xu, S. Peng, C. Geng, L. Mou, Z. Yan, J. Sun, H. Bao, and X. Zhou, \u201cRelightable and animatable neural avatar from sparse-view video,\u201d arXiv preprint arXiv:2308.07903, 2023. 2, 3",
          "ref_ids": [
            "36"
          ],
          "2": "With the advancement of neural implicit representations, recent methods [36], [104]\u2013[108] necessitate solely multi-view or even monocular video recordings obtained under constant unknown illumination conditions to model both human motion and light transport properties.",
          "3": "[36] utilize Hierarchical Distance Queries (HDQ) via sphere tracing to calculate correct SDF values under arbitrary human poses and then incorporate distant field soft shadow (DFSS) for estimating reasonable soft visibility maps."
        },
        "Meshavatar: Learning high-quality triangular human avatars from multi-view videos": {
          "authors": [
            "Y Chen",
            "Z Zheng",
            "Z Li",
            "C Xu",
            "Y Liu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73113-6_15.pdf",
          "ref_texts": "97. Xu, Z., Peng, S., Geng, C., Mou, L., Yan, Z., Sun, J., Bao, H., Zhou, X.: Relightable and animatable neural avatar from sparse-view video. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2024)",
          "ref_ids": [
            "97"
          ],
          "7": "387 to render) to render an image of512 \u00d7 512 resolution, in contrast to 40s in [57], \u223c20s in [88], 5s in [97] and 50s in [13], proving the effectiveness of our meshbased representation.",
          "12": "[97] on the quality of novel light synthesis, despite limited enhancements in geometry reconstruction."
        },
        "Intrinsicavatar: Physically based inverse rendering of dynamic humans from monocular videos via explicit ray tracing": {
          "authors": [
            "Shaofei Wang",
            "Bozidar Antic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_IntrinsicAvatar_Physically_Based_Inverse_Rendering_of_Dynamic_Humans_from_Monocular_CVPR_2024_paper.html",
          "ref_texts": "[78] Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Relightable and animatable neural avatar from sparse-view video. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024. 3",
          "ref_ids": [
            "78"
          ],
          "1": "[78] designs a hierarchical distance query algorithm and extends DFSS [53] to deformable neural SDF, achieving efficient light visibility computation using sphere tracing."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[30] Z. Xu, S. Peng, C. Geng, L. Mou, Z. Yan, J. Sun, H. Bao, and X. Zhou, \u201cRelightable and animatable neural avatar from sparseview video,\u201d in CVPR, 2024.",
          "ref_ids": [
            "30"
          ],
          "2": "Recently, High quality physical properties reconstruction of clothed human avatar [25], [26], [27], [29], [30], [79], [91], [92], [93], [94] have been widely explored with the development of Neural Radiance Fields [16]."
        },
        "Interactive Rendering of Relightable and Animatable Gaussian Avatars": {
          "authors": [
            "Y Zhan",
            "T Shao",
            "H Wang",
            "Y Yang",
            "K Zhou"
          ],
          "url": "https://arxiv.org/abs/2407.10707",
          "ref_texts": "[13] Z. Xu, S. Peng, C. Geng, L. Mou, Z. Yan, J. Sun, H. Bao, and X. Zhou, \u201cRelightable and animatable neural avatar from sparseview video,\u201d arXiv preprint arXiv:2308.07903, 2023.",
          "ref_ids": [
            "13"
          ],
          "2": "RelightableAvatar-Xu [13] proposes Hierarchical Distance Query on the SDF field for sphere tracing, and further utilizes Distance Field Soft Shadow (DFSS) for soft visibility.",
          "6": "As we use the entire image to calculate metrics, the results will be higher than those reported in past works [13].",
          "11": "Overall, although our results seem roughly similar to those of RALin [15] and RA-Xu [13], our results perform better in preserving the details."
        },
        "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video": {
          "authors": [
            "Y Hong",
            "Y Wu",
            "Z Shen",
            "C Guo",
            "Y Jiang",
            "Y Zhang"
          ],
          "url": "https://arxiv.org/abs/2502.08297",
          "ref_texts": "[71] Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Relightable and animatable neural avatar from sparse-view video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 990\u20131000, 2024. 3",
          "ref_ids": [
            "71"
          ],
          "1": "For human performance relighting, researchers [5, 6, 39, 43, 71, 83] extend mesh-based and neural relighting methods by incorporating body pose priors [29, 41]."
        },
        "HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos": {
          "authors": [
            "Q Chen",
            "R Xie",
            "K Huang",
            "Q Wang",
            "W Zheng"
          ],
          "url": "https://arxiv.org/abs/2405.11270",
          "ref_texts": "[59] Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. 2023. Relightable and Animatable Neural Avatar from Sparse-View Video. arXiv preprint arXiv:2308.07903 (2023).",
          "ref_ids": [
            "59"
          ],
          "1": "At the same time, inspired by neural reflectance decomposition[7, 66], Relighting4D [11] and Relightavatar [59] have attempted to recover human avatars with decoupled geometry and materials with implicit representation.",
          "2": "And Relighting4D and Relightavatar [11, 59] have attempted to recover human avatars with decoupled geometry and materials implicit representation."
        },
        "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars": {
          "authors": [
            "S Sasaki",
            "J Wu",
            "K Nishino"
          ],
          "url": "https://arxiv.org/abs/2412.04433",
          "ref_texts": "[32] Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Relightable and animatable neural avatar from sparse-view video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 990\u20131000, 2024. 2, 3",
          "ref_ids": [
            "32"
          ],
          "1": "CV] 6 Dec 2024 construction [6, 7, 12, 13, 19, 22\u201324, 27, 32, 35].",
          "2": "More recently, a number of 3DGS-based approaches have been proposed for creating animatable avatars from monocular [2, 3, 9] and multiview [6, 12, 19, 24, 32] video.",
          "3": "Many existing 3DGS-based avatar reconstruction methods use linear blend skinning (LBS) [8, 17] to change the pose of an avatar [6, 24, 32]."
        }
      }
    },
    {
      "title": "learning human mesh recovery in 3d scenes",
      "id": 32,
      "valid_pdf_number": "12/12",
      "matched_pdf_number": "11/12",
      "matched_rate": 0.9166666666666666,
      "citations": {
        "Recovering 3d human mesh from monocular images: A survey": {
          "authors": [
            "Y Tian",
            "H Zhang",
            "Y Liu",
            "L Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10195242/",
          "ref_texts": "[269] Z. Shen, Z. Cen, S. Peng, Q. Shuai, H. Bao, and X. Zhou, \u201cLearning human mesh recovery in 3D scenes,\u201d in CVPR, 2023, pp. 17 038\u201317 047.",
          "ref_ids": [
            "269"
          ],
          "1": "There are also scene-aware approaches [268], [269] to recovering plausible human motions in a pre-scaned 3D scene.",
          "2": "In [192], [195], [196], [269], the humanscene contact status is predicted to improve the plausibility."
        },
        "Tore: Token reduction for efficient human mesh recovery with transformer": {
          "authors": [
            "Zhiyang Dou",
            "Qingxuan Wu",
            "Cheng Lin",
            "Zeyu Cao",
            "Qiangqiang Wu",
            "Weilin Wan",
            "Taku Komura",
            "Wenping Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.html",
          "ref_texts": "[61] Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Learning human mesh recovery in 3d scenes. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 17038\u2013",
          "ref_ids": [
            "61"
          ],
          "1": "In future work, one of the promising directions could be applying the shown enhanced efficiency of HMR from monocular images to methods exhibiting high complexity for improving the model efficiency, especially in tasks such as human-environment/object interaction that perceives environments [37, 22, 18, 75, 73, 19, 61], as well as HMR from videos that involve temporal information [28, 53, 35, 63, 79, 72]."
        },
        "Dynamic inertial poser (dynaip): Part-based motion dynamics learning for enhanced human pose estimation with sparse inertial sensors": {
          "authors": [
            "Yu Zhang",
            "Songpengcheng Xia",
            "Lei Chu",
            "Jiarui Yang",
            "Qi Wu",
            "Ling Pei"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Dynamic_Inertial_Poser_DynaIP_Part-Based_Motion_Dynamics_Learning_for_Enhanced_CVPR_2024_paper.html",
          "ref_texts": "[35] Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Learning human mesh recovery in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 17038\u201317047, 2023. 1, 2",
          "ref_ids": [
            "35"
          ],
          "1": "Our paper examines HPE, a field marked by varied sensing modalities and methodologies [2, 6, 10], divided into three categories: 1) Vision-based HPE [35, 39], using single or multi-view images, known for its significant advancements; 2) Wireless-based HPE [3, 50], which addresses some vision-based challenges but is limited by environmental factors; 3) Wearable-based HPE [14, 17, 46], our focus, *Equal contribution \u2020Corresponding authors This work was supported by the National Nature Science Foundation of China (NSFC) under Grant 62273229.",
          "2": "Related Work HPE has been widely explored using different methods, including visual [8, 18, 35, 39], inertial [14, 17, 46], wireless [3, 43], and various hybrid approaches [32, 49]."
        },
        "Synergistic Global-space Camera and Human Reconstruction from Videos": {
          "authors": [
            "Yizhou Zhao",
            "Tuanfeng Yang",
            "Bhiksha Raj",
            "Min Xu",
            "Jimei Yang",
            "Hao Paul"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Synergistic_Global-space_Camera_and_Human_Reconstruction_from_Videos_CVPR_2024_paper.html",
          "ref_texts": "[49] Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Learning human mesh recovery in 3d scenes. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 17038\u2013",
          "ref_ids": [
            "49"
          ],
          "1": "Notably , this is achieved without requiring extra annotations or heuristic designs to decide which part of a human should be interacting with the scene [49] and which region in the scene is most likely to be in contact with humans [38, 62]."
        },
        "UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-view and Temporal Cues": {
          "authors": [
            "V Davoodnia",
            "S Ghorbani",
            "MA Carbonneau"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72640-8_2",
          "ref_texts": "54. Shen, Z., Cen, Z., Peng, S., Shuai, Q., Bao, H., Zhou, X.: Learning human mesh recovery in 3D scenes. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 17038\u201317047 (2023).https://doi.org/10.1109/ CVPR52729.2023.01634",
          "ref_ids": [
            "54"
          ],
          "1": "Next, we report the Translation Aligned error (TA-MPJPE) and PA-MPJPE for our OoD experiments on the RICH [16] dataset following prior works [26,28,34,35,54,59].",
          "2": "Please refer to our Supplementary Materials for a more in-depth comparison with weaklysupervised and semi-supervised approaches [9,27,53,56,62], multi-view methods without that do not rely on camera parameters [10,20,48], and monocular 3D pose estimation methods [26,28,34,35,54,59]."
        },
        "Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture": {
          "authors": [
            "Z Wang",
            "Z Hu",
            "R Guo",
            "H Pi",
            "Z Feng",
            "S Peng"
          ],
          "url": "https://arxiv.org/abs/2503.03222",
          "ref_texts": "[24] Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Learning human mesh recovery in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 17038\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "SA-HMR [24] recovers the absolute position of human meshes by utilizing a pre-scanned scene from a single image.",
          "4": "SA-HMR [24] has the most similar inputs to ours, allowing for a direct comparison of Abs-MPJPE, which does not align with the ground truth."
        },
        "Distribution and depth-aware transformers for 3d human mesh recovery": {
          "authors": [
            "J Bright",
            "B Balaji",
            "H Prakash",
            "Y Chen"
          ],
          "url": "https://assets.pubpub.org/nzhl89hf/f9h-11715796570485.pdf",
          "ref_texts": "[25] Z. Shen, Z. Cen, S. Peng, Q. Shuai, H. Bao, and X. Zhou, \u201cLearning human mesh recovery in 3d scenes,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 038\u201317 047.",
          "ref_ids": [
            "25"
          ],
          "1": "SAHMR [25] uses cross-attention between image and scene contact information to improve the posture of the regressed mesh."
        },
        "EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling": {
          "authors": [
            "S Xia",
            "Y Zhang",
            "Z Su",
            "X Zheng",
            "Z Lv",
            "G Wang"
          ],
          "url": "https://arxiv.org/abs/2412.10235",
          "ref_texts": "[33] Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Learning human mesh recovery in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 17038\u2013",
          "ref_ids": [
            "33"
          ],
          "1": "Scene-aware motion estimation and generation Recent advancements in human motion estimation and generation have expanded from solely analyzing body motion to integrating interactions with the surrounding environment, especially vision-based approaches [11, 12, 16, 33, 46, 51].",
          "2": "[33] used sparse 3D CNNs to estimate absolute positions and dense scene contacts, refining human mesh recovery via cross-attention with 3D scene cues.",
          "3": "Inspired by [26, 33], we employ a cross-attention module between the initial motion estimates and the cropped environmental point clouds to refine the human motion estimation."
        },
        "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction": {
          "authors": [
            "J Yang",
            "S Xia",
            "Z Lai",
            "L Sun",
            "Q Wu",
            "W Yu"
          ],
          "url": "https://arxiv.org/abs/2503.02375",
          "ref_texts": "[11] Z. Shen, Z. Cen, S. Peng, Q. Shuai, H. Bao, and X. Zhou, \u201cLearning human mesh recovery in 3d scenes,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17038\u2013",
          "ref_ids": [
            "11"
          ],
          "1": "Current mainstream human body reconstruction methods can be categorized into three primary approaches: vision-based [11], wearable-based [4], and wireless-based methods [2].",
          "2": "[11] Z."
        }
      }
    },
    {
      "title": "motion capture from internet videos",
      "id": 18,
      "valid_pdf_number": "37/43",
      "matched_pdf_number": "26/37",
      "matched_rate": 0.7027027027027027,
      "citations": {
        "Animatable neural radiance fields for modeling dynamic human bodies": {
          "authors": [
            "Sida Peng",
            "Junting Dong",
            "Qianqian Wang",
            "Shangzhan Zhang",
            "Qing Shuai",
            "Xiaowei Zhou",
            "Hujun Bao"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.html?ref=https://githubhelp.com",
          "ref_texts": "[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV, 2020. 2",
          "ref_ids": [
            "13"
          ],
          "1": "Based on SMPL, some works [48, 24, 27, 21, 13] reconstruct an animated human mesh from sparse camera views."
        },
        "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans": {
          "authors": [
            "Sida Peng",
            "Yuanqing Zhang",
            "Yinghao Xu",
            "Qianqian Wang",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Peng_Neural_Body_Implicit_Neural_Representations_With_Structured_Latent_Codes_for_CVPR_2021_paper.html",
          "ref_texts": "[13] Junting Dong, Qing Shuai, Y uanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InECCV, 2020.",
          "ref_ids": [
            "13"
          ],
          "1": "To obtain the 3D representation at a frame, we first transform the code locations based on the human pose, which can be reliably estimated from sparse camera views [3, 13, 15]."
        },
        "SPEC: Seeing people in the wild with an estimated camera": {
          "authors": [
            "Muhammed Kocabas",
            "Hao P. Huang",
            "Joachim Tesch",
            "Lea Muller",
            "Otmar Hilliges",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.html",
          "ref_texts": "[10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, 2020.",
          "ref_ids": [
            "10"
          ],
          "1": "Closely related to structure-from-motion and bundle adjustment, [2, 10, 37, 67] take videos as input and jointly estimate cameras and reconstruct human bodies; [17, 40] further ground the bodies in 3D scenes."
        },
        "A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose": {
          "authors": [
            "SY Su",
            "F Yu",
            "M Zollh\u00f6fer"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html",
          "ref_texts": "[11] J. Dong, Q. Shuai, Y . Zhang, X. Liu, X. Zhou, and H. Bao. Motion capture from internet videos. In ECCV, pages 210\u2013227. Springer, 2020.",
          "ref_ids": [
            "11"
          ],
          "1": "It also enables optimization within the bounds of the learned prior [11, 16, 25] and weak-supervision when integrated in a differentiable form [27] into neural training processes [4, 20, 23, 42, 45, 65]."
        },
        "Learning motion priors for 4d human body capture in 3d scenes": {
          "authors": [
            "Siwei Zhang",
            "Yan Zhang",
            "Federica Bogo",
            "Marc Pollefeys",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Learning_Motion_Priors_for_4D_Human_Body_Capture_in_3D_ICCV_2021_paper.html",
          "ref_texts": "[12] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, pages 210\u2013227. Springer, 2020.2",
          "ref_ids": [
            "12"
          ],
          "1": "SMPL [36]) to obtain complete 3D body meshes from multi-view [12, 15, 22, 25, 50, 64] or monocular RGB(D) sequences [10, 27, 31, 34, 37, 55, 67]."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[17] Junting Dong, Qing Shuai, Jingxiang Sun, Yuanqing Zhang, Hujun Bao, and Xiaowei Zhou. imocap: Motion capture from internet videos. IJCV, 2022.",
          "ref_ids": [
            "17"
          ],
          "1": "Based on the statistical human model, most works [8, 25, 26, 18, 17, 19] reconstruct the naked body mesh from various inputs and some works further add surface deformation to capture more details [63, 27, 48, 11, 61]."
        },
        "Human mesh recovery from multiple shots": {
          "authors": [
            "Georgios Pavlakos",
            "Jitendra Malik",
            "Angjoo Kanazawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Pavlakos_Human_Mesh_Recovery_From_Multiple_Shots_CVPR_2022_paper.html",
          "ref_texts": "[10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV, 2020. 3",
          "ref_ids": [
            "10"
          ],
          "1": ", from multiple views [10, 17] or monocular video [3, 24, 46, 50]."
        },
        "Gaussian shadow casting for neural characters": {
          "authors": [
            "Luis Bolanos",
            "Yang Su",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Bolanos_Gaussian_Shadow_Casting_for_Neural_Characters_CVPR_2024_paper.html",
          "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV, 2020. 6",
          "ref_ids": [
            "11"
          ],
          "1": "We capture the data using 3 cameras (Canon EOS R8, Canon EOS 70D, iPhone12) and obtain SMPL estimates using EasyMocap [1,10,11]."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
          "authors": [
            "Chiu Ma",
            "Anqi Joyce",
            "Shenlong Wang",
            "Raquel Urtasun",
            "Antonio Torralba"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.html",
          "ref_texts": "[22] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InECCV, 2020.3, 4",
          "ref_ids": [
            "22"
          ],
          "2": "With the flourishing of deep learning, these methods have made tremendous progress, either from a single image [41, 44, 47] or multi-view images [21,22,25,61]."
        },
        "Clip fusion with bi-level optimization for human mesh reconstruction from monocular videos": {
          "authors": [
            "Peng Wu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611978",
          "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. 2020. Motion capture from internet videos. In In Proceedings of the European Conference on Computer Vision. 210\u2013227.",
          "ref_ids": [
            "11"
          ],
          "2": "Compared to images and poses, though a single frame of monocular video gets no rid of depth ambiguity, the whole video contains depth information of the target human [11, 20]."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
          "authors": [
            "X Zhou",
            "S Peng",
            "Z Xu",
            "J Dong",
            "Q Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10401886/",
          "ref_texts": "[36] J. Dong, Q. Shuai, Y. Zhang, X. Liu, X. Zhou, and H. Bao, \u201cMotion capture from internet videos,\u201d in ECCV, 2020.",
          "ref_ids": [
            "36"
          ],
          "1": "Based on SMPL, some works [32], [33], [34], [35], [36] reconstruct an animated human mesh from sparse camera views."
        },
        "Enhancing self-supervised video representation learning via multi-level feature optimization": {
          "authors": [
            "Rui Qian",
            "Yuxi Li",
            "Huabin Liu",
            "John See",
            "Shuangrui Ding",
            "Xian Liu",
            "Dian Li",
            "Weiyao Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Qian_Enhancing_Self-Supervised_Video_Representation_Learning_via_Multi-Level_Feature_Optimization_ICCV_2021_paper.html",
          "ref_texts": "[14] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, pages 210\u2013227. Springer, 2020. 1",
          "ref_ids": [
            "14"
          ],
          "1": "To expand this pipeline to video domain, diverse spatiotemporal augmentation techniques are proposed to construct contrastive pairs and enhance motion modeling [17, 50, 64, 75, 14]."
        },
        "Learning compositional representation for 4d captures with neural ode": {
          "authors": [
            "Boyan Jiang",
            "Yinda Zhang",
            "Xingkui Wei",
            "Xiangyang Xue",
            "Yanwei Fu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Jiang_Learning_Compositional_Representation_for_4D_Captures_With_Neural_ODE_CVPR_2021_paper.html",
          "ref_texts": "[14] Junting Dong, Qing Shuai, Y uanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InEuropean Conference on Computer V ision , pages 210\u2013227. Springer, 2020.",
          "ref_ids": [
            "14"
          ],
          "1": "However, most works are developed based on strong assumptions [54, 42, 53, 31, 56], demand the costly multi-view inputs [36, 52, 35, 14]."
        },
        "Context-aware sequence alignment using 4d skeletal augmentation": {
          "authors": [
            "Taein Kwon",
            "Bugra Tekin",
            "Siyu Tang",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Kwon_Context-Aware_Sequence_Alignment_Using_4D_Skeletal_Augmentation_CVPR_2022_paper.html",
          "ref_texts": "[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 3",
          "ref_ids": [
            "13"
          ],
          "1": "Closely related to the sequence alignment problem, metrics for assessing human motion similarity have been actively explored by previous studies [4, 9, 13, 33, 34, 37, 52, 54, 54].",
          "2": "Conventional approaches for measuring similarity of human motion sequences are based on estimating the L2 displacement error [13, 34] or DTW [4]."
        },
        "Cat-nerf: Constancy-aware tx2former for dynamic body modeling": {
          "authors": [
            "Haidong Zhu",
            "Zhaoheng Zheng",
            "Wanrong Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.html",
          "ref_texts": "[7] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV, pages 210\u2013227. Springer, 2020. 2",
          "ref_ids": [
            "7"
          ],
          "1": "Recently, researchers have mainly developed two different methods: statisticbased methods [3, 7, 11, 14, 16, 28, 37, 55, 56] and databased methods [25, 26, 38, 39, 41, 52, 54]."
        },
        "Normalized human pose features for human action video alignment": {
          "authors": [
            "Jingyuan Liu",
            "Mingyi Shi",
            "Qifeng Chen",
            "Hongbo Fu",
            "Lan Tai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.html",
          "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, pages 210\u2013227. Springer, 2020. 1, 3",
          "ref_ids": [
            "11"
          ],
          "1": "A common approach to the problem of human action video alignment is to first estimate 2D or 3D human poses from two input videos, and then find the alignments by matching with features extracted from joint positions [48, 11], so as to reduce certain interference in video backgrounds and subjects\u2019 clothing.",
          "2": "Alignment of human action videos has been actively explored in recent years for many video analysis tasks, such as action detection in unconstrained videos [16], human reconstruction from uncalibrated multiview videos [11], action synchronization [12], few-shot video classification [6], etc."
        },
        "Pressim: An end-to-end framework for dynamic ground pressure profile generation from monocular videos using physics-based 3d simulation": {
          "authors": [
            "LSS Ray",
            "B Zhou",
            "S Suh"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10150221/",
          "ref_texts": "[26] J. Dong, Q. Shuai, Y . Zhang, X. Liu, X. Zhou, and H. Bao, \u201cMotion capture from internet videos,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 210\u2013227.",
          "ref_ids": [
            "26"
          ],
          "1": "For our second approach, we use EasyMocap, a volumetric pose estimation model that estimates both the 3D pose and body shape [26]."
        },
        "Nemo: Learning 3d neural motion fields from multiple video instances of the same action": {
          "authors": [
            "Chieh Wang",
            "Zhenzhen Weng",
            "Maria Xenochristou",
            "Joao Pedro",
            "Jeffrey Gu",
            "Karen Liu",
            "Serena Yeung"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.html",
          "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, pages 210\u2013227. Springer, 2020. 3",
          "ref_ids": [
            "11"
          ],
          "1": "iMoCap [11] studied a problem similar to ours by curating videos from the internet and also aimed to recover the 3D motion."
        },
        "Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos": {
          "authors": [
            "X Luo",
            "J Peng",
            "Z Cai",
            "L Yang",
            "F Yang",
            "Z Cao"
          ],
          "url": "https://arxiv.org/abs/2501.13335",
          "ref_texts": "[7] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV, 2020. 6",
          "ref_ids": [
            "7"
          ],
          "1": "To make the dataset more realistic, we use EasyMocap [7, 56] to re-calculate the human poses and the human masks of the synthesized blurred image sequences."
        },
        "Nemo: 3d neural motion fields from multiple video instances of the same action": {
          "authors": [
            "KC Wang",
            "Z Weng",
            "M Xenochristou",
            "JP Araujo"
          ],
          "url": "https://arxiv.org/abs/2212.13660",
          "ref_texts": "[10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision, pages 210\u2013227. Springer, 2020. 3",
          "ref_ids": [
            "10"
          ],
          "1": "iMoCap [10] studied a problem similar to ours by curating videos from the internet, and also aimed to recover the 3D motion."
        },
        "Music-and Lyrics-driven Dance Synthesis": {
          "authors": [
            "W Yin",
            "Q Yao",
            "Y Yu",
            "H Yin",
            "D Kragic"
          ],
          "url": "https://arxiv.org/abs/2310.00455",
          "ref_texts": "[8] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 210\u2013227. Springer, 2020.",
          "ref_ids": [
            "8"
          ],
          "1": "\u2022 Video Preprocessing: Utilizing EasyMocap [8], we achieved high-fidelity body estimation from these videos at a rate of 60 fps."
        },
        "Consistent 3D Human Shape from Repeatable Action": {
          "authors": [
            "Keisuke Shibata",
            "Sangeun Lee",
            "Shohei Nobuhara",
            "Ko Nishino"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021W/DynaVis/html/Shibata_Consistent_3D_Human_Shape_From_Repeatable_Action_CVPRW_2021_paper.html",
          "ref_texts": "[15] Junting Dong, Qing Shuai, Y uanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InProc. ECCV , 2020.",
          "ref_ids": [
            "15"
          ],
          "1": "synchronize videos of repeated action using 3D pose estimates from a single camera, and improves the 3D poses with iterative optimization [15]."
        },
        "Research on Communication Analysis between Vehicles and Pedestrians at Unsignalized Crosswalks Using Online Video": {
          "authors": [
            "G Bamba",
            "D Misaki"
          ],
          "url": "https://ph01.tci-thaijo.org/index.php/jrame/article/view/251426",
          "ref_texts": "[14] Dong J, Shuai Q, Zhang Y, Liu X, Zhou X , Bao H. Motion capture from internet videos . 16th European Conference on Computer Vision; 2020 Aug 23-28; Virtual platform. p. 210-227. ",
          "ref_ids": [
            "14"
          ],
          "1": "[14] Dong J, Shuai Q, Zhang Y, Liu X, Zhou X , Bao H."
        }
      }
    },
    {
      "title": "mapa: text-driven photorealistic material painting for 3d shapes",
      "id": 42,
      "valid_pdf_number": "5/6",
      "matched_pdf_number": "4/5",
      "matched_rate": 0.8,
      "citations": {
        "Sampart3d: Segment any part in 3d objects": {
          "authors": [
            "Y Yang",
            "Y Huang",
            "YC Guo",
            "L Lu",
            "X Wu",
            "EY Lam"
          ],
          "url": "https://arxiv.org/abs/2411.07184",
          "ref_texts": "[52] Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. Mapa: Text-driven photorealistic material painting for 3d shapes. arXiv:2404.17569, 2024. 7",
          "ref_ids": [
            "52"
          ],
          "1": "Applications Recent several works [7, 12, 33, 38, 49, 52] attempt to generate or edit the style or material of parts for 3D objects."
        },
        "Material Anything: Generating Materials for Any 3D Object via Diffusion": {
          "authors": [
            "X Huang",
            "T Wang",
            "Z Liu",
            "Q Wang"
          ],
          "url": "https://arxiv.org/abs/2411.15138",
          "ref_texts": "[51] Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. Mapa: Text-driven photorealistic material painting for 3d shapes. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201312, 2024. 1, 3",
          "ref_ids": [
            "51"
          ],
          "1": "These approaches either require specific optimizations for each case [43, 52] or rely on multi-modal models like GPT4-V [1] to retrieve materials for different parts of an object [12, 51].",
          "2": "Retrievalbased methods [12, 51] rely on large multi-modal models such as SAM [20] and GPT [1] for segmentation and material assignment, limiting their scalability."
        },
        "Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation": {
          "authors": [
            "B Song",
            "X Huang",
            "R Xie",
            "X Wang",
            "Q Wang"
          ],
          "url": "https://arxiv.org/abs/2412.03571",
          "ref_texts": "[59] Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. MaPa: Text-driven photorealistic material painting for 3D shapes. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201312, 2024. 3",
          "ref_ids": [
            "59"
          ],
          "1": "Several methods [1, 2, 28, 35, 59] focus on generating textures with varying styles for 3D objects, utilizing pre-trained diffusion models to generate textures based on input text prompts or images."
        },
        "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models": {
          "authors": [
            "J Kim",
            "D Kang",
            "J Choi",
            "J Wi",
            "J Gwon",
            "J Bae"
          ],
          "url": "https://arxiv.org/abs/2409.19989",
          "ref_texts": "[33] Zhang, S., Peng, S., Xu, T., Yang, Y ., Chen, T., Xue, N., Shen, Y ., Bao, H., Hu, R., Zhou, X.: Mapa: Text-driven photorealistic material painting for 3d shapes. arXiv preprint arXiv:2404.17569 (2024) 3",
          "ref_ids": [
            "33"
          ],
          "1": "In contrast, the second approach entails generating 2D diffusion images conditioned on viewpoints, which are subsequently projected onto 3D meshes [2, 3, 16, 21, 28, 31, 33]."
        }
      }
    },
    {
      "title": "prompting depth anything for 4k resolution accurate metric depth estimation",
      "id": 43,
      "valid_pdf_number": "4/5",
      "matched_pdf_number": "3/4",
      "matched_rate": 0.75,
      "citations": {
        "BANet: Bilateral Aggregation Network for Mobile Stereo Matching": {
          "authors": [
            "G Xu",
            "J Liu",
            "X Wang",
            "J Cheng",
            "Y Deng",
            "J Zang"
          ],
          "url": "https://arxiv.org/abs/2503.03259",
          "ref_texts": "[25] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. arXiv preprint arXiv:2412.14015, 2024. 1",
          "ref_ids": [
            "25"
          ],
          "1": "Currently, deep learning-based methods [12, 14, 16, 25, 30, 42, 51] have dominated stereo matching or depth benchmarks, consistently setting new \u2020Corresponding author."
        },
        "DuCos: Duality Constrained Depth Super-Resolution via Foundation Model": {
          "authors": [
            "Z Yan",
            "Z Wang",
            "H Dong",
            "J Li",
            "J Yang"
          ],
          "url": "https://arxiv.org/abs/2503.04171",
          "ref_texts": "[19] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. arXiv preprint arXiv:2412.14015, 2024. 2",
          "ref_ids": [
            "19"
          ],
          "1": "These models can provide strong priors for various depth perception tasks, including depth matching [10, 43] and depth completion [19, 21, 23, 27, 28, 38].",
          "2": "What\u2019s more, PromptDA [19] achieves 4K resolution depth estimation from low-quality depth data through concise prompt fusion and scaling designs."
        },
        "MonSter: Marry Monodepth to Stereo Unleashes Power": {
          "authors": [
            "J Cheng",
            "L Liu",
            "G Xu",
            "X Wang",
            "Z Zhang",
            "Y Deng"
          ],
          "url": "https://arxiv.org/abs/2501.08643",
          "ref_texts": "[19] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. arXiv preprint arXiv:2412.14015, 2024. 3",
          "ref_ids": [
            "19"
          ],
          "1": "Matching in illposed regions is challenging, previous methods [7, 17, 19, 31, 44, 47] tried to leverage structural priors to address this issue."
        }
      }
    },
    {
      "title": "neuralrecon: real-time coherent 3d reconstruction from monocular video",
      "id": 6,
      "valid_pdf_number": "243/276",
      "matched_pdf_number": "209/243",
      "matched_rate": 0.8600823045267489,
      "citations": {
        "Nice-slam: Neural implicit scalable encoding for slam": {
          "authors": [
            "Zihan Zhu",
            "Songyou Peng",
            "Viktor Larsson",
            "Weiwei Xu",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Martin R. Oswald",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html",
          "ref_texts": "[48] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, 2021. 1, 2",
          "ref_ids": [
            "48"
          ],
          "1": "In contrast, recent works [37, 48] demonstrate that establishing multi-level grid-based features can help to preserve geometric details and enable reconstructing complex scenes, but these are offline methods without real-time capability.",
          "2": "A few recent papers [1, 4, 9, 27, 48, 57, 61] attempt to predict scene-level geometry with RGB-(D) inputs, but they all assume given camera poses."
        },
        "Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving": {
          "authors": [
            "Yi Wei",
            "Linqing Zhao",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Jie Zhou",
            "Jiwen Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html",
          "ref_texts": "[50] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "50"
          ],
          "1": "3D Scene Reconstruction: 3D reconstruction [42, 49, 48, 41, 71, 16, 4, 38, 50, 5] is a traditional but important topic in computer vision.",
          "2": "Different from depth estimation, 3D scene reconstruction methods [22, 38, 50, 5, 7] directly reconstruct a comprehensive and accurate 3D geometry of a scene.",
          "3": "NeuralRecon [50] and TransformerFusion [5] fuse the learned image features from different views in an online manner for more accurate 3D reconstructions."
        },
        "A comprehensive review of vision-based 3d reconstruction methods": {
          "authors": [
            "Linglong Zhou",
            "Guoxin Wu",
            "Yunbo Zuo",
            "Xuanyu Chen",
            "Hongle Hu"
          ],
          "url": "https://www.mdpi.com/1424-8220/24/7/2314",
          "ref_texts": "217. Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; Bao, H. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 15598\u201315607.",
          "ref_ids": [
            "217"
          ],
          "1": "To replace the traditional TSDF fusion method, consider using the GRU module, which can self-learn to enhance the model\u2019s generalizability [217]."
        },
        "Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects": {
          "authors": [
            "Bowen Wen",
            "Jonathan Tremblay",
            "Valts Blukis",
            "Stephen Tyree",
            "Thomas Muller",
            "Alex Evans",
            "Dieter Fox",
            "Jan Kautz",
            "Stan Birchfield"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.html",
          "ref_texts": "[59] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 2",
          "ref_ids": [
            "59"
          ],
          "1": "For example, neural scene representations have achieved great success in creating high quality 3D object models from real data [3, 40, 44, 59, 68, 81].",
          "2": "With recent advances in neural scene representation, high quality 3D models can be reconstructed [3,40,44,59,68,81], though most of these methods assume known camera poses or ground-truth segmentation and often focus on static scenes with rich texture or geometric cues."
        },
        "Point-slam: Dense neural point cloud-based slam": {
          "authors": [
            "Erik Sandstrom",
            "Yue Li",
            "Luc Van",
            "Martin R. Oswald"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.html",
          "ref_texts": "[54] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u2013",
          "ref_ids": [
            "54"
          ],
          "1": "A number of recent works do not need depth input and accomplish dense online reconstruction from RGB cameras only [35, 10, 3, 50, 54, 47, 23].",
          "2": "The grid-based representation is perhaps the most explored one and can be further split into methods using dense grids [79, 36, 63, 64, 13, 54, 3, 24, 11, 77, 76, 66, 81], hierarchical octrees [69, 49, 29, 6, 26] and voxel hashing [38, 21, 15, 60, 33] to save memory."
        },
        "Eslam: Efficient dense slam system based on hybrid representation of signed distance fields": {
          "authors": [
            "Mohammad Mahdi",
            "Camilla Carta",
            "Francois Fleuret"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.html",
          "ref_texts": "[63] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "63"
          ],
          "1": "The exploitation of neural implicit representations for 3D reconstruction at real-world scale is studied in [1, 4, 9, 29, 43, 63, 70, 74, 80]."
        },
        "Neural rgb-d surface reconstruction": {
          "authors": [
            "Dejan Azinovic",
            "Ricardo Martin",
            "Dan B",
            "Matthias Niessner",
            "Justus Thies"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.html",
          "ref_texts": "[71] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 2",
          "ref_ids": [
            "71"
          ],
          "1": "To reduce artifacts from classical reconstruction methods, a series of methods was proposed that use learned spatial priors to predict depth maps from color images [24, 28, 39], to learn multiview stereo using 3D CNNs on voxel grids [34, 68, 82], or multi-plane images [22], to reduce the influence of noisy depth values [79], to complete incomplete scans [13, 15], to learn image features for SLAM [2, 10, 90] or feature fusion [4,71,80], to predict normals [89], or to predict objects or parts of a room from single images [11, 18, 27, 51, 75]."
        },
        "Viewdiff: 3d-consistent image generation with text-to-image models": {
          "authors": [
            "Lukas Hollein",
            "Aljaz Bozi",
            "Norman Muller",
            "David Novotny",
            "Yu Tseng",
            "Christian Richardt",
            "Michael Zollhofer",
            "Matthias Niessner"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hollein_ViewDiff_3D-Consistent_Image_Generation_with_Text-to-Image_Models_CVPR_2024_paper.html",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 4",
          "ref_ids": [
            "37"
          ],
          "1": "Inspired by multi-view stereo literature [3, 17, 37], we create a 3D feature voxel grid from all input spatial features Aggregator MLP V olume Renderer input posed features output features ScaleNet (1x1 CNN) 3D CNN ExpandNet (1x1 CNN) CompressNet (1x1 CNN) Figure 3."
        },
        "Neural 3d scene reconstruction with the manhattan-world assumption": {
          "authors": [
            "Haoyu Guo",
            "Sida Peng",
            "Haotong Lin",
            "Qianqian Wang",
            "Guofeng Zhang",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Guo_Neural_3D_Scene_Reconstruction_With_the_Manhattan-World_Assumption_CVPR_2022_paper.html",
          "ref_texts": "[47] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 2, 5",
          "ref_ids": [
            "47"
          ],
          "1": "NeuralRecon [47] improves the reconstruction speed through reconstructing local surfaces for each fragment sequence.",
          "2": "We consider F-score as the overall metric following [47]."
        },
        "Sparseneus: Fast generalizable neural surface reconstruction from sparse views": {
          "authors": [
            "X Long",
            "C Lin",
            "P Wang",
            "T Komura",
            "W Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_13",
          "ref_texts": "40. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "40"
          ],
          "1": "1 Multi-view stereo (MVS) Classical MVS methods utilize various 3D representations for reconstruction such as: voxel grids based [12,13,15,18,37,40], 3D point clouds based [7,19], and depth maps based [2,8,36,42,46,47,10,27,26,25]."
        },
        "Plgslam: Progressive neural scene represenation with local to global bundle adjustment": {
          "authors": [
            "Tianchen Deng",
            "Guole Shen",
            "Tong Qin",
            "Jianyu Wang",
            "Wentao Zhao",
            "Jingchuan Wang",
            "Danwei Wang",
            "Weidong Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Deng_PLGSLAM_Progressive_Neural_Scene_Represenation_with_Local_to_Global_Bundle_CVPR_2024_paper.html",
          "ref_texts": "[28] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15598\u201315607, June 2021. 2",
          "ref_ids": [
            "28"
          ],
          "1": "Other methods [1\u20133, 28] use various scene geometry representation methods, such as truncated signed distance function, voxel grid."
        },
        "Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing": {
          "authors": [
            "B Yang",
            "C Bao",
            "J Zeng",
            "H Bao",
            "Y Zhang",
            "Z Cui"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_34",
          "ref_texts": "49. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time Coherent 3D Reconstruction from Monocular Video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "49"
          ],
          "1": "Recently, more attention has been paid to neural network based scene reconstruction [29,49] and texture learning [52,8,59]."
        },
        "Gaussian-slam: Photo-realistic dense slam with gaussian splatting": {
          "authors": [
            "V Yugay",
            "Y Li",
            "T Gevers",
            "MR Oswald"
          ],
          "url": "https://arxiv.org/abs/2312.10070",
          "ref_texts": "64. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021) 3",
          "ref_ids": [
            "64"
          ],
          "1": "Recent research has successfully achieved dense online reconstruction using solely RGB cameras [3,8,27,43,55,58,64], bypassing the need for depth data.",
          "2": "They further divide into methods using dense grids [3,9,11,28,44,64,70\u201373,86,87,89], hierarchical octrees [6,30,31,36,57,78] and voxel hashing [13,20,41,46,67] for efficient memory management."
        },
        "Simplerecon: 3d reconstruction without 3d convolutions": {
          "authors": [
            "M Sayed",
            "J Gibson",
            "J Watson",
            "V Prisacariu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_1",
          "ref_texts": "65. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In: CVPR (2021)",
          "ref_ids": [
            "65"
          ],
          "2": "NeuralRecon [65] extended this to refine the TSDF in a coarse-to-fine manner using recurrent layers, while TransformerFusion [2] and VoRTX [64] further improved performance using transformers [69] to learn feature matching.",
          "5": "636 NeuralRecon [65] Yes 5.",
          "6": "636 NeuralRecon*[65] 3D Chunk Fusion + GRU 2D CNN (12ms) + GRU (78ms)90ms 0.",
          "8": "Notably, NeuralRecon [65] updates a chunk in world space when 9 keyframes have been received.",
          "9": "Although our method is slower than methods such as [65] on a per-keyframe basis, we can quickly perform updates to the reconstructed volume using online TSDF fusion methods, resulting in low update latencies.",
          "10": "Acknowledgments \u2014 We thank Alja\u02c7 z Bo\u02c7 zi\u02c7 c [2], Jiaming Sun [65] and Arda D\u00a8 uz\u00b8 ceker [12] for quickly providing useful information to help with baselines."
        },
        "Neural map prior for autonomous driving": {
          "authors": [
            "Xuan Xiong",
            "Yicheng Liu",
            "Tianyuan Yuan",
            "Yue Wang",
            "Yilun Wang",
            "Hang Zhao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.html",
          "ref_texts": "[31] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 3",
          "ref_ids": [
            "31"
          ],
          "1": "NeuralRecon [31] presents an approach for implicit neural 3D reconstruction that integrates reconstruction and fusion processes."
        },
        "Nerfusion: Fusing radiance fields for large-scale scene reconstruction": {
          "authors": [
            "Xiaoshuai Zhang",
            "Sai Bi",
            "Kalyan Sunkavalli",
            "Hao Su",
            "Zexiang Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_NeRFusion_Fusing_Radiance_Fields_for_Large-Scale_Scene_Reconstruction_CVPR_2022_paper.html",
          "ref_texts": "[39] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 2, 5",
          "ref_ids": [
            "39"
          ],
          "1": "Instead of estimating and fusing per-view depth, previous methods [6,19,39] introduce learning-based methods to aggregate per-view features and predict opacity volumes or signed distance volumes.",
          "2": "This fusion process is similar to previous 3D reconstruction pipelines [20, 33, 39] that focus on geometry reconstruction; in contrast, we instead reconstruct neural feature volumes to represent neural radiance fields for volume rendering, leading to photo-realistic novel view synthesis."
        },
        "RayMVSNet++: learning ray-based 1D implicit fields for accurate multi-view stereo": {
          "authors": [
            "Y Shi",
            "J Xi",
            "D Hu",
            "Z Cai",
            "K Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10185080/",
          "ref_texts": "[55] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "55"
          ],
          "1": "Our method is also relevant to [42], [55] in terms of reconstructing 3D object by estimating the SDFs."
        },
        "Nerf-det: Learning geometry-aware volumetric representation for multi-view 3d object detection": {
          "authors": [
            "Chenfeng Xu",
            "Bichen Wu",
            "Ji Hou",
            "Sam Tsai",
            "Ruilong Li",
            "Jialiang Wang",
            "Wei Zhan",
            "Zijian He",
            "Peter Vajda",
            "Kurt Keutzer",
            "Masayoshi Tomizuka"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html",
          "ref_texts": "[35] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 7",
          "ref_ids": [
            "35"
          ],
          "1": "NeuralRecon-Depth indicates NeuralRecon [35] pre-trained on ScanNetV2 is used to predict the depth.",
          "2": "Furthermore, we explore different scene geometry modeling methods, from using depth map to cost volume [35, 45], in Tab.",
          "3": "Thus, we try instead to render depth maps using outof-box geometry reconstruction from NeuralRecon [35]."
        },
        "Neo 360: Neural fields for sparse view synthesis of outdoor scenes": {
          "authors": [
            "Muhammad Zubair",
            "Sergey Zakharov",
            "Katherine Liu",
            "Vitor Guizilini",
            "Thomas Kollar",
            "Adrien Gaidon",
            "Zsolt Kira",
            "Rares Ambrus"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Irshad_NeO_360_Neural_Fields_for_Sparse_View_Synthesis_of_Outdoor_ICCV_2023_paper.html",
          "ref_texts": "[54] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 3, 4",
          "ref_ids": [
            "54"
          ],
          "1": "NeRF extensions focus on reducing aliasing effects via multiscale representations [3], modeling unbounded scenes [32], disentangled object-background representations and blending [42], compositional generative models [39] and improving reconstruction and depth estimation accuracy via multi-view consistent features [53, 54, 18].",
          "2": "Similar to prior works involving volumetric reconstruction [54, 38, 25], the obtained local features are projected backwards along every ray to the 3D feature volume (VF ) using camera pose \u03b3i and intrinsic Ki.",
          "3": "While volumetric reconstruction methods [54, 38], traditionally use the generated volume solely for indoor geometry reconstruction through TSDF, we show that it can also be employed in a computationally efficient way to estimate the entire scene\u2019s appearance and enable accurate neu9190 Volume Rendering Tri-plane Features Image Encoder 3D Feature Grid Feature Aggregation \ud835\udc53 Plane Feature Aggregators NeRF Decoder 2D Conv Z Volumetric Rendering Figure 6: Method: Our method effectively uses local features to infer an image-conditional triplanar representation for both backgrounds and foregrounds."
        },
        "Dp-nerf: Deblurred neural radiance field with physical scene priors": {
          "authors": [
            "Dogyoon Lee",
            "Minhyeok Lee",
            "Chajin Shin",
            "Sangyoun Lee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.html",
          "ref_texts": "[47] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "Due to the success of the NeRF in neural rendering, several studies have applied NeRF to other areas such as dynamic scenes [17\u201319, 29, 30, 33, 49, 55], generative models [28, 38], relighting [3, 23, 32, 42], human avatars [31, 45, 58], and 3D reconstruction [47, 50]."
        },
        "Transformerfusion: Monocular rgb scene reconstruction using transformers": {
          "authors": [
            "A Bozic",
            "P Palafox",
            "J Thies",
            "A Dai"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/0a87257e5308197df43230edf4ad1dae-Abstract.html",
          "ref_texts": "[39] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021.",
          "ref_ids": [
            "39"
          ],
          "1": "Recently, NeuralRecon [39] proposed a real-time 3D reconstruction framework, adding GRU units distributed in 3D to fuse reconstructions from different local windows of frames.",
          "3": "Reconstruction quality further improves with methods that directly predict the 3D surface geometry, such as NeuralRecon [39] and Atlas [28]."
        },
        "Neuris: Neural reconstruction of indoor scenes using normal priors": {
          "authors": [
            "J Wang",
            "P Wang",
            "X Long",
            "C Theobalt",
            "T Komura"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_9",
          "ref_texts": "31. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR (2021)",
          "ref_ids": [
            "31"
          ],
          "1": "the success of deep neural networks, data-driven (depth-based and TSDF-based) methods [34,17,10,33,22,31] have proven effective in alleviating the texture-less problem by exploiting various geometric priors learned from a large amount of data.",
          "5": "To reduce the computational burden, unlike Atlas that processes the whole image sequences at once, NeuralRecon [31] proposes a coarse-to-fine framework, that reconstructs the whole scene by processing local fragments incrementally.",
          "7": "For the depth based method DeepV2D, to address the scale ambiguity issue of it, we re-scale every predicted depth map according to the ground truth depth map using the median scale strategy [46] and then fuse its predicted depth maps following NeuralRecon [31] to construct global surface geometry.",
          "8": "For complete quantitative comparisons, we evaluate the 3D surface geometry results, following the metrics defined in NeuralRecon [31].",
          "9": "Among those metrics, F-score is usually considered as the most suitable metric to evaluate geometry quality [31]."
        },
        "FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes": {
          "authors": [
            "Y Wang",
            "T Huang",
            "H Chen"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/c2166d01fe4bcd694aba89f608737678-Abstract-Conference.html",
          "ref_texts": "[31] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Realtime coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "31"
          ],
          "2": "Consequently, inspired by previous methods [31, 40], we propose the Pixel-wise Triplet Fusion (PTF) module which can significantly remove redundant Gaussians in the overlapping regions and explicitly aggregate multiview observation features in the latent space."
        },
        "Shine-mapping: Large-scale 3d mapping using sparse hierarchical implicit neural representations": {
          "authors": [
            "X Zhong",
            "Y Pan",
            "J Behley"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160907/",
          "ref_texts": "[33] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Realtime coherent 3D reconstruction from monocular video. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "Due to such advantages, several recent works have used implicit representation for 3D scene reconstruction built from images data or RGB-D frames [2], [31], [33], [37], [40].",
          "2": "While such explicit geometric representations enable detailed reconstructions in large-scale environments [19], [36], [38], the recent emergence of neural representations, like NeRF [16] for novel view synthesis inspired researchers to leverage their capabilities for mapping and reconstruction [2], [8], [17], [27], [31], [32], [33], [34], [40]."
        },
        "Snap: Self-supervised neural maps for visual positioning and semantic understanding": {
          "authors": [
            "PE Sarlin",
            "E Trulls",
            "M Pollefeys"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/182c433412b33c14e32a7c4fc2c3e290-Abstract-Conference.html",
          "ref_texts": "[92] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. CVPR, 2021. 7",
          "ref_ids": [
            "92"
          ],
          "1": "3D voxel grids are more expressive and thus popular for reconstruction [66, 60, 92, 9, 119], rendering [59, 95], and semantic perception [17, 102, 13, 7] but are expensive to store and thus often restricted to small scenes."
        },
        "Surfelnerf: Neural surfel radiance fields for online photorealistic reconstruction of indoor scenes": {
          "authors": [
            "Yiming Gao",
            "Pei Cao",
            "Ying Shan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Gao_SurfelNeRF_Neural_Surfel_Radiance_Fields_for_Online_Photorealistic_Reconstruction_of_CVPR_2023_paper.html",
          "ref_texts": "[34] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In IEEE CVPR, pages 15598\u2013",
          "ref_ids": [
            "34"
          ],
          "1": "Recently, NeRFusion [50] followed NeuralRecon [34] to unproject input images into local sparse feature volumes, fusing them to a global volume via Gated Recurrent Units (GRUs), and then generating photorealistic results from the global feature volume via volume rendering.",
          "2": "Most online scene reconstruction methods [4,34,41,52,53] focus on 3D reconstruction only via TSDF-based fusion [4, 34], surfelbased fusion [41] and SLAM-based reconstruction [52].",
          "3": "Specifically, we use a MasNet following [34, 50] to extract multi-scale view-dependent features."
        },
        "Volrecon: Volume rendering of signed ray distance functions for generalizable multi-view reconstruction": {
          "authors": [
            "Yufan Ren",
            "Tong Zhang",
            "Marc Pollefeys",
            "Sabine Susstrunk",
            "Fangjinhua Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ren_VolRecon_Volume_Rendering_of_Signed_Ray_Distance_Functions_for_Generalizable_CVPR_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021. 2, 3, 8",
          "ref_ids": [
            "42"
          ],
          "3": "Instead, we believe it will be a promising direction to reconstruct progressively in small local volumes like NeuralRecon [42]."
        },
        "Go-surf: Neural feature grid optimization for fast, high-fidelity rgb-d surface reconstruction": {
          "authors": [
            "J Wang",
            "T Bleja",
            "L Agapito"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044463/",
          "ref_texts": "[31] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "Neural 3D Scene Reconstruction A recent trend includes architectures trained end-to-end on multiple sequences to aggregate image features and perform fusion over multiple frames using a volumetric representation and decoders to predict signed distance [31, 14, 2] or radiance [39] fields.",
          "2": "NeuralRecon [31] reconstructs surfaces sequentially from video fragments as TSDF volumes by performing feature fusion from previous fragments via recurrent units."
        },
        "Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding": {
          "authors": [
            "X Zuo",
            "P Samangouei",
            "Y Zhou",
            "Y Di",
            "M Li"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-024-02183-8",
          "ref_texts": "[50] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "50"
          ],
          "1": "To estimate the dense 3d voxel cells, probabilistic fusion methods were firstly [22] used and researchers also developed end-to-end learn-able methods [50], by using either depth sensors [22] or monocular camera systems [59]."
        },
        "I2-sdf: Intrinsic indoor scene reconstruction and editing via raytracing in neural sdfs": {
          "authors": [
            "Jingsen Zhu",
            "Yuchi Huo",
            "Qi Ye",
            "Fujun Luan",
            "Jifan Li",
            "Dianbing Xi",
            "Lisha Wang",
            "Rui Tang",
            "Wei Hua",
            "Hujun Bao",
            "Rui Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html",
          "ref_texts": "[31] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "NeuralRecon [31] proposes a coarse-to-fine framework to regress input images to TSDF incrementally."
        },
        "Real acoustic fields: An audio-visual room acoustics dataset and benchmark": {
          "authors": [
            "Ziyang Chen",
            "Israel D. Gebru",
            "Christian Richardt",
            "Anurag Kumar",
            "William Laney",
            "Andrew Owens",
            "Alexander Richard"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_Real_Acoustic_Fields_An_Audio-Visual_Room_Acoustics_Dataset_and_Benchmark_CVPR_2024_paper.html",
          "ref_texts": "[59] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 3",
          "ref_ids": [
            "59"
          ],
          "1": "Many approaches that focus on 3D scene reconstruction use representations such as (truncated) signed distance fields to combine multiple observations from RGB-D sensors [43, 44, 58, 69] or standard color videos [20, 25, 41, 59]."
        },
        "Vortx: Volumetric 3d reconstruction with transformers for voxelwise view selection and fusion": {
          "authors": [
            "N Stier",
            "A Rich",
            "P Sen",
            "T H\u00f6llerer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665948/",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021. 1, 2, 3, 6, 7",
          "ref_ids": [
            "37"
          ],
          "1": "We compare to Atlas [28], which fuses features by averaging, and NeuralRecon [37], which fuses locally by averaging and globally by RNN.",
          "2": "Recently, a number of works have addressed this by posing RGB-only 3D reconstruction as the direct prediction of a truncated signed-distance function (TSDF), using deep learning to fill in unobserved regions via learned priors [28, 37].",
          "3": "NeuralRecon [37] averages features only among nearby views, fusing across view clusters using a recurrent neural network (RNN).",
          "4": "It thus bears structural similarity to existing deep volumetric reconstruction methods [28, 37].",
          "6": "During training, we randomly drop out voxels to reduce memory cost, following [37].",
          "7": "con [37], and we use the provided pre-trained models."
        },
        "Finerecon: Depth-aware feed-forward network for detailed 3d reconstruction": {
          "authors": [
            "Noah Stier",
            "Anurag Ranjan",
            "Alex Colburn",
            "Yajie Yan",
            "Liang Yang",
            "Fangchang Ma",
            "Baptiste Angles"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Stier_FineRecon_Depth-aware_Feed-forward_Network_for_Detailed_3D_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[24] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 2, 3, 5, 6",
          "ref_ids": [
            "24"
          ],
          "1": "Recent works on 3D reconstruction from posed images [17, 23, 24] have demonstrated that direct inference of scene-level 3D geometry without test-time optimization is feasible using deep neural networks, showing remarkable promise and high efficiency.",
          "2": "Several methods have proposed improvements to this framework [1, 2, 23, 24], consistently pushing the state of the art in reconstruction accuracy.",
          "8": "Comparison of our method with NeuralRecon [24], V oRTX [23], and SimpleRecon [21].",
          "11": "For end-to-end 3D reconstruction methods we select Atlas [17], NeuralRecon [24], V oRTX [23], and TransformerFusion [1]."
        },
        "Scube: Instant large-scale scene reconstruction using voxsplats": {
          "authors": [
            "X Ren",
            "Y Lu",
            "JZ Wu",
            "H Ling",
            "M Chen"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/b111aa30ab71255946b19b6bd4e68939-Paper-Conference.pdf",
          "ref_texts": "[52] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "52"
          ],
          "1": "Hence previous methods [28, 50, 52] that broadcast the same features to all the voxels along the rays corresponding to the pixel are not suitable here to precisely locate the geometries such as vehicles."
        },
        "Applications of 3D reconstruction in virtual reality-based teleoperation: a review in the mining industry": {
          "authors": [
            "Alireza Kamran",
            "Amin Moniri",
            "Javad Sattarvand"
          ],
          "url": "https://www.mdpi.com/2227-7080/12/3/40",
          "ref_texts": "114. Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; Bao, H. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Virtual, 19\u201325 June 2021; pp. 15598\u201315607.",
          "ref_ids": [
            "114"
          ],
          "1": "[114] \u2713 \u2713 \u2713 Hybrid \u2713 \u2713 \u2713 \u2713 FBV , GRU Fusion, ScanNet S."
        },
        "Planemvs: 3d plane reconstruction from multi-view stereo": {
          "authors": [
            "Jiachen Liu",
            "Pan Ji",
            "Nitin Bansal",
            "Changjiang Cai",
            "Qingan Yan",
            "Xiaolei Huang",
            "Yi Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_PlaneMVS_3D_Plane_Reconstruction_From_Multi-View_Stereo_CVPR_2022_paper.html",
          "ref_texts": "[45] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 3",
          "ref_ids": [
            "45"
          ],
          "1": "Different from depth-map based MVS, Atlas [37] and NeuralRecon [45] propose to learn a TSDF [5] representation from posed images for 3D surface reconstruction which avoids multi-view depth fusion."
        },
        "Activermap: Radiance field for active mapping and planning": {
          "authors": [
            "H Zhan",
            "J Zheng",
            "Y Xu",
            "I Reid",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2211.12656",
          "ref_texts": "[60] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021. 2, 3",
          "ref_ids": [
            "60"
          ],
          "2": "Meanwhile, compelling results have been demonstrated for 3D object reconstruction [37, 43], surface reconstruction [3, 28], generative models [41,53], Structure-from-Motion [10,29,64], SLAM [58,60], etc."
        },
        "Volumefusion: Deep depth fusion for 3d scene reconstruction": {
          "authors": [
            "Jaesung Choe",
            "Sunghoon Im",
            "Francois Rameau",
            "Minjun Kang",
            "In So"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Choe_VolumeFusion_Deep_Depth_Fusion_for_3D_Scene_Reconstruction_ICCV_2021_paper.html",
          "ref_texts": "[38] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 1",
          "ref_ids": [
            "38"
          ],
          "1": "However, unlike the previous studies [43, 10, 32] and concurrent papers [38, 1], we integrate these two stages in an end-to-end manner."
        },
        "Cvrecon: Rethinking 3d geometric feature learning for neural reconstruction": {
          "authors": [
            "Ziyue Feng",
            "Liang Yang",
            "Pengsheng Guo",
            "Bing Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Feng_CVRecon_Rethinking_3D_Geometric_Feature_Learning_For_Neural_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[31] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021.",
          "ref_ids": [
            "31"
          ],
          "2": "In recent years, learningbased methods [1, 21, 24, 29, 31] have shown promising results for this task.",
          "4": "In recent years 3D computer vision research [27, 6, 5, 4, 42] have shown remarkable progress, especially volumetric-based 3D reconstruction [1, 21, 29, 31, 28].",
          "5": "NeuralRecon [31] is later proposed to introduce a fragmenting strategy and RNN-based global fusion to handle largescale environments.",
          "8": "Given the downstream operation-agnostic nature of our proposed RCCV feature, we have found that it can be seamlessly integrated with various inter-frame feature fusion techniques, such as the multi-head self-attention module [29] or naive averaging operation [21, 31].",
          "11": "Following NeuralRecon [31], we apply binary cross-entropy (BCE) loss function to the coarse and medium level occupancy predictions and l1 loss function to the fine level TSDF prediction."
        },
        "Dg-recon: Depth-guided neural 3d scene reconstruction": {
          "authors": [
            "Jihong Ju",
            "Ching Wei",
            "Oleksandr Bailo",
            "Georgi Dikov",
            "Mohsen Ghafoorian"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ju_DG-Recon_Depth-Guided_Neural_3D_Scene_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[42] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "42"
          ],
          "4": "In this work, we build our method upon the most scalable algorithm of the 3D volumetric reconstruction category, NeuralRecon [42], and revisit feature back projection and fusion with the help of depth priors.",
          "5": "Finally, the depth prior also helps improve the reconstruction completeness over the baseline [42] by replacing the overfitted occupancy prediction with the depth-derived occupancy mapping.",
          "6": "NeuralRecon [42] addresses the efficiency and scalability issue by adopting a sparse 3D CNN for TSDF prediction only in local fragments spanning the view frustum of a few neighboring frames.",
          "7": "Our method is built on top of the NeuralRecon [42] and addresses its issue of incomplete reconstruction and oversmoothed object shapes with the help of depth guidance and improved fusion mechanisms.",
          "15": "NeuralRecon [42], a real-time online volumetric 3D reconstruction method, is the direct baseline to compare DG-Recon against.",
          "19": "Compared to the NeuralRecon [42] baseline, DG-Recon improves its F-score from 0.",
          "20": "Compared to its baseline [42], DG-Recon produces more complete meshes (row 4, 6, and 7).",
          "24": "Table 2 shows that DG-Recon consistently improves the reconstruction F-score of the NeuralRecon [42] baseline by 18% and 11% on 7-Scenes and SUN3D respectively.",
          "31": "DGRecon achieves comparable accuracy as NeuralRecon [42] while improves the depth completeness from 89.",
          "32": "Starting from the NeuralRecon [42] baseline (first row in Table 4), adding the depth-guided back projection and occupancy mapping improves the F-score from 0."
        },
        "From sora what we can see: A survey of text-to-video generation": {
          "authors": [
            "R Sun",
            "Y Zhang",
            "T Shah",
            "J Sun",
            "S Zhang",
            "W Li"
          ],
          "url": "https://arxiv.org/abs/2405.10674",
          "ref_texts": "[149] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15598\u201315607, 2021.",
          "ref_ids": [
            "149"
          ],
          "1": "Several studies have demonstrated the effectiveness of 3D reconstruction from video streams, with notable contributions including DyNeRF [148] and NeuralRecon [149].",
          "2": "Particularly, the forefront of coherent 3D reconstruction research [149], [150] is characterized by its emphasis on the seamless generation of new scene blocks, leveraging the spatial and semantic continuity of pre-existing scene structures."
        },
        "Immesh: An immediate lidar localization and meshing framework": {
          "authors": [
            "J Lin",
            "C Yuan",
            "Y Cai",
            "H Li",
            "Y Ren",
            "Y Zou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10304337/",
          "ref_texts": "[80] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "80"
          ],
          "1": "5) Evaluation of correctness: For the quantitative evaluation of the methods\u2019 correctness in reconstructing the mesh, we utilized 3D geometry metrics as employed in works NeuralRecon [80] and Atlas [81]."
        },
        "Asynchronous hybrid reinforcement learning for latency and reliability optimization in the metaverse over wireless communications": {
          "authors": [
            "W Yu",
            "TJ Chua",
            "J Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10138588/",
          "ref_texts": "[5] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3D reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "5"
          ],
          "1": "[5], which obtains remarkable results in realtime 3D construction from videos.",
          "2": "In the context of current 3D real-time transmissions, only keyframes are used for generation [5] to lessen the computation overhead.",
          "3": "Even a single home edition GTX 2080 can handle this task in milliseconds [5], but we consider a server with much more computing power.",
          "5": "The domain ofct n is referenced from the experimental results from demos of NeuralRecon [5] and Monster Mash [9], which are two impressive 3D reconstruction techniques that are abreast of the times."
        },
        "Roboscript: Code generation for free-form manipulation tasks across real and simulation": {
          "authors": [
            "J Chen",
            "Y Mu",
            "Q Yu",
            "T Wei",
            "S Wu",
            "Z Yuan"
          ],
          "url": "https://arxiv.org/abs/2402.14623",
          "ref_texts": "[54] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021. 5",
          "ref_ids": [
            "54"
          ],
          "1": "TSDF fusion is often preferred over cloud fusion from depth maps in real-time reconstruction pipelines due to its simplicity, faster speed, and ease of parallelization, as discussed in [54]."
        },
        "Real-time dense 3d mapping of underwater environments": {
          "authors": [
            "W Wang",
            "B Joshi",
            "N Burgdorfer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160266/",
          "ref_texts": "[33] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-time coherent 3D reconstruction from monocular video,\u201d in CVPR, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "33"
          ],
          "1": "Recently, learning-based approaches [28]\u2013[33] have shown promising results at high frame rates."
        },
        "Benchmarking neural radiance fields for autonomous robots: An overview": {
          "authors": [
            "Y Ming",
            "X Yang",
            "W Wang",
            "Z Chen",
            "J Feng"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0952197624018438",
          "ref_texts": "[38] J. Sun, Y. Xie, L. Chen, X. Zhou, H. Bao, NeuralRecon: Real-time coherent 3D reconstruction from monocular video, CVPR (2021).",
          "ref_ids": [
            "38"
          ],
          "1": "Also using image features as priors, VolRecon [37] creates feature volumes similar to NeuralRecon [38]."
        },
        "Spacetime surface regularization for neural dynamic scene reconstruction": {
          "authors": [
            "Jaesung Choe",
            "Christopher Choy",
            "Jaesik Park",
            "In So",
            "Anima Anandkumar"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[64] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021.",
          "ref_ids": [
            "64"
          ],
          "1": "Recently, neural scene representation [9, 23,40, 43,45, 50,53, 64] gets noticeable attention with their promising quality of implicit shape representations, such as occupancy or signed distance function."
        },
        "Nis-slam: Neural implicit semantic rgb-d slam for 3d consistent scene understanding": {
          "authors": [
            "H Zhai",
            "G Huang",
            "Q Hu",
            "G Li",
            "H Bao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10673805/",
          "ref_texts": "[59] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Realtime coherent 3D reconstruction from monocular video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2",
          "ref_ids": [
            "59"
          ],
          "1": "Benefiting from deep learning technology, some learning-based approaches [3, 20, 59, 61, 62, 66] are proposed to improve the robustness and accuracy."
        },
        "Gennbv: Generalizable next-best-view policy for active 3d reconstruction": {
          "authors": [
            "Xiao Chen",
            "Quanyi Li",
            "Tai Wang",
            "Tianfan Xue",
            "Jiangmiao Pang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_GenNBV_Generalizable_Next-Best-View_Policy_for_Active_3D_Reconstruction_CVPR_2024_paper.html",
          "ref_texts": "[38] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1",
          "ref_ids": [
            "38"
          ],
          "1": "Introduction Recent advances in 3D reconstruction [23, 38, 39] and neural rendering [20, 37, 48] have significantly enhanced the quality of 3D digitization of large scenes, such as buildings and city landmarks [11, 17, 18, 43, 47]."
        },
        "Newton: Neural view-centric mapping for on-the-fly large-scale slam": {
          "authors": [
            "H Matsuki",
            "K Tateno",
            "M Niemeyer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10439614/",
          "ref_texts": "[25] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "25"
          ],
          "1": "While pioneering works such as KinectFusion [16] and SuperEight [29] tracks against underlying voxel gridbased Truncated Signed Distance Functions (TSDF) with classical depth map fusion, later works such as NeuralRecon [25] and Atlas [15] incorporate learned priors into the volumetric TSDF mapping."
        },
        "Gov-nesf: Generalizable open-vocabulary neural semantic fields": {
          "authors": [
            "Yunsong Wang",
            "Hanlin Chen",
            "Gim Hee"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wang_GOV-NeSF_Generalizable_Open-Vocabulary_Neural_Semantic_Fields_CVPR_2024_paper.html",
          "ref_texts": "[36] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 5",
          "ref_ids": [
            "36"
          ],
          "1": "Since Replica [35] does not provide the ground truth 3D semantic labels, we leverage the TSDF Fusion in [36] to generate the mesh with semantic labels."
        },
        "Fully sparse 3d occupancy prediction": {
          "authors": [
            "H Liu",
            "Y Chen",
            "H Wang",
            "Z Yang",
            "T Li",
            "J Zeng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_4",
          "ref_texts": "[47] Sun, J., Xie, Y ., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: CVPR (2021)",
          "ref_ids": [
            "47"
          ],
          "1": "Recent methods focus on more compact and efficient end-to-end 3D reconstruction pipelines [39, 47, 2, 46, 10].",
          "2": "NeuralRecon [47] directly reconstructs local surfaces as sparse TSDF volumes and uses a GRU-based TSDF fusion module to fuse features from previous fragments."
        },
        "Gnesf: Generalizable neural semantic fields": {
          "authors": [
            "H Chen",
            "C Li",
            "M Guo",
            "Z Yan"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/72d32f4fe0b7af03732bd227bf1c4a5f-Abstract-Conference.html",
          "ref_texts": "[40] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Realtime coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "40"
          ],
          "2": "8 34 % NeuralRecon [40] 37.",
          "3": ", Atlas [31], NeuralRecon [40], Joint Recon-Segment [29], PointConv [47], and PointNet++ [33], and approaches trained on 2D semantic supervision, i.",
          "4": "3 shows the performance of our method compared with Altas [31] and NeuralRecon [40] that also use color images as input when trained on ScanNet and test on Replica."
        },
        "What's the situation with intelligent mesh generation: A survey and perspectives": {
          "authors": [
            "N Lei",
            "Z Li",
            "Z Xu",
            "Y Li",
            "X Gu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10141677/",
          "ref_texts": "[90] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in CVPR, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "90"
          ],
          "4": "47 Neuralrecon [90] Accuracy, Consistency, and Real-time reconstruction Can generator accurate and coherent reconstruction in real-time Not an end-to-end differentiable process 2."
        },
        "Visfusion: Visibility-aware online 3d scene reconstruction from videos": {
          "authors": [
            "Huiyu Gao",
            "Wei Mao",
            "Miaomiao Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Gao_VisFusion_Visibility-Aware_Online_3D_Scene_Reconstruction_From_Videos_CVPR_2023_paper.html",
          "ref_texts": "[26] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "26"
          ],
          "11": "For online end-to-end 3D reconstruction methods, NeuralRecon [26] proposes the first real-time framework for dense reconstruction from posed monocular videos.",
          "29": "NeuralRecon [26] and TransformerFusion [2] are two end-to-end incremental volumetric reconstruction frameworks that directly predict the surface geometry and are the most relevant ones to our approach.",
          "35": "Although due to the extra computation of our visibility-aware feature fusion and ray-based local sparsification, our method tends to be slower than neuralrecon [26], our model still achieves a real-time reconstruction of 25 key frames per second (FPS) on an NVIDIA RTX 2080Ti GPU and 45 FPS on an NVIDIA RTX 4090 GPU.",
          "37": "Compared to NeuralRecon [26], our reconstruction results are more complete and contain more details (highlighted in the red boxes)."
        },
        "Planarrecon: Real-time 3d plane detection and reconstruction from posed monocular videos": {
          "authors": [
            "Yiming Xie",
            "Matheus Gadelha",
            "Fengting Yang",
            "Xiaowei Zhou",
            "Huaizu Jiang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xie_PlanarRecon_Real-Time_3D_Plane_Detection_and_Reconstruction_From_Posed_Monocular_CVPR_2022_paper.html",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, 2021. 1, 3, 4, 6, 7",
          "ref_ids": [
            "37"
          ],
          "3": "Our work adopts the framework from [37] for the initial 3D geometry estimation due to its high efficiency.",
          "12": "Specifically, our method runs \u223c2\u00d7 faster than ESTDepth [24] + PEAC [13] and \u223c15\u00d7 faster than NeuralRecon [37] + Seq-RANSAC."
        },
        "Reality3dsketch: Rapid 3d modeling of objects from single freehand sketches": {
          "authors": [
            "T Chen",
            "C Ding",
            "L Zhu",
            "Y Zang",
            "Y Liao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10295995/",
          "ref_texts": "[51] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in CVPR, 2021, pp.",
          "ref_ids": [
            "51"
          ],
          "1": "3D Reconstruction and In-Situ 3D Modeling We next apply the sketch-to-model process in a real environment, which is enabled by a state-of-the-art real-time indoor 3D reconstruction algorithm [51] and our customized acquisition application."
        },
        "3dvnet: Multi-view depth prediction and volumetric refinement": {
          "authors": [
            "A Rich",
            "N Stier",
            "P Sen",
            "T H\u00f6llerer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665838/",
          "ref_texts": "[24] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. InConference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 3, 6",
          "ref_ids": [
            "24"
          ],
          "2": "NeuralRecon [24] improves on the memory consumption and run-time of Atlas by reconstructing local fragments using the most recent keyframes, then fusing the local fragments to a global volume using an RNN.",
          "3": "Baselines: We compare our method to seven state of the art baselines: Point-MVSNet (PMVS) [2], FastMVSNet (FMVS) [32], DeepVideoMVS pair/fusion networks (DVMVS pair/fusion) [6], GPMVS batched [11], Atlas [18], and NeuralRecon [24]."
        },
        "Panorecon: Real-time panoptic 3d reconstruction from monocular video": {
          "authors": [
            "Dong Wu",
            "Zike Yan",
            "Hongbin Zha"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_PanoRecon_Real-Time_Panoptic_3D_Reconstruction_from_Monocular_Video_CVPR_2024_paper.html",
          "ref_texts": "[7] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 3, 4, 5, 6",
          "ref_ids": [
            "7"
          ],
          "1": "[7, 9\u201311] adopt a sparse 3D CNN to perform feature volume-based reconstruction fragment by fragment for better efficiency and scalability, and utilize a GRU to fuse fragment feature volumes over time for better coherence.",
          "8": "Our method outperforms existing online feature fusion methods [7, 9], and slightly better than the state of the art depth fusion method [5] in terms of Fscore metric.",
          "9": "With the assistance of MVS depth, our method can recover more complete and detailed geometry than the pure feature fusion method [7]."
        },
        "Fast monocular scene reconstruction with global-sparse local-dense grids": {
          "authors": [
            "Wei Dong",
            "Christopher Choy",
            "Charles Loop",
            "Or Litany",
            "Yuke Zhu",
            "Anima Anandkumar"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Dong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023_paper.html",
          "ref_texts": "[38] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021. 1, 3",
          "ref_ids": [
            "38"
          ],
          "2": "A variety of classical and learning-based methods [27, 38, 46, 50] have been developed to achieve high quality multi-view depth reconstruction from monocular images."
        },
        "Joint depth prediction and semantic segmentation with multi-view sam": {
          "authors": [
            "Mykhailo Shvets",
            "Dongxu Zhao",
            "Marc Niethammer",
            "Roni Sengupta",
            "Alexander C. Berg"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.html",
          "ref_texts": "[43] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 3",
          "ref_ids": [
            "43"
          ],
          "1": "On the other end of the spectrum, full 3D recognition [24] and reconstruction [36,43] models showed impressive performance on a range of tasks.",
          "2": "Atlas [36] and NeuralRecon [43] use back-projection to lift 2D features into 3D space and regress a TSDF in a voxel grid.",
          "3": "Atlas [36] performs a running average aggregation and produces a scene-level voxel grid, while NeuralRecon [43] predicts fragments of the scene and fuses them together through an RNN."
        },
        "Nerfvs: Neural radiance fields for free view synthesis via geometry scaffolds": {
          "authors": [
            "Chen Yang",
            "Peihao Li",
            "Zanwei Zhou",
            "Shanxin Yuan",
            "Bingbing Liu",
            "Xiaokang Yang",
            "Weichao Qiu",
            "Wei Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.html",
          "ref_texts": "[22] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "22"
          ],
          "1": "In contrast, some neural reconstruction methods can recover the holistic scene geometry successfully with various priors [9,22,25,34], while the synthesized images from these methods contain plenty of artifacts and are over-smoothed."
        },
        "AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings": {
          "authors": [
            "Jamie Watson",
            "Filippo Aleotti",
            "Mohamed Sayed",
            "Zawar Qureshi",
            "Oisin Mac",
            "Gabriel Brostow",
            "Michael Firman",
            "Sara Vicente"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Watson_AirPlanes_Accurate_Plane_Estimation_via_3D-Consistent_Embeddings_CVPR_2024_paper.html",
          "ref_texts": "[70] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 3, 6",
          "ref_ids": [
            "70"
          ],
          "1": "Subsequent methods [6, 70] have extended neural TSDF estimation to the online setting.",
          "2": "16 NeuralRecon [70] + RANSAC 9.",
          "3": "92 NeuralRecon [70] + RANSAC 9.",
          "4": "In addition, we also apply the sequential RANSAC method to geometry from [45, 67, 70].",
          "5": "To validate the usefulness of our 3D embeddings, we use them in combination with different geometry estimation methods [45, 58, 67, 70]."
        },
        "Efficient implicit neural reconstruction using lidar": {
          "authors": [
            "D Yan",
            "X Lyu",
            "J Shi",
            "Y Lin"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160322/",
          "ref_texts": "[25] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "25"
          ],
          "1": "Others abandon the traditional feature matching process and build a cost volume to mimic the multi-view stereo matching using 3D CNN [23]\u2013[25]."
        },
        "Input-level inductive biases for 3d reconstruction": {
          "authors": [
            "Wang Yifan",
            "Carl Doersch",
            "Relja Arandjelovic",
            "Joao Carreira",
            "Andrew Zisserman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yifan_Input-Level_Inductive_Biases_for_3D_Reconstruction_CVPR_2022_paper.html",
          "ref_texts": "[52] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, pages 15598\u2013",
          "ref_ids": [
            "52"
          ],
          "1": "Online methods typically rely on more explicit but expensive 3D representations like voxel grids [25, 40, 52, 67]."
        },
        "Sgaligner: 3d scene alignment with scene graphs": {
          "authors": [
            "Sayan Deb",
            "Ondrej Miksik",
            "Marc Pollefeys",
            "Daniel Barath",
            "Iro Armeni"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Sarkar_SGAligner_3D_Scene_Alignment_with_Scene_Graphs_ICCV_2023_paper.html",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 9",
          "ref_ids": [
            "37"
          ],
          "1": "The evaluation metrics we use focus on the geometric aspects of accuracy and completeness [37] of the resulting reconstruction, as well as on precision, recall, and F1-score of registered point clouds."
        },
        "Hybridocc: Nerf enhanced transformer-based multi-camera 3d occupancy prediction": {
          "authors": [
            "X Zhao",
            "B Chen",
            "M Sun",
            "D Yang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10564099/",
          "ref_texts": "[34] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Realtime coherent 3d reconstruction from monocular video,\u201d2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.",
          "ref_ids": [
            "34"
          ],
          "1": "Implicit reconstruction works [34], [35] based on image features extends object-level reconstruction to indoor scenes and is committed to building a generalized implicit network.",
          "2": "[34] and [35] adopt a coarse-to-fine approach fuse multi-scale features to obtain more accurate 3D reconstruction of indoor scenes."
        },
        "Cross-dimensional refined learning for real-time 3D visual perception from monocular video": {
          "authors": [
            "Ziyang Hong",
            "Patrick Yue"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/Hong_Cross-Dimensional_Refined_Learning_for_Real-Time_3D_Visual_Perception_from_Monocular_ICCVW_2023_paper.html",
          "ref_texts": "[39] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "39"
          ],
          "1": "Our model is more accurate and coherent in real time, compared to two baseline methods with input from monocular video, Atlas [27] and NeuralRecon [39] + Semantic-Heads.",
          "4": "NeuralRecon [39] adopted sparse 3D convolutions and the gated recurrent unit (GRU) to achieve a real-time 3D reconstruction on cellphones, without the capability of semantic deduction.",
          "5": "NeuralRecon [39] achieved a real-time 3D reconstruction learning capability by utilizing sparse 3D convolutions and recurrent networks with key frames as input.",
          "12": "Our method is capable of reconstructing consistent and detailed geometry which is neither overly smooth as the one from Atlas [27] nor eroded with holes as from NeuralRecon [39].",
          "16": "To evaluate how much robustness a model can achieve while targeting 3D perception tasks in real time, we define the 3D perception efficiency metric\u03b73D = FPS \u00d7mIoU \u00d7 F-score, since F-score is regarded as the most suitable 3D metric for evaluating 3D reconstruction quality by considering Precision and Recall at the same time [27, 39, 36]."
        },
        "Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors": {
          "authors": [
            "ZX Zou",
            "SS Huang",
            "YP Cao",
            "TJ Mu",
            "Y Shan"
          ],
          "url": "https://arxiv.org/abs/2209.15153",
          "ref_texts": "[22] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Realtime coherent 3d reconstruction from monocular video,\u201d in IEEE CVPR, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "22"
          ],
          "1": "The recent work of NeuralRecon [22] proposes to reconstruct the 3D geometry with a neural network instead of multi-view depth maps, and has achieved coherent 3D surface reconstruction results from monocular videos.",
          "6": "We adopt the sparse feature volume data structure from NeuralRecon [22] to organize the 3D scene\u2019s geometry content, where the entire 3D space is divided into a set of sparse voxels.",
          "7": "These feature latent vectors Fl \u03b8 are further merged with the previous feature latent vectors using a GRU module after sparse 3D CNN, as global feature latent vectors [22].",
          "8": "Finally, we train our NISR similar to NeuralRecon [22] but with a geometric prior guided loss function: L= Lo + \u03bbsLs + \u03bbnLn + \u03bbfLf, (6) where Lo = \u2211L l=1 \u03bbl oBCE(ol,\u02c6 ol) denotes the binary crossentropy (BCE) loss, Ls = |clamp(s,\u03c4) \u2212clamp(\u02c6s,\u03c4)| represents the clipped L1 loss with clamp(x,\u03c4) = min(\u03c4,max(\u2212\u03c4,x)), and Lf = 1 M \u2211M i=1 ||fi||2 2 is used to regularize scene feature vectors fi \u2208Fl \u03b8."
        },
        "Retr: Modeling rendering via transformer for generalizable neural surface reconstruction": {
          "authors": [
            "Y Liang",
            "H He",
            "Y Chen"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/c47ec10bc135be5c3663ba344d29a6a5-Abstract-Conference.html",
          "ref_texts": "[50] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "50"
          ],
          "1": "To overcome this limitation, inspired by NeuralRecon [50], we further propose Hybrid Extractor."
        },
        "Human body shape completion with implicit shape and flow learning": {
          "authors": [
            "Boyao Zhou",
            "Di Meng",
            "Sebastien Franco",
            "Edmond Boyer"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.html",
          "ref_texts": "[43] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. 3",
          "ref_ids": [
            "43"
          ],
          "1": "Depth Feature Encoder Recent literature has shown the robustness of hierarchical feature encoding for both 2D object detection [20] and 3D reconstruction [43] tasks."
        },
        "Geometry-guided feature learning and fusion for indoor scene reconstruction": {
          "authors": [
            "Ruihong Yin",
            "Sezer Karaoglu",
            "Theo Gevers"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yin_Geometry-guided_Feature_Learning_and_Fusion_for_Indoor_Scene_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[30] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "30"
          ],
          "1": "Based on Atlas, NeuralRecon [30] designs a learning-based TSDF fusion to transfer features from previous to current fragments.",
          "2": "NeuralRecon [30] concatenates the projected depth to 3D features after multiview fusion.",
          "5": "132 NeuralRecon [30] 1.",
          "6": "Like [30], the reconstruction time of a local fragment is divided by the number of keyframes."
        },
        "Multi-scale hash encoding based neural geometry representation": {
          "authors": [
            "Z Deng",
            "H Xiao",
            "Y Lang",
            "H Feng",
            "J Zhang"
          ],
          "url": "https://link.springer.com/article/10.1007/s41095-023-0340-x",
          "ref_texts": "[57] Sun, J. M.; Xie, Y. M.; Chen, L. H.; Zhou, X. W.; Bao, H. J. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15593\u201315602, 2021.",
          "ref_ids": [
            "57"
          ],
          "1": "NeuralRecon [57] offers a neural network to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially."
        },
        "SuperPrimitive: Scene reconstruction at a primitive level": {
          "authors": [
            "Kirill Mazur",
            "Gwangbin Bae",
            "Andrew J. Davison"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Mazur_SuperPrimitive_Scene_Reconstruction_at_a_Primitive_Level_CVPR_2024_paper.html",
          "ref_texts": "[48] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.2",
          "ref_ids": [
            "48"
          ],
          "1": "In the recent years, deep-learning based approaches sought to replace explicit multi-view geometry estimating with learning-based methods [3, 42, 48]."
        },
        "Geobench: Benchmarking and analyzing monocular geometry estimation models": {
          "authors": [
            "Y Ge",
            "G Xu",
            "Z Zhao",
            "L Sun",
            "Z Huang",
            "Y Sun"
          ],
          "url": "https://arxiv.org/abs/2406.12671",
          "ref_texts": "[SXC+21] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3d reconstruction from monocular video. In IEEE Conf. Comput. Vis. Pattern Recog., pages 15598\u201315607, 2021.",
          "ref_ids": [
            "SXC\\+21"
          ],
          "1": "Its significance is underscored by its broad utility across various downstream tasks, including object detection [HWSH22, WYK+20, DHY+20], visual navigation [TTLN17, YSWC20, SYX+22, YWSC18], novel view synthesis [DLZR22, RBM+22], controllable image generation [ZRA23, ECA+23, ZCC+24], and 3D scene reconstruction [SXC+21, DT20]."
        },
        "Incremental dense reconstruction from monocular video with guided sparse feature volume fusion": {
          "authors": [
            "X Zuo",
            "N Yang",
            "N Merrill",
            "B Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10120911/",
          "ref_texts": "[2] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. \u201cNeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021, pp. 15598\u201315607.",
          "ref_ids": [
            "2"
          ],
          "1": "The incremental variant [2] can even achieve real-time performance on a desktop with commercial-level GPU.",
          "2": "The closest work to our method is NeuralRecon [2], which is also a baseline of our method.",
          "3": "Following [2, 14], we utilize a 2D feature extraction network composed of an MnasNet encoder [25] and feature pyramid network (FPN) [26] style decoder.",
          "5": "Fragment to Global Fusion We follow NeuralRecon [2] to fuse the fragment feature volume into a global feature volume incrementally via GRU fusion [6].",
          "9": "The proposed method, Ours, can recover more 3D structures than the incremental feature-volume-based method NeuralRecon [2].",
          "14": "We compare the proposed method, Ours, with state-of-the-art traditional structure-from-motion method, COLMAP [31], multi-view stereo networks including MVDepthNet [9], DPSNet [12], and GPMVS [10], as well as all the open-source feature-volumebased methods including Atlas [1], V ortx [4], and NeuralRecon [2].",
          "17": "We can find that Ours outperforms NeuralRecon [2].",
          "21": "5, where we can easily find that our methods can recover more geometric details than NeuralRecon [2]."
        },
        "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation": {
          "authors": [
            "Y Chen",
            "Y Nie",
            "B Ummenhofer",
            "R Birkl"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72673-6_10",
          "ref_texts": "59. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021) 2",
          "ref_ids": [
            "59"
          ],
          "1": "Radiance fields have emerged as a powerful learning representation for 3D reconstruction and generation, as evidenced by their effectiveness in various studies [6,16,29,34,41,42,59,66]."
        },
        "Multi-view 3D object reconstruction and uncertainty modelling with neural shape prior": {
          "authors": [
            "Ziwei Liao",
            "Steven L. Waslander"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.html",
          "ref_texts": "[44] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": ", NeRF [28], visual SLAM [42, 56, 55] and scene reconstruc3099 tion [44]."
        },
        "Depth field networks for generalizable multi-view scene representation": {
          "authors": [
            "V Guizilini",
            "I Vasiljevic",
            "J Fang",
            "R Ambru"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_15",
          "ref_texts": "45. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 15598\u201315607 (June 2021)",
          "ref_ids": [
            "45"
          ],
          "1": "NeuralRecon [45] moves beyond depth-based architectures to learn Truncated Signed Distance Field (TSDF) volumes as a way to improve surface consistency.",
          "3": "The only method that outperforms DeFiNe in terms of speed is NeuralRecon [45], which uses a sophisticated TSDF integration strategy."
        },
        "MonoSelfRecon: Purely self-supervised explicit generalizable 3D reconstruction of indoor scenes from monocular RGB views": {
          "authors": [
            "Runfa Li",
            "Upal Mahbub",
            "Vasudev Bhaskaran",
            "Truong Nguyen"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/3DMV/html/Li_MonoSelfRecon_Purely_Self-Supervised_Explicit_Generalizable_3D_Reconstruction_of_Indoor_Scenes_CVPRW_2024_paper.html",
          "ref_texts": "[35] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021. 1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "35"
          ],
          "5": "We use the same evaluation metrics as Atlas [27] and NeuralRecon [35], and we evaluate both 3D geometric mesh metrics and 2D rendered depth metrics.",
          "6": "For the generalizable supervised voxel-SDF methods [27, 35], we directly compare both 3D geometric mesh and 2D rendered depth.",
          "14": "While all supervised voxel-SDF methods [27, 34, 35] use voxel-SDF annotations for a L1 loss, NeuralRecon also uses a coarse binary voxel-mask annotations as weakly supervised signal, where the mask tells if each voxel-SDF is within truncation distance.",
          "17": "361 NeuralRecon[35] sup 82.",
          "18": "346 Neucon weak[35] weak 78."
        },
        "Volumetric bundle adjustment for online photorealistic scene capture": {
          "authors": [
            "Ronald Clark"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Clark_Volumetric_Bundle_Adjustment_for_Online_Photorealistic_Scene_Capture_CVPR_2022_paper.html",
          "ref_texts": "[18] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2, 5",
          "ref_ids": [
            "18"
          ],
          "3": "For monocular systems we compare against MVDepthNet [23], ATLAS, [11] and NeuralRecon [18] In terms of neural volume rendering approaches we compare against NeRF [10], NSVF [8], MVSNeRF [1] and IBRNet [24]."
        },
        "Learning-based multi-view stereo: a survey": {
          "authors": [
            "F Wang",
            "Q Zhu",
            "D Chang",
            "Q Gao",
            "J Han"
          ],
          "url": "https://arxiv.org/abs/2408.15235",
          "ref_texts": "[47] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in CVPR, 2021.",
          "ref_ids": [
            "47"
          ],
          "3": "Specifically, Atlas [46] and NeuralRecon [47] attempt to predict the TSDF volume from the 3D feature volume constructed by lifting 2D image features."
        },
        "Neural 3D Scene Reconstruction With Indoor Planar Priors": {
          "authors": [
            "X Zhou",
            "H Guo",
            "S Peng",
            "Y Xiao",
            "H Lin"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10476755/",
          "ref_texts": "[61] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-time coherent 3D reconstruction from monocular video,\u201d inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 15593\u201315602. Authorized licensed use limited to: Zhejiang University. Downloaded on December 29,2024 at 15:06:12 UTC from IEEE Xplore. Restrictions apply. ",
          "ref_ids": [
            "61"
          ],
          "1": "NeuralRecon[61] improves the reconstruction speed through reconstructing local surfaces for each fragment sequence.",
          "2": "We consider F-score as the overall metric following[61]."
        },
        "Pmvc: Promoting multi-view consistency for 3d scene reconstruction": {
          "authors": [
            "Chushan Zhang",
            "Jinguang Tong",
            "Tao Jun",
            "Chuong Nguyen",
            "Hongdong Li"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PMVC_Promoting_Multi-View_Consistency_for_3D_Scene_Reconstruction_WACV_2024_paper.html",
          "ref_texts": "[45] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video.CVPR, 2021. 7",
          "ref_ids": [
            "45"
          ],
          "1": "It is worth noting that the F-score is often regarded as the most appropriate metric for assessing geometry quality, as argued in [45]."
        },
        "Parf: Primitive-aware radiance fusion for indoor scene novel view synthesis": {
          "authors": [
            "Haiyang Ying",
            "Baowei Jiang",
            "Jinzhi Zhang",
            "Di Xu",
            "Tao Yu",
            "Qionghai Dai",
            "Lu Fang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ying_PARF_Primitive-Aware_Radiance_Fusion_for_Indoor_Scene_Novel_View_Synthesis_ICCV_2023_paper.html",
          "ref_texts": "[34] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.2, 3",
          "ref_ids": [
            "34"
          ],
          "1": "NeuralRecon [34] establishes a learning-based TSDF fusion module based on GRU to guide the network to fuse features from previous fragments in real time."
        },
        "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
          "authors": [
            "Y Xin",
            "X Zuo",
            "D Lu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10316359/",
          "ref_texts": "[51] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "In addition to the cost-volume based MVS techniques mentioned above, a new category of learning-based volumetric methods has recently emerged, which directly extract 3D scene structures from deep feature volumes [2, 30, 49, 51].",
          "2": "6 NeuralRecon [51] Yes 5."
        },
        "Diffindscene: Diffusion-based high-quality 3d indoor scene generation": {
          "authors": [
            "Xiaoliang Ju",
            "Zhaoyang Huang",
            "Yijin Li",
            "Guofeng Zhang",
            "Yu Qiao",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Ju_DiffInDScene_Diffusion-based_High-Quality_3D_Indoor_Scene_Generation_CVPR_2024_paper.html",
          "ref_texts": "[41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.1, 2, 5, 8",
          "ref_ids": [
            "41"
          ],
          "2": "Our proposed diffusion method can be also utilized to refine indoor scene meshes such as the reconstruction results from multi-view stereo, such as NeuralRecon [41].",
          "3": "For scene refinement or recovery on multi-view stereo (MVS), we trained DiffInDScene using the training split of the ScanNet dataset, with the NeuralRecon [41] as the MVS module."
        },
        "Vidar: Data quality improvement for monocular 3d reconstruction through in-situ visual interaction": {
          "authors": [
            "H Gao",
            "Y Liu",
            "F Cao",
            "H Wu",
            "F Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610260/",
          "ref_texts": "[8] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Realtime coherent 3d reconstruction from monocular video,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "8"
          ],
          "2": "NeuralRecon [8] predicts TSDF values with coarse-to-fine structure and introduces a 3D gated recurrent unit (GRU) to help fuse local features into global space instead of traditional TSDF fusion methods."
        },
        "ObjectFusion: Accurate object-level SLAM with neural object priors": {
          "authors": [
            "Xin Zou"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1524070322000418",
          "ref_texts": "[48] J. Sun, Y. Xie, L. Chen, X. Zhou, H. Bao, NeuralRecon: Real-time coherent 3D reconstruction from monocular video, in: IEEE CVPR, 2021, pp. 15598\u201315607.",
          "ref_ids": [
            "48"
          ],
          "1": "Given camera poses estimation, FroDo [23] and NeuralRecon [48] can generate 3D object or scene reconstruction from monocular RGB frames with impressively high surface reconstruction quality."
        },
        "Svr-net: A sparse voxelized recurrent network for robust monocular slam with direct tsdf mapping": {
          "authors": [
            "Rongling Lang",
            "Ya Fan",
            "Qing Chang"
          ],
          "url": "https://www.mdpi.com/1424-8220/23/8/3942",
          "ref_texts": "18. Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; Bao, H. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video.arXiv 2021, arXiv:2104.00681.",
          "ref_ids": [
            "18"
          ],
          "2": "NeuralRecon directly reconstructs local surfaces, represented as sparse TSDF volumes for each video fragment sequentially, by a neural network [18]."
        },
        "Smurf: Continuous dynamics for motion-deblurring radiance fields": {
          "authors": [
            "J Lee",
            "D Lee",
            "M Lee",
            "D Kim",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2403.07547",
          "ref_texts": "51. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "51"
          ],
          "1": "Many recent studies have extensively applied NeRF across various fields, including dynamic 3D scene modeling [3,27,28,30,37,38,43,54,61], scene relighting [2,32,41,48,53], and 3D reconstruction [51,55,57]."
        },
        "DoubleTake: Geometry Guided Depth Estimation": {
          "authors": [
            "M Sayed",
            "F Aleotti",
            "J Watson",
            "Z Qureshi"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73464-9_8",
          "ref_texts": "61. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In: CVPR (2021)",
          "ref_ids": [
            "61"
          ],
          "1": "In contrast, feedforward volumetric methods [18,45,61,79] directly estimate the volume occupancy (usually encoded as a TSDF), often leveraging expensive 3D CNNs.",
          "2": "579 NeuralRecon [61] Yes 7.",
          "3": "615 NeuralRecon [61] Yes 5.",
          "4": "58ms per frame update of [52] or 90ms of [61].",
          "5": "NeuralRecon [61] SR [52] (online) Ours (incremental) Ground truth Fig."
        },
        "2S-UDF: a novel two-stage UDF learning method for robust non-watertight model reconstruction from multi-view images": {
          "authors": [
            "Junkai Deng",
            "Fei Hou",
            "Xuhui Chen",
            "Wencheng Wang",
            "Ying He"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Deng_2S-UDF_A_Novel_Two-stage_UDF_Learning_Method_for_Robust_Non-watertight_CVPR_2024_paper.html",
          "ref_texts": "[33] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular V ideo. InIEEE Conf. Comput. V is. P attern Recog. , pages 15593\u201315602, 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "V oxel-based methods [3, 8, 20, 21, 33] divide the 3D space into voxels and determine which ones belong to the object."
        },
        "A robust inlier identification algorithm for point cloud registration via -minimization": {
          "authors": [
            "Y Jiang",
            "X Tang",
            "C Cheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/735c847a07bf6dd4486ca1ace242a88c-Abstract-Conference.html",
          "ref_texts": "[29] Sun, J., Xie, Y ., Chen, L., Zhou, X., Bao, H., 2021. Neuralrecon: Real-time coherent 3d reconstruction from monocular video, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15598\u201315607.",
          "ref_ids": [
            "29"
          ],
          "1": "Inspired by the success of deep learning in 3D perception [34, 32, 37, 29], recent works have adopted learning networks for point cloud registration [1, 8, 19, 20, 44]."
        },
        "A survey on deep learning approaches for data integration in autonomous driving system": {
          "authors": [
            "X Zhu",
            "L Wang",
            "C Zhou",
            "X Cao",
            "Y Gong"
          ],
          "url": "https://arxiv.org/abs/2306.11740",
          "ref_texts": "[209] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 2021, pp. 15 598\u201315 607. [Online]. Available: https://openaccess.thecvf.com/ content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_",
          "ref_ids": [
            "209",
            "Online"
          ],
          "1": "Besides, most 3D reconstruction methods [209], [239]\u2013[245] integrate at feature level because of two reasons."
        },
        "Object slam-based active mapping and robotic grasping": {
          "authors": [
            "Y Wu",
            "Y Zhang",
            "D Zhu",
            "X Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665905/",
          "ref_texts": "[26] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 8",
          "ref_ids": [
            "26"
          ],
          "1": "The proposed SLAM-based object map promotes augmented reality to the next level, in which the object map provides more complete environment information for augmented reality, allowing for a more realistic immersive experience [38, 9, 26]."
        },
        "WaterMono: Teacher-Guided Anomaly Masking and Enhancement Boosting for Robust Underwater Self-Supervised Monocular Depth Estimation": {
          "authors": [
            "Y Ding",
            "K Li",
            "H Mei",
            "S Liu",
            "G Hou"
          ],
          "url": "https://arxiv.org/abs/2406.13344",
          "ref_texts": "[2] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "2"
          ],
          "1": "I NTRODUCTION A CQUIRING depth information is a fundamental task in underwater visual perception, serving as a prerequisite for underwater observation and operation tasks such as visual navigation [1], visual reconstruction [2], visual localization and mapping [3], etc."
        },
        "A Probability-Guided Sampler for Neural Implicit Surface Rendering": {
          "authors": [
            "GD Pais",
            "V Piedade",
            "M Chatterjee",
            "M Greiff"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72913-3_10",
          "ref_texts": "44. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR). pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "44"
          ],
          "1": "T o solve this problem, occupancy approaches [23, 25, 29, 30, 32, 35] and SDF approaches [2, 7, 11, 21, 44, 50, 51, 57, 58, 61, 62, 65] were proposed."
        },
        "Planarnerf: Online learning of planar primitives with neural radiance fields": {
          "authors": [
            "Z Chen",
            "Q Yan",
            "H Zhan",
            "C Cai",
            "X Xu",
            "Y Huang"
          ],
          "url": "https://arxiv.org/abs/2401.00871",
          "ref_texts": "[41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "41"
          ],
          "1": "We compare our method with four types of approaches: (1) Single view plane recovering [9]; (2) Multi-view depth estimation [44] with depth based plane detection [45]; (3) V olume-based 3D reconstruction [41] with Sequential RANSAC [42]; and (4) Learning-based 3D planar reconstruction [14].",
          "2": "(GB) \u2193 Time (ms) \u2193 NeuralRecon [41] + Seq-RANSAC [42] [43] 0.",
          "3": "Method VOI \u2193 RI \u2191 SC \u2191 NeuralRecon [41] + Seq-RANSAC [42] 8."
        },
        "Omni-Recon: Harnessing Image-Based Rendering for General-Purpose Neural Radiance Fields": {
          "authors": [
            "Y Fu",
            "H Qu",
            "Z Ye",
            "C Li",
            "K Zhao",
            "Y Lin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72640-8_9",
          "ref_texts": "63. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "63"
          ],
          "1": "Next, we construct a 3D feature volume V \u2208 RM\u00d7M\u00d7M\u00d7C, which serves similar roles as cost volumes for multiview stereo [10,83], to aggregate geometry information from different source views, following [44,55,63].",
          "2": "Mesh extraction:We utilize TSDF fusion [45,63] to reconstruct scene meshes from predicted source view depths, following [34,55].",
          "3": ", the depth of each source view from each scene\u2019s training set is estimated and fused into a mesh using TSDF fusion [45,63].",
          "4": "This involves baking the complex geometry branch into meshes using TSDF fusion [45,63] as discussed in Sec."
        },
        "Shapematcher: Self-supervised joint shape canonicalization segmentation retrieval and deformation": {
          "authors": [
            "Yan Di",
            "Chenyangguang Zhang",
            "Chaowei Wang",
            "Ruida Zhang",
            "Guangyao Zhai",
            "Yanyan Li",
            "Bowen Fu",
            "Xiangyang Ji",
            "Shan Gao"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Di_ShapeMatcher_Self-Supervised_Joint_Shape_Canonicalization_Segmentation_Retrieval_and_Deformation_CVPR_2024_paper.html",
          "ref_texts": "[49] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021. 1",
          "ref_ids": [
            "49"
          ],
          "1": "Introduction In recent years, there has been a notable surge in research interest focused on generating high-quality 3D models from scans of complex scenes [6, 15, 16, 21, 35, 49, 73]."
        },
        "NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation": {
          "authors": [
            "Ziyi Chen",
            "Xiaolong Wu",
            "Yu Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_NC-SDF_Enhancing_Indoor_Scene_Reconstruction_Using_Neural_SDFs_with_View-Dependent_CVPR_2024_paper.html",
          "ref_texts": "[41] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 2",
          "ref_ids": [
            "41"
          ],
          "1": "Some data-driven methods [7, 13, 28, 39, 41] alleviate this limitation by directly predicting a truncated signed distance field (TSDF) from multi-view images.",
          "2": "Alternatively, other learning-based methods [7, 13, 28, 39, 41] directly predict the TSDF and then extract the mesh from the TSDF volume."
        },
        "Crim-gs: Continuous rigid motion-aware gaussian splatting from motion blur images": {
          "authors": [
            "J Lee",
            "D Kim",
            "D Lee",
            "S Cho",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2407.03923",
          "ref_texts": "[46] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "46"
          ],
          "1": "NeRF has led to a wide range of studies, including 3D mesh reconstruction [22, 46, 49, 50], dynamic scene [20, 21, 31, 32, 38, 47], and human avatars [14, 37, 53]."
        },
        "Ray-Distance Volume Rendering for Neural Scene Reconstruction": {
          "authors": [
            "R Yin",
            "Y Chen",
            "S Karaoglu",
            "T Gevers"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72630-9_22",
          "ref_texts": "37. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "37"
          ],
          "4": "For instance, NeuralRecon [37] designs a learnable TSDF fusion module to integrate features from previous frames and predicts the TSDF for sparse volumes."
        },
        "XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM": {
          "authors": [
            "X Wang",
            "N Wang",
            "G Zhang"
          ],
          "url": "https://arxiv.org/abs/2410.23690",
          "ref_texts": "[29] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 15598\u201315607, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "NeuralRecon [29] is integrated into XRDSLAM through this approach."
        },
        "Heightfields for efficient scene reconstruction for AR": {
          "authors": [
            "Jamie Watson",
            "Sara Vicente",
            "Oisin Mac",
            "Clement Godard",
            "Gabriel Brostow",
            "Michael Firman"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.html",
          "ref_texts": "[48] Sun, J., Xie, Y ., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3d reconstruction from monocular video. In: CVPR (2021)",
          "ref_ids": [
            "48"
          ],
          "3": "Unlike Atlas [30], which maintains a scene-level 3D feature volume, NeuralRecon [48] first performs local surface estimation by chopping the input scene into fragments which are later fused into a single global volume.",
          "7": "Our reconstructions are more complete than NeuralRecon [48] and are qualitatively comparable to Atlas [30], despite the fact that we only estimate 2D heightfields.",
          "8": "Comparison to 2D and 3D baselines We compare to three recent state-of-the-art methods that reason in 3D: Atlas [30], NeuralRecon [48] and TransformerFusion [3].",
          "11": "NeuralRecon [48] computes voxel reconstructions in fragments, which saves memory."
        },
        "Livepose: Online 3d reconstruction from monocular video with dynamic camera poses": {
          "authors": [
            "Noah Stier",
            "Baptiste Angles",
            "Liang Yang",
            "Yajie Yan",
            "Alex Colburn",
            "Ming Chuang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Stier_LivePose_Online_3D_Reconstruction_from_Monocular_Video_with_Dynamic_Camera_ICCV_2023_paper.html",
          "ref_texts": "[24] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. zju3dv.github.io/neuralrecon. 2, 3, 4, 5, 8",
          "ref_ids": [
            "24"
          ],
          "3": "In particular, we develop a novel, non-linear de-integration method based on deep learning to support online reconstruction for methods such as NeuralRecon [24] that use a learned, non-linear update rule.",
          "4": "Implicit volumetric reconstruction Recently, a number of methods have demonstrated a new paradigm in which a unified 3D model is estimated directly in scene space [2, 4, 10, 18, 23, 24], alleviating the above limitations of depth prediction.",
          "5": "NeuralRecon [24] uses a recurrent neural network (RNN) to integrate new views, yielding improved reconstruction quality relative to integration based on unweighted averaging.",
          "6": "Learned de-integration for neural features The third category of approach that we demonstrate is volumetric TSDF reconstruction with non-linear multi-view fusion, as exemplified by NeuralRecon [24].",
          "7": "Recent trends show that there is a significant advantage to data-driven integration approaches [2, 23, 24], allowing the network to learn to attend to the most informative image inputs.",
          "9": "We propose to fill the role of the intermediate layer using the fragment generation system from NeuralRecon [24], which we extend in order to incorporate pose update dynamics."
        },
        "Dionysus: Recovering scene structures by dividing into semantic pieces": {
          "authors": [
            "Likang Wang",
            "Lei Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Dionysus_Recovering_Scene_Structures_by_Dividing_Into_Semantic_Pieces_CVPR_2023_paper.html",
          "ref_texts": "[57] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 3, 5, 8",
          "ref_ids": [
            "57"
          ],
          "2": "Pursuing a higher candidate sampling density, CasMVSNet [15] shrinks the depth range in a coarse-to-fine fashion; IS-MVSNet [63] searches for new candidates according to the estimated error distribution; NeuralRecon [57] prunes the voxels thought to be unreliable in the previous predictions.",
          "3": "The mainstream real-time methods can be categorized into depth-based [9, 16, 40, 61] and volume-based [57].",
          "4": "Besides, similar to most existing methods [9, 57, 73], we assume the camera parameters from time t \u2212 n to t are available.",
          "5": "Disp = r |transition|2 + 2 3T race(I \u2212 rotation) (1) Datasets: Following the mainstream settings [9, 57], we train and evaluate our model on ScanNet [5], which is a large-scale real-world video dataset containing 1201 scenes for training, 321 for validation, and 100 for testing."
        },
        "Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding": {
          "authors": [
            "FG Zanjani",
            "H Cai",
            "Y Zhu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10648072/",
          "ref_texts": "[27] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao, \u201cNeuralRecon: Real-time coherent 3D reconstruction from monocular video,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, 2021, pp. 15598\u201315607.",
          "ref_ids": [
            "27"
          ],
          "1": "Baseline: Since there are only a few prior works that focus on learning-based multi-view 3D planar reconstruction, we compare our method to two types of approaches: (1) PlanarRecon [6] trained with 3D geometry and 3D plane supervisions, (2) dense 3D reconstruction methods like NeuralRecon[27] with 3D geometry supervision and MonoSDF [13] with 2D image supervision, followed by the Sequential RANSAC to extract planes [15]."
        },
        "Fine-detailed Neural Indoor Scene Reconstruction using multi-level importance sampling and multi-view consistency": {
          "authors": [
            "X Li",
            "Y Ji",
            "X Lai",
            "W Zhang",
            "L Zeng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10648179/",
          "ref_texts": "[4] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in CVPR, 2021, pp. 15598\u201315607. 1, 2, 4, 5",
          "ref_ids": [
            "4"
          ],
          "1": "Some other methods, such as volumetric methods [3, 4] use explicit voxels to model 3D scenes and directly regress input images to TSDF value or sparse occupancy.",
          "2": "[4] divides the space into multiple fragments and utilizes a recurrent network to fuse the features from previous fragments sequentially.",
          "5": "Baselines We compare against: (1) classic MVS method: COLMAP [19], (2) TSDF based method: NeuralRecon [4], (3) neural volume rendering methods: NeRF [5], NeuS [6], Manhattan-SDF [8], MonoSDF [9], NeuRIS [10] and HelixSurf [20]."
        },
        "SP-SLAM: Neural Real-Time Dense SLAM With Scene Priors": {
          "authors": [
            "Z Hong",
            "B Wang",
            "H Duan",
            "Y Huang",
            "X Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10830563/",
          "ref_texts": "[54] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15598\u201315607, 2021.",
          "ref_ids": [
            "54"
          ],
          "1": "Recently, NeRF has shown promising results in tasks such as novel view synthesis [44]\u2013[47], object-level reconstruction [48]\u2013[50], and large-scale scene reconstruction [16], [21], [51]\u2013[54]."
        },
        "Deep Cost Ray Fusion for Sparse Depth Video Completion": {
          "authors": [
            "J Kim",
            "S Kim",
            "J Park",
            "S Lee"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73347-5_19",
          "ref_texts": "45. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR (2021)",
          "ref_ids": [
            "45"
          ],
          "1": "To compare the local cost volume fusion and our raybased fusion, we adopt a convolutional variant of gated recurrent unit (GRU) used in [45] to fuse two aligned cost volumes.",
          "2": "\u2018B2\u2019 denotes the convolutional GRU-based cost volume fusion [45]."
        },
        "Unified Scene Representation and Reconstruction for 3D Large Language Models": {
          "authors": [
            "T Chu",
            "P Zhang",
            "X Dong",
            "Y Zang",
            "Q Liu"
          ],
          "url": "https://arxiv.org/abs/2404.13044",
          "ref_texts": "38. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3d reconstruction from monocular video. In: CVPR (2021) 4, 6, 9, 10",
          "ref_ids": [
            "38"
          ],
          "2": "NeuralRecon [38] is proposed to predict a discrete TSDF volume using GRU fusion.",
          "5": "We adopt GRU Fusion [38] following SPVConv [40] as our 3D modulemi, which is used to fuse features of each voxel in 3D space at leveli.",
          "7": "We choose NeuralRecon [38] as the baseline for our reconstruction.",
          "8": "Compared to the baseline method [38] (second column), our method (third column) predicts the reconstruction results with more semantic details."
        },
        "Toward cooperative 3d object reconstruction with multi-agent": {
          "authors": [
            "X Li",
            "Z Wen",
            "L Zhou",
            "C Li",
            "Y Zhou",
            "T Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160714/",
          "ref_texts": "[30] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "30"
          ],
          "1": "When the input is a video stream [6], [30], temporal correlation can be used to improve the smoothness and inter-frame consistency of the reconstruction."
        },
        "Continuous 3D Perception Model with Persistent State": {
          "authors": [
            "Q Wang",
            "Y Zhang",
            "A Holynski",
            "AA Efros"
          ],
          "url": "https://arxiv.org/abs/2501.12387",
          "ref_texts": "[92] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15598\u201315607, 2021. 2, 3",
          "ref_ids": [
            "92"
          ],
          "1": "Our approach is related to learning based methods [18, 42, 92, 127], such as 3D-R2N2 [18], which utilize recurrent neural network architectures [25, 36, 41] for online 3D reconstruction."
        },
        "Learning online multi-sensor depth fusion": {
          "authors": [
            "E Sandstr\u00f6m",
            "MR Oswald",
            "S Kumar",
            "S Weder"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_6",
          "ref_texts": "56. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15598\u201315607 (2021)",
          "ref_ids": [
            "56"
          ],
          "1": "Several recent works do not require depth input and instead perform online reconstruction from RGB-cameras such as Atlas [40], VolumeFusion [10], TransformerFusion [4] and NeuralRecon [56]."
        },
        "Range-agnostic multi-view depth estimation with keyframe selection": {
          "authors": [
            "A Conti",
            "M Poggi",
            "V Cambareri"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550456/",
          "ref_texts": "[28] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021. 3",
          "ref_ids": [
            "28"
          ],
          "1": "[28] improves such approach by means of 3D recurrent layers and a coarse-to-fine approach."
        },
        "Virtual Agent Positioning Driven by Personal Characteristics": {
          "authors": [
            "J Liu",
            "Y Zheng",
            "K Zhou"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3680634",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. 2021. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .",
          "ref_ids": [
            "37"
          ],
          "1": "It should be noted that the historical scene and the posture and information of agents can be obtained based on three-dimensional reconstruction[37, 42], or can be directly captured based on the RGB camera of the VR device[39], such as HoloLens."
        },
        "VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction": {
          "authors": [
            "A Gassol Puigjaner",
            "E Mello Rella",
            "E Sandstr\u00f6m"
          ],
          "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/713602/1/vf_nerf_3dv.pdf",
          "ref_texts": "[48] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. CVPR, 2021. 6",
          "ref_ids": [
            "48"
          ],
          "1": "For 3D surface reconstruction, we focus on evaluating our method with median Chamfer distance (CD) and F1score [48] with a threshold of 5cm."
        },
        "DISORF: A Distributed Online 3D Reconstruction Framework for Mobile Robots": {
          "authors": [
            "C Li",
            "H Fan",
            "X Huang",
            "R Liang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10803046/",
          "ref_texts": "[16] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607. 2",
          "ref_ids": [
            "16"
          ],
          "1": "Recent neural reconstruction methods also enable promising online 3D reconstruction results from monocular video streams [16], [17]."
        },
        "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion": {
          "authors": [
            "V Guizilini",
            "MZ Irshad",
            "D Chen"
          ],
          "url": "https://arxiv.org/abs/2501.18804",
          "ref_texts": "[92] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 15598\u201315607, 2021. 7, 8",
          "ref_ids": [
            "92"
          ],
          "1": "vious state of the art by a large margin, even though it does not rely on geometry-preserving 3D augmentations [36], architectural equivariance [112], or TSDF volumes [92].",
          "2": "168 NeuralRecon [92] 0."
        },
        "LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations": {
          "authors": [
            "M Xu",
            "M Wu",
            "Y Zhao",
            "JCL Li",
            "W Ou"
          ],
          "url": "https://arxiv.org/abs/2412.06322",
          "ref_texts": "[48] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15598\u201315607, 2021. 3",
          "ref_ids": [
            "48"
          ],
          "1": "Recent deep learning approaches, such as ATLAS [37], NeuralRecon [48], and TransformerFusion [5], bypass traditional depth estimation by backprojecting 2D features into 3D space, though they incur high computational costs."
        },
        "AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes": {
          "authors": [
            "J Jang",
            "I Lee",
            "M Kim",
            "K Joo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10463130/",
          "ref_texts": "[28] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in CVPR, 2021.",
          "ref_ids": [
            "28"
          ],
          "1": "Among various implicit representations, SDF has gained much attention in that it can implicitly encode surface information using continuous values [27], [28], [29], [30]."
        },
        "Odd-One-Out: Anomaly Detection by Comparing with Neighbors": {
          "authors": [
            "A Bhunia",
            "C Li",
            "H Bilen"
          ],
          "url": "https://arxiv.org/abs/2406.20099",
          "ref_texts": "[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, 2021. 2, 3",
          "ref_ids": [
            "37"
          ],
          "1": "Another related area involves taking multi-view images as input and training a feedforward model for 3D volume-based reconstruction [27, 37, 49] and novel view synthesis [12, 20, 39, 41, 48].",
          "2": "These feature volumes are aggregated over all input views using an average operation as in [27, 37]."
        },
        "Improving neural indoor surface reconstruction with mask-guided adaptive consistency constraints": {
          "authors": [
            "X Yu",
            "L Lu",
            "J Rong",
            "G Xu",
            "L Ou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611101/",
          "ref_texts": "[14] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "14"
          ],
          "2": "TSDF-based methods like NeuralRecon [14] propose a novel framework to lift the 2D features, fuse them spatially and temporally in 3D space, and predict the TSDF volume directly.",
          "3": "(2) NeuralRecon [14]: A learning-based TSDF fusion module."
        },
        "UniPlane: Unified Plane Detection and Reconstruction from Posed Monocular Videos": {
          "authors": [
            "Y Huang",
            "F Morstatter"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10888522/",
          "ref_texts": "[33] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "Recently, NeuralRecon [33] proposed an approach that avoids estimating view-dependent 2D depth maps by projecting 2D image features into a 3D voxel grid, allowing for tracking and fusion of multiple input frames.",
          "2": "Recently, PlanarRecon [40] builds a learning-based multi-view plane reconstruction system following the volumetric reconstruction approach of NeuralRecon [33].",
          "4": "Since there are no previous work that focus on learning-based multi-view 3D plane detection, we compare our method with the following three types of approaches: (1) single-view plane recovering [44]; (2) multi-view depth estimation [20] + depth-based plane detection [8]; and (3) volume-based 3D reconstruction [33, 23] + Sequential RANSAC [9].",
          "5": "For (3), we first employ [33, 23] to estimate the 3D mesh of the scene, and perform sequential RANSAC to group the oriented vertices of the mesh into planes.",
          "6": "For [33, 23], we run sequential RANSAC every time when a new 3D reconstruction is completed to achieve incremental 3D plane detection."
        },
        "PSDF for Neural Indoor Scene Reconstruction": {
          "authors": [
            "J Li",
            "J Yu",
            "R Wang",
            "Z Li",
            "Z Zhang",
            "L Cao"
          ],
          "url": "https://arxiv.org/abs/2303.00236",
          "ref_texts": "[31] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "31"
          ],
          "1": "NeuralRecon [31] uses recurrent networks to fuse the features from different views, which is much faster.",
          "2": "Following [5], [31], [55], we use RGB-D fusion results as 3D reconstruction ground truth and evaluate our method using five standard metrics defined in [30]: accuracy, completeness, precision, recall, and F-score with a threshold of 5cm."
        },
        "Learning 3D Robotics Perception using Inductive Priors": {
          "authors": [
            "MZ Irshad"
          ],
          "url": "https://arxiv.org/abs/2405.20364",
          "ref_texts": "[247] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "247"
          ],
          "1": "NeRF extensions focus on reducing aliasing effects via multiscale representations [35], modeling unbounded scenes [246], disentangled object-background representations and blending [230], compositional generative models [173] and improving reconstruction and depth estimation accuracy via multi-view consistent features [107, 247, 241].",
          "3": "While volumetric reconstruction methods [247, 265], traditionally use the generated volume solely for indoor geometry reconstruction through TSDF, we show that it can also be employed in a computationally efficient way to estimate the entire scene\u2019s appearance and enable accurate neural rendering.",
          "4": "126 These include reducing aliasing effects through multiscale representations [35], modeling unbounded scenes [246], creating disentangled object-background representations [230], employing compositional generative models [173], and improving reconstruction and depth estimation accuracy through multi-view consistent features [107, 247, 241]."
        },
        "Neural Radiance Fields for the Real World: A Survey": {
          "authors": [
            "W Xiao",
            "R Chierchia",
            "RS Cruz",
            "X Li"
          ],
          "url": "https://arxiv.org/abs/2501.13104",
          "ref_texts": "[190] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. 2021. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 15598\u201315607.",
          "ref_ids": [
            "190"
          ],
          "1": "NeuralRecon [190], on the other hand, utilizes Truncated Signed Distance Functions (TSDF) to represent local surfaces rather than optimizing the whole geometry at once."
        },
        "Uni-SLAM: Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction": {
          "authors": [
            "S Wang",
            "Y Xie",
            "CP Chang",
            "C Millerdurai"
          ],
          "url": "https://arxiv.org/abs/2412.00242",
          "ref_texts": "[57] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "57"
          ],
          "1": "Most common scene representation for dense mapping are grid-based (including voxel grids [8, 39, 57], octrees [59, 71], voxel hashing [37, 41]), surfel clouds [6, 65, 68] and multi layer perceptron (MLP)-based [2, 45, 72]."
        },
        "PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D Matching": {
          "authors": [
            "C Ziwen",
            "Z Xu",
            "L Fuxin"
          ],
          "url": "https://arxiv.org/abs/2410.23245",
          "ref_texts": "[30] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 1, 2, 7",
          "ref_ids": [
            "30"
          ],
          "1": "Depth prediction-based approaches [2, 8, 12, 13, 16, 26, 33, 34] accumulate features from multiple views into a per-view cost volume to regress depth maps, while volumetric methods [1, 10, 15, 23, 28, 30] aggregate features into a global voxel grid to predict occupancy or TSDF values.",
          "2": "NeuralRecon [30] introduces an online approach that incrementally constructs and fuses local grids with a global grid using a GRU, while also sparsifying fine grids based on coarse predictions."
        },
        "3D Reconstruction Based on Iterative Optimization of Moving Least-Squares Function": {
          "authors": [
            "Saiya Li",
            "Jinhe Su",
            "Guoqing Jiang",
            "Ziyu Huang",
            "Xiaorong Zhang"
          ],
          "url": "https://www.mdpi.com/1999-4893/17/6/263",
          "ref_texts": "32. Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; Bao, H. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 15598\u201315607.",
          "ref_ids": [
            "32"
          ],
          "1": "This significantly improved the generalization ability and accuracy in invisible point clouds; real-time coherent 3D reconstruction from monocular video (NeuralRecon) [32] used gated recurrent units to capture local smoothing Algorithms 2024, 17, 263 4 of 19 priors and global shape priors of three-dimensional surfaces during sequential surface reconstruction, thereby achieving accurate, coherent, and real-time surface reconstruction of images.",
          "2": "This significantly improved the generalization ability and accuracy in invisible point clouds; real-time coherent 3D reconstruction from monocular video (NeuralRecon) [32] used gated recurrent units to capture local smoothing priors and global shape priors of three-dimensional surfaces during sequential surface reconstruction, thereby achieving accurate, coherent, and real-time surface reconstruction of images."
        },
        "Dynamic voxel grid optimization for high-fidelity rgb-d supervised surface reconstruction": {
          "authors": [
            "X Xu",
            "L Chen",
            "C Cai",
            "H Zhan",
            "Q Yan",
            "P Ji"
          ],
          "url": "https://arxiv.org/abs/2304.06178",
          "ref_texts": "[34] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2",
          "ref_ids": [
            "34"
          ],
          "1": "Neural RGB-D Surface Reconstruction Neural surface reconstruction [1, 12, 22, 34, 35, 36, 39, 40], which reduces artifacts from traditional 3D reconstruction through training end-to-end networks, has been a trend for high-fidelity 3D surface reconstruction."
        },
        "Inner-outer aware reconstruction model for monocular 3D scene reconstruction": {
          "authors": [
            "YK Qiu",
            "GH Xu",
            "WS Zheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/27c852e9d6c76890ca633f111c556a4f-Abstract-Conference.html",
          "ref_texts": "[9] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d inConference on Computer Vision and Pattern Recognition, pp. 15598\u201315607, IEEE, 2021.",
          "ref_ids": [
            "9"
          ],
          "1": "Recently, some researchers [8, 9, 10, 11, 12] attempted to regress the 3D surface of the entire scene directly and have achieved promising results.",
          "2": "NeuralRecon [9] achieves real-time 3D reconstruction by reconstructing the scene in each local window and then merges the result using a GRU [32] module."
        },
        "3d semantic label transfer in human-robot collaboration": {
          "authors": [
            "David Rozenberszki",
            "Gabor Soros",
            "Szilvia Szeier",
            "Andras Lorincz"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/html/Rozenberszki_3D_Semantic_Label_Transfer_in_Human-Robot_Collaboration_ICCVW_2021_paper.html",
          "ref_texts": "[39] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video.Conference on Computer Vision and Pattern Recognition, 2021.",
          "ref_ids": [
            "39"
          ],
          "1": "Additionally, there are promising recent works [23, 39] that project CNN feature maps from posed monocular images from 2D to 3D to reconstruct a consistent sparse TSDF map close to real time, but as for other learning-based approaches, generalization to arbitrary scenes is still a challenge."
        },
        "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images": {
          "authors": [
            "J Lee",
            "S Cho",
            "T Kim",
            "HD Jang",
            "M Lee",
            "G Cha"
          ],
          "url": "https://arxiv.org/abs/2412.16028",
          "ref_texts": "[32] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 3",
          "ref_ids": [
            "32"
          ],
          "1": "NeRF and 3DGS has enabled a wide range of related research, including dynamic scene repre2 sentation [14, 15, 23, 24, 28, 33], human avatars [8, 27, 39], 3D mesh reconstruction [16, 32, 35, 36], 3D scene representation from sparse-view images [22, 34, 42, 43], and 3D scene representation from blurry images [4, 10\u201313, 18, 25, 26, 37, 46]."
        },
        "FAWN: Floor-and-Walls Normal Regularization for Direct Neural TSDF Reconstruction": {
          "authors": [
            "A Sokolova",
            "A Vorontsova",
            "B Gabdullin"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10647694/",
          "ref_texts": "[3] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao, \u201cNeuralRecon: Real-time coherent 3D reconstruction from monocular video,\u201d inCVPR, 2021.",
          "ref_ids": [
            "3"
          ],
          "1": "NeuralRecon [3] runs a hierarchical fusion strategy on sequential frames, averaging features of nearby views and fusing across clusters with an RNN network.",
          "4": "RNNbased NeuralRecon [3]directly reconstructs local surfaces as multiple sparse TSDFs, and fuses them with an RNN module.",
          "6": "Ground truth scan Atlas [2] NeuralRecon [3] V oRTX [1] VisFusion [4] Coverage: 97."
        },
        "UNIKD: UNcertainty-Filtered Incremental Knowledge Distillation for Neural Implicit Representation": {
          "authors": [
            "M Guo",
            "C Li",
            "H Chen",
            "GH Lee"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_14",
          "ref_texts": "53. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In: CVPR (2021)",
          "ref_ids": [
            "53"
          ],
          "1": "Theneuralimplicitrepresentations(NIRs)[11, 28,30,58,62] have shown remarkable potential in various computer vision tasks, such as novel view synthesis [6,7,38] and 3D reconstruction [4,53,65]."
        },
        "Planar Gaussian Splatting": {
          "authors": [
            "FG Zanjani",
            "H Cai",
            "H Ackermann"
          ],
          "url": "https://arxiv.org/abs/2412.01931",
          "ref_texts": "[33] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 6, 8",
          "ref_ids": [
            "33"
          ],
          "2": ", PlanarRecon [40], which is trained with 3D geometry and 3D plane supervisions, and NMF [46] which is an optimizationbased approach using depth and normal supervision, (2) dense 3D reconstruction methods like NeuralRecon [33] with 3D geometry supervision and 3DGS [16] with 2D RGB supervision, followed by Sequential RANSAC to extract planes [10]."
        },
        "Leveraging 360\u00b0 camera in 3D reconstruction: A vision-based approach": {
          "authors": [
            "Hoi Chuen",
            "Babar Hussain",
            "Ziyang Hong",
            "Patrick Yue"
          ],
          "url": "https://www.ijsps.com/vol12/IJSPS-V12-1.pdf",
          "ref_texts": "[17] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Realtime coherent 3D reconstructio n from monocular video,\u201d in Proc. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15598\u201315607, 2021. ",
          "ref_ids": [
            "17"
          ],
          "1": "Recent methods such as [9 , 17, 18] use neural networks to directly regress a Truncated Signed Distance Function (TSDF) volume for 3D model generation."
        },
        "MuSR: Multi-Scale 3D Scenes Reconstruction based on Monocular Video": {
          "authors": [
            "H Gao",
            "H Wu",
            "P Dong",
            "Y Xu",
            "F Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10446982/",
          "ref_texts": "[2] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15598\u201315607.",
          "ref_ids": [
            "2"
          ],
          "3": "Our MuSR\u2019s design shares a similar idea with NeuralRecon[2], the SOTA 3D scene reconstruction methods, regarding applying coarse-to-fine network to obtain 3D spatial features and fusing new backprojected local features with global features."
        },
        "Neural 3D Scene Reconstruction from Multiple 2D Images without 3D Supervision": {
          "authors": [
            "Y Guo",
            "C Sun",
            "Y Jia",
            "Y Wu"
          ],
          "url": "https://arxiv.org/abs/2306.17643",
          "ref_texts": "[33] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021. 1, 2, 6",
          "ref_ids": [
            "33"
          ],
          "2": "NeuralRecon [33] regresses TSDF directly using a coarse-to-fine framework which achieves real-time indoor scene reconstruction."
        },
        "LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes": {
          "authors": [
            "J Zhang",
            "Z Yang",
            "M Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.02313",
          "ref_texts": "[20] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralrecon: Real-time coherent 3d reconstruction from monocular video,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 598\u201315 607.",
          "ref_ids": [
            "20"
          ],
          "1": "Contrast to the explicit scene representation, recent works leveraging neural implicit scene representation, such as NGLOD [18], Di-fusion [19], and NeuralRecon [20], achieve significant success."
        },
        "Pyramidal Signed Distance Learning for Spatio-Temporal Human Shape Completion": {
          "authors": [
            "Boyao Zhou",
            "Sebastien Franco",
            "Edmond Boyer"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Pyramidal_Signed_Distance_Learning_for_Spatio-Temporal_Human_Shape_Completion_ACCV_2022_paper.html",
          "ref_texts": "45. Sun, J., Xie, Y., Chen, L., Zhou, X., Bao, H.: NeuralRecon: Real-time coherent 3D reconstruction from monocular video. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2021)",
          "ref_ids": [
            "45"
          ],
          "1": "Taking inspiration from pyramidal strategies applied successfully to other prominent vision problems such as object detection [23], multi-view stereo [48], 3D reconstruction [45] and optical flow [44] we devise a method that aggregates spatio-temporal information in a coarse to fine manner, propagating features from low to high resolution through up-sampling, concatenation with higher resolution features and the addition of residuals."
        },
        "Data-driven Non-rigid Reconstruction": {
          "authors": [
            "A Bo\u017ei\u010d"
          ],
          "url": "https://mediatum.ub.tum.de/1711341",
          "ref_texts": "[39] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. CVPR, 2021.",
          "ref_ids": [
            "39"
          ],
          "8": "Recently, NeuralRecon [39] proposed a real-time 3D reconstruction framework, adding GRU units distributed in 3D to fuse reconstructions from different local windows of frames.",
          "10": "Reconstruction quality further improves with methods that directly predict the 3D surface geometry, such as NeuralRecon [39] and Atlas [28]."
        },
        "Robot Mapping with 3D LiDARs": {
          "authors": [
            "Ignacio Martin"
          ],
          "url": "https://bonndoc.ulb.uni-bonn.de/xmlui/handle/20.500.11811/11536",
          "ref_texts": "[177] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: RealTime Coherent 3D Reconstruction From Monocular Video. InProc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "177"
          ],
          "1": "More than a decade later, Newcombe massively popularized 3D surface reconstruction as a mapping technique [126] and established a new standard for 3D robotics mapping, mainly for indoor scenes and employing RGB-D sensors [17, 21, 22, 57, 87, 94, 113, 121, 130, 132, 134, 146, 157, 161, 177, 179, 187, 203, 204]."
        },
        "Back2Future-SIM: Creating Real-Time Interactable Immersive Virtual World For Robot Teleoperation": {
          "authors": [
            "S Akturk"
          ],
          "url": "https://era.library.ualberta.ca/items/72cb0e47-601b-4065-98a4-d2fc0b19b8d8",
          "ref_texts": "[48] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-time coherent 3D reconstruction from monocular video,\u201d CVPR, 2021.",
          "ref_ids": [
            "48"
          ],
          "1": "The level of detail in the resulting mesh reconstruction varies depending on the density of the sparse point cloud [2, 48, 49]."
        },
        "Advancing Visual Geometric Perception: Camera-Based Depth, Reconstruction, and Active Vision": {
          "authors": [
            "Ziyue Feng"
          ],
          "url": "https://open.clemson.edu/all_dissertations/3823/",
          "ref_texts": "[20] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao, \u201cNeuralRecon: Real-time coherent ",
          "ref_ids": [
            "20"
          ],
          "1": "In recent years, learning-based methods[19], [20], [21], [22] have demonstrated great potential in this task.",
          "3": "Compared to traditional reconstruction [19], [20], [21], [22], active reconstruction[25], [26] not only extracts geometric information from image data but also actively guides the intelligent agent to optimize the observation path for better reconstruction results.",
          "4": "The training is based on enforcing the re-projection photometric [20] consistency."
        },
        "CNN-based Scene Modeling: From Depth Estimation to 3D Reconstruction": {
          "authors": [
            "Z Niu"
          ],
          "url": "https://naist.repo.nii.ac.jp/record/10969/files/R018015.pdf",
          "ref_texts": "[100] Sun, J., Xie, Y., Chen, L., Zhou, X., and Bao, H.NeuralRecon: Real-time coherent 3D reconstruction from monocular video. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2021), pp. 15598\u201315607.",
          "ref_ids": [
            "100"
          ],
          "1": "NeuralRecon [100] further improves the performance of real-time scene reconstruction using 3D gated recurrent units (GRUs)."
        },
        "GNeSF: Generalizable Neural Semantic Fields Supplementary Material": {
          "authors": [
            "H Chen",
            "C Li",
            "M Guo",
            "Z Yan",
            "GH Lee"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/72d32f4fe0b7af03732bd227bf1c4a5f-Supplemental-Conference.pdf",
          "ref_texts": "[8] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Realtime coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15598\u201315607, 2021.",
          "ref_ids": [
            "8"
          ],
          "2": "We explicitly construct feature volume grids locally with the resolution 96 \u00d7 96 \u00d7 96 based on the extracted features of some nearby views, and then fuse it into a global volume, following Neuralrecon [8] and Mononeuralfusion [17]."
        },
        "Monocular 3D Reconstruction in Poorly Visible Environments": {
          "authors": [
            "ITA Ilesinghe",
            "N Lekamge",
            "G Samarutilake"
          ],
          "url": "http://192.248.104.6/handle/345/7636",
          "ref_texts": "[14] J. Sun, Y. Xie, L. Chen, X. Zhou and H. Bao, \"NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video,\" CVPR, 2021. ",
          "ref_ids": [
            "14"
          ],
          "1": "NeuralRecon [14] is a neural network that processes a sequence of images from a moving camera and their corresponding camera poses to generate a 3D representation of the scene as a TSDF volume.",
          "2": "For the base of the 3D reconstruction model, the other frameworks tested were NeuralRecon [14], TransformerFusion [16], and FineRecon [17].",
          "3": "This application was introduced and utilized in NeuralRecon [14] for monocular video recording purposes."
        },
        "Supplementary Material for Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception from Monocular Video": {
          "authors": [
            "Z Hong",
            "CP Yue"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/supplemental/Hong_Cross-Dimensional_Refined_Learning_ICCVW_2023_supplemental.pdf",
          "ref_texts": "[15] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In CVPR, pages 15598\u201315607, 2021. 4, 5",
          "ref_ids": [
            "15"
          ],
          "2": "Thanks to the feature refinement brought by the utilization of both depth prior and semantic prior, our method yields better 3D reconstruction and 3D semantic segmentation results at the same time, comparing to two main baseline methods, Atlas [10] and NeuralRecon [15]."
        },
        "EE381V: Active 3D Reconstruction by Using Ego-centric Camera": {
          "authors": [
            "M Seo",
            "Z Fang"
          ],
          "url": "https://zhou-spec.github.io/files/EE381V_mseo_zfang.pdf",
          "ref_texts": "[23] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15598\u201315607, 2021. 2, 3, 5",
          "ref_ids": [
            "23"
          ],
          "3": "3D Reconstruction Model To take advantage of GPU accelerated real-time construction of 3D shapes, we extended the model design of NeuralRecon, a data-driven model for generating 3D shapes [23]."
        }
      }
    },
    {
      "title": "totalselfscan: learning full-body avatars from self-portrait videos of faces, hands, and bodies",
      "id": 27,
      "valid_pdf_number": "18/22",
      "matched_pdf_number": "16/18",
      "matched_rate": 0.8888888888888888,
      "citations": {
        "Econ: Explicit clothed humans optimized via normal integration": {
          "authors": [
            "Yuliang Xiu",
            "Jinlong Yang",
            "Xu Cao",
            "Dimitrios Tzionas",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.html",
          "ref_texts": "[13] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning fullbody avatars from self-portrait videos of faces, hands, and bodies. In Conference on Neural Information Processing Systems (NeurIPS), 2022. 7",
          "ref_ids": [
            "13"
          ],
          "1": "ECON\u2019s reconstructions, together with its underneath SMPL-X body, could be useful as 3D shape prior to learn neural avatars [13, 24, 32]."
        },
        "Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling": {
          "authors": [
            "Zhe Li",
            "Zerong Zheng",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[12] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In NeurIPS, 2022. 3, 7",
          "ref_ids": [
            "12"
          ],
          "1": "Besides the body avatar, TotalSelfScan [12], X-Avatar [71] and AvatarReX [104] propose compositional full-body avatars for expressive control of the human body, hands and face.",
          "2": "Full-body avatars including TotalSelfScan [12], X-Avatar [71] and AvatarReX [104] can realize expressive control of the body, hands and face."
        },
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[64] J. Dong, Q. Fang, Y . Guo, S. Peng, Q. Shuai, X. Zhou, and H. Bao, \u201cTotalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies,\u201d in NeurIPS, 2022. 3, 11",
          "ref_ids": [
            "64"
          ],
          "1": "Besides the body avatar, TotalSelfScan [64], X-Avatar [65] and AvatarReX [66] propose compositional full-body avatars for expressive control of the human body, hands and face.",
          "2": "2) NeRF-based Full-body Avatars: Full-body avatars including TotalSelfScan [64], X-Avatar [65] and AvatarReX [66] can realize expressive control of the body, hands and face."
        },
        "Tela: Text to layer-wise 3d clothed human generation": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Z Huang",
            "X Xu",
            "J Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_2",
          "ref_texts": "10. Dong, J., Fang, Q., Guo, Y., Peng, S., Shuai, Q., Zhou, X., Bao, H.: Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. Advances in Neural Information Processing Systems35, 13654\u201313667 (2022) 4",
          "ref_ids": [
            "10"
          ],
          "1": "Benefiting from advancements in implicit functions [26,27,30], recent methods [7,10,32,33,44,45,47,49,52] have presented impressive clothed huTELA: Text to Layer-wise 3D Clothed Human Generation 5 man reconstruction or generation from images and 3D scans."
        },
        "Xagen: 3d expressive human avatars generation": {
          "authors": [
            "Z Xu",
            "J Zhang",
            "JH Liew",
            "J Feng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6d6f9908ea35313dd7566f5ce8c6e815-Abstract-Conference.html",
          "ref_texts": "[15] J. Dong, Q. Fang, Y . Guo, S. Peng, Q. Shuai, X. Zhou, and H. Bao. Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In NeurIPS, 2022.",
          "ref_ids": [
            "15"
          ],
          "1": "The most recent work [15, 54] proposed to learn a single full-body avatar from multi-part portrait videos or 3D scans."
        },
        "Meshavatar: Learning high-quality triangular human avatars from multi-view videos": {
          "authors": [
            "Y Chen",
            "Z Zheng",
            "Z Li",
            "C Xu",
            "Y Liu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73113-6_15.pdf",
          "ref_texts": "16. Dong, J., Fang, Q., Guo, Y., Peng, S., Shuai, Q., Zhou, X., Bao, H.: Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In: NeurIPS (2022)",
          "ref_ids": [
            "16"
          ],
          "1": "To capture the avatar appearance, recent works leverage NeRF as the underlying representation [16,19,23,37,38,50,51,70,72,77,82,86,90,93,106,107] for its impressive learning capability."
        },
        "TexVocab: texture vocabulary-conditioned human avatars": {
          "authors": [
            "Yuxiao Liu",
            "Zhe Li",
            "Yebin Liu",
            "Haoqian Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_TexVocab_Texture_Vocabulary-conditioned_Human_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[8] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning fullbody avatars from self-portrait videos of faces, hands, and bodies. Advances in Neural Information Processing Systems, 35:13654\u201313667, 2022. 2",
          "ref_ids": [
            "8"
          ],
          "1": "AniNeRF [44], ARAH [64], NeuralBody [45] TotalSelfScan [8] and PoseV ocab [26] auto-decode [40] latent embeddings to encode the dynamic appearances with perframe latent code or joint-structured feature lines."
        },
        "Expressive Gaussian Human Avatars from Monocular RGB Video": {
          "authors": [
            "H Hu",
            "Z Fan",
            "T Wu",
            "Y Xi",
            "S Lee",
            "G Pavlakos"
          ],
          "url": "https://arxiv.org/abs/2407.03204",
          "ref_texts": "[6] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In NeurIPS, pages 13654\u201313667, 2022.",
          "ref_ids": [
            "6"
          ],
          "1": "Early works [32, 2, 37, 38, 3, 15, 49, 46, 39, 42, 6, 22, 41] mainly resort to the combination of implicit neural representations like NeRF [29] and parametric models to represent human avatar with high fidelity and flexibility."
        },
        "Weakly-Supervised 3D Reconstruction of Clothed Humans via Normal Maps": {
          "authors": [
            "J Wu",
            "D Thomas",
            "R Fedkiw"
          ],
          "url": "https://arxiv.org/abs/2311.16042",
          "ref_texts": "[14] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. Advances in Neural Information Processing Systems, 35:13654\u201313667, 2022. 3",
          "ref_ids": [
            "14"
          ],
          "1": "Instead of a single input image, other works aim to construct animatable avatars from a sparse set of cameras [27, 71, 98, 104], video [14, 18, 23, 29\u201331, 60, 76, 85, 94], depth [15, 83, 90, 100], point clouds [35, 49], 4D capture [28], or scans [42, 45, 72]."
        },
        "GAS: Generative Avatar Synthesis from a Single Image": {
          "authors": [
            "Y Lu",
            "J Dong",
            "Y Kwon",
            "Q Zhao",
            "B Dai"
          ],
          "url": "https://arxiv.org/abs/2502.06957",
          "ref_texts": "[8] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning fullbody avatars from self-portrait videos of faces, hands, and bodies. In Advances in Neural Information Processing Systems, 2022. 2",
          "ref_ids": [
            "8"
          ],
          "1": "To address these challenges, some methods [8, 9, 21, 22, 34, 35, 59] use 3D human templates, such as the SMPL model [30], to anchor features accurately on the human form."
        },
        "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans": {
          "authors": [
            "A Chatziagapi",
            "B Chaudhuri",
            "A Kumar"
          ],
          "url": "https://arxiv.org/abs/2409.16666",
          "ref_texts": "11. Dong, J., Fang, Q., Guo, Y., Peng, S., Shuai, Q., Zhou, X., Bao, H.: Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In: Advances in Neural Information Processing Systems (2022)",
          "ref_ids": [
            "11"
          ],
          "1": "(a) Our training data consists of monocularfrontal-only videos of talking humans, whereas existing approaches leverage information from side and back views [11,13,32,42,51,68,71].",
          "2": "Subsequent works propose NeRF-based approaches [11,14,15,59,64,71], discussed in the following paragraphs.",
          "3": "TotalSelfScan [11] represents body, hand, and head pose, omitting the non-rigid motion of facial muscles, and requires multiple videos per subject."
        },
        "Representing Animatable Avatar via Factorized Neural Fields": {
          "authors": [
            "C Song",
            "Z Wu",
            "B Wandt",
            "L Sigal",
            "H Rhodin"
          ],
          "url": "https://arxiv.org/abs/2406.00637",
          "ref_texts": "[52] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies. In NeurIPS, 2022.",
          "ref_ids": [
            "52"
          ],
          "1": "Other methods [50, 44, 51, 52] aim to improve results with an image-to-image translation network and a per-frame latent code."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "211. Dong, J., Fang, Q., Guo, Y ., Peng, S., Shuai, Q., Zhou, X., Bao, H.: TotalSelfScan: learning full-body avatars from self-portrait videos of faces, hands, and bodies. In: NeurIPS (2022)",
          "ref_ids": [
            "211"
          ],
          "1": "Integrating the study of hand and face reconstruction with full-body reconstruction can help address the challenges posed by complex human poses and enhance local details [16, 210, 211]."
        },
        "DEGAS: Detailed Expressions on Full-Body Gaussian Avatars": {
          "authors": [
            "Z Shao",
            "D Wang",
            "QY Tian",
            "YD Yang",
            "H Meng"
          ],
          "url": "https://arxiv.org/abs/2408.10588",
          "ref_texts": "[13] Junting Dong, Qi Fang, Yudong Guo, Sida Peng, Qing Shuai, Xiaowei Zhou, and Hujun Bao. TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies. In Advances in Neural Information Processing Systems, 2022. 3",
          "ref_ids": [
            "13"
          ],
          "1": "TA V A [35], TotalSelfScan [13], PoseV ocab [37], InstantAvatar [29], and INSTA [80] propose to build NeRF aligned with the canonical mesh.",
          "2": "NeRF-based methods [13, 29, 35, 37], on the other hand, usually require the conversion of multiple samples on each ray from the posed space back to the canonical space [5, 6]."
        }
      }
    },
    {
      "title": "compact neural volumetric video representations with dynamic codebooks",
      "id": 44,
      "valid_pdf_number": "4/5",
      "matched_pdf_number": "4/4",
      "matched_rate": 1.0,
      "citations": {
        "Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps": {
          "authors": [
            "Z Fan",
            "K Wang",
            "K Wen",
            "Z Zhu",
            "D Xu"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/fd881d3b625437354d4421818f81058f-Abstract-Conference.html",
          "ref_texts": "[50] Haoyu Guo, Sida Peng, Yunzhi Yan, Linzhan Mou, Yujun Shen, Hujun Bao, and Xiaowei Zhou. Compact neural volumetric video representations with dynamic codebooks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.",
          "ref_ids": [
            "50"
          ],
          "1": "VQ has thus been widely applied in image synthesis [47], text-to-image generation [48], and novel view synthesis [18, 49, 50]."
        },
        "Tetrirf: Temporal tri-plane radiance fields for efficient free-viewpoint video": {
          "authors": [
            "Minye Wu",
            "Zehao Wang",
            "Georgios Kouros",
            "Tinne Tuytelaars"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_TeTriRF_Temporal_Tri-Plane_Radiance_Fields_for_Efficient_Free-Viewpoint_Video_CVPR_2024_paper.html",
          "ref_texts": "[11] Haoyu Guo, Sida Peng, Y unzhi Y an, Linzhan Mou, Y ujun Shen, Hujun Bao, and Xiaowei Zhou. Compact neural volumetric video representations with dynamic codebooks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.",
          "ref_ids": [
            "11"
          ],
          "1": "Recent works extend the compression into dynamic scene by using tensor decomposition [34], residual radiance fields with specialized video codecs [41], and reducing spatio-temporal redundancies of feature grids [11]."
        },
        "Rate-aware Compression for NeRF-based Volumetric Video": {
          "authors": [
            "Z Zhang",
            "G Lu",
            "H Liang",
            "Z Cheng",
            "A Tang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3680970",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Yunzhi Yan, Linzhan Mou, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 2024. Compact neural volumetric video representations with dynamic codebooks. Advances in Neural Information Processing Systems36 (2024).",
          "ref_ids": [
            "16"
          ],
          "1": "For dynamic NeRF compression, the recent works [16, 48, 49, 52, 56, 58] adopt frame-by-frame modeling, which enables high-quality reconstruction for long-duration sequences.",
          "2": "[16] adopts a vector quantization approach to eliminate the spatial-temporal redundancy of the radiance field."
        },
        "AI-Driven Innovations in Volumetric Video Streaming: A Review": {
          "authors": [
            "E Entezami",
            "H Guan"
          ],
          "url": "https://arxiv.org/abs/2412.12208",
          "ref_texts": "[29] Haoyu Guo, Sida Peng, Yunzhi Yan, Linzhan Mou, Yujun Shen, Hujun Bao, and Xiaowei Zhou. Compact neural volumetric video representations with dynamic codebooks. Advances in Neural Information Processing Systems , 36, 2024. 7",
          "ref_ids": [
            "29"
          ],
          "1": "3 Multi-plane and Feature Grids Methods A group of methods [9, 22, 29, 36, 39, 80, 90] have proposed representing scenes using feature grids.",
          "2": "In number of methods [9, 22, 29, 36, 90], the 4D spacetime plane, which includes X, Y , and Z as spatial parameters along with T as the time dimension, is decomposed into six planes: three spatial planes (XY , XZ , and Y Z) and three time-dependent planes (XT , Y T, and ZT )."
        }
      }
    },
    {
      "title": "neuralrecon: real-time coherent 3d scene reconstruction from monocular video",
      "id": 45,
      "valid_pdf_number": "3/4",
      "matched_pdf_number": "2/3",
      "matched_rate": 0.6666666666666666,
      "citations": {
        "A survey on occupancy perception for autonomous driving: The information fusion perspective": {
          "authors": [
            "H Xu",
            "J Chen",
            "S Meng",
            "Y Wang",
            "LP Chau"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1566253524004494",
          "ref_texts": "[64] X. Chen, J. Sun, Y . Xie, H. Bao, X. Zhou, Neuralrecon: Real-time coherent 3d scene reconstruction from monocular video, IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).",
          "ref_ids": [
            "64"
          ],
          "1": "3D Reconstruction from images 3D reconstruction is a traditional but important topic in the computer vision and robotics communities [63, 64, 65, 66]."
        },
        "Hierarchical Context Alignment with Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction": {
          "authors": [
            "B Li",
            "X Jin",
            "J Deng",
            "Y Sun",
            "X Wang",
            "W Zeng"
          ],
          "url": "https://arxiv.org/abs/2412.08243",
          "ref_texts": "[61] X. Chen, J. Sun, Y . Xie, H. Bao, and X. Zhou, \u201cNeuralrecon: Real-time coherent 3d scene reconstruction from monocular video,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.",
          "ref_ids": [
            "61"
          ],
          "1": "3 Temporal Modeling in 3D Visual Perception The incorporation of temporal information has gained prominence in applications such as temporal 3D object detection [49, 50, 51, 52, 53, 54, 55, 56] and video depth estimation [57, 58, 59, 60, 61, 62, 63], enhancing overall prediction accuracy."
        }
      }
    },
    {
      "title": "animatable neural radiance fields for modeling dynamic human bodies",
      "id": 4,
      "valid_pdf_number": "320/415",
      "matched_pdf_number": "267/320",
      "matched_rate": 0.834375,
      "citations": {
        "Nerf: Neural radiance field in 3d vision, a comprehensive review": {
          "authors": [
            "K Gao",
            "Y Gao",
            "H He",
            "D Lu",
            "L Xu",
            "J Li"
          ],
          "url": "https://arxiv.org/abs/2210.00379",
          "ref_texts": "[178] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , October 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "178"
          ],
          "1": "[175],DoubleField [176], LISA [177], Animatable NeRF [178], TA V A [179], NeuMan [61] Fig.",
          "2": "State of the art models from top tier conferences in 2021/2022 such as A-NeRF [183] (Feb 2021), Animatable NeRF [178] (May 2021), DoubleField [176] (Jun 2021), HumanNeRF [174] (Jan 2022), Zheng et al.",
          "3": "[178] S."
        },
        "Ref-nerf: Structured view-dependent appearance for neural radiance fields": {
          "authors": [
            "D Verbin",
            "P Hedman",
            "B Mildenhall"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9879796/",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "NeRF has inspired many subsequent works, which extend its neural volumetric scene representation to application domains including dynamic and deformable scenes [26], avatar animation [11, 27], and even phototourism [21]."
        },
        "Humannerf: Free-viewpoint rendering of moving people from monocular video": {
          "authors": [
            "Yi Weng",
            "Brian Curless",
            "Pratul P. Srinivasan",
            "Jonathan T. Barron",
            "Ira Kemelmacher"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.html",
          "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 1, 2, 4",
          "ref_ids": [
            "47"
          ],
          "1": "Human-specific methods typically assume a SMPL template [33] as a prior, which helps constrain the motion space but also introduces artifacts in clothing and complex motions that are not captured by the SMPL model [47, 48].",
          "2": "Other than free-viewpoint rendering, there is another related active research field that focus on human motion retargeting either in 2D [1, 6, 34, 41, 52, 65, 66] or 3D [18, 19, 24, 31, 47, 51, 67, 72].",
          "3": "Similar approaches have been applied to human modeling [3, 9, 13, 24, 39, 47, 50, 61, 72]."
        },
        "Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians": {
          "authors": [
            "Shenhan Qian",
            "Tobias Kirschstein",
            "Liam Schoneveld",
            "Davide Davoli",
            "Simon Giebenhain",
            "Matthias Niessner"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_GaussianAvatars_Photorealistic_Head_Avatars_with_Rigged_3D_Gaussians_CVPR_2024_paper.html",
          "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "33"
          ],
          "1": "[33] deform points with the skeleton of SMPL and neural blending weights."
        },
        "A survey on 3d gaussian splatting": {
          "authors": [
            "G Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2401.03890",
          "ref_texts": "[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "307"
          ],
          "1": "47 AnimNeRF [307][ICCV21] 29.",
          "2": "[307] S."
        },
        "Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling": {
          "authors": [
            "Zhe Li",
            "Zerong Zheng",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[58] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 4",
          "ref_ids": [
            "58"
          ],
          "1": "In the past few years, with the rise of implicit representations, particularly neural radiance fields (NeRF) [54], many researchers tend to represent the 3D human as a pose-conditioned NeRF [40, 44, 58, 103] to automatically learn a neural avatar from RGB videos.",
          "3": "Animatable NeRF [58] introduces SMPL deformation into NeRF for animatable human modeling."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of ICCV, 2021. 1, 2, 3, 6",
          "ref_ids": [
            "35"
          ],
          "1": "Recent advances in implicit neural fields [27, 30, 32, 36, 47, 50, 51, 53, 55, 65, 66] have enabled high-quality reconstruction of geometry [8, 38, 57, 61] and appearance [13, 20, 22, 31, 35, 37, 42, 58, 70] of clothed human bodies from sparse multi-view or monocular videos.",
          "2": "Animation of such reconstructed clothed human bodies is also possible by learning the geometry and appearance representations in a predefined canonical pose [13, 20, 35, 57, 58, 70].",
          "3": "To achieve state-of-the-art rendering quality, existing methods rely on training a neural radiance field (NeRF) [27] combined with either explicit body articulation [8, 12, 13, 20, 35, 38, 57, 58, 70] or conditioning the NeRF on human body related encodings [31, 37, 48, 61].",
          "4": "The majority of the works focus on either learning a NeRF conditioned on human body related encodings [31, 48, 61], or learning a canonical NeRF representation and warp camera rays from the observation space to the canonical space to query radiance and density values from the canonical NeRF [8, 12, 13, 20, 35, 38, 57, 58, 70].",
          "5": "To model human articulations, a widely adopted paradigm is to represent geometry and appearance in a shared canonical space [8, 12, 13, 20, 35, 38, 57, 58] and use Linear Blend Skinning (LBS) [2, 9, 24, 33, 34, 60] to deform the parametric human body under arbitrary poses.",
          "6": "For fair comparison, we use the provided poses optimized by Anim-NeRF [35] and do not further optimize it during our training."
        },
        "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields": {
          "authors": [
            "L Song",
            "A Chen",
            "Z Li",
            "Z Chen",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10049689/",
          "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, October 2021. 2",
          "ref_ids": [
            "61"
          ],
          "1": "The scene representation in NeRF inspired a number of works focusing on 3D modeling, such as human face and body capture [24, 42, 55, 61, 62, 72], relighting [4, 5, 71] and 3D content generation [9, 10, 22, 25, 31, 69, 78]."
        },
        "Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting": {
          "authors": [
            "Zhijing Shao",
            "Zhaolong Wang",
            "Zhuang Li",
            "Duotun Wang",
            "Xiangru Lin",
            "Yu Zhang",
            "Mingming Fan",
            "Zeyu Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "34"
          ],
          "1": "Recent advances in the field have seen a shift towards using Neural Radiance Fields (NeRF) [32], especially for capturing high-frequency details in 3D avatar modeling [3, 18, 21, 26, 27, 34, 35, 53].",
          "3": "T o achieve convincing rendering beyond the limitation of triangle mesh, especially on the hair, glasses, and clothes, some recent works [3, 16, 21, 26, 34, 35, 44, 47, 53] focus on constructing NeRF in the canonical space (usually T -pose of SMPL [28] or neutral expression of FLAME [25]) and conduct volume rendering at the posed space."
        },
        "Gart: Gaussian articulated template models": {
          "authors": [
            "Jiahui Lei",
            "Yufu Wang",
            "Georgios Pavlakos",
            "Lingjie Liu",
            "Kostas Daniilidis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Con19885",
          "ref_ids": [
            "51"
          ],
          "1": "To address this issue, recent studies [7\u20139, 11\u201313, 16, 20\u201325, 32, 33, 38, 40, 48, 51, 54, 57\u201359, 64, 67, 72, 73, 82, 86, 88] propose to use neural representations, such as NeRF, to capture high-fidelity humans from multiple views or videos."
        },
        "Structured local radiance fields for human avatar modeling": {
          "authors": [
            "Zerong Zheng",
            "Han Huang",
            "Tao Yu",
            "Hongwen Zhang",
            "Yandong Guo",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.html",
          "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "54"
          ],
          "1": "Recently, neural radiance representations, which implicitly encode shape and appearance using neural networks, are also applied in pursuit of higher-fidelity results [35,49, 54].",
          "3": "Recently, neural scene representations and rendering techniques are adopted for higher-fidelity results [35,54, 55].",
          "9": "Compared to [54], our method can produce more appearance details, and generate the non-rigid mo15898 Figure 6.",
          "11": "1 also prove that our method can achieve higher-quality results than [54]."
        },
        "Neuman: Neural human radiance field from a single video": {
          "authors": [
            "W Jiang",
            "KM Yi",
            "G Samei",
            "O Tuzel",
            "A Ranjan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_24",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021) 1, 3, 4, 6",
          "ref_ids": [
            "32"
          ],
          "1": "Recent efforts also focus on animation of these radiance field models [19,33,32,12,40] of human, with the aid of large controlled datasets, further extending the application domain of radiance-field-based modeling to enable augmented reality experiences.",
          "2": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [22] or 3D skeleton [19,33,32,12,40].",
          "3": "NeRF [32] learns a blending weight field in both observation space and canonical space, and optimize for a new blending weight field for novel poses."
        },
        "Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition": {
          "authors": [
            "Chen Guo",
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "41"
          ],
          "1": "Recent works [20, 22, 29, 39, 41, 54, 59, 60] attempt to reconstruct humans from more sparse settings by deploying neural rendering."
        },
        "Humannerf: Efficiently generated human radiance field from sparse inputs": {
          "authors": [
            "Fuqiang Zhao",
            "Wei Yang",
            "Jiakai Zhang",
            "Pei Lin",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "More recent implicit manner based work [23, 29, 32,40,42,49,56] achieves impressive results for novel view synthesis for a specific scene."
        },
        "Monohuman: Animatable human neural field from monocular video": {
          "authors": [
            "Zhengming Yu",
            "Wei Cheng",
            "Xian Liu",
            "Wayne Wu",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 1, 2, 3, 4",
          "ref_ids": [
            "32"
          ],
          "4": "NeuralBody [32] uses structured pose features generated from SMPL [25] to condition the radiance field, enabling it to recover human performers and produce free-viewpoint images from sparse multi-view videos.",
          "5": "[32] create an animatable model by conditioning the NeRF with pose-dependent latent code."
        },
        "Instantavatar: Learning avatars from monocular video in 60 seconds": {
          "authors": [
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.html",
          "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "48"
          ],
          "1": "Recently, neural representations [37, 41, 45, 46] have emerged as a powerful tool to model 3D humans [3,6,8,10, 11, 13, 14, 22\u201326, 30, 31, 34, 38, 39, 39, 43, 44, 48, 49, 52, 53, 57, 59\u201363, 63, 64, 67, 69, 70].",
          "2": "Using neural representations, many works [6, 18, 26, 27, 30, 34, 43, 48, 49, 61, 62, 64, 69] can directly reconstruct high fidelity neural human avatars from a sparse set of views or a monocular video without prescanning personalized template."
        },
        "Efficientnerf efficient neural radiance fields": {
          "authors": [
            "Tao Hu",
            "Shu Liu",
            "Yilun Chen",
            "Tiancheng Shen",
            "Jiaya Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.html",
          "ref_texts": "[22] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2",
          "ref_ids": [
            "22"
          ],
          "1": "With Neural Radiance Fields (NeRF) [17] proposed, NVS tasks [20, 24], like large-scale or dynamic synthesis [21,22,25], were successfully dealt with in high quality.",
          "2": "Neural Actor [13] and Animatable-NeRF [22] also adopt similar functions to synthesize human body with novel poses."
        },
        "Ash: Animatable gaussian splats for efficient and photoreal human rendering": {
          "authors": [
            "Haokai Pang",
            "Heming Zhu",
            "Adam Kortylewski",
            "Christian Theobalt",
            "Marc Habermann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Pang_ASH_Animatable_Gaussian_Splats_for_Efficient_and_Photoreal_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Int. Conf. Comput. Vis., pages 14314\u201314323, 2021. 1, 3",
          "ref_ids": [
            "45"
          ],
          "1": "Hybrid approaches usually attach a neural radiance field (NeRF) [38] onto a (deformable) human model [15, 32, 45].",
          "2": "To better model the pose-dependent appearance of humans, recent studies [10, 15, 27, 33, 45, 76, 80, 81] further introduce motion-aware residual deformations in the canonicalized space."
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
          "authors": [
            "Chen Geng",
            "Sida Peng",
            "Zhen Xu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.html",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
          "ref_ids": [
            "56"
          ],
          "1": "We extend the technique of displacement map [14, 15] to represent human motions by restricting the originally 3D deformation field [40, 56, 59] on the 2D surface of a parametric human model, such as SMPL [43].",
          "2": "Another line of works [28, 30, 34, 36, 40, 56, 58, 65, 92, 93, 95, 101\u2013103, 105] exploits dynamic implicit neural representations and differentiable renderers to reconstruct 3D human models from 8760.",
          "3": "In contrast to [40, 56] which use a single neural radiance field (NeRF) to represent the canonical human model, we decompose the human body into multiple parts with different complexity and adopt a structured set of MHE-augmented NeRF with varying resolutions as the body representation.",
          "4": "(8) In contrast to [40, 56] that represent the body with a single NeRF network, our part-based voxelized human representation can assign different densities of model parameters to different human parts with different complexity, thereby enabling us to efficiently distribute the representational power of the network.",
          "5": "Our method is implemented purely with the PyTorch framework [53] to demonstrate the effectiveness of our representation It also enables us to fairly compare with baseline methods [34, 56, 58] implemented in PyTorch.",
          "6": "Animatable NeRF (AN) [56] deforms the canonical NeRF with the skeleton-driven framework and models non-rigid deformations by learning blend weight fields.",
          "7": "[57] extend [56] with a signed distance field and pose-dependent deformation field to better model the residual deformation and geometric 8763.",
          "8": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
          "9": "[57, 93] exhibit better results than [56, 58].",
          "10": "Although [56,58] have shown impressive rendering results given 4-view videos, they do not perform well on monocular inputs.",
          "11": "18 AN [56] \u02dc10 h 29.",
          "12": "[56] uses a learnable blend weight field to model human motion, which has a higher dimension and could be hard to converge well given single-view supervision."
        },
        "CelebV-HQ: A large-scale video facial attributes dataset": {
          "authors": [
            "H Zhu",
            "W Wu",
            "W Zhu",
            "L Jiang",
            "S Tang",
            "L Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_38",
          "ref_texts": "62. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 16",
          "ref_ids": [
            "62"
          ],
          "1": "These features on video modality could not only be further exploited to improve the quality of current models, but also stimulate the emerging of several budding topics, such as Dynamic NeRF [63] and Animatable NeRF [62]."
        },
        "Rignerf: Fully controllable neural 3d portraits": {
          "authors": [
            "Rukh Athar",
            "Zexiang Xu",
            "Kalyan Sunkavalli",
            "Eli Shechtman",
            "Zhixin Shu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "37"
          ],
          "1": "The photorealism of volumetric and implicit representations have encouraged works that combine them with classical representations in order improve reconstruction [5] or lend control over foreground [12, 30, 37].",
          "2": "Similarly, [37] learns a 3D skinning field to accurately deform points according to the target pose."
        },
        "Banmo: Building animatable 3d neural models from many casual videos": {
          "authors": [
            "Gengshan Yang",
            "Minh Vo",
            "Natalia Neverova",
            "Deva Ramanan",
            "Andrea Vedaldi",
            "Hanbyul Joo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "Similar to our goal, some recent works [24, 32, 36, 37, 44] produce posecontrollable NeRFs, but they rely on a human body model, or synchronized multi-view video inputs."
        },
        "Fourier plenoctrees for dynamic radiance field rendering in real-time": {
          "authors": [
            "Liao Wang",
            "Jiakai Zhang",
            "Xinhang Liu",
            "Fuqiang Zhao",
            "Yanshun Zhang",
            "Yingliang Zhang",
            "Minye Wu",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "40"
          ],
          "1": "skeleton [40] or parametric models [27, 41]) to explicitly calculate stable motion flows from model animations."
        },
        "Gomavatar: Efficient animatable human modeling from monocular video using gaussians-on-mesh": {
          "authors": [
            "Jing Wen",
            "Xiaoming Zhao",
            "Zhongzheng Ren",
            "Alexander G. Schwing",
            "Shenlong Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. InICCV, 2021.3",
          "ref_ids": [
            "53"
          ],
          "1": "To address this limitation, human modeling from videos has received a lot of attention from the community: many prior efforts utilize implicit representations and differentiable renderers for either non-animatable [54] or animatable [16, 22, 25, 37, 39, 53, 55, 64, 69, 70, 75, 81, 83, 89] scene-specific human modeling while other efforts focus on scene-agnostic modeling [9, 14, 21, 31, 34, 35, 56, 85, 87]."
        },
        "Tava: Template-free animatable volumetric actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_25",
          "ref_texts": "34. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021) TAVA: Template-free Animatable Volumetric Actors 23",
          "ref_ids": [
            "34"
          ],
          "2": "The follow-up work Animatable-NeRF [34] establishes a transformation between view and canonical space through optimizing the inverse deformation field.",
          "7": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 SMPL-based Methods Animatable-NeRF [34] 30.",
          "8": "We compare our work with two types of previous methods: 1) Templatefree methods, including NARF [29] and A-NeRF [42], as well as 2) SMPL-based methods, including Animatable-NeRF [34] and NeuralBody [35].",
          "10": "2, for the template-based baselines Animatable-NeRF [34] and NeuralBody [35], we use their official implementations.",
          "11": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [34,35,45].",
          "12": "TAVA: Template-free Animatable Volumetric Actors 19 Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 Subject 313 Animatable-NeRF [34] 29.",
          "13": "957 Subject 315 Animatable-NeRF [34] 27.",
          "14": "960 Subject 377 Animatable-NeRF [34] 32.",
          "15": "980 Subject 386 Animatable-NeRF [34] 34."
        },
        "Dreamwaltz: Make a scene with complex 3d animatable avatars": {
          "authors": [
            "Y Huang",
            "J Wang",
            "A Zeng",
            "H Cao"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0e769ec2c2cd99b6ad69c9d75113e386-Abstract-Conference.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "The advancement of deep learning methods has enabled promising methods which can reconstruct 3D human models from monocular images [36, 45] or videos [44, 14, 46, 41, 12, 30]."
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
          "authors": [
            "Y Kwon",
            "D Kim",
            "D Ceylan"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/cf866614b6b18cda13fe699a3a65661b-Abstract.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "32"
          ],
          "1": "Despite the promising results, these general deformable NeRF [17, 53] and human-specific NeRF [11, 9, 33, 32] methods must be optimized for each new video separately, and generalize poorly on unseen scenarios."
        },
        "Fenerf: Face editing in neural radiance fields": {
          "authors": [
            "Jingxiang Sun",
            "Xuan Wang",
            "Yong Zhang",
            "Xiaoyu Li",
            "Qi Zhang",
            "Yebin Liu",
            "Jue Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "41"
          ],
          "1": "Various follow-ups extend NeRF to faster training and testing [8,13,17,43,49], pose-free [26,31], dynamic scenes [5,50] and animating avatars [15,27,41]."
        },
        "Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering": {
          "authors": [
            "Wei Cheng",
            "Ruixiang Chen",
            "Siming Fan",
            "Wanqi Yin",
            "Keyu Chen",
            "Zhongang Cai",
            "Jingbo Wang",
            "Yang Gao",
            "Zhengming Yu",
            "Zhengyu Lin",
            "Daxuan Ren",
            "Lei Yang",
            "Ziwei Liu",
            "Chen Change",
            "Chen Qian",
            "Wayne Wu",
            "Dahua Lin",
            "Bo Dai",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "To reconstruct the human body from sequences, AnimatableNeRF [43] learns a static canonical radiance field together with a ray blending network from the current frame to canonical space.",
          "2": "Recent dynamic human rendering works like NeuralBody [44], A-NeRF [53], AnimatableNeRF [43], and NeuralV olumes [32] obtained impressive results by training on a single case with multi-view Interaction Simple Hard Medium No Deformation Simple Hard Medium Instant-NGP NeuS Neural Volumes A-NeRF Neural Body AnimatableNeRF HumanNeRF Scaled 1/PSNR Scaled 1/SSIM Scaled LPIPS Motion Simple Hard Medium Texture Simple Hard Medium Instant-NGP NeuS Neural Volumes A-NeRF Neural Body AnimatableNeRF HumanNeRF Figure 4: Quantitative results visualization of novel view synthesis test across benchmarks splits and difficulties.",
          "3": "For a fair comparison, we unify the training setting of NeuralV olumes [32], A-NeRF [53], NeuralBody [44], AnimatableNeRF [43], and HumanNeRF [61] with 42 dense training views.",
          "4": "Similar to the novel view synthesis task, we conduct novel pose animation benchmark on the four dynamic methods [44, 43, 53, 61].",
          "5": "Besides, for the SMPLguided pose animation methods [44, 43, 61], we provide the SMPL parameters of test images for the models to infer rendering.",
          "6": "We abbreviate NeuralV olumes [32] as \u2018NV\u2019, A-NeRF [53] as \u2018AN\u2019, NeuralBody [44] as \u2018NB\u2019, AnimatableNeRF [43] as \u2018AnN\u2019 and HumanNeRF [61] as \u2018HN\u2019.",
          "7": "From top to bottom, we illustrate the reposing results generated by(a-e): NeuralV olumes [32], A-NeRF [53], NeuralBody [44] AnimatableNeRF [43], and HumanNeRF [61]."
        },
        "Devrf: Fast deformable voxel radiance fields for dynamic scenes": {
          "authors": [
            "JW Liu",
            "YP Cao",
            "W Mao",
            "W Zhang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eeb57fdf745eb31a3c7ef22c59a4661d-Abstract-Conference.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "26"
          ],
          "1": "Lastly, several NeRF-based approaches have been proposed for modeling dynamic humans [8, 41, 17, 26, 32] but can not directly generalize to other scenes."
        },
        "Representing volumetric videos as dynamic mlp maps": {
          "authors": [
            "Sida Peng",
            "Yunzhi Yan",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "To model high-resolution scenes, [14, 16, 31, 42, 44, 46, 47, 71] extend NeRF to represent dynamic scenes."
        },
        "Dp-nerf: Deblurred neural radiance field with physical scene priors": {
          "authors": [
            "Dogyoon Lee",
            "Minhyeok Lee",
            "Chajin Shin",
            "Sangyoun Lee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "Due to the success of the NeRF in neural rendering, several studies have applied NeRF to other areas such as dynamic scenes [17\u201319, 29, 30, 33, 49, 55], generative models [28, 38], relighting [3, 23, 32, 42], human avatars [31, 45, 58], and 3D reconstruction [47, 50]."
        },
        "Deforming radiance fields with cages": {
          "authors": [
            "T Xu",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_10",
          "ref_texts": "37. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "37"
          ],
          "1": "Harada For some specific object categories, such as the human body or articulated objects, recent studies [24,32,33,37,38,41,46] enable the generation of the unseen scene by controlling the body shape or bone pose.",
          "2": "For the specific task of human body modeling, various works proposed to combine NeRF with a parametric human model to enable human body reposing [37, 38], shape control [24] or even clothing changes [46]."
        },
        "Sifu: Side-view conditioned implicit function for real-world usable clothed human reconstruction": {
          "authors": [
            "Zechuan Zhang",
            "Zongxin Yang",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.html",
          "ref_texts": "[62] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "62"
          ],
          "1": "The rise of Neural Radiance Fields (NeRF) has seen methods [18, 20, 33, 34, 56, 59, 62, 63, 78, 88] using videos or multi-view images to optimize NeRF for human form capture."
        },
        "Nerfacc: A general nerf acceleration toolbox": {
          "authors": [
            "R Li",
            "M Tancik",
            "A Kanazawa"
          ],
          "url": "https://arxiv.org/abs/2210.04847",
          "ref_texts": "[6] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang , Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human b odies. In Proceedings of the IEEE/CVF International Conference on Computer V ision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "6"
          ],
          "1": "In the past two years, they have been proven to be quite powerful in many downstream applications in 3D such as static/dynamic scene reconstruction [3, 4, 5, 6], relighting [7, 8, 9, 10, 11] and content generation [12, 13, 14]."
        },
        "Vmrf: View matching neural radiance fields": {
          "authors": [
            "J Zhang",
            "F Zhan",
            "R Wu",
            "Y Yu",
            "W Zhang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3548078",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "25"
          ],
          "1": "Due to the multi-view consistency of the generated images, NeRF as well as its variants [1, 3, 9, 17, 20, 28, 42] has attracted increasing attention in different tasks such as dynamic scene representation [7, 8, 10, 24, 31], color and shape editing [18, 35], compositional scene modeling [23], scene relighting [2, 30] and skeleton-driven synthesis [25]."
        },
        "Learning neural light fields with ray-space embedding": {
          "authors": [
            "Benjamin Attal",
            "Bin Huang",
            "Michael Zollhofer",
            "Johannes Kopf",
            "Changil Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Attal_Learning_Neural_Light_Fields_With_Ray-Space_Embedding_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "36"
          ],
          "1": "Others leverage coordinate embedding for modeling articulated objects such as the human body [24, 36]."
        },
        "Robust e-nerf: Nerf from sparse & noisy events under non-uniform motion": {
          "authors": [
            "Weng Fei",
            "Gim Hee"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Low_Robust_e-NeRF_NeRF_from_Sparse__Noisy_Events_under_Non-Uniform_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "This trend is further driven by the exceptional capabilities and photorealism of Neural Radiance Field (NeRF) [24]-based works [49, 23, 45, 30, 31, 32]."
        },
        "Physavatar: Learning the physics of dressed 3d avatars from visual observations": {
          "authors": [
            "Y Zheng",
            "Q Zhao",
            "G Yang",
            "W Yifan",
            "D Xiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72913-3_15",
          "ref_texts": "86. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323",
          "ref_ids": [
            "86"
          ],
          "1": "Several different types of representations have been explored for clothed avatars, including meshes [68] with dynamic textures [4,33,131], neural surface [16,90,107] and radiance fields [14,18,24,27,44\u201346, 86,87,99,133], point sets [66,67,69], and 3D Gaussians [34,58,82,134]."
        },
        "Cagenerf: Cage-based neural radiance field for generalized 3d deformation and animation": {
          "authors": [
            "Y Peng",
            "Y Yan",
            "S Liu",
            "Y Cheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb78e6b5246b03e0b82b4acc8b11cc21-Abstract-Conference.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303, 2021.",
          "ref_ids": [
            "32"
          ],
          "4": "6M), and compare with the state-of-the-art method AniNeRF [32] as well as other human sythesis methods [41, 49]."
        },
        "Synbody: Synthetic dataset with layered human models for 3d human perception and modeling": {
          "authors": [
            "Zhitao Yang",
            "Zhongang Cai",
            "Haiyi Mei",
            "Shuai Liu",
            "Zhaoxi Chen",
            "Weiye Xiao",
            "Yukun Wei",
            "Zhongfei Qing",
            "Chen Wei",
            "Bo Dai",
            "Wayne Wu",
            "Chen Qian",
            "Dahua Lin",
            "Ziwei Liu",
            "Lei Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3, 9",
          "ref_ids": [
            "40"
          ],
          "2": "NeuralBody [41] incorporates prior from a statistical body template to learn dynamic sequence, while Animatable NeRF [40] proposes to reconstruct an animatable human model that generalizes to new poses.",
          "4": "We benchmark five methods in total, including the vanilla NeRF [34], NeuralBody [41] and HumanNeRF [50] for novel view synthesis, AnimNeRF [40] for novel pose synthesis, NHP [26] for generalizable human NeRF (novel identity synthesis)."
        },
        "Unsupervised learning of efficient geometry-aware neural articulated representations": {
          "authors": [
            "A Noguchi",
            "X Sun",
            "S Lin",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_36",
          "ref_texts": "56. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "56"
          ],
          "5": "In order to train a single tri-plane for all parts, the second change is to further transform the local coordinates xl k into a canonical space defined by a canonical pose oc, similar to Animatable NeRF [56].",
          "7": "1 Training on a Dynamic Scene Following the training setting in Animatable NeRF [56], we train our ENARF model on synchronized multi-view videos of a single moving articulated object.",
          "9": "First, we compare our method with the state-of-the-art supervised methods NARF [51] and Animatable NeRF [56]."
        },
        "Skinned motion retargeting with residual perception of motion semantics & geometry": {
          "authors": [
            "Jiaxu Zhang",
            "Junwu Weng",
            "Di Kang",
            "Fang Zhao",
            "Shaoli Huang",
            "Xuefei Zhe",
            "Linchao Bao",
            "Ying Shan",
            "Jue Wang",
            "Zhigang Tu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.html",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "19"
          ],
          "1": "[19] introduced the neural blend weight fields to reconstruct an animatable human model from a multi-view video."
        },
        "Dinar: Diffusion inpainting of neural textures for one-shot human avatars": {
          "authors": [
            "David Svitov",
            "Dmitrii Gudkov",
            "Renat Bashirov",
            "Victor Lempitsky"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "38"
          ],
          "1": "The highest photorealism and temporal consistency in the video avatar task are currently achieved by NeRF-based methods [38, 54, 24]."
        },
        "Headgas: Real-time animatable head avatars via 3d gaussian splatting": {
          "authors": [
            "H Dhamo",
            "Y Nie",
            "A Moreau",
            "J Song",
            "R Shaw"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72627-9_26",
          "ref_texts": "38. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. ICCV (2021) 4",
          "ref_ids": [
            "38"
          ],
          "1": "NeRFs have also been used to represent dynamic scenes including human bodies [9,38,50], human heads [14,62], and generic time-varying scenes [12,25,36,37,39,46]."
        },
        "Fast-snarf: A fast deformer for articulated neural fields": {
          "authors": [
            "X Chen",
            "T Jiang",
            "J Song",
            "M Rietmann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10112633/",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "These methods serve as a foundation for many tasks such as generative modeling of articulated objects or humans [3, 8, 12, 20, 40, 63], and reconstructing animatable avatars from scans [9, 13, 27, 34, 35, 51, 56], depth [14, 42, 57], videos [7, 24, 25, 26, 29, 39, 45, 47, 58, 59, 64] or a single image [18, 21, 60]."
        },
        "Anifacegan: Animatable 3d-aware face image generation for video avatars": {
          "authors": [
            "Y Wu",
            "Y Deng",
            "J Yang",
            "F Wei"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eae78bf2712f222f101bd7d12f875a57-Abstract-Conference.html",
          "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "48"
          ],
          "1": "The original NeRF and most of its successors [43, 33, 45, 49, 48, 40, 32, 73] focus on learning scene-specific representation using a set of posed images or a video sequence of a static or dynamic scene."
        },
        "gdna: Towards generative detailed neural avatars": {
          "authors": [
            "Xu Chen",
            "Tianjian Jiang",
            "Jie Song",
            "Jinlong Yang",
            "Michael J. Black",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_gDNA_Towards_Generative_Detailed_Neural_Avatars_CVPR_2022_paper.html",
          "ref_texts": "[49] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "49"
          ],
          "1": "To overcome the topology and resolution limitations of meshes, other representations, including point clouds [35, 37,65], implicit surfaces [12,47,52,55,58,60], and radiance fields [32, 45, 49, 56, 63], have been explored."
        },
        "Gm-nerf: Learning generalizable model-based neural radiance fields from multi-view images": {
          "authors": [
            "Jianchuan Chen",
            "Wentao Yi",
            "Liqian Ma",
            "Xu Jia",
            "Huchuan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303. IEEE, 2021. 2, 6, 8",
          "ref_ids": [
            "30"
          ],
          "1": "To alleviate this limitation, some works [6, 30, 31, 41, 48, 51] combine neural radiance fields [27] with SMPL [23] to represent the human body, which can be rendered to 2D images by differentiable rendering.",
          "3": "We also compare with per-scene optimization methods NB [31], Ani-NeRF [30], A-NeRF [41], ARAH [48]."
        },
        "Hosnerf: Dynamic human-object-scene neural radiance fields from a single video": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Tianyuan Yang",
            "Zhongcong Xu",
            "Jussi Keppo",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "Subsequent works have further improved on the generalizability [17, 4, 9] and animatability [31, 18] of human bodies."
        },
        "Surmo: surface-based 4D motion modeling for dynamic human rendering": {
          "authors": [
            "Tao Hu",
            "Fangzhou Hong",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_SurMo_Surface-based_4D_Motion_Modeling_for_Dynamic_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "42"
          ],
          "1": "Firstly, in contrast to existing pose-guided methods [25, 42, 43, 61] that focus on static poses as a conditional variable, we extract an expressive 4D motion input from the 3D body mesh sequences obtained from training video as our input, which includes both a static pose represented by a spatial 3D mesh and its temporal dynamics.",
          "2": "For stable view synthesis, recent papers [6, 25, 38, 42, 43, 53] propose to unify geometry reconstruction with view synthesis by volume rendering, which, however, is computationally heavy."
        },
        "Pina: Learning a personalized implicit neural avatar from a single rgb-d video sequence": {
          "authors": [
            "Zijian Dong",
            "Chen Guo",
            "Jie Song",
            "Xu Chen",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.html",
          "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, October 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "Furthermore, several methods that learn a neural avatar for a specific outfit from watertight meshes [13,16,24,32,47,53,56] have been proposed.",
          "2": "These methods either require complete full-body scans with accurate surface normals and registered poses [13, 16, 53, 56] or rely on complex and intrusive multi-view setups [24,32,47]."
        },
        "HandNeRF: Neural radiance fields for animatable interacting hands": {
          "authors": [
            "Zhiyang Guo",
            "Wengang Zhou",
            "Min Wang",
            "Li Li",
            "Houqiang Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 4, 6, 8",
          "ref_ids": [
            "25"
          ],
          "1": "To address the above issues and push the boundary of realistic human hand modeling, motivated by the recent success of NeRF [17] in modeling human body [11, 25, 26], we propose HandNeRF, a novel framework that unifiedly models the geometry and texture of animatable interacting This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "2": "Similar LBS-based pipelines are adopted by a lot of works [11,12,25,32,37,40].",
          "3": "Most methods [11, 37] regress an extra point-wise offset for samples, while some works like Animatable-NeRF [25] try to jointly optimize NeRF with the LBS weights for deformation.",
          "4": "Therefore, we follow previous works on NeRF for dynamic human body [11, 25, 37, 40] to leverage the parameterized human priors.",
          "5": "(5) Note that different from previous works [25, 32] relying on per-pose latent code to guide the deformation, we use pose representation instead, ensuring robustness to unseen poses.",
          "6": "1) Pose-NeRF: we modify Mip-NeRF [1] to learn a NeRF conditioned on pose; 2) Ani-NeRF: we adapt [25] to the setup of human hands; 3) NeuMan: we re-implement the \u201cHuman NeRF\u201d module of [11] on the settings of hands while preserving its various training losses.",
          "7": "Since AniNeRF [25] cannot directly generalize to unseen poses, we report its pose adaptation performance after re-training with blend weight consistency."
        },
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[5] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021, pp. 14 314\u201314 323. 1, 2, 5",
          "ref_ids": [
            "5"
          ],
          "1": "In the past few years, with the rise of implicit representations, particularly neural radiance fields (NeRF) [4], many researchers tend to represent the 3D human as a poseconditioned NeRF [5]\u2013[8] to automatically learn a neural avatar from RGB videos.",
          "2": ", signed distance function (SDF) [35], [36], occupancy [37], and radiance (NeRF) [5] fields.",
          "3": "Animatable NeRF [5] introduces SMPL deformation JOURNAL OF LATEX CLASS FILES, VOL.",
          "4": "2) Template-guided Parameterization: Previous human avatar representations in NeRF-based approaches [5]\u2013[7] necessitate the coordinate-based MLPs for the formulation of the implicit NeRF function.",
          "5": "We also modulate the output color attributes on Gaussian maps with a view direction map V to model view-dependent variance like NeRF-based approaches [5]."
        },
        "Tela: Text to layer-wise 3d clothed human generation": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Z Huang",
            "X Xu",
            "J Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_2",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021) 4",
          "ref_ids": [
            "32"
          ],
          "1": "Benefiting from advancements in implicit functions [26,27,30], recent methods [7,10,32,33,44,45,47,49,52] have presented impressive clothed huTELA: Text to Layer-wise 3D Clothed Human Generation 5 man reconstruction or generation from images and 3D scans."
        },
        "Gaussianbody: Clothed human reconstruction via 3d gaussian splatting": {
          "authors": [
            "M Li",
            "S Yao",
            "Z Xie",
            "K Chen"
          ],
          "url": "https://arxiv.org/abs/2401.09720",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Extensions of NeRF [14] into dynamic scenes [35\u201337] and methods for animatable 3D human models in multi-view scenarios [18\u201321, 38, 39] or monocular videos [15\u201317, 40] have shown promising results."
        },
        "Reacto: Reconstructing articulated objects from a single video": {
          "authors": [
            "Chaoyue Song",
            "Jiacheng Wei",
            "Chuan Sheng",
            "Guosheng Lin",
            "Fayao Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Song_REACTO_Reconstructing_Articulated_Objects_from_a_Single_Video_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.3",
          "ref_ids": [
            "44"
          ],
          "1": "To address these challenges, several works [16, 37, 44, 61] employ the parametric 3D human models, such as SMPL [29], while other methods [28, 44, 45] utilize synchronized multi-view video inputs."
        },
        "Editablenerf: Editing topologically varying neural radiance fields by key points": {
          "authors": [
            "Chengwei Zheng",
            "Wenbin Lin",
            "Feng Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zheng_EditableNeRF_Editing_Topologically_Varying_Neural_Radiance_Fields_by_Key_Points_CVPR_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.3",
          "ref_ids": [
            "29"
          ],
          "1": "By using human body parametric models and skinning techniques such as SMPL [22], neural radiance fields have been extended to model the human body and can be animated by controlling skeleton poses [3, 19, 25, 29, 36]."
        },
        "One-shot implicit animatable avatars with model-based priors": {
          "authors": [
            "Yangyi Huang",
            "Hongwei Yi",
            "Weiyang Liu",
            "Haofan Wang",
            "Boxi Wu",
            "Wenxiao Wang",
            "Binbin Lin",
            "Debing Zhang",
            "Deng Cai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 2, 6, 7",
          "ref_ids": [
            "37"
          ],
          "1": ", multi-view videos captured by well-calibrated multi-camera systems [39, 62, 58, 37, 68], or long monocular videos [57] where almost all parts of the human body are visible.",
          "2": "Method Subject data Extra training data Invisible area completion Animatable NeuralBody [39] Ani-NeRF [37] HumanNeRF [57] multi-view images, monocular videos data-free \u2717 \u2713 PiFU [47] PaMIR [72] ARCH [16] ARCH++ [13] PHORHUM [1] monocular images 3D scans \u2713 \u2713 MPS-NeRF [11] NHP [22] sparse videos, multi-view images multi-view videos \u2717 \u2713 MonoNHR [7] monocular images multi-view images \u2713 \u2717 EV A3D [14] monocular images monocular images \u2713 \u2713 ELICIT (ours) monocular images data-free \u2713 \u2713 Table 1: Recent human rendering methods that are most relevant to our work.",
          "3": "Among which [39] learns structured latent codes on SMPL [29] mesh vertices, other methods construct the representation in a canonical space by modeling pose-driven deformation [57, 62, 52, 71, 38, 37].",
          "4": "We selected three state-of-the-art methods as baselines: Neural Body [39] (NB) and Animatable NeRF [37] (AniNeRF) from per-subject optimization methods, and Neural Human Performer [22] (NHP) from generalizable methods.",
          "5": "Compared with state-of-the-art NeRF based methods[39, 22, 37] on novel view synthesis and novel pose synthesis, ELICIT generates human 3D renderings with more consistent appearance and realistic details from a single image.",
          "6": "For per-subject optimization methods NB [39] and Ani-NeRF [37], we optimized one model for each frame.",
          "7": "264 Ani-NeRF[37] 20."
        },
        "Mps-nerf: Generalizable 3d human rendering from multiview images": {
          "authors": [
            "X Gao",
            "J Yang",
            "J Kim",
            "S Peng",
            "Z Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9888037/",
          "ref_texts": "[6] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323. 1, 2, 3, 5, 6, 7, 10",
          "ref_ids": [
            "6"
          ],
          "1": "While traditional methods [1], [2], [3] use dense multiview camera rigs or depth sensors to accomplish this task, recent neural rendering approaches [4], [5], [6], [7] have shown that free-view rendering and animation can be achieved using sparse color cameras, which could significantly reduce the device setup and capture cost.",
          "2": "In particular, promising results have been shown by methods [5], [6], [7] that are based on the neural radiance field (NeRF) [8] representation.",
          "3": "However, due to the high complexity of human motion and appearance, existing methods [5], [6], [7] are typically trained in a person-specific setup, i.",
          "4": "Some methods [6], [40], [41] also leverage SMPL to deform the 3D space to a canonical pose, where posedependent residual deformation can be considered.",
          "5": "To render the target image, we follow recent works [5], [6], [7] and base our rendering scheme on NeRF [8], which is a compact yet powerful representation for neural rendering.",
          "6": "The notion of a canonical space has been used in previous deformable NeRF schemes such as [6], [22], [23].",
          "7": "Following [6], [38], [49], for each point in the volume, we assign the skinning weights of its closest body vertex.",
          "8": ", a residual skinning weight field is learned in the personspecific model of [6]).",
          "9": "In [6], a per-frame latent code is jointly learned to alleviate this issue, and a test-time optimization is further needed to optimize the latent code for a novel pose.",
          "10": "Following [6], we conduct experiments on 7 subject: S1, S5, S6, S7, S8, S9, and S11.",
          "11": "As in AniNeRF [6], we use three views (#0, #1, #2) out of the four as the input to our method and all four views for supervision during training.",
          "12": "We train and test our method using fitted SMPL parameters and image masks provided by [6] which are obtained using [30] and [59], respectively.",
          "13": "Instead of directly calculating PSNR and SSIM for the whole image, we follow AniNeRF [6] and NeuralBody [5] to project the 3D bounding box of a body onto image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "14": "TABLE 2: Comparison of our method with NB [5], AniNeRF [6] on the Human3.",
          "15": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera parameters to render a novel view of these trained subjects.",
          "16": "Hence, for reference purpose, we compare our method with recent person-specific models NeuralBody (NB) [5] and Animatable NeRF (AniNeRF) [6].",
          "17": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera and pose parameters to render these trained subjects.",
          "18": "TABLE 3: Comparison of NB [5], AniNeRF [6], and our method on the THuman dataset.",
          "19": "Note that we did not apply any post-processing on the extracted 3D shapes such as the Gaussian smoothing used in AniNeRF [6].",
          "20": "1, 2, 3, 5, 6, 7, 10 [6] S."
        },
        "Omg: Towards open-vocabulary motion generation via mixture of controllers": {
          "authors": [
            "Han Liang",
            "Jiacheng Bao",
            "Ruichi Zhang",
            "Sihan Ren",
            "Yuecheng Xu",
            "Sibei Yang",
            "Xin Chen",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liang_OMG_Towards_Open-vocabulary_Motion_Generation_via_Mixture_of_Controllers_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human 491",
          "ref_ids": [
            "53"
          ],
          "1": "With CLIP\u2019s strong ability, many works are able to generate high-quality zero-shot text-driven images [17, 51, 83] or 3D objects [31, 32, 49, 53, 65, 80]."
        },
        "Humangen: Generating human radiance fields with explicit priors": {
          "authors": [
            "Suyi Jiang",
            "Haoran Jiang",
            "Ziyu Wang",
            "Haimin Luo",
            "Wenzheng Chen",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.html",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "56"
          ],
          "1": "Embracing the developing of NeRF techniques [8,9, 40\u201342, 44, 45, 48, 69, 71, 73, 82, 86], the human shape prior augmented NeRFs achieve modeling realistic human bodies [32, 37, 50, 57, 89], learning animatable avatars [34, 56, 74] and generalizing across different persons [32, 70, 89] from temporal data."
        },
        "Actorsnerf: Animatable few-shot human rendering with generalizable nerfs": {
          "authors": [
            "Jiteng Mu",
            "Shen Sang",
            "Nuno Vasconcelos",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303, 2021. 1, 2",
          "ref_ids": [
            "32"
          ],
          "1": "However, to achieve high-quality rendering, existing approaches [33, 23, 32, 29, 42] require a combination of synchronized multi-view videos and an instance-level NeRF network, trained on a specific human video sequence.",
          "2": "Recently, NeRF-based human representations have shown promise for high-quality view synthesis [33, 32, 50, 29, 19, 23, 42, 48, 55, 15, 10, 20, 45, 40, 41].",
          "3": "To better animate the human actor, subsequent works [23, 32] introduce a canonical space to align different body poses."
        },
        "Transhuman: A transformer-based human representation for generalizable neural human rendering": {
          "authors": [
            "Xiao Pan",
            "Zongxin Yang",
            "Jianxin Ma",
            "Chang Zhou",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.1, 2, 6, 15",
          "ref_ids": [
            "31"
          ],
          "1": "Recent works [33, 31, 44, 38] integrate the Neural Radiance Fields (NeRF) [29] technology with parametric human prior models (e.",
          "2": "With the recent success of Neural Radiance Fields (NeRF) [29, 2], many works [33, 31, 44, 38] have attempted to learn the 3D human representation from image inputs via differentiable rendering."
        },
        "Npc: Neural point characters from video": {
          "authors": [
            "Yang Su",
            "Timur Bagautdinov",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "34"
          ],
          "2": "Recent work applies NeRF to reconstruct dynamic human appearance [16, 27, 35, 38, 52] and learning animatable human models [19, 22, 30, 34, 43, 44, 50, 62] from video sequences.",
          "8": "Experiments We quantify the improvements our NPC brings over the most recent surface-free approach TA V A [19], DANBO [43] and A-NeRF [44], as well as most recent and established approaches that leverage template or scan-based prior, including ARAH [50], Anim-NeRF [34] and NeuralBody [35]."
        },
        "Structured 3d features for reconstructing controllable avatars": {
          "authors": [
            "Enric Corona",
            "Mihai Zanfir",
            "Thiemo Alldieck",
            "Eduard Gabriel",
            "Andrei Zanfir",
            "Cristian Sminchisescu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR, 2021. 3",
          "ref_ids": [
            "45"
          ],
          "1": "NeRFs have been recently explored for novel human view synthesis [11, 14, 25, 45, 46, 55, 60, 61, 63]."
        },
        "Surface-aligned neural radiance fields for controllable 3d human synthesis": {
          "authors": [
            "Tianhan Xu",
            "Yasuhiro Fujita",
            "Eiichi Matsumoto"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.html",
          "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "35"
          ],
          "1": "Because manually designing high-quality 3D human models is usually labor-intensive, increasing studies [1\u20133,24,27,30,35,36] have proposed the reconstruction of 3D human models using only 2D observations.",
          "2": "Several approaches [24, 30, 35, 36] have been proposed to incorporate knowledge from a statistical 3D human model and its pose estimation with NeRF.",
          "3": "Deformationbased approaches [24, 35] use a deformation field to transform the query point from the observation space to a poseindependent canonical space and then build NeRF in the canonical space.",
          "4": "For modeling a dynamic human body, recent studies [24, 30, 35, 36] have proposed the use of prior knowledge of human pose and skinning weights of SMPL [26] to ease the learning of a deformation field.",
          "5": "Animatable NeRF [35] uses the skinning weight of SMPL [26] to predict the neural blend shape fields.",
          "6": "Animatable NeRF [35] and Neural Actor [24] rely on this way of projection for learning a deformation field and/or utilizing a texture map.",
          "7": "Following the previous studies [28,35,36], we minimize the per-pixel mean squared error (MSE) between the rendered image and the ground truth image.",
          "8": "For detailed settings, we refer to [35].",
          "9": "6M dataset, we compare with Animatable NeRF [35], which learns the neural blend weight field and builds a NeRF within a canonical space.",
          "10": "6M dataset, our approach outperforms both [30] and [35] by a large margin.",
          "11": "For both datasets, the performance of our approach almost consistently outperforms [36], [35], and [30].",
          "12": "Also, while we do not explicitly model the time-varying deformation components (such as using per-frame embedding in [35,36]), neural networks could implicitly model such components by inferring time from the skeleton pose information."
        },
        "Relightable and animatable neural avatar from sparse-view video": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Chen Geng",
            "Linzhan Mou",
            "Zihan Yan",
            "Jiaming Sun",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_Relightable_and_Animatable_Neural_Avatar_from_Sparse-View_Video_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 7",
          "ref_ids": [
            "44"
          ],
          "1": "Albeit showing the capability of novel pose synthesis, the reconstructed avatars in these works [38, 44, 60] are not relightable as they This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "2": "Note that it is not trivial to combine DFSS with previous methods [44, 60, 63], as they cannot produce world-space distance values from 3D points to the scene surface along an arbitrary direction.",
          "3": "Several methods [14, 26, 44, 49] opt to optimize personalized skinning weights for the target human subject, where they represent the skinning weights as an MLP network and learn it from input data, such as human shapes [14, 49] or multiview videos [26, 37, 44].",
          "4": "We formulate the relightable and animatable avatar using a set of canonical space neural fields and a warping between world and canonical space defined by the linear blend skinning algorithm [35] and a displacement field [38, 44, 45, 61].",
          "5": "We follow the previous literature[38, 44] and use the linear blend skinning algorithm [35] to perform the inverse warping.",
          "6": "Note that we evaluate PSNR using the same protocal as [44], only computing metrics on the human region."
        },
        "Generalizable human gaussians for sparse view synthesis": {
          "authors": [
            "Y Kwon",
            "B Fang",
            "Y Lu",
            "H Dong",
            "C Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73229-4_26",
          "ref_texts": "35. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "Whileutilizing3Dhumanpriorhasprovenits effectiveness in the human rendering task [11,25,35,36,43], representing the geometry gap between human template and the real geometry (e."
        },
        "Ohta: One-shot hand avatar via data-driven implicit priors": {
          "authors": [
            "Xiaozheng Zheng",
            "Chao Wen",
            "Zhuo Su",
            "Zeran Xu",
            "Zhaohu Li",
            "Yang Zhao",
            "Zhou Xue"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_OHTA_One-shot_Hand_Avatar_via_Data-driven_Implicit_Priors_CVPR_2024_paper.html",
          "ref_texts": "[55] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "55"
          ],
          "1": "NeRF-based methods [24, 55, 66, 71, 73, 78] typically require dense inputs but have evolved to accommodate few-shot or one-shot scenarios by incorporating various priors."
        },
        "Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos": {
          "authors": [
            "R Jena",
            "GS Iyer",
            "S Choudhary",
            "B Smith"
          ],
          "url": "https://arxiv.org/abs/2311.10812",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, human-specific neural rendering methods have demonstrated state-of-the-art results in controllable human synthesis [3, 4, 6, 7, 28, 31, 32, 45, 50].",
          "5": "For ZJU MoCap, we consider SMPLPix which uses deferred rendering, NeuralBody, Animatable NeRF (AniNeRF) [28], Surface-Aligned NeRF (SA-NeRF) [50], and HumanNeRF [45] which are state-of-the-art neural rendering methods for animatable humans."
        },
        "Human101: Training 100+ fps human gaussians in 100s from 1 view": {
          "authors": [
            "M Li",
            "J Tao",
            "Z Yang",
            "Y Yang"
          ],
          "url": "https://arxiv.org/abs/2312.15258",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 6, 7, 12, 13, 16, 17",
          "ref_ids": [
            "43"
          ],
          "1": "Different from the usual inverse skinning used by [15, 22, 43, 45] this forward skinning deformation method avoids searching for the corresponding canonical points of the target pose points but directly deform the canonical points into observation space.",
          "3": "For dynamic human reconstruction, studies like [15, 43, 62] use neural networks to enhance the deformation process, applying residuals to point coordinates or blending weights.",
          "8": "AnimatableNeRF(AnimNeRF) [43] and AnimatableSDF(AnimSDF) [45] use SMPL deformation and posedependent neural blend weight field to model dynamic humans."
        },
        "Uv volumes for real-time rendering of editable free-view human performance": {
          "authors": [
            "Yue Chen",
            "Xuan Wang",
            "Xingyu Chen",
            "Qi Zhang",
            "Xiaoyu Li",
            "Yu Guo",
            "Jue Wang",
            "Fei Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 3, 6",
          "ref_ids": [
            "38"
          ],
          "2": "To validate our method, we compare it against several state-of-the-art free-view video synthesis techniques: 1) DN: DyNeRF [25], which takes time-varying latent codes as the conditions for dynamic scenes; and 2) NB: NeuralBody [39], which takes as input the posed human model with structured time-invariant latent codes and generates a pose-conditioned neural radiance field; 3) AN: Animatable-NeRF [38], which uses neural blend weight fields to generate correspondences between observation and canonical space."
        },
        "Total-recon: Deformable scene reconstruction for embodied view synthesis": {
          "authors": [
            "Chonghyuk Song",
            "Gengshan Yang",
            "Kangle Deng",
            "Yan Zhu",
            "Deva Ramanan"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE International Conference on Computer Vision (ICCV), 2021. 3",
          "ref_ids": [
            "38"
          ],
          "1": "One group of work leverages human-specific priors [38, 53, 32, 39, 24, 16, 19, 37] such as human body models (e."
        },
        "Rendering humans from object-occluded monocular videos": {
          "authors": [
            "Tiange Xiang",
            "Adam Sun",
            "Jiajun Wu",
            "Ehsan Adeli",
            "Li Fei"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Rendering_Humans_from_Object-Occluded_Monocular_Videos_ICCV_2023_paper.html",
          "ref_texts": "[50] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "50"
          ],
          "1": "Since the emergence of Neural Radiance Fields (NeRF) [43], different extensions have been recently developed to enable high-quality rendering of static scenes [22, 57, 2, 3, 63, 61, 58, 44], moving objects [18, 36, 47, 48, 52, 46], and dynamic humans [51, 4, 7, 11, 9, 13, 14, 17, 20, 21, 26, 27, 35, 37, 45, 50, 62, 64, 68, 49, 28, 30]."
        },
        "Neuraldome: A neural modeling pipeline on multi-view human-object interactions": {
          "authors": [
            "Juze Zhang",
            "Haimin Luo",
            "Hongdi Yang",
            "Xinru Xu",
            "Qianyang Wu",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "Existing works equip NeRF with pose-embeddings [23, 27, 40, 45, 80], learnable skinning weights [25, 44, 70] and even generalization across individuals [23, 66, 80]."
        },
        "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints": {
          "authors": [
            "M Mihajlovic",
            "A Bansal",
            "M Zollhoefer",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19784-0_11",
          "ref_texts": "44. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "44"
          ],
          "1": "Recent approaches have incorporated priors specific to human faces [8, 9, 15, 17, 50, 61, 72] and human bodies [44,45,64,66,67,71,73,74] to reduce the dependence on multi-view captures."
        },
        "Learning implicit templates for point-based clothed human modeling": {
          "authors": [
            "S Lin",
            "H Zhang",
            "Z Zheng",
            "R Shao",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_13",
          "ref_texts": "49. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "49"
          ],
          "1": "Recently, based on neural radiance fields (NeRF) [43], attempts have been made to bypass the underlying geometry and synthesize rendered images of clothed humans directly [49,50,62,67]."
        },
        "Caphy: Capturing physical properties for animatable human avatars": {
          "authors": [
            "Zhaoqi Su",
            "Liangxiao Hu",
            "Siyou Lin",
            "Hongwen Zhang",
            "Shengping Zhang",
            "Justus Thies",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_CaPhy_Capturing_Physical_Properties_for_Animatable_Human_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "31"
          ],
          "1": "Especially, recent methods [38, 24, 4, 51, 20, 27, 31] that rely on deep neural networks to represent appearance and geometry information show promising results.",
          "2": "Some methods leverage Nerf representation [22, 31, 51, 42, 53, 58, 18] for generating articulated human Nerf models, which propose pose-deformable neural radiance fields for representing human dynamics."
        },
        "PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling": {
          "authors": [
            "Xiaoyun Zheng",
            "Liwei Liao",
            "Xufeng Li",
            "Jianbo Jiao",
            "Rongjie Wang",
            "Feng Gao",
            "Shiqi Wang",
            "Ronggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_PKU-DyMVHumans_A_Multi-View_Video_Benchmark_for_High-Fidelity_Dynamic_Human_Modeling_CVPR_2024_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "29"
          ],
          "1": "To address this limitation, some recent studies [15, 29] propose using human body priors to assist in learning human representations."
        },
        "Nsf: Neural surface fields for human modeling from monocular depth": {
          "authors": [
            "Yuxuan Xue",
            "Bharat Lal",
            "Riccardo Marin",
            "Nikolaos Sarafianos",
            "Yuanlu Xu",
            "Gerard Pons",
            "Tony Tung"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xue_NSF_Neural_Surface_Fields_for_Human_Modeling_from_Monocular_Depth_ICCV_2023_paper.html",
          "ref_texts": "[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "46"
          ],
          "1": "There\u2019s a plethora of NeRFbased approaches for humans modeling that provide animatable avatars starting from monocular RGB videos [16, 19, 46, 53, 61, 72]."
        },
        "Nvfi: Neural velocity fields for 3d physics learning from dynamic videos": {
          "authors": [
            "J Li",
            "Z Song",
            "B Yang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html",
          "ref_texts": "[44] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. ICCV, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "In parallel, there are also a number of domain-specific NeRFs to model particular dynamic objects such as human bodies [44, 50, 45, 70, 68] and faces [4, 20, 66, 67]."
        },
        "Meshavatar: Learning high-quality triangular human avatars from multi-view videos": {
          "authors": [
            "Y Chen",
            "Z Zheng",
            "Z Li",
            "C Xu",
            "Y Liu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73113-6_15.pdf",
          "ref_texts": "71. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "71"
          ],
          "1": "Previous works project the spatial points onto the fitted SMPL surface to obtain an initial guess, and learn some residuals for refinements [71].",
          "2": "1 Evaluations on Synthetic Data For the task of intrinsic decomposition, we further evaluate our method on a synthetic dataset SyntheticHuman++ [71] and compare with state-of-the-art methods [13,97,103]."
        },
        "Danbo: Disentangled articulated neural body representations via graph neural networks": {
          "authors": [
            "SY Su",
            "T Bagautdinov",
            "H Rhodin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_7",
          "ref_texts": "40. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "40"
          ],
          "3": "While the skinning weights in SMPL provide an initialization, [40] showed that fine-tuning the deformation fields via self-supervision helps rendering unseen poses.",
          "4": "4 Experiments In the following, we evaluate the improvements upon the most recent surface-free neural body model A-NeRF [46], and compare against recent model-based solutions NeuralBody [41] and Anim-NeRF [40].",
          "8": "Table 1 verifies these improvements on the test set of Anim-NeRF [40]."
        },
        "Avatarcap: Animatable avatar conditioned monocular human volumetric capture": {
          "authors": [
            "Z Li",
            "Z Zheng",
            "H Zhang",
            "C Ji",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_19",
          "ref_texts": "53. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "53"
          ],
          "1": "They create animatable avatars from various inputs, including scans [9, 58, 44, 46, 8], multi-view RGB videos [53, 39] and monocular depth measurements [7, 71].",
          "2": "Recent works proposed to directly learn an animatable avatar from the database, including scans [45, 58, 44, 46, 8], multi-view RGB videos [39, 53] and depth frames [7, 71, 10]."
        },
        "Deformable 3d gaussian splatting for animatable human avatars": {
          "authors": [
            "HJ Jung",
            "N Brasch",
            "J Song",
            "E Perez-Pellitero"
          ],
          "url": "https://arxiv.org/abs/2312.15059",
          "ref_texts": "[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 3",
          "ref_ids": [
            "46"
          ],
          "1": "Recent works utilize implicit representations combined with volume rendering [8, 31, 46] which enables photo-realistic rendering with a significantly higher amount of details at the cost of denser camera views and human poses during training.",
          "2": "Animatable NeRF [46] and HumanNerf [69] integrates motion priors in the form of SMPL [35] parameters to regularize field prediction."
        },
        "Rana: Relightable articulated neural avatars": {
          "authors": [
            "Umar Iqbal",
            "Akin Caliskan",
            "Koki Nagano",
            "Sameh Khamis",
            "Pavlo Molchanov",
            "Jan Kautz"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3, 9",
          "ref_ids": [
            "45"
          ],
          "1": "Existing methods commonly aim to learn such neural avatars using monocular videos [47, 46, 45, 54, 66, 58, 31].",
          "2": "Hence, the generated images may not be the true representation of the 23143 novel viewnovel posegeneralizablerelightableMethod \u2713 NeuralBody [46], SelfRecon [31] \u2713 \u2713 AnimatableNeRf [45, 14], NeuMan [32] \u2713 \u2713 \u2713 ANR [47], TNA [52], StylePeople [22] \u2713 \u2713 Relighting4D [16] \u2713 \u2713 \u2713 \u2713 RANA (Ours) Table 1.",
          "3": "The 3D neural rendering methods represent the person using neural radiance fields [42] and render the target images using volume rendering [46, 62, 32, 45, 14, 58, 24, 38].",
          "4": "Thanks to the design of RANA, we can pretrain on as many subjects as available, which is not possible with most of the state-of-the-art methods for human synthesis [45, 46, 58, 31]."
        },
        "AniPortraitGAN: animatable 3D portrait generation from 2D image collections": {
          "authors": [
            "Y Wu",
            "S Xu",
            "J Xiang",
            "F Wei",
            "Q Chen",
            "J Yang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3610548.3618164",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 3, 4",
          "ref_ids": [
            "42"
          ],
          "1": "The related techniques have undergone a significant growth recently, with a variety of promising methods being proposed [1, 7, 8, 16, 21, 38, 40, 42, 51, 52, 54, 60, 62].",
          "2": "Human Image and Video ManipulationOur method is also related to human image and video manipulation approaches [13, 16, 17, 23, 27, 31, 42, 43, 48, 55, 58, 59, 63, 64] that also produce human animation videos.",
          "3": "In fact, this strategy is widely used in state-of-the-art animatable human body modeling and generation methods [2, 12, 17, 22, 42, 65]."
        },
        "Get3dhuman: Lifting stylegan-human into a 3d generative model using pixel-aligned reconstruction priors": {
          "authors": [
            "Zhangyang Xiong",
            "Di Kang",
            "Derong Jin",
            "Weikai Chen",
            "Linchao Bao",
            "Shuguang Cui",
            "Xiaoguang Han"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xiong_Get3DHuman_Lifting_StyleGAN-Human_into_a_3D_Generative_Model_Using_Pixel-Aligned_ICCV_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "Some other representations are also proposed for dealing with more complex clothed bodies, like point clouds [33, 35, 58], radiance fields [29, 41, 45, 54] and implicit fields [7, 37, 43, 57]."
        },
        "Smplitex: A generative model and dataset for 3d human texture estimation from single image": {
          "authors": [
            "D Casas",
            "M Comino-Trinidad"
          ],
          "url": "https://arxiv.org/abs/2309.01855",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. InProc. of IEEE International Conference on Computer Vision (ICCV), 2021.",
          "ref_ids": [
            "42"
          ],
          "1": "Similarly, other works leverage neural rendering pipelines [23, 41, 42, 44, 58] to generate view-dependent posed avatars but do not output 3D texture maps either."
        },
        "Watch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects": {
          "authors": [
            "Atsuhiro Noguchi",
            "Umar Iqbal",
            "Jonathan Tremblay",
            "Tatsuya Harada",
            "Orazio Gallo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "40"
          ],
          "1": "They allow novel view and pose synthesis, but require ground truth poses [8,36,49], or dense 3D meshes [24, 30, 40, 41, 53] annotations for the training image."
        },
        "Novel-view synthesis and pose estimation for hand-object interaction from sparse views": {
          "authors": [
            "Wentian Qu",
            "Zhaopeng Cui",
            "Yinda Zhang",
            "Chenyu Meng",
            "Cuixia Ma",
            "Xiaoming Deng",
            "Hongan Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Although existing neural rendering approaches perform well on static scenes [31, 2], rigid objects [54, 12] and human models [39, 38, 45], they barely considered scene context in interaction (such as contact [63] and model penetration [4, 24]).",
          "2": "Prior arts [9, 30, 6, 23, 39, 38, 35] use implicitly shape representation to reconstruct articulated human body shape.",
          "3": "In order to learn generative novel view synthesis, several methods [39, 38, 35, 45, 51, 64, 50, 8] integrate bone transformation [45] and linear blend skinning [38, 64, 50, 8] with neural radiance fields.",
          "4": "Compared to implicit articulated shape representation such as NASA [9], Animatable NeRF [38] and DD-NeRF [56] that need parametric shape models or skinning weight supervision, our method can learn the geometry and appearance of hand and object with sparse-view only."
        },
        "Reloo: Reconstructing humans dressed in loose garments from monocular video in the wild": {
          "authors": [
            "C Guo",
            "T Jiang",
            "M Kaufmann",
            "C Zheng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72673-6_2",
          "ref_texts": "37. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "37"
          ],
          "1": "Recent works employ neural rendering to fit neural fields to videos to obtain an articulated human model [9,14,19,21,22,29,37,38,40,48,50]."
        },
        "Generalizing neural human fitting to unseen poses with articulated se (3) equivariance": {
          "authors": [
            "Haiwen Feng",
            "Peter Kulits",
            "Shichen Liu",
            "Michael J. Black",
            "Victoria Fernandez"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Feng_Generalizing_Neural_Human_Fitting_to_Unseen_Poses_With_Articulated_SE3_ICCV_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 1",
          "ref_ids": [
            "36"
          ],
          "1": "Introduction The three-dimensional (3D) capture of humans in varied poses is increasingly common and has many applications including synthetic data generation [35], human health analysis [57], apparel design and sizing [51], and avatar creation [10, 36, 50, 55]."
        },
        "Instant continual learning of neural radiance fields": {
          "authors": [
            "Ryan Po",
            "Zhengyang Dong",
            "Alexander W. Bergman",
            "Gordon Wetzstein"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Po_Instant_Continual_Learning_of_Neural_Radiance_Fields_ICCVW_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
          "ref_ids": [
            "43"
          ],
          "1": "The success of NeRFs has spawned a line of works on improving the quality and efficiency of the method [5, 4, 11, 20, 24, 6, 32, 33, 38, 40, 45, 55, 56, 60, 61, 64, 65, 67], while extending the method to a range of applications [62, 12, 34, 43, 19, 22, 42, 53, 68]."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "39"
          ],
          "1": "Recently, optimizing a network to represent a person-specific model shows impressive results [14, 46, 10, 41, 39].",
          "2": "Furthermore, in order to achieve better animatable effects, learning blend weights automatically from data [39] and incorporating articulated structures [36] are explored.",
          "3": "98 AniNeRF [39] 1.",
          "4": "2 Comparison with the baselines Since most previous methods only focus on body modeling, we extend the state-of-the-art methods AniNeRF [39] and AniSDF [40] with hands to compare with our method.",
          "5": "239 AniNeRF [39] 20."
        },
        "Geometry transfer for stylizing radiance fields": {
          "authors": [
            "Hyunyoung Jung",
            "Seonghyeon Nam",
            "Nikolaos Sarafianos",
            "Sungjoo Yoo",
            "Alexander Sorkine",
            "Rakesh Ranjan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jung_Geometry_Transfer_for_Stylizing_Radiance_Fields_CVPR_2024_paper.html",
          "ref_texts": "[60] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "60"
          ],
          "1": "Building on this concept, subsequent studies [16, 59, 60, 70] have addressed the view synthesis challenge in dynamic scenes."
        },
        "Interactive nerf geometry editing with shape priors": {
          "authors": [
            "YJ Yuan",
            "YT Sun",
            "YK Lai",
            "Y Ma",
            "R Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10252034/",
          "ref_texts": "[19] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "19"
          ],
          "2": "However, NeRF still has shortcomings and plenty of work has extended the original NeRF, including better synthesis effects [2], [43], [44], applicable to dynamic scenes [8], [14], [18], [19], [45], [46], [47], [48], [49], faster training and rendering speed [3], [4], [50], [51], [52], generalization to different scenes [53], [54], relighting [6], [7], [55], [56], and various kinds of editing [20], [57], [58], [59], [60], [61]."
        },
        "Livehand: Real-time and photorealistic neural hand rendering": {
          "authors": [
            "Akshay Mundra",
            "Mallikarjun B",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "28"
          ],
          "1": "Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9].",
          "2": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_1",
          "ref_texts": "58. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
          "ref_ids": [
            "58"
          ],
          "3": "Clothed Humans as Implicit Functions: Neural implicit functions [13, 44, 45, 55, 61] have been used to model clothed humans from various sensor inputs including monocular images [22,23,25,33,64\u201366,72,80,93], multi-view videos [30, 38, 52, 58, 60, 81], sparse point clouds [6, 14, 16, 77, 78, 94], or 3D meshes [11, 12, 15,47,48,67,74].",
          "4": "[38, 52, 58, 60, 81] take multi-view videos as inputs and do not need ground-truth geometry during training.",
          "5": "Neural Rendering of Animatable Clothed Humans: Differentiable neural rendering has been extended to model animatable human bodies by a number of recent works [52, 58, 60, 63, 72, 81].",
          "6": "Several recent works [52, 58, 72] propose to model the radiance field in canonical space and use a pre-defined or learned backward mapping to map query points from observation space to this canonical space.",
          "8": "4 Experiments We validate the generalization ability and reconstruction quality of our proposed method against several recent baselines [58, 60, 72].",
          "9": "For completeness, we also tested our approach on the H36M dataset [26] and report a quantitative comparison to [52,58] in Appendix G.",
          "10": "Baselines: We compare against three major baselines: Neural Body [60](NB), Ani-NeRF [58](AniN), and A-NeRF [72](AN).",
          "11": "For inference, we follow [58, 60] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
          "12": "For unseen test poses we follow the practice of [58, 60] and use the latent code Z of the last training frame as the input.",
          "13": "Neural Body [60], Ani-NeRF [58], and A-NeRF [72].",
          "15": "We also report quantitative results on the H36M dataset [26], following the testing protocols proposed by [58] in Table G."
        },
        "Mimo: Controllable character video synthesis with spatial decomposed modeling": {
          "authors": [
            "Y Men",
            "Y Yao",
            "M Cui",
            "L Bo"
          ],
          "url": "https://arxiv.org/abs/2409.16160",
          "ref_texts": "[23] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "23"
          ],
          "1": ", NeRF [22] and 3D Gaussian splatting [12]), a series of works [8, 15, 17, 23, 27] tend to represent the dynamic human as a pose-conditioned NeRF or Gaussian to learn animatable avatars in high-fidelity rendering quality."
        },
        "Uv gaussians: Joint learning of mesh deformation and gaussian textures for human avatar modeling": {
          "authors": [
            "Y Jiang",
            "Q Liao",
            "X Li",
            "L Ma",
            "Q Zhang",
            "C Zhang"
          ],
          "url": "https://arxiv.org/abs/2403.11589",
          "ref_texts": "39. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "39"
          ],
          "1": "Subsequently, a significant amount of work [9,14,26,39,40,46,47,50] has been explored to utilize NeRF representation for human modeling.",
          "2": "As a result, these representations have been also adopted for modelingclothedhumans[9,10,14,18,26,28,34,39,40,44,46\u201348,50].",
          "3": "To map posed space into a canonical pose, Animatable NeRF [39] learns neural blend weight fields and Neural Actor [28] utilizes a coarse body model as a proxy.",
          "6": "2 Comparison We compare our algorithm with several baselines: Neural Body [40], AnimNeRF [39], UV Volumes [9], and a recent state-of-the-art human avatar methods using GS with source code available, 3DGS-Avatar [41]."
        },
        "Manus: Markerless grasp capture using articulated 3d gaussians": {
          "authors": [
            "Chandradeep Pokhariya",
            "Ishaan Nikhil",
            "Angela Xing",
            "Zekun Li",
            "Kefan Chen",
            "Avinash Sharma",
            "Srinath Sridhar"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Pokhariya_MANUS_Markerless_Grasp_Capture_using_Articulated_3D_Gaussians_CVPR_2024_paper.html",
          "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "Several methods specifically address articulated shapes [32] like human bodies [32, 35, 47, 48, 66], or hands [14, 28, 34, 45, 50]."
        },
        "PointNeRF++: a multi-scale, point-based neural radiance field": {
          "authors": [
            "W Sun",
            "E Trulls",
            "YC Tseng",
            "S Sambandam"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72920-1_13",
          "ref_texts": "36. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021) 3",
          "ref_ids": [
            "36"
          ],
          "1": "Among many applications [14], NeRFs have been used to reconstruct individual objects [29] and unbounded scenes [2], in uncontrolled [8,28,52] or dynamic environments [22,33,34,36,37], in few-shot settings [7,21,31,51,54] and large urban landscapes [42,46,50]."
        },
        "Differentiable visual computing for inverse problems and machine learning": {
          "authors": [
            "A Spielberg",
            "F Zhong",
            "K Rematas"
          ],
          "url": "https://www.nature.com/articles/s42256-023-00743-0",
          "ref_texts": "[7] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "7"
          ],
          "1": "Recently, [7] demonstrated the extraction of geometry, animation and rendering parameters from multiview video, effectively combining all elements of the differentiable pipeline."
        },
        "Neural transformation fields for arbitrary-styled font generation": {
          "authors": [
            "Bin Fu",
            "Junjun He",
            "Jianjun Wang",
            "Yu Qiao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "29"
          ],
          "1": "For example, Animatable NeRF [29] employs NeRF to embed 3D static human bodies and utilizes a deformation field to model body movement by transforming observation-space points to the canonical space."
        },
        "MetaCap: Meta-learning Priors from Multi-view Imagery for Sparse-View Human Performance Capture and Rendering": {
          "authors": [
            "G Sun",
            "R Dabral",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72952-2_20",
          "ref_texts": "59. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "59"
          ],
          "1": "Several volumetric rendering-based methods focus on learning the human geometry and appearance in the canonical space [29,54,59,61,63,69,80,81], which learns shared features across different poses."
        },
        "Hvtr: Hybrid volumetric-textural rendering for human avatars": {
          "authors": [
            "T Hu",
            "T Yu",
            "Z Zheng",
            "H Zhang",
            "Y Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044408/",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 6, 11, 13",
          "ref_ids": [
            "56"
          ],
          "1": "Existing methods parameterize poses by global pose parameter conditioning [28, 39, 54, 84], 3D sparse points [57], or skinning weights [7, 29, 56].",
          "2": "2D : EDN [5], vid2vid [76] GAN \u0017 \u0013\n2D Plus : SMPLpix [58], DNR [73], ANR[61] GAN \u0017 \u0013\n3D : NB[57], AniNeRF[56] V olR \u0013 \u0017\n3D : Ours Hybrid \u0013 \u0013 Table 1: A set of recent human synthesis approaches classified by feature representations (2D/3D) and renderers.",
          "3": "For stable view synthesis, recent papers [7, 29, 48, 56, 57, 71, 83] propose to unify geometry reconstruction with view 2 Figure 1: We illustrate the differences between (left) GAN-based methods (DNR), (middle) our hybrid approach, and (right) NeRF methods (Neural Body [57]).",
          "7": "The geometry-guided ray marching algorithm and UV conditioned architecture enable us to train a PD-NeRF\n12 Figure 14: Comparisons of backward skinning method (AniNeRF [56]) and forward skinning method (ours).",
          "8": "LPIPS \u2193 FID \u2193 SSIM\u2191 PSNR\u2191 AniNeRF [56] ."
        },
        "Dynamic plenoctree for adaptive sampling refinement in explicit nerf": {
          "authors": [
            "Haotian Bai",
            "Yiqi Lin",
            "Yize Chen",
            "Lin Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Bai_Dynamic_PlenOctree_for_Adaptive_Sampling_Refinement_in_Explicit_NeRF_ICCV_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human 8794",
          "ref_ids": [
            "29"
          ],
          "1": "Introduction Rendering photo-realistic scenes and objects is crucial for providing users with an immersive and interactive experience in virtual reality [11, 45] and metaverse [19, 29, 50]."
        },
        "Entity-nerf: Detecting and removing moving entities in urban scenes": {
          "authors": [
            "Takashi Otonari",
            "Satoshi Ikehata",
            "Kiyoharu Aizawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Otonari_Entity-NeRF_Detecting_and_Removing_Moving_Entities_in_Urban_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021.2",
          "ref_ids": [
            "25"
          ],
          "1": "To address this issue, subsequent research has proposed methods that either explicitly learn scene dynamics by category-specific methods [6, 8, 14, 15, 25, 26, 30, 34, 37, 46, 50, 57], detection [10, 22], deformation [18, 23, 24, 27, 41, 44, 48, 54, 56], flow [4, 5, 7, 12, 55], multiple synchronized videos [11, 47, 58], depth-based approaches [52], or treat moving objects as outliers in a robust approach [33]."
        },
        "Implicit neural head synthesis via controllable local deformation fields": {
          "authors": [
            "Chuhan Chen",
            "Matthew O",
            "Gaurav Bharaj",
            "Pablo Garrido"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Implicit_Neural_Head_Synthesis_via_Controllable_Local_Deformation_Fields_CVPR_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303. IEEE, 2021. 4",
          "ref_ids": [
            "37"
          ],
          "1": "Local Deformation Fields Inspired by advances in part-based implicit rigging [33, 37, 67, 70], we overcome limitations of previous work by decomposing the global deformation field into multiple local fields, each centered around a pre-defined facial landmark location, to model non-linear local expression deformations with higher level of details, as shown in Fig."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "27. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "27"
          ],
          "1": "Following the training setup in previous work [27,29,31], we use 4 cameras for training and the rest 19 cameras for testing."
        },
        "RoGUENeRF: a robust geometry-consistent universal enhancer for NeRF": {
          "authors": [
            "S Catley-Chandar",
            "R Shaw",
            "G Slabaugh"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73254-6_4",
          "ref_texts": "30. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "30"
          ],
          "1": "The NeRF paradigm has been very popular in recent years [53], with active research in the field proposing new functionalities [27,30,31,48], applications [10,21,24] and also tackling some of the open challenges present in [23]."
        },
        "Clothed human performance capture with a double-layer neural radiance fields": {
          "authors": [
            "Kangkan Wang",
            "Guofeng Zhang",
            "Suxu Cong",
            "Jian Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021. 2, 3, 4, 5",
          "ref_ids": [
            "24"
          ],
          "1": "Clothed humans can be reconstructed through implicit representation-based methods such as voxel representation [33, 42], implicit function [27, 28], or neural radiance fields (NeRFs) [20, 24, 25].",
          "3": "Double-layer NeRFs for Dynamic Humans Unlike a single NeRFs for clothed humans [17,24,25,34, 41], we represent the clothing with an independent NeRFs on the body, which forms a double-layer NeRFs.",
          "4": "The color network Fc in canonical frame is formulated as, ci(x)= Fc(\u03b3x(x),\u03d5i), (3) where \u03d5i is an appearance latent code [24, 25] for framei.",
          "5": ", peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) as [24, 34]."
        },
        "Animal avatars: Reconstructing animatable 3D animals from casual videos": {
          "authors": [
            "R Sabathier",
            "NJ Mitra",
            "D Novotny"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72986-7_16",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Int. Conf. Comput. Vis. pp. 14294\u201314303 (2021)",
          "ref_ids": [
            "32"
          ],
          "1": "Additionally, there are several works [5,11,13,32] targeting human reconstruction with texture from monocular and multi-view sources achieving high rendering quality by leveraging off-the-shelf human pose estimation models."
        },
        "TexVocab: texture vocabulary-conditioned human avatars": {
          "authors": [
            "Yuxiao Liu",
            "Zhe Li",
            "Yebin Liu",
            "Haoqian Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_TexVocab_Texture_Vocabulary-conditioned_Human_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 6",
          "ref_ids": [
            "44"
          ],
          "2": "On the other end of the spectrum, lots of works focus on creating animatable textured avatars from RGB videos [21, 22, 24, 26, 29, 44, 64, 71, 72].",
          "6": "Pioneer works like AniNeRF [44], ARAH [64] and PoseV ocab [26] assign global latent codes or jointstructured feature lines as the pose conditions.",
          "9": "Then we compare our method against other approaches, such as TA V A [24], ARAH [64], AniNeRF [44], PoseV ocab [26] and NeuralActor [29]."
        },
        "RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control": {
          "authors": [
            "Xiang Deng",
            "Zerong Zheng",
            "Yuxiang Zhang",
            "Jingxiang Sun",
            "Chao Xu",
            "Xiaodong Yang",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Deng_RAM-Avatar_Real-time_Photo-Realistic_Avatar_from_Monocular_Videos_with_Full-body_Control_CVPR_2024_paper.html",
          "ref_texts": "[49] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "49"
          ],
          "1": "1996 tensive exploration of neural radiance fields (NeRF) [45] for acquiring a 3D neural representation of human avatars [5, 21, 23, 38, 39, 49, 57, 66, 68, 69].",
          "2": "[49] introduce a neural blend weight field, which recovers animatable human models by combining NeRF and 3D human skeletons."
        },
        "Neural radiance fields: Past, present, and future": {
          "authors": [
            "A Mittal"
          ],
          "url": "https://arxiv.org/abs/2304.10050",
          "ref_texts": "171. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323",
          "ref_ids": [
            "171"
          ],
          "1": "\u2022 Animatable Neural Radiance Fields [171] is an extension of NeRF that focuses on modeling dynamic human bodies by incorporating a parametric human body model into the neural radiance field framework."
        },
        "Gravitationally lensed black hole emission tomography": {
          "authors": [
            "Aviad Levis",
            "Pratul P. Srinivasan",
            "Andrew A. Chael",
            "Ren Ng",
            "Katherine L. Bouman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Levis_Gravitationally_Lensed_Black_Hole_Emission_Tomography_CVPR_2022_paper.html",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Our work is related to extensions of NeRF to dynamic scenes [17, 32, 38, 39, 45].",
          "2": "Previous methods use priors on deformation sparsity and rigidity [38, 45], pre-trained monocular depth estimation [32], or explicit models of human faces and bodies [17, 39] to relate 3D points across time."
        },
        "Slimmerf: Slimmable radiance fields": {
          "authors": [
            "S Yuan",
            "H Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550817/",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "45"
          ],
          "1": "Scene Representation with Radiance Fields Recently, models based on Neural Radiance Fields (NeRF) [38] have become popular in areas such as generative modelling [2, 3, 13, 29, 33\u201335, 41, 47, 48, 53, 61], pose estimation [8, 21, 30, 52, 57, 60, 64, 73], human body modelling [9,22,28,45,46,54,69,72], mobile 3D rendering [6,12, 25, 42, 49, 62, 63], and so on."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
          "authors": [
            "X Zhou",
            "S Peng",
            "Z Xu",
            "J Dong",
            "Q Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10401886/",
          "ref_texts": "[21] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021.",
          "ref_ids": [
            "21"
          ],
          "1": "A preliminary version of this work appeared in ICCV 2021 [21].",
          "2": "Similar to [21], [75], [76], [78], [80] leverage the LBS model to establish observation-to-canonical correspondences, which enables them to aggregate temporal observations in the input video.",
          "3": "We decompose the human motion into articulated and non-rigid deformations, which is represented by the LBS model [5], [21] and a neural displacement field, respectively.",
          "4": "Table 3 compares our method with [17], [21], [89] in terms of the P2S and CD metrics.",
          "5": "[17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRFPDF\u201d representations model the human geometry with the volume density field, while our \u201cSDF-PDF\u201d representation adopt the signed distance field.",
          "6": "We empirically set the threshold of volume density to extract the geometry of [17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRF-PDF\u201d.",
          "7": "Our representation \u201cSDF-PDF\u201d significantly outperforms baseline methods [17], [21], [89] by a margin of at least 0.",
          "8": "Our method adopts the network of [26], while the preliminary version of this work [21] uses the network of NeRF [6].",
          "9": "[21] S.",
          "10": "Following [21], we use three camera views for training and test on the remaining view.",
          "11": "[21] select video clips from the action \u201cPosing\u201d of S1, S5, S6, S7, S8, S9, and S11."
        },
        "SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image": {
          "authors": [
            "Yunhao Li",
            "Xiaodong Wang",
            "Ping Wang",
            "Xin Yuan",
            "Peidong Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_SCINeRF_Neural_Radiance_Fields_from_a_Snapshot_Compressive_Image_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "28"
          ],
          "1": "Others focus on developing NeRF which can deal with non-rigid object reconstruction [1, 9, 28, 29]."
        },
        "Flexnerf: Photorealistic free-viewpoint rendering of moving humans from sparse views": {
          "authors": [
            "Vinoj Jayasundara",
            "Amit Agrawal",
            "Nicolas Heron",
            "Abhinav Shrivastava",
            "Larry S. Davis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.html",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14294\u201314303, 2021. 1, 2",
          "ref_ids": [
            "27"
          ],
          "1": "Human-specific NeRFs have recently become popular for learning models using input videos [27, 40].",
          "2": "Hence, most methods [27, 28, 41] begin with assuming SMPL template as a prior [18]."
        },
        "Motion-oriented compositional neural radiance fields for monocular dynamic human modeling": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72933-1_27",
          "ref_texts": "50. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "50"
          ],
          "1": "We present our extensive results on ZJU-MoCap [52] and MonoCap [17,18,50] to demonstrate the effectiveness of the proposed MoCo-NeRF in learning photo-realistic representation and modeling complex non-rigid motions from monocular videos.",
          "2": "In recent days, there have been many works learning an implicit human representation for novel pose synthesis [22,23, 25,27,34,50,56,64,71,87\u201389] or for a novel-view synthesis [27,28,31,37,51,52, 64,73,74,78,85,86].",
          "4": "Experiments We conduct extensive experiments on ZJU-MoCap [52] and MonoCap [17,18,50] to verify the effectiveness of our proposed approach on both singleand multisubject settings."
        },
        "Dual-space nerf: Learning animatable avatars and scene lighting in separate spaces": {
          "authors": [
            "Y Zhi",
            "S Qian",
            "X Yan",
            "S Gao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044388/",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 5, 6, 7, 11, 12",
          "ref_ids": [
            "19"
          ],
          "2": "Animatable NeRF [19] resolves novel-pose synthesis by mapping observed points into a canonical space with inverse linear blend skinning (LBS).",
          "4": "AniNeRF [19] learns a neural blending weight field to learn the LBS weights for each particular pose.",
          "10": "5, our method produces fewer artifacts than AniNeRF [19], indicating a better correspondences across frames.",
          "15": "Our method exceeds AniNeRF [19], which explicitly builds correspondences across frames like our method, by a large margin in all metrics.",
          "17": "Our method outperforms AniNeRF [19] and is comparable to Neural Body [20] on the perceptual metric."
        },
        "Generalizable neural human renderer": {
          "authors": [
            "M Masuda",
            "J Park",
            "S Iwase",
            "R Khirodkar"
          ],
          "url": "https://arxiv.org/abs/2404.14199",
          "ref_texts": "74. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "74"
          ],
          "1": "Following its developments, many NeRF-based animatable human rendering methods have been proposed for both body part rendering[7,15,18,26,43,95]andfullbodyrendering[17,37,38,47,55,63,68,74,75,86,89\u201391,98,100,104,107,112,115]."
        },
        "Ghunerf: Generalizable human nerf from a monocular video": {
          "authors": [
            "C Li",
            "J Lin",
            "GH Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550750/",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani11 matable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "To model the motion of human body, existing human NeRFs [34, 41, 30, 23, 14, 29, 39] rely on human-prior information, such as a skeleton or a parametric model.",
          "2": "The other line of works [41, 23, 14, 29, 39] map all observations to a shared canonical space to model the large deformation of human bodies.",
          "3": "We follow previous works [29, 13, 2] to model the blending weights by leveraging prior knowledge from the SMPL model."
        },
        "iVS-Net: learning human view synthesis from internet videos": {
          "authors": [
            "Junting Dong",
            "Qi Fang",
            "Tianshuo Yang",
            "Qing Shuai",
            "Chengyu Qiao",
            "Sida Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3",
          "ref_ids": [
            "36"
          ],
          "1": "four views), recent works [37, 36] have achieved photo-realistic novel view synthesis based on the neural radiance field (NeRF) [32]i na per-scene optimization setting.",
          "2": "Following the NeRF [31] that represents the scene as a neural radiance field, some works propose to overfit a multi-view human video via per-scene optimization [37, 36, 28, 34].",
          "3": "To improve the generalization ability to novel human poses, [36, 28, 34, 7] propose to define the human model in the canonical space and build the correspondence between canonical space and observation space.",
          "4": "Human model Based on the constructed feature of the sample points, we aim to reconstruct the 3D human model, which is represented as the neural radiance field similar to [37, 36]."
        },
        "NDF: Neural deformable fields for dynamic human modelling": {
          "authors": [
            "R Zhang",
            "J Chen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_3",
          "ref_texts": "24. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "24"
          ],
          "2": "To further simplify the learning of the deformation fields, Animatable NeRF [24] resorts to a parametric human body model as a strong geometry prior to the deformation fields.",
          "3": "Animatable NeRF [24] can animate the performer to novel poses however it requires fine-tuning on the novel pose frames.",
          "4": "1 SMPL as Projection Reference with Non-linear Deformation To decrease NeRF\u2019s high requirement of camera numbers, a typical solution is to learn a deformation function \u03a6t(x) : R3 7\u2192 R3 to map sample points x in frame t to a shared canonical space [24] [26].",
          "5": "Following typical protocols [20] and works most related to us [24] [25], we evaluate our method on image synthesis using two metrics: peak signal-tonoise ratio (PSNR) and structural similarity index (SSIM).",
          "6": "2 Performance on NVS and NPS We compare our method with state-of-the-art view synthesis methods [25,24] that also use SMPL models and can handle dynamic scenes.",
          "7": "Animatable NeRF [24] predicts the blend weights for each sample point and aggregates observations across frames to a shared canonical representation and further improves on novel pose synthesis by fine-tuning on novel pose images.",
          "8": "Table 1 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on ZJU-MoCap dataset.",
          "9": "Our method outperforms Animatable NeRF [24] by a margin of 0.",
          "10": "Figure 3 presents the qualitative comparison of our method with [25,24] on the ZJU-MoCap dataset.",
          "14": "Table 2 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on novel pose synthesis.",
          "15": "Note that Animatable NeRF [24] needs to be fine-tuned on novel pose images while our method can be directly applied to novel pose synthesis."
        },
        "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting": {
          "authors": [
            "R Shaw",
            "M Nazarczuk",
            "J Song",
            "A Moreau"
          ],
          "url": "https://arxiv.org/abs/2312.13308",
          "ref_texts": "45. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021)",
          "ref_ids": [
            "45"
          ],
          "1": "Such an approach has been popular [12,13,28,32,33,44], and has also been used for dynamic human reconstruction [45,74]."
        },
        "Avatarone: Monocular 3d human animation": {
          "authors": [
            "Akash Karthikeyan",
            "Robert Ren",
            "Yash Kant",
            "Igor Gilitschenski"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 1, 2",
          "ref_ids": [
            "44"
          ],
          "1": "While some other works [21, 44, 57] utilizes the blend weights from template models, such as SMPL [31].",
          "2": "Several studies have proposed controllable animatable NeRFs [29, 38, 44, 45, 56, 64], introducing an array of techniques such as pose-dependent radiance fields, latent codes anchored on deformable meshes, and transformation optimization between view and canonical space.",
          "3": "With the advent of NeRFs, new methods have proposed modeling human geometry as radiance fields [10, 21, 23, 41, 42, 44, 45, 64, 78] or distance functions [59, 67], offering more flexibility and improved rendering quality."
        },
        "Moda: Modeling deformable 3d objects from casual videos": {
          "authors": [
            "C Song",
            "J Wei",
            "T Chen",
            "Y Chen",
            "CS Foo",
            "F Liu"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-024-02310-5",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "38"
          ],
          "2": "To extend NeRF to dynamic scenes, recent methods [40, 35, 36, 66, 38] introduce a canonical neural radiance field that models the shape and appearance, and a deformation model that achieves 3D point transformation between the observation space and the canonical space.",
          "4": "With the popularity of neural radiance fields [29], there are many works [25, 39, 38, 32, 51, 57, 11, 36, 35, 71, 3, 50, 19, 21] learning to reconstruct the shape and appearance from images or videos with a NeRF-based template."
        },
        "Neural novel actor: Learning a generalized animatable neural representation for human actors": {
          "authors": [
            "Q Gao",
            "Y Wang",
            "L Liu",
            "L Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10221769/",
          "ref_texts": "[44] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14314\u201314323, 2021.",
          "ref_ids": [
            "44"
          ],
          "2": "Leveraging the Skinned Multi-Person Linear (SMPL) model [36], several works [3], [6], [31], [44], [56] manage to obtain an animatable neural representation for humans.",
          "3": "To evaluate the performance of this task, we compare our method with Neural Body (NB), Animatable Nerf (AN) [44], Neural Actor (NA) [31] and MPS-NeRF (MPS) [17] on the ZJU-MoCap dataset.",
          "6": "Our method achieves the best performance in two metrics, compared to three person-specific animatable human models, NeuralBody (NB) [45], AnimatbleNerf (AN) [44], and Neural Actor (NA) [31] and MPS-NeRF (MPS) [17]."
        },
        "Recovering a Molecule's 3D Dynamics from Liquid-phase Electron Microscopy Movies": {
          "authors": [
            "Enze Ye",
            "Yuhang Wang",
            "Hong Zhang",
            "Yiqin Gao",
            "Huan Wang",
            "He Sun"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ye_Recovering_a_Molecules_3D_Dynamics_from_Liquid-phase_Electron_Microscopy_Movies_ICCV_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "26"
          ],
          "1": "It has achieved remarkable success in graphical rendering of static and dynamic scenes from limited views[5, 13, 25, 26, 27, 36], as well as in scientific imaging applications, including clinical X-ray CT[4], surgical endoscopy[39], optical microscopy[1, 14], and black hole emission tomography[10]."
        },
        "UNIF: United neural implicit functions for clothed human reconstruction and animation": {
          "authors": [
            "S Qian",
            "J Xu",
            "Z Liu",
            "L Ma",
            "S Gao"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_8",
          "ref_texts": "[28] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "28"
          ],
          "2": "2 Human Body Reconstruction and Animation As the most popular mesh-based human body model, SMPL [17] and its variations [12, 30, 27] dominate the area of human body reconstruction for its expressiveness and flexibility, supporting innumerable downstream task [15, 26, 2, 9, 29, 28, 14]."
        },
        "Neural puppeteer: Keypoint-based neural rendering of dynamic shapes": {
          "authors": [
            "Simon Giebenhain",
            "Urs Waldmann",
            "Ole Johannsen",
            "Bastian Goldluecke"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.html",
          "ref_texts": "42. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "42"
          ],
          "5": "Compared to A-NeRF [52] and AniNeRF [42] the fundamental differences in the rendering pipeline result in a significant speed increase."
        },
        "Humannerf-se: A simple yet effective approach to animate humannerf with diverse poses": {
          "authors": [
            "Caoyuan Ma",
            "Lun Liu",
            "Zhixiang Wang",
            "Wu Liu",
            "Xinchen Liu",
            "Zheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Ma_HumanNeRF-SE_A_Simple_yet_Effective_Approach_to_Animate_HumanNeRF_with_CVPR_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, 2021.1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "40"
          ],
          "6": "We also take Ani-NeRF [40] as one of the baselines because this work presents SMPL-based neural blend weight that can better generalize novel poses."
        },
        "Cat-nerf: Constancy-aware tx2former for dynamic body modeling": {
          "authors": [
            "Haidong Zhu",
            "Zhaoheng Zheng",
            "Wanrong Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 3, 4, 5, 6, 7, 11, 12",
          "ref_ids": [
            "32"
          ],
          "1": "To align the points between these two spaces, researchers use SE(3) [29] or a translation vector field [32,35] for building correspondences between the canonical and observation spaces.",
          "2": "To solve this problem, we separate appearance constancy and uniqueness between the frames based on the neural blend weight fields [32] with Constancy Awareness Tx2Former, abbreviated as CAT-NeRF.",
          "3": "Recently some papers [20, 27, 29, 32, 33, 35] introduce decomposing the neural radiance from the observation space to canonical space for modeling the movement of an object.",
          "4": "By predicting the correspondences in the canonical space [29,32,35], deformable NeRF finds the connection between every point in the observation space and the canonical space and uses the moving object in the video for the construction of a unique object.",
          "5": "To animate the body shapes and render them in the scene, animatable NeRF, different from the deformable methods, projects the body shape into a canonical space [18, 32, 33, 49] or a common shape shapes [34, 36] for projecting the human body shape from different frames into a shared shape or space.",
          "6": "SNARF [4] and Animatable NeRF [32] utilize the neural blend weight field with frame-level correction for building such correspondence with statistical methods, while TA V A [18] uses the linear blend skinning (LBS) and apply a constant change for modeling muscles and clothing dynamics.",
          "7": "We first briefly review Animatable NeRF [32] in 3.",
          "8": "Animatable NeRF for Human Modeling To represent a dynamic scene or object in the video, Animatable NeRF [32] constructs two spaces: one observation space representing the shape we observe for each individual frame and one canonical space shared by all the frames describing the same object with a default pose.",
          "9": "Objective To train the model, we follow [32] to build the objective function for training \u03c8c, \u03c8u i , Fconst \u2206w , Funique \u2206w , G(\u00b7), F\u03c3 and Fc jointly.",
          "10": "3, we follow [32] for establishing Lnsf to minimize the difference.",
          "11": "We follow [32] to select the frames and generate the splits for training and inference in our experiment.",
          "12": "In our experiment, we follow [32] to select the videos from subjects S1, S5, S6, S7, S8, S9 and S11.",
          "13": "We follow [32] to use the videos in four categories, \u201cTwirl\u201d, \u201cTaichi\u201d, \u201cWarmup\u201d, and \u201cPunch1\u201d in our experiment.",
          "14": "To train the network for novel camera viewpoints, we follow [32] to implement our network.",
          "15": "For both training steps, we use the Adam optimizer [17] and set the initial learning rate as 5e \u2212 4 and decay it to 1 10 after 1,000 epochs with an exponential training scheduler following [32].",
          "16": "In our experiment, we compare our method with Neural Texture [44], NHR [50] and Animated NeRF [32].",
          "17": "We follow [32] to use the body shape reconstruction from SMPL [22] as the input for Neural Texture [44] as the coarse mesh to render the image and use the points sampled from the SMPL body shape reconstruction as the input for NHR.",
          "18": "For the results in the tables, NT is the result for Neural Texture [44] and AN is the result from Animatable NeRF [32].",
          "19": "For the results on the novel view setting shown in Table 1, we outperform the state-of-the-art baseline method, Animatable NeRF [32], on all splits in the dataset for both metrics we assessed.",
          "20": "In addition to the Neural Texture [44], NHR [50] and Animatable NeRF [32], we also compared with D-NeRF [35] for reconstruction."
        },
        "Free-viewpoint rgb-d human performance capture and rendering": {
          "authors": [
            "P Nguyen-Ha",
            "N Sarafianos",
            "C Lassner"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_27",
          "ref_texts": "50. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021) 1, 4",
          "ref_ids": [
            "50"
          ],
          "2": "Given multi-view input frames or videos, recent works on rendering animate humans from novel views show impressive results [46,50,51,66]."
        },
        "Human 3d avatar modeling with implicit neural representation: A brief survey": {
          "authors": [
            "M Sun",
            "D Yang",
            "D Kou",
            "Y Jiang",
            "W Shan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10218567/",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3, 4, 7",
          "ref_ids": [
            "38"
          ],
          "3": "[38] develop NeRF for the re-animation of an avatar."
        },
        "Nephi: Neural deformation fields for approximately diffeomorphic medical image registration": {
          "authors": [
            "L Tian",
            "H Greer",
            "RS Jos\u00e9 Est\u00e9par",
            "R Sengupta"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73223-2_13",
          "ref_texts": "35. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "2 Neural Deformation Models Functional representations of deformation fields (parameterized via MLPs) for natural images in dynamic scenes [14,24,25,25,33,34,36,51], for dynamic objects [13,23,31, 47, 56], and for animatable humans [8, 15, 26, 35, 40, 59, 61, 62] have attracted significant recent interest."
        },
        "A comprehensive benchmark for neural human radiance fields": {
          "authors": [
            "K Liu",
            "D Jin",
            "A Zeng",
            "X Han"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6e566c91d381bd7a45647d9a90838817-Abstract-Datasets_and_Benchmarks.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "37"
          ],
          "1": "The extended NeRFs for humans [38, 49, 12, 54, 10, 37, 32, 8, 24] follow similar pathways of development.",
          "2": "While these methods progress greatly, some emerging problems should be taken seriously.",
          "3": "Among these variants, NeRFs for human body rendering [38, 49, 12, 54, 10, 37, 32, 8, 24] have attracted a lot of attention due to their broad applications.",
          "4": "Scene-specific methods [38, 37, 47, 49, 54, 10, 20] for human body rendering require only sparse view videos for training as different video frames can be treated equivalent to dense view images by exploiting the human body prior.",
          "5": "To strengthen the performance of novel pose rendering, some works [37, 54] introduce additional constraints to learn more reasonable skinning weights."
        },
        "Vaxnerf: Revisiting the classic for voxel-accelerated neural radiance field": {
          "authors": [
            "N Kondo",
            "Y Ikeda",
            "A Tagliasacchi",
            "Y Matsuo"
          ],
          "url": "https://arxiv.org/abs/2111.13112",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1",
          "ref_ids": [
            "51"
          ],
          "1": "Many extensions and applications quickly ensued, including dynamic scene rendering [16, 31, 47, 50, 51, 67], controllable relighting [3, 61, 77, 87], latent appearance and shape priors [6, 57, 68], scene composition [21, 45, 49, 79], and pose estimation [24, 32, 38, 62, 82]."
        },
        "Localised-NeRF: Specular Highlights and Colour Gradient Localising in NeRF": {
          "authors": [
            "Dharmendra Selvaratnam",
            "Dena Bazazian"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NRI/html/Selvaratnam_Localised-NeRF_Specular_Highlights_and_Colour_Gradient_Localising_in_NeRF_CVPRW_2024_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "34"
          ],
          "1": "Subsequent models have extended NeRF\u2019s application to dynamic scenes [33], avatar animation [34], and phototourism [26], by focusing on improving view-dependent appearance and geometry\u2019s smoothness."
        },
        "High-fidelity eye animatable neural radiance fields for human face": {
          "authors": [
            "H Wang",
            "Z Zhang",
            "Y Cheng",
            "HJ Chang"
          ],
          "url": "https://arxiv.org/abs/2308.00773",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "28"
          ],
          "1": "Further explorations [11, 17, 26, 28, 29] adapt NeRF to represent dynamic scenes."
        },
        "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters": {
          "authors": [
            "M Sun",
            "J Chen",
            "J Dong",
            "Y Chen",
            "X Jiang",
            "S Mao"
          ],
          "url": "https://arxiv.org/abs/2411.17423",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "32"
          ],
          "1": "However, human-centered 3D reconstruction methods focus on high-fidelity digitization and reconstruction of clothed humans from minimal inputs [7, 10, 11, 14, 16, 17, 32, 40, 41, 53\u201355, 70]."
        },
        "Placing human animations into 3d scenes by learning interaction-and geometry-driven keyframes": {
          "authors": [
            "James F. Mullen",
            "Divya Kothandaraman",
            "Aniket Bera",
            "Dinesh Manocha"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14294\u201314303, Montreal, QC, Canada, Oct. 2021. IEEE.",
          "ref_ids": [
            "38"
          ],
          "1": "[38, 34, 58, 47] worked towards extending NeRF to articulated objects like humans."
        },
        "Wild2avatar: Rendering humans behind occlusions": {
          "authors": [
            "T Xiang",
            "A Sun",
            "S Delp",
            "K Kozuka",
            "L Fei-Fei"
          ],
          "url": "https://arxiv.org/abs/2401.00431",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "40"
          ],
          "1": "While past works were able to achieve good quality renderings of humans from dense [5, 10, 54] and sparse [14, 25, 27, 40, 41, 47, 55] camera views, a recent research focus involves rendering a moving human from a single camera angle [1, 2, 9, 11, 15, 17, 18, 20, 44, 51].",
          "2": "With the above transformations, we are able to first optimize a static neural field for the dynamic human in the canonical space and then deform the ray samples to the observation space for volume rendering [39, 40, 42]."
        },
        "Meil-nerf: Memory-efficient incremental learning of neural radiance fields": {
          "authors": [
            "J Chung",
            "K Lee",
            "S Baik",
            "KM Lee"
          ],
          "url": "https://arxiv.org/abs/2212.08328",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "43"
          ],
          "1": "Among the attempts, neural radiance fields (NeRF) [36] has emerged as one of promising methods for its high reconstruction quality, spurring a large number of research works on improvements [3, 4, 9, 18, 22, 27, 28, 33, 37, 44, 52\u201355, 57\u201359] and a wide range of applications [56]: controllable human avatar [10, 29, 43], realistic game [17], robotics [20, 41, 50, 62], 3D-aware image generation [6, 7], and data compression [13]."
        },
        "NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via Generative Prior": {
          "authors": [
            "G Kim",
            "K Seo",
            "S Cha",
            "J Noh"
          ],
          "url": "https://arxiv.org/abs/2405.05749",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "43"
          ],
          "1": "Furthermore, by embedding facial features such as landmarks and parameters of a facial model [6, 14, 15, 18, 22, 43, 62, 72] or audio features [20, 30, 32, 49, 56, 67, 69], these methods facilitate the representation of dynamics in the desired direction through the learned features."
        },
        "Dynamic NeRFs for soccer scenes": {
          "authors": [
            "S Lewin",
            "M Vandegar",
            "T Hoyoux",
            "O Barnich"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3606038.3616158",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14314\u201314323.",
          "ref_ids": [
            "28"
          ],
          "1": "They often work by learning the motion of a skinned multi-person linear model (SMPL [22]) along with its appearance [28, 29, 42]."
        },
        "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction": {
          "authors": [
            "L Qiu",
            "S Zhu",
            "Q Zuo",
            "X Gu",
            "Y Dong",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2412.02684",
          "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "54"
          ],
          "1": "Other works focus on animatable avatar reconstruction from monocular videos [31, 43, 58, 72] or multi-view videos [13, 41, 54]."
        },
        "Generalizable neural voxels for fast human radiance fields": {
          "authors": [
            "T Yi",
            "J Fang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://arxiv.org/abs/2303.15387",
          "ref_texts": "[58] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 4",
          "ref_ids": [
            "58"
          ],
          "1": "Some works [60, 86, 72, 12, 59, 58, 15, 33, 89] successfully apply NeRF methods to human body rendering frameworks.",
          "2": "To solve this problem, Animatable NeRF [58, 59] maps human poses from the observation space to the predefined canonical space.",
          "3": "Representing Canonical Bodies with Neural Voxels Most previous NeRF-based methods for human bodies [60, 86, 72, 12, 59, 58, 15, 33, 89] adopt purely implicit representations."
        },
        "Dynamic nerf: A review": {
          "authors": [
            "J Lin"
          ],
          "url": "https://arxiv.org/abs/2405.08609",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "53"
          ],
          "1": "[53] also proposed a reconstruction method of dynamic human body model.",
          "2": "[53] proposed a novel method developed based on traditional human armature animation deformation, which is abbreviated as Skeleton-NeRF in this paper."
        },
        "Human image generation: A comprehensive survey": {
          "authors": [
            "Z Jia",
            "Z Zhang",
            "L Wang",
            "T Tan"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3665869",
          "ref_texts": "[127] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. InICCV. 14314\u201314323.",
          "ref_ids": [
            "127"
          ],
          "1": "Subsequent works [23, 36, 127] leverage NeRF to predict the color and density of 3D human bodies."
        },
        "Deformable NeRF using Recursively Subdivided Tetrahedra": {
          "authors": [
            "Z Qiu",
            "C Ren",
            "K Song",
            "X Zeng",
            "L Yang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681019",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV.",
          "ref_ids": [
            "29"
          ],
          "1": "NeRF has found extensive applications in versatile computer graphics and computer vision tasks, such as human avatar creation [8, 29], pose estimation [2, 6, 21], reconstruction [3, 33], robotics [1, 20] and simulation [18, 19]."
        },
        "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale Environments": {
          "authors": [
            "Leif Van",
            "Patrick Stotko",
            "Stefan Krumpen",
            "Reinhard Klein",
            "Michael Weinmann"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Van_Holland_Efficient_3D_Reconstruction_Streaming_and_Visualization_of_Static_and_Dynamic_ICCVW_2023_paper.html",
          "ref_texts": "[118] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV. 2021. 3",
          "ref_ids": [
            "118"
          ],
          "1": "In particular, this includes implicit scene representations based on Neural Radiance Fields (NeRFs) [97] and respective extensions towards speeding up model training [125, 39, 16, 26, 10, 164, 147, 105, 37, 9, 11, 188, 179, 102] with training times of seconds, the adaptation to unconstrained image collections [94, 13, 63], deformable scenes [115, 123, 43, 158, 124, 111, 160, 118, 116, 12, 87, 58, 83, 37] and video inputs [82, 174, 30, 119, 44, 81, 151, 79], the refinement or complete estimation of camera pose parameters for the input images [181, 165, 146, 20, 192, 191, 130, 187, 95, 84, 57, 173, 90, 6, 17, 15, 14, 52, 148, 86], combining NeRFs with semantics regarding objects in the scene [163, 189, 40], incorporating depth cues [166, 26, 128, 126, 3] to guide the training and allow handling textureless regions, handling large-scale scenarios [150, 161, 96], and streamable representations [18, 149]."
        },
        "Inv: Towards streaming incremental neural videos": {
          "authors": [
            "S Wang",
            "A Supikov",
            "J Ratcliff",
            "H Fuchs"
          ],
          "url": "https://arxiv.org/abs/2302.01532",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "Some recent works [15, 21, 32, 33, 40] focus on animating clothed humans only."
        },
        "PGAHum: prior-guided geometry and appearance learning for high-fidelity animatable human reconstruction": {
          "authors": [
            "H Wang",
            "Q Xu",
            "H Chen",
            "R Ma"
          ],
          "url": "https://arxiv.org/abs/2404.13862",
          "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "33"
          ],
          "1": "The skeleton pose has also been used to define a pose-driven deformation field [33, 34] for the observation-to-canonical space transformation so that the geometry and color can be optimized in the canonical space.",
          "2": "NB[35] GT Ours Ani-NeRF[33] ARAH[40] InstantNVR[6] Figure 3: Qualitative results on ZJU-MoCap dataset for novel view synthesis on training poses.",
          "5": "086 Example Input Frame Ours ARAH[40] NB[35] Ani-NeRF[33] A-NeRF[38] GT Figure 4: Qualitative results on ZJU-MoCap dataset for geometry reconstruction."
        },
        "Explicifying neural implicit fields for efficient dynamic human avatar modeling via a neural explicit surface": {
          "authors": [
            "R Zhang",
            "J Chen",
            "Q Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611707",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "28"
          ],
          "1": "On the other hand, other methods [28, 39, 40, 44] aggregate information across video frames and demonstrate better ability to handle novel poses.",
          "4": "AniSDF is an extended version of AniNeRF [28] that replaces the neural radiance fields in AniNeRF with signed distance fields, leading to better performance."
        },
        "Canonical fields: Self-supervised learning of pose-canonicalized neural fields": {
          "authors": [
            "Rohith Agaram",
            "Shaurya Dewan",
            "Rahul Sajnani",
            "Adrien Poulenard",
            "Madhava Krishna",
            "Srinath Sridhar"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/html/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1",
          "ref_ids": [
            "33"
          ],
          "1": "In particular, neural radiance fields (NeRFs) [24], have been successfully used in problems such as novel view synthesis [3, 4, 66], scene geometry extraction [55, 61], capturing dynamic scenes [19, 29, 30, 33, 52], 3D semantic segmentation [53, 68], and robotics [1, 13, 21]."
        },
        "Crim-gs: Continuous rigid motion-aware gaussian splatting from motion blur images": {
          "authors": [
            "J Lee",
            "D Kim",
            "D Lee",
            "S Cho",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2407.03923",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "37"
          ],
          "1": "NeRF has led to a wide range of studies, including 3D mesh reconstruction [22, 46, 49, 50], dynamic scene [20, 21, 31, 32, 38, 47], and human avatars [14, 37, 53]."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[77] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "77"
          ],
          "1": "In addition to the textured mesh, NeRFs [16] also became a useful representation for photo-realistic clothed avatar reconstruction [34], [35], [38], [77], [78], [79], [80], [81], [82], [83], [84] from monocular or multi-view videos.",
          "2": "Animatable NeRF [77] further proposed a pose-dependent deformation network to fit the clothed human conditioned on human."
        },
        "Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos": {
          "authors": [
            "S Jeon",
            "I Cho",
            "M Kim",
            "WO Cho",
            "SJ Kim"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-72684-2_23.pdf",
          "ref_texts": "33. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "33"
          ],
          "1": "Recent methods [19,33,51,52] have suggested an alternative yet effective approach for general users: building animatable models from casually captured videos.",
          "5": "Recent advances in NeRF have also spurred active research in these approaches [19,23,33,34,36,43,52]."
        },
        "Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering": {
          "authors": [
            "Chuanyue Shen",
            "Letian Zhang",
            "Zhangsihao Yang",
            "Masood Mortazavi",
            "Xiyun Song",
            "Liang Peng",
            "Heather Yu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021. 1, 2",
          "ref_ids": [
            "24"
          ],
          "1": "It catalyzes a wave of human neural rendering methods that deliver high fidelity results [9,15,24,26,32,33].",
          "2": "Due to its high-quality performance while being simple and extendable, the use of NeRF as a core algorithm has been widely explored in a variety of scene representation and rendering tasks, such as pose estimation [24,26,29,32], lighting [1,2,37], scene labeling and understanding [30,39], and scene composition [23, 34].",
          "3": "Animatable NeRF [24] introduces a per-frame neural blend weight field to be combined with NeRF, while using human priors from SMPL to regularize the learned blend weight."
        },
        "Neural kaleidoscopic space sculpting": {
          "authors": [
            "Byeongjoo Ahn",
            "Michael De",
            "Ioannis Gkioulekas",
            "Aswin C. Sankaranarayanan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "26"
          ],
          "1": "The most successful among these work use class-specific priors for faces and the human body [17,26,27,32,35]."
        },
        "Hdhuman: High-quality human novel-view rendering from sparse views": {
          "authors": [
            "T Zhou",
            "J Huang",
            "T Yu",
            "R Shao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10168294/",
          "ref_texts": "[36] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proc. IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323. 2.2",
          "ref_ids": [
            "36"
          ],
          "1": "For human rendering from sparse views, some works [33], [34], [35], [36], [37] use SMPL template as a prior, which helps to constrain the motion space and improve the rendering quality."
        },
        "Efficient view synthesis with neural radiance distribution field": {
          "authors": [
            "Yushuang Wu",
            "Xiao Li",
            "Jinglu Wang",
            "Xiaoguang Han",
            "Shuguang Cui",
            "Yan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wu_Efficient_View_Synthesis_with_Neural_Radiance_Distribution_Field_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "These include the handling of dynamic scenes [10, 46, 27, 33, 28], human digitization [39, 14, 37, 31, 20], shape and appearance modeling [9, 17, 51, 5], 3D-aware synthesis [6, 8], and many others."
        },
        "A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields": {
          "authors": [
            "K Ye",
            "H Wu",
            "X Tong",
            "K Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10584300/",
          "ref_texts": "[31] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u2013",
          "ref_ids": [
            "31"
          ],
          "1": "dynamic scenes [32], human bodies [31] or illumination variations [27])."
        },
        "PAV: Personalized Head Avatar from Unstructured Video Collection": {
          "authors": [
            "A Caliskan",
            "B Kicanaoglu",
            "H Kim"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72940-9_7",
          "ref_texts": "27. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "27"
          ],
          "1": "For human body, a series of approaches have been proposed to target this challenge [20,27]."
        },
        "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time rendering of temporally complex dynamic scenes": {
          "authors": [
            "J Yan",
            "R Peng",
            "L Tang",
            "R Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681463",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "40"
          ],
          "1": "The photo-realistic view synthesis capability of NeRF has inspired a series of works across various domains, including enhancing rendering quality[4\u20136, 8, 54, 55, 72, 74], sparse inputs[3, 34, 63], surface representation and segmentation[38, 59, 79], accelerating training and rendering[6, 14, 19, 20, 33, 43, 44, 49, 52, 69, 73], as well as human modeling[40, 41, 60, 71, 80], among others."
        },
        "SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations": {
          "authors": [
            "Y Jiang",
            "Q Liao",
            "Z Wang",
            "X Lin",
            "Z Lu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10687388/",
          "ref_texts": "[16] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021.",
          "ref_ids": [
            "16"
          ],
          "1": "Recent works suggest that we can learn the deformation of a general character template from scanned data [11], [14] or RGB video data [15], [16] to get a drivable avatar directly.",
          "2": "Comparison with Baselines Additionally, We conduct comparisons with two baselines, Neural Body (NB) [29] and Ani-NeRF (AN) [16]."
        },
        "FastHuman: Reconstructing High-Quality Clothed Human in Minutes": {
          "authors": [
            "L Lin",
            "S Peng",
            "Q Gan",
            "J Zhu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550690/",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), pages 14294\u201314303, 2021. 1, 2, 7",
          "ref_ids": [
            "43"
          ],
          "1": "In order to model human avatars, some approaches [6, 43, 45, 60, 61] incorporate the estimated human skeleton and neural rendering to model animatable human avatars in an implicit Recovered Mesh Textured MeshReposed Mesh Input Video Figure 1.",
          "2": "[43, 45, 61] dynamically synthesize the human image."
        },
        "High-degrees-of-freedom dynamic neural fields for robot self-modeling and motion planning": {
          "authors": [
            "L Schulze",
            "H Lipson"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611047/",
          "ref_texts": "[11] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "11"
          ],
          "1": "Different from methods using deformation from a canonical representation [11], we propose a DOFencoder-based dynamic neural density field, which is suitable for modeling complex changing scenes beyond robotics."
        },
        "ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation": {
          "authors": [
            "H Li",
            "HX Yu",
            "J Li",
            "J Wu"
          ],
          "url": "https://arxiv.org/abs/2412.18600",
          "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "61"
          ],
          "1": "Neural rendering techniques have significantly advanced the synthesis of realistic human appearances [12, 36, 42, 44, 46, 54, 61, 62, 83].",
          "2": "AnimatableNeRF [61] introduces a neural blend weight field and achieves superior novel view and novel pose synthesis results."
        },
        "Efficient Integration of Neural Representations for Dynamic Humans": {
          "authors": [
            "W Li",
            "L Zeng",
            "C Gao",
            "N Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10666828/",
          "ref_texts": "[35] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "35"
          ],
          "5": "AN [35] uses neural blend weight fields with human skeletons for deformation fields, enabling the conversion between canonical and observation representations.",
          "6": "Additionally, compared to [1], [35], [4], our reconstruction results exhibit fewer noisy points."
        },
        "Neural texture puppeteer: A framework for neural geometry and texture rendering of articulated shapes, enabling re-identification at interactive speed": {
          "authors": [
            "Urs Waldmann",
            "Ole Johannsen",
            "Bastian Goldluecke"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Waldmann_Neural_Texture_Puppeteer_A_Framework_for_Neural_Geometry_and_Texture_WACVW_2024_paper.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, October 2021. 1, 2, 3, 4",
          "ref_ids": [
            "30"
          ],
          "2": "Good results can be achieved with NeRF-based [26] approaches [30, 38], approaches based on implicit neural representations [31, 36] or approximate differentiable rendering [47]."
        },
        "Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View": {
          "authors": [
            "D Lee",
            "D Kim",
            "J Lee",
            "M Lee",
            "S Lee",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2407.06613",
          "ref_texts": "[36] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "36"
          ],
          "1": "In addition, its implicit representation capability leads to explosive development of other graphical tasks such as modeling dynamic scenes [28]\u2013[31], relighting [32], [33], 3D reconstructions [34], [35], and human avatar [36]."
        },
        "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text": {
          "authors": [
            "G Shim",
            "S Lee",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2502.11642",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "26"
          ],
          "1": "Building on diverse 3D representations [16, 22, 38, 40], numerous studies have explored the reconstruction of 3D human avatars from various data sources, including 3D scans [35, 36, 48], video sequences [7, 26, 42], single images [9, 11], and even text [1, 17, 19, 21].",
          "2": "These studies draw inspiration from human deformation concepts derived from deformable neural representations [2, 26, 27, 42], which address how 3D coordinates on a human model are deformed across different poses."
        },
        "Semantic-preserved point-based human avatar": {
          "authors": [
            "L Lin",
            "J Zhu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S107731422500030X",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), pages 14294\u201314303, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "Also, there are some works that model dynamic humans from multiview videos [27,29]."
        },
        "Report on Methods and Applications for Crafting 3D Humans": {
          "authors": [
            "L Liu",
            "K Zhao"
          ],
          "url": "https://arxiv.org/abs/2406.01223",
          "ref_texts": "[60] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "60"
          ],
          "1": "Further advancements include deformable and animatable NeRFs, such as those proposed by previous works [60]\u2013[62], which synthesize humans from novel poses and views."
        },
        "Dreamo: articulated 3d reconstruction from a single casual video": {
          "authors": [
            "T Tu",
            "MF Li",
            "CH Lin",
            "YC Cheng",
            "M Sun"
          ],
          "url": "https://arxiv.org/abs/2312.02617",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "While some of these methods can generate plausible 3D models, they require specific inputs such as multi-view videos [36, 63], predefined 3D skeletons [36, 46, 49, 50], or 3D rest-pose point clouds [47]."
        },
        "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
          "authors": [
            "H Wang",
            "W Zhang",
            "S Liu",
            "X Zhou",
            "J Li",
            "Z Tang"
          ],
          "url": "https://arxiv.org/abs/2405.12477",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 6, 7",
          "ref_ids": [
            "34"
          ],
          "1": "Quantitative Results To verify the effectiveness of our method in solving the geometric distortion problem in reconstructing the human body, we compare our method with NeuralBody [35], HumanNeRF [43] AnimateNeRF [34], InstantNVR [5] InstantAvatar [15] GauHuman [11] on the ZJU dataset and the Monocap dataset, as shown in Table 1."
        },
        "Diversity-Aware Sign Language Production through a Pose Encoding Variational Autoencoder": {
          "authors": [
            "MI Lakhal",
            "R Bowden"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10581951/",
          "ref_texts": "[30] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the International Conference on Computer Vision (ICCV) , 2021.",
          "ref_ids": [
            "30"
          ],
          "1": "Human-NeRF [48], [30], [31] uses a human 3D mesh template (usually SMPL [24]) which is deformed with a given body pose and rendered to the target viewpoint using Volume Rendering [23]."
        },
        "Flnerf: 3d facial landmarks estimation in neural radiance fields": {
          "authors": [
            "H Zhang",
            "T Dai",
            "YW Tai",
            "CK Tang"
          ],
          "url": "https://arxiv.org/abs/2211.11202",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "53"
          ],
          "1": "Further studies [32, 67, 68, 30, 77, 59, 83, 39, 92, 56, 38, 72, 44, 23, 58, 88, 28, 12] have been done to improve the performance, efficiency and generalization of NeRF, with its variants quickly and widely adopted in dynamic scene reconstruction [52, 82, 37, 55, 18], novel scene composition [51, 89, 48, 25, 41, 85, 46, 35], articulated 3D shape reconstruction [86, 61, 78, 93, 31, 94, 84, 11, 49, 53], and various computer vision tasks, including face NeRFs [21, 5, 50, 69, 16, 29], the focus of this paper."
        },
        "Neural Radiance Fields with Torch Units": {
          "authors": [
            "B Ni",
            "H Wang",
            "D Bai",
            "M Weng",
            "D Qi",
            "W Qiu"
          ],
          "url": "https://arxiv.org/abs/2404.02617",
          "ref_texts": "[37] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H., 2021a. Animatable neural radiance fields for modeling dynamic human bodies, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14314\u201314323.",
          "ref_ids": [
            "37"
          ],
          "1": "Besides, neural radiance fields are also applied to digital human body [10, 14, 26, 37, 55, 59].",
          "2": "Besides, Animatable [37] introduces neural radiance fields to generate a deformation field and enables human body modeling."
        },
        "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training": {
          "authors": [
            "R Yin",
            "V Yugay",
            "Y Li",
            "S Karaoglu",
            "T Gevers"
          ],
          "url": "https://arxiv.org/abs/2411.02229",
          "ref_texts": "[25] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Bao, H., Zhou, X.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "25"
          ],
          "1": "Due to its capabilities, NeRF became widely adopted for 3D scene reconstruction [1, 18, 8, 34, 14], human body modeling [26, 25, 36, 16], robotics [41, 28], and medical imaging [7]."
        },
        "LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes": {
          "authors": [
            "Z Qu",
            "K Xu",
            "GP Hancke",
            "RWH Lau"
          ],
          "url": "https://arxiv.org/abs/2411.06757",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "36"
          ],
          "1": ", accelerating the training and rendering of NeRF [14, 18, 40, 41], handling dynamic scenes [21, 34, 39, 44, 61] and digital humans body [1, 34, 36, 37, 10] or human head [53, 68, 16] modeling, and the manipulation [4, 38, 28, 23] or generation [8, 15, 25, 33] of scene contents.",
          "2": "For example, [14, 18, 40, 41] are proposed to accelerate the NeRF training procedure, [21, 34, 39, 44, 61] are applied to render dynamic scenarios, [4, 38, 28, 23] are focused on the NeRF relighting methods, [1, 34, 36, 37, 53] are expanded to the non-rigid object rendering, [8, 15, 25, 33] are used for the generation models."
        },
        "TEDRA: Text-based Editing of Dynamic and Photoreal Actors": {
          "authors": [
            "B Sunagad",
            "H Zhu",
            "M Mendiratta",
            "A Kortylewski"
          ],
          "url": "https://arxiv.org/abs/2408.15995",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 4",
          "ref_ids": [
            "45"
          ],
          "1": "Introduction Digital avatars of real humans play a vital role in various applications, including augmented and virtual reality, gaming, movie production, and synthetic data generation [10, 11, 13, 29, 30, 35, 45, 66, 72].",
          "2": "To better model the pose-dependent appearance of humans, recent studies [10, 13, 29, 35, 45, 66, 72] incorporate motion-aware residual deformations in the canonicalized space."
        },
        "Representing Animatable Avatar via Factorized Neural Fields": {
          "authors": [
            "C Song",
            "Z Wu",
            "B Wandt",
            "L Sigal",
            "H Rhodin"
          ],
          "url": "https://arxiv.org/abs/2406.00637",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "Other methods [50, 44, 51, 52] aim to improve results with an image-to-image translation network and a per-frame latent code."
        },
        "Few-Shot Multi-Human Neural Rendering Using Geometry Constraints": {
          "authors": [
            "VF Abrevaya",
            "F Multon",
            "A Boukhayma"
          ],
          "url": "https://arxiv.org/abs/2502.07140",
          "ref_texts": "[73] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "73"
          ],
          "1": "For humans, this has been leveraged to obtain geometry and appearance from monocular video [12, 37], RGB-D video [19], and sparse multi-view video [46, 52, 73, 76, 91, 93, 99, 116].",
          "2": "between 2 and 15), where the lack of views and presence of wide baselines is compensated by tracking a pre-scanned template [10, 17, 23, 88, 97], using a parametric body model [6, 32], or more recently, by the use of deep learning [33, 46, 50, 52, 73, 76, 91, 93, 99].",
          "3": "For generating free-viewpoint video, image-based rendering has been considered as an alternative or complement to 3D reconstruction [10, 46, 52, 52, 73, 93, 98, 99]."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "149. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV , pp. 14294\u201314303 (2021).https://doi. org/10.1109/ICCV48922.2021.01405",
          "ref_ids": [
            "149"
          ],
          "3": "AnimatableNeRF [149] avails a consistency loss between the blend weights of the posed-to-canonical and canonical-to-posed transformations to optimize the canonical neural blend weight field and novel pose latent code.",
          "4": "Typically, a neural network is enlisted to predict the offset stemming from nonrigid pose-dependent deformation [10, 149, 152, 153, 163]."
        },
        "AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation": {
          "authors": [
            "M Li",
            "S Yao",
            "C Kai",
            "Z Xie",
            "K Chen",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2502.19441",
          "ref_texts": "[45] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "45"
          ],
          "1": "Extensions of NeRF [38] into dynamic scenes [39]\u2013[41] and methods for animatable 3D human models in multi-view scenarios [20], [43]\u2013[45], [58], [67] or monocular videos [8], [11], [14], [36] have shown promising results."
        },
        "Anipixel: Towards animatable pixel-aligned human avatar": {
          "authors": [
            "J Fan",
            "J Zhang",
            "Z Hou",
            "D Tao"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3612058",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV. 14314\u201314323.",
          "ref_ids": [
            "29"
          ],
          "2": "With the recent success of neural radiance field (NeRF) representation [23], a line of works has tried to reconstruct volumetric avatars in radiance field [29, 30, 44].",
          "4": "CV] 17 Oct 2023 MM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada Jinlong Fan, Jing Zhang, Zhi Hou, & Dacheng Tao human reconstruction, skeleton motion is often taken as prior to constraining the deformation field [29, 30, 50].",
          "5": "To account for dynamic humans, deformation fields are devised to deform the posed body in target space to canonical space, where the density and color are predicted [29, 30].",
          "6": "The other way is to predict the per-point weights using a neural network [17, 29, 44].",
          "8": "For each part, there are (a) the input three views and results of (b) AniNeRF [29], (c) NeuralBody [30], (d) MPS-NeRF [6], (e) our method, and (f) the ground truth.",
          "9": "893 Table 2: Comparison of our method with NeuralBody [30], AniNeRF [29], MPS-NeRF [6] on the Human3.",
          "10": "895 Table 3: Comparison of NeuralBody [30], AniNeRF [29], KeypointNeRF [22], and our method on ZJUMoCAP.",
          "11": "We predict two versions of masks, one is rendered by volume density accumulation, and the other is generated by minimum SDF rendering as in [29].",
          "12": "Following [6, 29], we conduct experiments on 7 subjects: S1, S5, S6, S7, S8, S9 and S11.",
          "13": "for the whole image, we follow previous methods [6, 29, 30] to project the 3D bounding box of the fitted SMPL mesh onto the image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "14": "We compare our method with recent two animatable methods, NeuralBody [30] and AniNeRF [29], and two generalizable methods, KeypointNeRF [22] and MPS-NeRF [6]."
        },
        "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene": {
          "authors": [
            "S Biswas",
            "Q Wu",
            "B Banerjee",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2409.17459",
          "ref_texts": "[7] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "7"
          ],
          "1": "In NeRF-based dynamic scene reconstruction, the focus has predominantly been on human reconstruction [7, 8, 9, 10], utilizing template models such as SMPL [11], and CAPE [12].",
          "5": "Also, we evaluate our method for single-object reconstruction, including arbitrary deformable entities, Human reconstruction: We compare our human surface reconstruction results with TA V A [28] and AnimatableNeRF [7] on the ZJU-MoCap dataset [9] (Tab.",
          "6": "SDF modeling contributes smoother surface reconstructions compared to models like [28, 7].",
          "8": "Ours TA V A [28] AnimatableNeRF[7]I/P Figure 4: Qualitative comparison on ZJUMocap dataset [9]."
        },
        "DiHuR: Diffusion-Guided Generalizable Human Reconstruction": {
          "authors": [
            "J Chen",
            "C Li",
            "GH Lee"
          ],
          "url": "https://arxiv.org/abs/2411.11903",
          "ref_texts": "[17] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 7",
          "ref_ids": [
            "17"
          ],
          "1": "Ani-NeRF [17] learns a canonical NeRF model and a backward LBS network which predicts residuals to the deterministic SMPLbased backward LBS (Linear Blending Skinning) to animate the learned human NeRF model.",
          "2": "For THuman dataset, we follow [5] and compare with existing human NeRF methods [4, 5, 17, 18]."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[35] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "35"
          ],
          "1": "Animatable NeRF [35] defines a per-frame latent code to capture appearance variations across each frame."
        },
        "RustNeRF: Robust Neural Radiance Field with Low-Quality Images": {
          "authors": [
            "M Li",
            "M Lu",
            "X Li",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2401.03257",
          "ref_texts": "[18] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "18"
          ],
          "1": "There are also plenty of works extending NeRF to various application scenarios such as scalable large scene [23, 25], 3D human face [6, 7], 3D human body [18, 42], and few-shot reconstruction [33, 36]."
        },
        "Editing Implicit and Explicit Representations of Radiance Fields: A Survey": {
          "authors": [
            "A Hubert",
            "G Elghazaly",
            "R Frank"
          ],
          "url": "https://arxiv.org/abs/2412.17628",
          "ref_texts": "[117] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
          "ref_ids": [
            "117"
          ],
          "1": "Geometry editing can be used to modify facial expressions [113], body pose [117] or human movement in a video [104], both of which can be extended with human body parametric models [118]."
        },
        "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors": {
          "authors": [
            "J Yin",
            "W Yin",
            "H Chen",
            "X Ren",
            "Z Ma",
            "J Guo"
          ],
          "url": "https://arxiv.org/abs/2311.15171",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Int. Conf. Comput. Vis., pages 14314\u201314323, 2021.",
          "ref_ids": [
            "30"
          ],
          "2": "Animatable NeRF [30] uses a parametric human body model as a strong geometry prior to canonical NeRF model, and achieves impressive visual fidelity on novel view and pose synthesis results.",
          "19": "5, our method produces higher visual quality with fewer artifacts than existing methods [30,32,33,38,40], which also indicates a better correspondence across frames.",
          "20": "87% improvement for PSNR on subject \u201c313\u201d with seen poses than Ani-NeRF [30].",
          "23": "Compared with Ani-NeRF [30], our approach achieves a better performance of novel view synthesis on seen and unseen poses.",
          "25": "Specifically, compared with Ani-NeRF [30] and Ani-SDF [31], our approach not only reconstructs smoother geometry results with high-quality but also captures more geometry details (e."
        },
        "NeRF-FF: a plug-in method to mitigate defocus blur for runtime optimized neural radiance fields": {
          "authors": [
            "Tristan Wirth"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03507-y",
          "ref_texts": "38. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human 123",
          "ref_ids": [
            "38"
          ],
          "1": "Some publications attempt to overcome the static nature of NeRFs regarding baked lighting and rigid scene geometry by applying them to generative models [34, 44], enabling dynamic relighting of the captured scenes [32, 46, 54, 64, 67], using them for animatable human avatar generation [13, 38, 47, 68] and through approaches that allow scene editing [5, 12, 19, 24, 34, 60, 63]."
        },
        "UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing": {
          "authors": [
            "J Fan",
            "J Zhang",
            "D Tao"
          ],
          "url": "https://arxiv.org/abs/2304.06969",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "NeRF [26] \u0014 \u0018 \u0018 \u0018 Neumesh [51] \u0014 \u0018 \u0014 \u0014 Ani-NeRF [31] \u0014 \u0014 \u0018 \u0018 UV-V olumes [6] \u0014 \u0014 \u0014 \u0018 Ours (UV A) \u0014 \u0014 \u0014 \u0014 Table 1: Comparison of the rendering and editing abilities of different methods.",
          "2": "Typically, a motion field such as a displacement field [27, 28] or a skinning field based on human prior [31] is used to align the dynamic human with a template in canonical space, where the canonical radiance field captures the human in coordinate-based networks.",
          "4": "Novel view and Novel pose synthesis For novel view synthesis and novel pose rendering, we compare our method against five existing approaches: 1) Neural Body (NB) [32] diffuses per-SMPL-vertex latent codes in observation space to condition the NeRF model and achieves high-quality novel view synthesis results on training poses; 2) Ani-NeRF [31] learns a backward LBS weight field and a canonical NeRF to reconstruct the human avatar;"
        },
        "Interactive Rendering of Relightable and Animatable Gaussian Avatars": {
          "authors": [
            "Y Zhan",
            "T Shao",
            "H Wang",
            "Y Yang",
            "K Zhou"
          ],
          "url": "https://arxiv.org/abs/2407.10707",
          "ref_texts": "[40] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "40"
          ],
          "1": "In recent years, many works [34], [35], [36], [37], [38], [38], [39], [40], [41], [42], [43] have used NeRF [7] to represent the human body by learning from multi-view videos, achieving pleasant rendering results.",
          "2": "Previous methods have successfully constructed the body geometry from the multiview or monocular videos [36], [40], [43]."
        },
        "TIFu: Tri-directional Implicit Function for High-Fidelity 3D Character Reconstruction": {
          "authors": [
            "B Lim",
            "SW Lee"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-8705-0_10",
          "ref_texts": "26. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao: Animatable neural radiance fields for modeling dynamic human bodies, in ICCV, 2021",
          "ref_ids": [
            "26"
          ],
          "1": "NeRF-based techniques [26] are gaining popularity for creating photorealistic renderings of novel viewpoints from a single or few images."
        },
        "LokiTalk: Learning Fine-Grained and Generalizable Correspondences to Enhance NeRF-based Talking Head Synthesis": {
          "authors": [
            "T Li",
            "R Zheng",
            "B Li",
            "Z Zhang",
            "M Wang",
            "J Chen"
          ],
          "url": "https://arxiv.org/abs/2411.19525",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE /CVF International Conference on Computer Vision, pages 14314\u2013",
          "ref_ids": [
            "24"
          ],
          "1": "The inherent limitation of vanilla NeRF in modeling solely static scenes has motivated the development of diverse approaches [17, 18, 23, 24, 25, 26] to address the representation of dynamic scenes."
        },
        "Neural rendering of humans in novel view and pose from monocular video": {
          "authors": [
            "T Wang",
            "N Sarafianos",
            "MH Yang",
            "T Tung"
          ],
          "url": "https://arxiv.org/abs/2204.01218",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 6, 7",
          "ref_ids": [
            "34"
          ],
          "1": "Given a monocular video, we predict novel views with body poses unseen from training with fine-level details (wrinkles) that works such as NeuralBody [34] or HumanNeRF [46] struggle to obtain.",
          "5": "The qualitative comparisons are provided in Fig 4 where our approach captures fine-level details on the body (1st,3rd rows) and head (2nd row) better than prior works [34, 35, 46]."
        },
        "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images": {
          "authors": [
            "J Lee",
            "S Cho",
            "T Kim",
            "HD Jang",
            "M Lee",
            "G Cha"
          ],
          "url": "https://arxiv.org/abs/2412.16028",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "NeRF and 3DGS has enabled a wide range of related research, including dynamic scene representation [14, 15, 23, 24, 28, 33], human avatars [8, 27, 39], 3D mesh reconstruction [16, 32, 35, 36], 3D scene representation from sparse-view images [22, 34, 42, 43], and 3D scene representation from blurry images [4, 10\u201313, 18, 25, 26, 37, 46]."
        },
        "SAGA: Surface-Aligned Gaussian Avatar": {
          "authors": [
            "R Chen",
            "Y Cong",
            "J Liu"
          ],
          "url": "https://arxiv.org/abs/2412.00845",
          "ref_texts": "[37] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in IEEE/CVF International Conference on Computer Vision , 2021, pp.",
          "ref_ids": [
            "37"
          ],
          "1": "Given the unprecedented success of Neural Radiance Fields [6], many methods have applied NeRF to reconstruct and render humans from videos [7], [8], [10], [11], [37]\u2013[40].",
          "7": "For NeRF-based methods: NeuralBody [7] anchors latent codes on the SMPL mesh and diffuse in 3D space with 3D convolution networks for neural volume rendering; AnimatableNeRF [37] represents the scene with a large MLP, and learns a forward and backward blending weight MLP for animation; HumanNeRF [8] further incorporates a non-rigid deformation network and achieves SOTA performance; InstantAvatar [19] applies the efficient Instant-NGP [12] as the canonical representation; InstantNVR [18] designs multi-part hash encoder based on [12]."
        },
        "2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting": {
          "authors": [
            "Q Yan",
            "M Sun",
            "L Zhang"
          ],
          "url": "https://arxiv.org/abs/2503.02452",
          "ref_texts": "[16] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "16"
          ],
          "1": "Learning Gaussian Parameters Directly Methods [9], [11] that directly learn Gaussian parameters typically follow a pipeline that is similar to NeRF-based approaches [2], [16], [17], where the avatar is represented in a canonical space and subsequently transformed into the posed space using LBS, after which the Gaussian primitives are rendered into images through the 3DGS rasterizer."
        },
        "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars": {
          "authors": [
            "S Sasaki",
            "J Wu",
            "K Nishino"
          ],
          "url": "https://arxiv.org/abs/2412.04433",
          "ref_texts": "[22] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "22"
          ],
          "1": "Following the release of NeRF, the earliest works in this area [7, 13, 22, 23, 27, 35] rely on the SMPL model [14] to skin an optimized NeRF of a human from a canonical space (unskinned) space to the observation space (skinned) and vice versa, which is key to both animation and dynamic reconstruction."
        },
        "NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction": {
          "authors": [
            "Z Zhang",
            "J Song",
            "E P\u00e9rez-Pellitero"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550790/",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "30"
          ],
          "1": "NeRF has later been adapted to represent dynamic human body [20, 30, 31, 37, 38, 44], achieving compelling results on modeling a clothed human.",
          "3": "Further explorations [20, 30, 31, 37, 38, 44] adapt NeRF to deformable humans."
        },
        "DRaCoN--Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars": {
          "authors": [
            "A Raj",
            "U Iqbal",
            "K Nagano",
            "S Khamis"
          ],
          "url": "https://arxiv.org/abs/2203.15798",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR, 2021. 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "25"
          ],
          "1": "The state-of-the-art methods in this direction use implicit functions [31, 33] to model human avatars and use volumetric rendering for image generation [14,25,26,39].",
          "2": "The most recent methods take inspiration from NeRF [21] or its variants [3,13,24,43,50] and represent human avatars using pose-conditioned implicit 3D representations [11, 14, 25, 26, 42].",
          "3": "A-NeRF [39] and AnimatableNeRF [25] use body pose information to canonicalize the sampled rays and learn neural radiance fields in the canonical space, which helps the learned avatar to generalize across different poses."
        },
        "ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events": {
          "authors": [
            "K Chen",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2409.14103",
          "ref_texts": "[57] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of ICCV, 2021.",
          "ref_ids": [
            "57"
          ],
          "1": "Recently, some pioneering works [58, 57, 15] are proposed to learn dynamic humans based on the body poses and observation frames by deforming the neural radiance fields (NeRF) [44] and 3D gausian splatting (3DGS) [26].",
          "2": "Following the emergence of Neural Radiance Fields (NeRF) [44], a variety of advancements have been made to facilitate the highfidelity rendering of static scenes [1, 73, 70, 68, 46], moving subjects [13, 34, 53, 54, 59, 50], and dynamic humans [12, 20, 33, 39, 57, 76, 85, 22, 23]."
        },
        "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video": {
          "authors": [
            "H Wang",
            "X Cai",
            "X Sun",
            "J Yue",
            "Z Tang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2405.12806",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 7",
          "ref_ids": [
            "38"
          ],
          "1": "2 Comparison methods Compare MOSS with two categories of SOTA methods: Human NeRF-based methods, such as NeuralBody [39], HumanNeRF [50] AnimateNeRF [38], InstantNVR [10] InstantAvatar [17].",
          "2": "Evaluation To validate the effectiveness of MOSS in solving the problem of lacking global constraints for clothed human reconstruction, we compare our method with some previous human reconstruction methods [10, 15, 17, 26, 38, 39, 50] on the ZJU-MoCap and MonoCap datasets.",
          "3": "To ensure a fair comparison, we compare [10, 15, 26, 38, 39, 50] at 512 \u00d7 512 resolution."
        },
        "TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering": {
          "authors": [
            "Sadia Mubashshira",
            "Kevin Desai"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2025W/ImageQuality/html/Mubashshira_TE-NeRF_Triplane-Enhanced_Neural_Radiance_Field_for_Artifact-Free_Human_Rendering_WACVW_2025_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 4",
          "ref_ids": [
            "28"
          ],
          "1": "Similar to the approach in Animatable Neural Radiance Fields [28], this allows modeling complex deformations such as cloth wrinkles and muscle bulges."
        },
        "Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video": {
          "authors": [
            "Y Rao",
            "EP Pellitero",
            "B Busam",
            "Y Zhou"
          ],
          "url": "https://arxiv.org/abs/2312.04784",
          "ref_texts": "52. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "52"
          ],
          "1": "monocular video, approaches like SelfRecon [21] and NeuMan [22] as well as Animatable NeRF [52] and HumanNeRF [70] integrate motion priors for regularization to allow this even with a single input video."
        },
        "PixelHuman: Animatable Neural Radiance Fields from Few Images": {
          "authors": [
            "G Shim",
            "J Lee",
            "J Hyung",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2307.09070",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 6",
          "ref_ids": [
            "19"
          ],
          "1": "Creating human avatars has been developed in various ways, ranging from creating textured human scans using various 3D representations [23, 24, 36, 26, 6], to rendering human images using implicit representations [19, 31, 14].",
          "2": "As neural radiance fields [18] has emerged with promising rendering performance on 3D objects, various human rendering methods [19, 31, 14] have been proposed to learn 3D human bodies only from images.",
          "3": "Similarly, Animatable NeRF [19] is suggested to generate novel human poses by deforming human body into a canonical human model represented by a neural radiance field.",
          "4": "2 Skeletal Deformation Following previous studies [19, 31, 14], we define the skeletal deformation by utilizing the linear blend skinning (LBS) function [30] to transform coordinates between different pose spaces.",
          "5": "We select HumanNeRF [31], Ani-NeRF [19], and KeypointNeRF [17] as our baselines which are state-of-theart human rendering methods."
        },
        "Bringing Telepresence to Every Desk": {
          "authors": [
            "S Wang",
            "Z Wang",
            "R Schmelzle",
            "L Zheng"
          ],
          "url": "https://arxiv.org/abs/2304.01197",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "45"
          ],
          "1": "Some recent works [46, 31, 54, 45, 26] exclusively focus on animating clothed humans."
        },
        "Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera": {
          "authors": [
            "H Lee",
            "J Cha",
            "Y Ku",
            "JS Yoon",
            "S Baek"
          ],
          "url": "https://arxiv.org/abs/2312.16842",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "Animatable NeRF [29] designed a novel transformation field between view and canonical space to better memorize the seen poses."
        },
        "Human View Synthesis using a Single Sparse RGB-D Input": {
          "authors": [
            "P Nguyen",
            "N Sarafianos",
            "C Lassner",
            "J Heikkila"
          ],
          "url": "https://3dvar.com/Nguyen2021Human.pdf",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2",
          "ref_ids": [
            "51"
          ],
          "2": "Given multi-view input frames or videos, recent works on rendering animatable humans from novel views show impressive results [49,51, 52, 71]."
        },
        "Modularizing deep learning for geometry-aware registration and reconstruction": {
          "authors": [
            "Wei Jiang"
          ],
          "url": "https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0427395",
          "ref_texts": "[195] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV, 2021. \u2192pages 3, 83, 86",
          "ref_ids": [
            "195"
          ],
          "3": "Animatable NeRF [195] learns a blending weight field in both observation space and canonical space, and optimize for a new blending weight field for novel poses."
        },
        "Accelerating Human Avatar Creation: Pose-dependent Hybrid Representations for Efficient Rendering of Clothed Human Avatars": {
          "authors": [
            "Z Qian"
          ],
          "url": "https://www.research-collection.ethz.ch/handle/20.500.11850/610316",
          "ref_texts": "[64] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of ICCV, 2021.",
          "ref_ids": [
            "64"
          ],
          "1": "With recent advances in implicit neural rendering [52, 82, 86, 85, 91] and implicit surface reconstruction [56, 105, 104, 93, 59] from RGB images, drastic improvements have been made regarding rendering fidelity [66, 64, 57, 39, 71, 35, 30] and geometric quality [101, 65, 97, 23] for clothed human avatars created using sparse multi-view or monocular acquisition setups.",
          "2": "For learning avatars directly from RGB inputs, volumetric rendering with implicit representations has been combined with human priors to achieve sparse multi-view reconstruction and rendering by a number of recent works [84, 101, 66, 64, 65, 57, 110, 99, 28, 97, 35, 49, 33, 109, 39].",
          "8": "2) with recent stateof-the-art methods [97, 66, 64, 99], showing that the proposed approach achieves comparable rendering and geometry reconstruction quality while providing 100 times faster rendering than the most competitive baseline [97]."
        },
        "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos\u2014Supplementary Material": {
          "authors": [
            "S Hu",
            "T Hu",
            "Z Liu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_GauHuman_Articulated_Gaussian_CVPR_2024_supplemental.pdf",
          "ref_texts": "[11] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1",
          "ref_ids": [
            "11"
          ],
          "1": "Animatable NeRF (AN) [11] learns a canonical human NeRF through skeleton-driven deformation and learned blend weight fields.",
          "2": "AS[13] further extends [11] by learning a signed distance field and a posedependent deformation field for residual information and geometric details of dynamic 3D humans."
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors* Supplemental Material": {
          "authors": [
            "Y Huang",
            "H Yi",
            "W Liu",
            "H Wang",
            "B Wu",
            "W Wang",
            "B Lin"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Huang_One-shot_Implicit_Animatable_ICCV_2023_supplemental.pdf",
          "ref_texts": "[11] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 1, 2, 3",
          "ref_ids": [
            "11"
          ],
          "1": "1 Data splitting For per-subject optimization methods Animatable NeRF[11] (Ani-NeRF) and NeuralBody[13] (NB), we use all subjects of ZJU-MoCap data-set (313, 315, 377, 386, 387, 390, 392, 393, 394) and the \"Posing\" sequences of Human 3.",
          "4": "5, we replaced the HumanNeRF model used in ELICIT with an SDF-based model from Animatable NeRF[12, 11]."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
          "authors": [
            "JA Rahim"
          ],
          "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProc. of International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021.",
          "ref_ids": [
            "29"
          ],
          "3": "Below we discuss three important works in this category: Animatable NeRF [29], Neural Body [30], and Structured Local Radiance Fields for Human Avatar Modeling [44].",
          "4": "Animatable NeRF [29] renders a novel view of a human model in a target pose, by leveraging SMPL as human body prior and using deformations to canonical pose (which in this case is a T-pose).",
          "5": "Efforts have been made in adapting NeRF to rendering human subjects [30, 21, 18, 29, 42, 44, 6].",
          "6": "3, prior works have been proposed to achieve eithersubjectagnostic [18, 6] oranimatable [21, 29, 42, 44] human avatar NeRF-based rendering.",
          "8": "We compare against three methods; Structured Local Radiance Fields for Human Avatar Modeling (SLRF) [44], Animatable Nerf (AN) [29], Neural Body (NB) [30]."
        },
        "Posed Neural Radiance Fields for Human Animation/submitted by Paul Knoll": {
          "authors": [
            "P Knoll"
          ],
          "url": "https://epub.jku.at/urn/urn:nbn:at:at-ubl:1-74368",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies This approach [43] proposes a novel method for creating animatable human models from multi-view video by decomposing a non-rigidly deforming scene into a canonical NeRF and a set of deformation fields that map observation-space points to the canonical space."
        },
        "Supplementary for ARAH: Animatable Volume Rendering of Articulated Human SDFs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920001-supp.pdf",
          "ref_texts": "12. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
          "ref_ids": [
            "12"
          ],
          "1": "For inference, we follow [12, 13] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
          "2": "For unseen test poses we follow the practice of [12, 13] and use the latent code Z of the last training frame as the input.",
          "5": "We also report quantitative results on the H36M dataset [5], following the testing protocols proposed by [12] in Table 2."
        },
        "Supplemental Materials for TAVA: Template-free Animatable Volumetric Actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall",
            "A Kanazawa"
          ],
          "url": "https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920417-supp.pdf",
          "ref_texts": "5. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021)",
          "ref_ids": [
            "5"
          ],
          "2": "2, for the template-based baselines Animatable-NeRF [5] and NeuralBody [6], we use their official implementations.",
          "3": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [5,6,8].",
          "4": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 Subject 313 Animatable-NeRF [5] 29.",
          "5": "957 Subject 315 Animatable-NeRF [5] 27.",
          "6": "960 Subject 377 Animatable-NeRF [5] 32.",
          "7": "980 Subject 386 Animatable-NeRF [5] 34."
        }
      }
    },
    {
      "title": "gift: learning transformation-invariant dense visual descriptors via group cnns",
      "id": 12,
      "valid_pdf_number": "54/68",
      "matched_pdf_number": "47/54",
      "matched_rate": 0.8703703703703703,
      "citations": {
        "LoFTR: Detector-free local feature matching with transformers": {
          "authors": [
            "Jiaming Sun",
            "Zehong Shen",
            "Yuang Wang",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Sun_LoFTR_Detector-Free_Local_Feature_Matching_With_Transformers_CVPR_2021_paper.html",
          "ref_texts": "[25] Y uan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning transformation-invariant dense visual descriptors via group cnns.NeurIPS, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "Many learning-based local features along this line [32, 11, 25, 28, 47] also adopt the detector-based design."
        },
        "Aliked: A lighter keypoint and descriptor extraction network via deformable transformation": {
          "authors": [
            "X Zhao",
            "X Wu",
            "W Chen",
            "PCY Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10111017/",
          "ref_texts": "[13] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Advances in Neural Information Processing Systems , vol. 32, 2019.",
          "ref_ids": [
            "13"
          ],
          "1": "GIFT [13] first generates groups of images with different scales and orientations, and then extracts features from these images to produce scale and orientation invariant descriptors.",
          "2": "To address this issue, some methods explicitly rotate and scale the images [13] or convolution kernels [27] with predefined degrees and scales."
        },
        "Learning feature descriptors using camera pose supervision": {
          "authors": [
            "Q Wang",
            "X Zhou",
            "B Hariharan",
            "N Snavely"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58452-8_44",
          "ref_texts": "34. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: Gift: Learning transformationinvariant dense visual descriptors via group cnns. In: NeurIPS (2019)",
          "ref_ids": [
            "34"
          ],
          "1": "Dense descriptor methods [9,13,14,16,34,50,55] instead use fully-convolutional neural networks [35] to extract dense feature descriptors for the whole image in one forward pass."
        },
        "BEVPlace: Learning LiDAR-based place recognition using bird's eye view images": {
          "authors": [
            "Lun Luo",
            "Shuhang Zheng",
            "Yixuan Li",
            "Yongzhi Fan",
            "Beinan Yu",
            "Yuan Cao",
            "Junwei Li",
            "Liang Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Luo_BEVPlace_Learning_LiDAR-based_Place_Recognition_using_Birds_Eye_View_Images_ICCV_2023_paper.html",
          "ref_texts": "[18] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning transformation-invariant dense visual descriptors via group cnns. In Conference and Workshop on Neural Information Processing Systems, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "Group convolution has been well-developed for a few years, and there are some mature group convolution designs [31, 18, 33].",
          "2": "We implemented our network based on GIFT [18]."
        },
        "Local feature matching using deep learning: A survey": {
          "authors": [
            "S Xu",
            "S Chen",
            "R Xu",
            "C Wang",
            "P Lu",
            "L Guo"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1566253524001222",
          "ref_texts": "[90] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, X. Zhou, Gift: Learning transformation-invariant dense visual descriptors via group cnns, Advances in Neural Information Processing Systems 32 (2019).",
          "ref_ids": [
            "90"
          ],
          "1": "For GIFT [90] and COLD [91], the former underscores the importance of incorporating underlying structural information from group features to construct potent descriptors."
        },
        "Recurrent homography estimation using homography-guided image warping and focus transformer": {
          "authors": [
            "Yuan Cao",
            "Runmin Zhang",
            "Lun Luo",
            "Beinan Yu",
            "Zehua Sheng",
            "Junwei Li",
            "Liang Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.html",
          "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group CNNs. Advances in Neural Information Processing Systems, 32, 2019. 1, 3, 4",
          "ref_ids": [
            "25"
          ],
          "6": "For example, GIFT [25] produces rotation and scale invariant features using warped images with predefined rotation angles and scale ratios."
        },
        "A case for using rotation invariant features in state of the art feature matchers": {
          "authors": [
            "Georg Bokman",
            "Fredrik Kahl"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.html",
          "ref_texts": "[31] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning TransformationInvariant Dense Visual Descriptors via Group CNNs. arXiv:1911.05932 [cs], Nov. 2019. 2",
          "ref_ids": [
            "31",
            "cs"
          ],
          "1": "CNN-based approaches for detection and description of keypoints include for instance D2-Net, R2D2-Net and DISK [15, 36, 44], but most related to our approach are the ones that use rotation invariant features such as LIFT, GIFT and others [26, 31, 49, 51]."
        },
        "Online invariance selection for local feature descriptors": {
          "authors": [
            "R Pautrat",
            "V Larsson",
            "MR Oswald"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58536-5_42",
          "ref_texts": "19. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: Gift: Learning transformationinvariant dense visual descriptors via group cnns. In: Advances in Neural Information Processing Systems (NeurIPS) (2019)",
          "ref_ids": [
            "19"
          ],
          "3": "We consider the following baselines: Root SIFT with the default Kornia [32] implementation; HardNet [24] (trained on the PS-dataset [26]), SOSNet [40] (trained on the Liberty dataset of UBC Phototour [6]), SuperPoint (SP) [8], D2-Net [9], R2D2 [31] and GIFT [19] with the authors implementation."
        },
        "Self-supervised equivariant learning for oriented keypoint detection": {
          "authors": [
            "Jongmin Lee",
            "Byungjin Kim",
            "Minsu Cho"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Lee_Self-Supervised_Equivariant_Learning_for_Oriented_Keypoint_Detection_CVPR_2022_paper.html",
          "ref_texts": "[26] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems, 32:6992\u20137003, 2019. 2, 7",
          "ref_ids": [
            "26"
          ],
          "1": "The most similar work, GIFT [26], use equivariant networks to obtain dense local descriptors, but [26] constructs the group representation with augmented images, whereas we construct the representation through 4848 steerable kernels [67] without rotating images at runtime.",
          "5": "Our model with GIFT descriptor [26] achieves better MMAs compared to the SuDet.",
          "7": "In particular, our model with the rotation-invariant descriptors [26] achieves the best MMAs, which shows that the rotation-invariant representation contributes to improving the accuracy of correspondences."
        },
        "Learning rotation-equivariant features for visual correspondence": {
          "authors": [
            "Jongmin Lee",
            "Byungjin Kim",
            "Seungwook Kim",
            "Minsu Cho"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_Learning_Rotation-Equivariant_Features_for_Visual_Correspondence_CVPR_2023_paper.html",
          "ref_texts": "[26] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns.Advances in Neural Information Processing Systems, 32:6992\u20137003, 2019.1, 2, 5, 6, 7",
          "ref_ids": [
            "26"
          ],
          "2": "GIFT [26] constructs groupequivariant features by rotating or rescaling the images, and then collapses the group dimension using bilinear pooling to obtain invariant local descriptors.",
          "3": "Note that this dataset generation protocol is the same as that of GIFT [26] for a fair comparison.",
          "6": "We use SuperPoint keypoint detector [8] same to the GIFT descriptor [26].",
          "12": "Overall, incorporating group aligning demonstrates the best results in terms of MMA compared to average pooling, max pooling or bilinear pooling [26].",
          "13": "We evaluate the descriptors using their own keypoint detectors [8, 9, 27, 39, 40, 44, 50], or combined with off-the-shelf detectors [24, 26, 42].",
          "14": "In particular, our rotation-invariant descriptor shows consistently higher matching accuracy than GIFT [26], which is a representative learning-based group-invariant descriptor."
        },
        "TransVLAD: Multi-scale attention-based global descriptors for visual geo-localization": {
          "authors": [
            "Yifan Xu",
            "Pourya Shamsolmoali",
            "Eric Granger",
            "Claire Nicodeme",
            "Laurent Gardes",
            "Jie Yang"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Xu_TransVLAD_Multi-Scale_Attention-Based_Global_Descriptors_for_Visual_Geo-Localization_WACV_2023_paper.html",
          "ref_texts": "[24] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Adv. Neural Inform. Process. Syst., 2019.",
          "ref_ids": [
            "24"
          ],
          "1": "As shown in Figure 7, we estimate matching results on both day and night image pairs and compare them with the results of three state-of-the-art models \u2013 DFM, GIFT [24], and Superpoint [5]."
        },
        "Vs-net: Voting with segmentation for visual localization": {
          "authors": [
            "Zhaoyang Huang",
            "Han Zhou",
            "Yijin Li",
            "Bangbang Yang",
            "Yan Xu",
            "Xiaowei Zhou",
            "Hujun Bao",
            "Guofeng Zhang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.html",
          "ref_texts": "[28] Y uan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. InAdvances in Neural Information Processing Systems , pages 6990\u20137001, 2019.",
          "ref_ids": [
            "28"
          ],
          "1": "Traditional visual localization frameworks [4, 16, 26, 61, 44, 12] build a map by SfM techniques [62, 1, 67, 46, 55] with general feature detectors and descriptors [30, 6, 43, 35, 15, 17, 28, 41]."
        },
        "Multiple planar object tracking": {
          "authors": [
            "Zhicheng Zhang",
            "Shengzhe Liu",
            "Jufeng Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Multiple_Planar_Object_Tracking_ICCV_2023_paper.html",
          "ref_texts": "[53] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformationinvariant dense visual descriptors via group cnns. NeurIPS, 2019.",
          "ref_ids": [
            "53"
          ],
          "1": "Methods Traditional Deep OursMotion Patterns Metrics CMT [61] NCC [71] CCRE [76] MI [17] GOP [8] Gracker [79] STM [62] PoST [60] LISRD [64] SMask [78] SRPN+ [38] SPoint [16] SOS [73] GIFT [53] HDN [95] Overall S0."
        },
        "P2-net: Joint description and detection of local features for pixel and point matching": {
          "authors": [
            "Bing Wang",
            "Changhao Chen",
            "Zhaopeng Cui",
            "Jie Qin",
            "Chris Xiaoxuan",
            "Zhengdi Yu",
            "Peijun Zhao",
            "Zhen Dong",
            "Fan Zhu",
            "Niki Trigoni",
            "Andrew Markham"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.html",
          "ref_texts": "[30] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In NeurIPS, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "2D Local Features Description and Detection Previous learning-based methods in 2D domain simply replaced the descriptor [50, 51, 30, 19, 38] or detector [43, 59, 4] with a learnable alternative.",
          "2": "1 Image Matching In the image matching experiment, we use the HPatches dataset [3], which has been widely adopted to evaluate the quality of image matching [33, 16, 40, 30, 51, 38, 53]."
        },
        "Deltas: Depth estimation by learning triangulation and densification of sparse points": {
          "authors": [
            "A Sinha",
            "Z Murez",
            "J Bartolozzi"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58589-1_7",
          "ref_texts": "27. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: Gift: Learning transformation-invariant dense visual descriptors via group cnns. In: Advances in Neural Information Processing Systems. pp. 6990\u20137001 (2019)",
          "ref_ids": [
            "27"
          ],
          "1": "General purpose descriptors learnt by methods such as SuperPoint [9], LIFT [46], GIFT [27] aim to bridge the gap towards differentiable SLAM."
        },
        "Improving transformer-based image matching by cascaded capturing spatially informative keypoints": {
          "authors": [
            "Chenjie Cao",
            "Yanwei Fu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cao_Improving_Transformer-based_Image_Matching_by_Cascaded_Capturing_Spatially_Informative_Keypoints_ICCV_2023_paper.html",
          "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems, 32, 2019. 3",
          "ref_ids": [
            "25"
          ],
          "1": "After the wave of deep learning, many learning-based methods [15, 62, 8, 11, 25, 27] were proposed based on the detector-dependent pipeline with better performance."
        },
        "Steerers: A framework for rotation equivariant keypoint descriptors": {
          "authors": [
            "Georg Bokman",
            "Johan Edstedt",
            "Michael Felsberg",
            "Fredrik Kahl"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Bokman_Steerers_A_Framework_for_Rotation_Equivariant_Keypoint_Descriptors_CVPR_2024_paper.html",
          "ref_texts": "[34] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns.Advances in Neural Information Processing Systems, 32, 2019. 2",
          "ref_ids": [
            "34"
          ],
          "1": "Equivariant ConvNets have also been used for rotation-robust keypoint detection without estimating the rotation frame [2, 46], keypoint description [2, 34] and end-to-end image matching [9]."
        },
        "Bev-locator: An end-to-end visual semantic localization network using multi-view images": {
          "authors": [
            "Z Zhang",
            "M Xu",
            "W Zhou",
            "T Peng",
            "L Li"
          ],
          "url": "https://link.springer.com/article/10.1007/s11432-023-4114-6",
          "ref_texts": "[25] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Advances in Neural Information Processing Systems , vol. 32, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": ", Group Invariant Feature Transform (GIFT) [25], HardNet [23], and SOSNet [46], etc."
        },
        "Self-supervised learning of image scale and orientation": {
          "authors": [
            "J Lee",
            "Y Jeong",
            "M Cho"
          ],
          "url": "https://arxiv.org/abs/2206.07259",
          "ref_texts": "[20] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. arXiv preprint arXiv:1911.05932, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "More recent studies [2, 10, 20, 32] aim to extract local descriptors that are invariant or covariant with respect to geometric variations within a local region."
        },
        "Matching in the dark: A dataset for matching image pairs of low-light scenes": {
          "authors": [
            "Wenzheng Song",
            "Masanori Suganuma",
            "Xing Liu",
            "Noriyuki Shimobayashi",
            "Daisuke Maruta",
            "Takayuki Okatani"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Song_Matching_in_the_Dark_A_Dataset_for_Matching_Image_Pairs_ICCV_2021_paper.html",
          "ref_texts": "[30] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Proc. NeurIPS, 2019. https: //github.com/zju3dv/GIFT. 6",
          "ref_ids": [
            "30"
          ],
          "1": "We use SuperPoint [17], Reinforced SuperPoint [9], GIFT [30], R2D2 [26], and RF-Net [50] as representative neural network-based methods."
        },
        "PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking": {
          "authors": [
            "Xinran Liu",
            "Xiaoqiong Liu",
            "Ziruo Yi",
            "Xin Zhou",
            "Thanh Le",
            "Libo Zhang",
            "Yan Huang",
            "Qing Yang",
            "Heng Fan"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_PlanarTrack_A_Large-scale_Challenging_Benchmark_for_Planar_Object_Tracking_ICCV_2023_paper.html",
          "ref_texts": "[24] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. NeurIPS, 2019. 6, 7",
          "ref_ids": [
            "24"
          ],
          "1": "Specifically, these trackers are Gracker [35], GIFT [24], ESM [3], LISRD [29], SOL [15], SIFT [25], IC [1], SCV [30], HDN [41], and WOFT [32]."
        },
        "Planar object tracking via weighted optical flow": {
          "authors": [
            "Jonas Serych",
            "Jiri Matas"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Serych_Planar_Object_Tracking_via_Weighted_Optical_Flow_WACV_2023_paper.html",
          "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems, 32, 2019.",
          "ref_ids": [
            "25"
          ],
          "1": "The best ones use the SIFT keypoint detector, a deep learning descriptor such as GIFT [25], M ATCH NET [13], SOSN ET [43], or LISRD [32], followed by RANSAC."
        },
        "Scalenet: A shallow architecture for scale estimation": {
          "authors": [
            "Axel Barroso",
            "Yurun Tian",
            "Krystian Mikolajczyk"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Barroso-Laguna_ScaleNet_A_Shallow_Architecture_for_Scale_Estimation_CVPR_2022_paper.html",
          "ref_texts": "[22] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformationinvariant dense visual descriptors via group cnns. arXiv preprint arXiv:1911.05932, 2019. 2",
          "ref_ids": [
            "22"
          ],
          "1": "The rotation has been addressed by correcting input patches [13, 23, 30] before extracting local descriptors [28,48,49], or by designing robust architectures [4, 22, 24]."
        },
        "Sim2e: Benchmarking the group equivariant capability of correspondence matching algorithms": {
          "authors": [
            "S Su",
            "Z Zhao",
            "Y Fei",
            "S Li",
            "Q Chen",
            "R Fan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25056-9_47",
          "ref_texts": "32. Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning transformation-invariant dense visual descriptors via group CNNs. Advances in Neural Information Processing Systems, 32, 2019. 3, 7, 8, 9, 11, 12",
          "ref_ids": [
            "32"
          ],
          "1": "GIFT [32] is a rotation-equivariant and scaling-equivariant descriptor based on G-CNN.",
          "2": "GIFT [32] is trained on the MS-COCO [18] dataset and finetuned on the GL3D [53] dataset (consisting of indoor and outdoor scenes).",
          "3": "GIFT [32] uses SuperPoint as the feature detector."
        },
        "Hybrid Gromov\u2013Wasserstein Embedding for Capsule Learning": {
          "authors": [
            "P Shamsolmoali",
            "M Zareapoor",
            "S Das"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10413503/",
          "ref_texts": "[77] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Proc. Adv. Neural Inf. Process. Syst. , vol. 32, 2019.",
          "ref_ids": [
            "77"
          ],
          "1": "For this experiment, We use the Aachen dataset [67], and compare the performance of our model with three strong methods; DISK [78], GIFT [77], and Superpoint [76].",
          "2": "The first column uses key points detected by Superpoint [76], the second column uses GIFT [77], the third column uses DISK [78], and the last one is our proposed model.",
          "3": "We also compare the HGWCapsule with three baselines: DISK [78], GIFT [77], and Superpoint [76]."
        },
        "PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching": {
          "authors": [
            "H Nie",
            "B Luo",
            "J Liu",
            "Z Fu",
            "H Zhou",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2502.18104",
          "ref_texts": "[39] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.",
          "ref_ids": [
            "39"
          ],
          "2": "Our PromptMID was compared with twelve algorithms: SIFT [10], RIFT [11], LNIFT [12], SRIF [37], LoFTR [16], XoFTR [18], RoMa [42], ReDFeat [17], HardNet [14], HyNet [15], SoSNet [38] and GIFT [39].",
          "4": "894 GIFT [39] 71."
        },
        "ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer": {
          "authors": [
            "Y Xu",
            "P Shamsolmoali",
            "M Zareapoor"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10772618/",
          "ref_texts": "[49] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d inAdv. Neural Inform. Process. Syst., 2019.",
          "ref_ids": [
            "49"
          ],
          "1": "We compared the results with those of three strong baseline models\u2013 DFM, GIFT [49], and Superpoint [50]."
        },
        "Scale-net: Learning to reduce scale differences for large-scale invariant image matching": {
          "authors": [
            "Y Fu",
            "Y Wu"
          ],
          "url": "https://arxiv.org/abs/2112.10485",
          "ref_texts": "[36] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d in Advances in neural information processing systems (NIPS) , 2019.",
          "ref_ids": [
            "36"
          ],
          "1": "GIFT uses group convolutions to fuse features extracted from the transformed versions of an image to obtain a descriptor which is invariant to scale changes [36].",
          "5": "We select SIFT [6], ASLFeat [13] and GIFT-SP [36] as baselines.",
          "6": "Following GIFT [36], we use Percentage of Correctly Matched Keypoints (PCK) to quantify the performance for correspondence estimation.",
          "7": "We select SIFT [6], ASLFeat [13] and GIFT-SP [36] as baselines."
        },
        "Feature lenses: Plug-and-play neural modules for transformation-invariant visual representations": {
          "authors": [
            "S Li",
            "X Sui",
            "J Fu",
            "Y Liu",
            "RSM Goh"
          ],
          "url": "https://arxiv.org/abs/2004.05554",
          "ref_texts": "21. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: Gift: Learning transformation-invariant dense visual descriptors via group cnns. In: Advances in Neural Information Processing Systems 32, pp. 6992\u20137003 (2019)",
          "ref_ids": [
            "21"
          ],
          "1": "Group invariant feature transform [21] extracts visual features that are both discriminative and robust to geometric transformations."
        },
        "DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant Descriptors in Local Feature Matching": {
          "authors": [
            "R Huang",
            "J Cai",
            "C Li",
            "Z Wu",
            "X Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10341994/",
          "ref_texts": "[17] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Advances in Neural Information Processing Systems , vol. 32, 2019.",
          "ref_ids": [
            "17"
          ],
          "2": "To obtain transformation invariant descriptors, GIFT [17] uses group convolutions [13] to exploit underlying structures of the extracted features from the transformed versions of an image.",
          "4": "To validate the performance of ASLFeat [12] and GIFT [17] we use the trained model provided by the original authors.",
          "5": "GIFT [17] and RoRD [16] are trained with homography augmentations and are specifically designed to improve rotation invariance, therefore they overtake other methods on Rotated Hpatches.",
          "7": "We compare our DRKF with SIFT and learning-based methods, including SuperPoint [9] + SuperGlue [21], D2-Net [10], ASLFeat [12], LoFTR [22], GIFT [17], and RoRD [16] in Fig."
        },
        "Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects\u2014Supplementary Material\u2014": {
          "authors": [
            "T Cheng",
            "WC Ma",
            "K Guan",
            "A Torralba",
            "S Wang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/3764a9c8abc84c7482f778fefc24f10b-Supplemental-Conference.pdf",
          "ref_texts": "[4] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems, 32, 2019.",
          "ref_ids": [
            "4"
          ],
          "1": "We believe that combining some transformation-invariant feature extractor (such as GIFT[4]) can solve this problem faster."
        },
        "Learning Rotation-Equivariant Features for Visual Correspondence-supplementary material": {
          "authors": [
            "N L"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Lee_Learning_Rotation-Equivariant_Features_CVPR_2023_supplemental.pdf",
          "ref_texts": "[14] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns.Advances in Neural Information Processing Systems, 32:6992\u20137003, 2019.1, 2, 3, 4, 5, 9",
          "ref_ids": [
            "14"
          ],
          "5": "Our descriptor consistently performs better than SuperPoint [5] descriptors under varying number of keypoints, and obtains comparable results with GIFT [14] descriptors."
        },
        "A stricter constraint produces outstanding matching: Learning more reliable image matching using a quadratic hinge triplet loss network": {
          "authors": [
            "M Xu",
            "C Shen",
            "J Zhang",
            "Z Wang",
            "Z Ruan",
            "S Poslad"
          ],
          "url": "https://graphicsinterface.org/wp-content/uploads/gi2021-23.pdf",
          "ref_texts": "[15] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns.arXiv preprint arXiv:1911.05932, 2019.",
          "ref_ids": [
            "15"
          ],
          "1": "Traditionally, detectors and descriptors are separately applied in the pipeline, SIFT [16] (and RootSIFT [3]) and SURF [4] are most popular detectors while some descriptors are followed, in which LogPolar [12] shows better performance than ContextDesc [17] from the relative pose error, while SOSNet [26] and HardNet [18] surpass GIFT [15] in the public validation set."
        }
      }
    },
    {
      "title": "implicit neural representations with structured latent codes for human body modeling",
      "id": 29,
      "valid_pdf_number": "18/20",
      "matched_pdf_number": "15/18",
      "matched_rate": 0.8333333333333334,
      "citations": {
        "Gart: Gaussian articulated template models": {
          "authors": [
            "Jiahui Lei",
            "Yufu Wang",
            "Georgios Pavlakos",
            "Lingjie Liu",
            "Kostas Daniilidis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html",
          "ref_texts": "[54] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2",
          "ref_ids": [
            "54"
          ],
          "1": "To address this issue, recent studies [7\u20139, 11\u201313, 16, 20\u201325, 32, 33, 38, 40, 48, 51, 54, 57\u201359, 64, 67, 72, 73, 82, 86, 88] propose to use neural representations, such as NeRF, to capture high-fidelity humans from multiple views or videos."
        },
        "Deliffas: Deformable light fields for fast avatar synthesis": {
          "authors": [
            "Y Kwon",
            "L Liu",
            "H Fuchs"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/805c06617d2b643278936daadfde4280-Abstract-Conference.html",
          "ref_texts": "[42] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.",
          "ref_ids": [
            "42"
          ],
          "1": "To address this issue, a large body of work [32, 41, 8, 63, 51, 39, 20, 26, 11, 66, 56, 57, 29, 50, 42, 21, 64, 27] propose neural animatable implicit representations.",
          "2": "Also, we can use recent avatar NeRF works [64, 21, 42] to compute the canonical mesh (template mesh) and then use skinning-based deformation to obtain the deformed mesh for the new pose."
        },
        "Videorf: Rendering dynamic radiance fields as 2d feature video streams": {
          "authors": [
            "Liao Wang",
            "Kaixin Yao",
            "Chengcheng Guo",
            "Zhirui Zhang",
            "Qiang Hu",
            "Jingyi Yu",
            "Lan Xu",
            "Minye Wu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wang_VideoRF_Rendering_Dynamic_Radiance_Fields_as_2D_Feature_Video_Streams_CVPR_2024_paper.html",
          "ref_texts": "[46] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling.IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(8):9895\u20139907, 2023. 2",
          "ref_ids": [
            "46"
          ],
          "1": "Some methods [10, 15, 29, 31\u201333, 43, 48, 63, 69, 77, 79] handle spatial change directly conditions on time and [14, 45, 46, 73, 82] use feature latent code to represent time information."
        },
        "Neural Visibility Field for Uncertainty-Driven Active Mapping": {
          "authors": [
            "Shangjie Xue",
            "Jesse Dill",
            "Pranay Mathur",
            "Frank Dellaert",
            "Panagiotis Tsiotra",
            "Danfei Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xue_Neural_Visibility_Field_for_Uncertainty-Driven_Active_Mapping_CVPR_2024_paper.html",
          "ref_texts": "[35] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2",
          "ref_ids": [
            "35"
          ],
          "1": "Along this direction, significant progress has been made in novel view rendering [27, 29, 45], 3D reconstruction [3, 22], 3D generation [17, 36] and videos [10, 23, 24, 35, 47]."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "28. Peng, S., Geng, C., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Zhou, X., Bao, H.: Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)",
          "ref_ids": [
            "28"
          ],
          "1": "Previous efforts in dynamic human neural rendering have focused on digitalizing human avatars and modeling human motion [2,4,13,28,29,32,33,39,40,44].",
          "5": "3 Comparison with State of the Arts We compare with state-of-the-art counterparts, including Neural Body [28], AniNeRF [29], AniSDF [29], HumanNeRF [40] and 3DGS-Avatar [33]."
        },
        "Ghnerf: Learning generalizable human features with efficient neural radiance fields": {
          "authors": [
            "Arnab Dey",
            "Di Yang",
            "Rohith Agaram",
            "Antitza Dantcheva",
            "Andrew I. Comport",
            "Srinath Sridhar",
            "Jean Martinet"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NRI/html/Dey_GHNeRF_Learning_Generalizable_Human_Features_with_Efficient_Neural_Radiance_Fields_CVPRW_2024_paper.html",
          "ref_texts": "[33] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE TPAMI, 2023. 2",
          "ref_ids": [
            "33"
          ],
          "1": "Similarly, [17, 33, 37, 40] uses pre-existing skeleton data or pose estimator or information from the SMPL model [23] to reconstruct novel views or novel poses."
        },
        "Joint2human: High-quality 3d human generation via compact spherical embedding of 3d joints": {
          "authors": [
            "Muxin Zhang",
            "Qiao Feng",
            "Zhuo Su",
            "Chao Wen",
            "Zhou Xue",
            "Kun Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Joint2Human_High-Quality_3D_Human_Generation_via_Compact_Spherical_Embedding_of_CVPR_2024_paper.html",
          "ref_texts": "[39] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE Trans. Pattern Anal. Mach. Intell., 2023. 2",
          "ref_ids": [
            "39"
          ],
          "1": "3D Human Generation with 2D Generators Many approaches [12, 19, 20, 22, 35, 46] try to learn the 3D shape from 2D images via various NeRF representations [25, 28, 31, 33, 38, 39] and differentiable volume rendering [32, 45, 51]."
        },
        "TutteNet: Injective 3D Deformations by Composition of 2D Mesh Deformations": {
          "authors": [
            "Bo Sun",
            "Thibault Groueix",
            "Chen Song",
            "Qixing Huang",
            "Noam Aigerman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Sun_TutteNet_Injective_3D_Deformations_by_Composition_of_2D_Mesh_Deformations_CVPR_2024_paper.html",
          "ref_texts": "[67] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling.IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.8",
          "ref_ids": [
            "67"
          ],
          "1": "SHERF [29]), while we use humans as a benchmark for comparing and measuring the accuracy of our method, as well as showing its generality: unlike these other techniques [8, 35, 37, 42, 66, 67, 75, 82, 84, 95], we did not use any human-specific priors in the design of the representation, and the same exact method could be applied as-is to any other deformation dataset."
        },
        "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition": {
          "authors": [
            "A Chatziagapi",
            "GG Chrysos",
            "D Samaras"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72691-0_22",
          "ref_texts": "68. Peng, S., Geng, C., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Zhou, X., Bao, H.: Implicit Neural Representations with Structured Latent Codes for Human Body Modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)",
          "ref_ids": [
            "68"
          ],
          "1": "They have been extended to dynamic scenes [44,45,71], making them a popular choice for modeling human bodies [16,27,31,47,59,68,69,93] and faces [15,64,65]."
        },
        "AirNeRF: 3D reconstruction of human with drone and NeRF for future communication systems": {
          "authors": [
            "A Kotcov",
            "M Dronova",
            "V Cheremnykh"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10711766/",
          "ref_texts": "[3] S. Peng, C. Geng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, X. Zhou, and H. Bao, \u201cImplicit neural representations with structured latent codes for human body modeling,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023.",
          "ref_ids": [
            "3"
          ],
          "1": "This focus is imperative for the rendering of human subjects within a scene, especially in the context of developing visual and immersive experiences [2], [3].",
          "2": "While these approaches have produced intriguing and promising outcomes, they often necessitate the separate training of editable instances [8] or manual curation of training data [3]."
        },
        "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans": {
          "authors": [
            "A Chatziagapi",
            "B Chaudhuri",
            "A Kumar"
          ],
          "url": "https://arxiv.org/abs/2409.16666",
          "ref_texts": "50. Peng, S., Geng, C., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Zhou, X., Bao, H.: Implicit Neural Representations with Structured Latent Codes for Human Body Modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)",
          "ref_ids": [
            "50"
          ],
          "1": "Many NeRF-based approaches capture the 4D dynamics and appearance of humans from multi-view videos [16,27,29,37,44,46, 47,50,51,66], while recent works use monocular videos [15,21,28,42,60,62,64,68]."
        },
        "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features": {
          "authors": [
            "A Dey",
            "CY Lu",
            "AI Comport",
            "S Sridhar",
            "CT Lin"
          ],
          "url": "https://arxiv.org/abs/2411.03086",
          "ref_texts": "[46] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE TPAMI, 2023. 2",
          "ref_ids": [
            "46"
          ],
          "1": "Likewise, [26, 46, 54, 61] utilize existing skeletal data, state-of-the-art pose estimators, or pose data to generate novel views and poses."
        },
        "MV2MP: Segmentation Free Performance Capture of Humans in Direct Physical Contact from Sparse Multi-Cam Setups": {
          "authors": [
            "Sergei Eliseev",
            "Leonid Shtanko",
            "Rasim Akhunzianov",
            "Yaroslav Romanenko",
            "Anatoly Starostin"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2024/html/Eliseev_MV2MP_Segmentation_Free_Performance_Capture_of_Humans_in_Direct_Physical_ACCV_2024_paper.html",
          "ref_texts": "23. Peng, S., Geng, C., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Zhou, X., Bao, H.: Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023)",
          "ref_ids": [
            "23"
          ],
          "1": "As a solution, emerging systems [8,17,23] leverage volumetric rendering [21], while others [13,24,37] use 3D Gaussians [14] as a color and volume barrier.",
          "2": "Neuralbody [23] anchors a set of latent codes to the vertices of the SMPL model, a deformable human body model.",
          "3": "But also utilise the [23] approach and register features in SMPL nodes."
        },
        "AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation": {
          "authors": [
            "M Li",
            "S Yao",
            "C Kai",
            "Z Xie",
            "K Chen",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2502.19441",
          "ref_texts": "[81] S. Peng, C. Geng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, X. Zhou, and H. Bao, \u201cImplicit neural representations with structured latent codes for human body modeling,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.",
          "ref_ids": [
            "81"
          ],
          "1": "ZJU-MoCap Dataset [81] contains several multi-view video captures around the motion humans."
        },
        "DEGAS: Detailed Expressions on Full-Body Gaussian Avatars": {
          "authors": [
            "Z Shao",
            "D Wang",
            "QY Tian",
            "YD Yang",
            "H Meng"
          ],
          "url": "https://arxiv.org/abs/2408.10588",
          "ref_texts": "[49] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit Neural Representations with Structured Latent Codes for Human Body Modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 3",
          "ref_ids": [
            "49"
          ],
          "1": "NeuralBody [48, 49] encodes learnable latent codes to SMPL mesh vertices."
        }
      }
    },
    {
      "title": "autorecon: automated 3d object discovery and reconstruction",
      "id": 33,
      "valid_pdf_number": "8/11",
      "matched_pdf_number": "7/8",
      "matched_rate": 0.875,
      "citations": {
        "Language embedded 3d gaussians for open-vocabulary scene understanding": {
          "authors": [
            "Chuan Shi",
            "Miao Wang",
            "Bin Duan",
            "Hua Guan"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Shi_Language_Embedded_3D_Gaussians_for_Open-Vocabulary_Scene_Understanding_CVPR_2024_paper.html",
          "ref_texts": "[47] Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, and Xiaowei Zhou. Autorecon: Automated 3d object discovery and reconstruction. In CVPR, 2023. 3",
          "ref_ids": [
            "47"
          ],
          "1": "Conversely, DINO [5] exhibits autonomous object decomposition without requiring labeled data [1], as several studies [21, 22, 26, 47] have shown that DINO effectively enhances language feature grouping without reliance on labels or prior knowledge."
        },
        "Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation": {
          "authors": [
            "Jiahao Chen",
            "Yipeng Qin",
            "Lingjie Liu",
            "Jiangbo Lu",
            "Guanbin Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_NeRF-HuGS_Improved_Neural_Radiance_Fields_in_Non-static_Scenes_Using_Heuristics-Guided_CVPR_2024_paper.html",
          "ref_texts": "[49] Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, and Xiaowei Zhou. Autorecon: Automated 3d object discovery and reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21382\u201321391, 2023. 3",
          "ref_ids": [
            "49"
          ],
          "1": "Recent works have also used it to estimate the scene depth [43], locate target objects [49, 55] or initialize the set of 3D Gaussians [13]."
        },
        "Multi-view aggregation network for dichotomous image segmentation": {
          "authors": [
            "Qian Yu",
            "Xiaoqi Zhao",
            "Youwei Pang",
            "Lihe Zhang",
            "Huchuan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Yu_Multi-view_Aggregation_Network_for_Dichotomous_Image_Segmentation_CVPR_2024_paper.html",
          "ref_texts": "[37] Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, and Xiaowei Zhou. Autorecon: Automated 3d object discovery and reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21382\u201321391, 2023. 3",
          "ref_ids": [
            "37"
          ],
          "1": "In recent years, the integration of multi-view information with deep learning has garnered significant attention in many areas, such as 3D object recognition [33, 43], 3D reconstruction [17, 36, 37, 41], and feature matching [12, 32]."
        },
        "Nto3d: Neural target object 3d reconstruction with segment anything": {
          "authors": [
            "Xiaobao Wei",
            "Renrui Zhang",
            "Jiarui Wu",
            "Jiaming Liu",
            "Ming Lu",
            "Yandong Guo",
            "Shanghang Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wei_NTO3D_Neural_Target_Object_3D_Reconstruction_with_Segment_Anything_CVPR_2024_paper.html",
          "ref_texts": "[50] Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, and Xiaowei Zhou. Autorecon: Automated 3d object discovery and reconstruction. In CVPR, 2023. 3",
          "ref_ids": [
            "50"
          ],
          "1": "AutoRecon [50] leverages self-supervised 2D vision transformer features and reconstruct decomposed neural scene representations with decomposed point clouds, to achieve accurate object reconstruction and segmentation."
        },
        "Unsupervised object localization in the era of self-supervised vits: A survey": {
          "authors": [
            "O Sim\u00e9oni",
            "\u00c9 Zablocki",
            "S Gidaris",
            "G Puy"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-024-02167-8",
          "ref_texts": "[116] Y . Wang, X. He, S. Peng, H. Lin, H. Bao, and X. Zhou. Autorecon: Automated 3d object discovery and reconstruction. In CVPR, 2023.",
          "ref_ids": [
            "116"
          ],
          "1": "For instance, AutoRecon [116] advances unsupervised 3D object localization in object-centric videos."
        },
        "DORec: Decomposed Object Reconstruction and Segmentation Utilizing 2D Self-Supervised Features": {
          "authors": [
            "J Wu",
            "S Li",
            "S Ji",
            "Y Yang",
            "Y Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10777610/",
          "ref_texts": "[34] Y . Wang, X. He, S. Peng, H. Lin, H. Bao, and X. Zhou, \u201cAutorecon: Automated 3d object discovery and reconstruction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.",
          "ref_ids": [
            "34"
          ],
          "1": "AutoRecon [34] aggregates DINO features to point clouds for coarse foreground segmentation but assumes a planar background for decomposition, making it difficult to handle complex environments.",
          "3": "BlendedMVS Dataset: For the BlendedMVS dataset [38], we use the data published by [35] and [34] for fair comparisons."
        },
        "A Qualitative Analysis Strategy Towards AI-Enabled 3D City Reconstruction": {
          "authors": [
            "A Christodoulides"
          ],
          "url": "https://www.swansea.ac.uk/media/Andreas-Christodoulides---Thesis.pdf",
          "ref_texts": "[54] Y. Wang, X. He, S. Peng, H. Lin, H. Bao, and X. Zhou, \u201cAutorecon: Automated 3d object discovery and reconstruction,\u201d inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 21382\u201321391.",
          "ref_ids": [
            "54"
          ],
          "3": "Authors of AutoRecon [54] utilise a self-supervised ViT to combine features from images on top of point clouds.",
          "5": "The transformer-based approaches [50,54,56,73] are also seen to utilise multi-view images.",
          "6": "AutoRecon [54] combines the features learned from the different views and decomposes them into a point cloud representation.",
          "7": "Among the approaches presented, it is particularly common to compute an implicit representation and then extract meshes through marching cubes, such as [4,12,47,49,50,52,54,56,58,63 \u201366].",
          "9": "Many approaches directly output SDFs through directly using MLPs with SDF loss functions [45\u201347,49,54,56,64 \u201366].",
          "11": "Approaches [45 \u201347,54] whilst having semantic capabilities in that they can understand object classes, they are limited to single object reconstructions."
        }
      }
    },
    {
      "title": "learning neural volumetric representations of dynamic humans in minutes",
      "id": 15,
      "valid_pdf_number": "57/67",
      "matched_pdf_number": "48/57",
      "matched_rate": 0.8421052631578947,
      "citations": {
        "A survey on 3d gaussian splatting": {
          "authors": [
            "G Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2401.03890",
          "ref_texts": "[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2023, pp. 8759\u2013",
          "ref_ids": [
            "306"
          ],
          "1": "The same testing views following [306] are adopted.",
          "2": "38 Instant-NVR [306][CVPR23] 31."
        },
        "Hugs: Human gaussian splats": {
          "authors": [
            "Muhammed Kocabas",
            "Hao Rick",
            "James Gabriel",
            "Oncel Tuzel",
            "Anurag Ranjan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Kocabas_HUGS_Human_Gaussian_Splats_CVPR_2024_paper.html",
          "ref_texts": "[56] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 7",
          "ref_ids": [
            "56"
          ],
          "2": "We compare with recent previous work that report their evaluation on this dataset which include NeuralBody [9], HumanNerf [8], InstantNVR [56], and MonoHuman [14]."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[7] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proc. of CVPR , 2023. 2, 6, 8",
          "ref_ids": [
            "7"
          ],
          "4": "Instant-NVR [7] and InstantAvatar [12] achieve instant training within 5 minutes.",
          "5": "[7] also utilizes iNGP and represents non-rigid deformations in the UV space, which enables fast training and modeling of posedependent non-rigid deformations."
        },
        "Gauhuman: Articulated gaussian splatting from monocular human videos": {
          "authors": [
            "Shoukang Hu",
            "Tao Hu",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.html",
          "ref_texts": "[30] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8759\u20138770, 2023. 1, 2, 3, 6",
          "ref_ids": [
            "30"
          ],
          "1": "On the contrary, a recent line of research focuses on learning efficient 3D human representations [30, 49] to improve training speed.",
          "2": "20418 coordinate-based neural networks, methods like [30, 49] incorporate 3D human modelling with multi-resolution hash encoding (MHE) [78] to accelerate the training.",
          "3": "To improve the training efficiency, recent research [30, 49] apply multi-hashing encoding (MHE) to reduce the training time to several minutes, while their rendering remains comparatively slow.",
          "10": "To accelerate the training of 3D human modelling, InstantNVR [30] and InstantAvatar [49] reduce the training to several minutes with multi-hashing encoding representation.",
          "11": "InstantNVR [30] and InstantAvatar [49] reduce the training time to several minutes with a multi20423 GTOursInstantAvatarInstantNVRHumanNeRFASNBNHP 1 min189 FPS3 min4."
        },
        "Gart: Gaussian articulated template models": {
          "authors": [
            "Jiahui Lei",
            "Yufu Wang",
            "Georgios Pavlakos",
            "Lingjie Liu",
            "Kostas Daniilidis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html",
          "ref_texts": "[13] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 2, 5",
          "ref_ids": [
            "13"
          ],
          "2": "and Instant-NVR [13], which demonstrate better fidelity than classical mesh-based representations [2]."
        },
        "Human gaussian splatting: Real-time rendering of animatable avatars": {
          "authors": [
            "Arthur Moreau",
            "Jifei Song",
            "Helisa Dhamo",
            "Richard Shaw",
            "Yiren Zhou",
            "Eduardo Perez"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Moreau_Human_Gaussian_Splatting_Real-time_Rendering_of_Animatable_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[7] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 2",
          "ref_ids": [
            "7"
          ],
          "1": "Neural Actor [29], Animatable NeRF [43] and InstantNVR [7] use pose-dependent networks to learn deformation or blendweights fields, but they have been observed to generalize poorly to novel body poses."
        },
        "Gomavatar: Efficient animatable human modeling from monocular video using gaussians-on-mesh": {
          "authors": [
            "Jing Wen",
            "Xiaoming Zhao",
            "Zhongzheng Ren",
            "Alexander G. Schwing",
            "Shenlong Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.html",
          "ref_texts": "[16] Chen Geng, Sida Peng, Zhenqi Xu, Hujun Bao, and Xiaowei Zhou. Learning Neural V olumetric Representations of Dynamic Humans in Minutes. InCVPR, 2023.1, 3, 4",
          "ref_ids": [
            "16"
          ],
          "1": "Reconstruction of digital humans from monocular videos has been studied intensively recently [16, 25, 64, 70, 81].",
          "2": "Neural fields based avatars [16, 27, 70, 81] offer photorealism, but they are challenging to articulate and lack explicit geometry, making them less compatible with game engines.",
          "3": "To address this limitation, human modeling from videos has received a lot of attention from the community: many prior efforts utilize implicit representations and differentiable renderers for either non-animatable [54] or animatable [16, 22, 25, 37, 39, 53, 55, 64, 69, 70, 75, 81, 83, 89] scene-specific human modeling while other efforts focus on scene-agnostic modeling [9, 14, 21, 31, 34, 35, 56, 85, 87].",
          "4": "Inspired by the success, concurrent works explore efficient NeRF rendering for humans [16, 58]."
        },
        "Cg-slam: Efficient dense rgb-d slam in a consistent uncertainty-aware 3d gaussian field": {
          "authors": [
            "J Hu",
            "X Chen",
            "B Feng",
            "G Li",
            "L Yang",
            "H Bao"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_6",
          "ref_texts": "15. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: CVPR (2023) 3",
          "ref_ids": [
            "15"
          ],
          "1": "2 Neural Implict Radiance Field based SLAM Neural radiance fields [26] have shown promising potential in many 3D computer vision applications, such as novel view synthesis [3,4,14,27], dynamic scene modeling [13,15,38,45], and generalization [17,37,48,50,55]."
        },
        "Sifu: Side-view conditioned implicit function for real-world usable clothed human reconstruction": {
          "authors": [
            "Zechuan Zhang",
            "Zongxin Yang",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.html",
          "ref_texts": "[18] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 3",
          "ref_ids": [
            "18"
          ],
          "1": "The rise of Neural Radiance Fields (NeRF) has seen methods [18, 20, 33, 34, 56, 59, 62, 63, 78, 88] using videos or multi-view images to optimize NeRF for human form capture."
        },
        "Physavatar: Learning the physics of dressed 3d avatars from visual observations": {
          "authors": [
            "Y Zheng",
            "Q Zhao",
            "G Yang",
            "W Yifan",
            "D Xiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72913-3_15",
          "ref_texts": "27. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8759\u20138770 (2023)",
          "ref_ids": [
            "27"
          ],
          "1": "Several different types of representations have been explored for clothed avatars, including meshes [68] with dynamic textures [4,33,131], neural surface [16,90,107] and radiance fields [14,18,24,27,44\u201346, 86,87,99,133], point sets [66,67,69], and 3D Gaussians [34,58,82,134]."
        },
        "Surmo: surface-based 4D motion modeling for dynamic human rendering": {
          "authors": [
            "Tao Hu",
            "Fangzhou Hong",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_SurMo_Surface-based_4D_Motion_Modeling_for_Dynamic_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[9] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 6",
          "ref_ids": [
            "9"
          ],
          "1": "438 Instant-NVR [9].",
          "2": "We compare our method against SOTA methods including Neural Body [43], HumanNeRF [61], InstantNVR [9], ARAH [58], DV A [47] and HVTR++ [18]."
        },
        "Human101: Training 100+ fps human gaussians in 100s from 1 view": {
          "authors": [
            "M Li",
            "J Tao",
            "Z Yang",
            "Y Yang"
          ],
          "url": "https://arxiv.org/abs/2312.15258",
          "ref_texts": "[15] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 1, 2, 3, 4, 6, 7, 8, 12, 13, 16, 17, 18",
          "ref_ids": [
            "15"
          ],
          "19": "Utilizing [39]\u2019s voxel grid representation, Instantnvr [15] manage to shorten the convergence time into 5 minutes.",
          "21": "Remarkably, our approach achieves optimization within approximately 100 seconds, yielding results that are comparable with or surpass [15, 22, 24] in terms of PSNR and LPIPS.",
          "22": "Moreover, for inference speed, our model is 67 times faster than InstantNvr [15] and 11 times faster than InstantAvatar [22] in 512 \u00d7 512 resolution.",
          "39": "8, Human101 demonstrates notable memory efficiency compared to prior methods [15, 22].",
          "41": "Results demonstrate that our model utilizes much less GPU memory and disk usage than [15] while maintaining comparable or better visual quality."
        },
        "Learning unsigned distance functions from multi-view images with volume rendering priors": {
          "authors": [
            "W Zhang",
            "K Shi",
            "YS Liu",
            "Z Han"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_22",
          "ref_texts": "15. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning Neural Volumetric Representations of Dynamic Humans in Minutes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8759\u20138770 (2023) 1",
          "ref_ids": [
            "15"
          ],
          "1": "Using coordinate based deep neural networks, a mapping from locations to attributes at these locations like geometry [39,43], color [10,38], and motion [15] can be learned as an implicit representation."
        },
        "Dynamic nerf: A review": {
          "authors": [
            "J Lin"
          ],
          "url": "https://arxiv.org/abs/2405.08609",
          "ref_texts": "[20] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. 2023. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 8759\u20138770.",
          "ref_ids": [
            "20"
          ],
          "1": "[20] proposed a novel method to make the efficient Dynamic NeRF for humans in minutes with a high visual quality, by designing a voxelized and part-based representation method."
        },
        "Occfusion: Rendering occluded humans with generative diffusion priors": {
          "authors": [
            "A Sun",
            "T Xiang",
            "S Delp",
            "FF Li"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/a7bfdee9544cea324cf183ac03c7d5c0-Abstract-Conference.html",
          "ref_texts": "[6] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8759\u20138770, 2023.",
          "ref_ids": [
            "6"
          ],
          "1": "To speed up NeRF-based models, multi-resolution hash encoding [40, 43, 6, 18], and generalizability [41, 2, 13, 27] have been proposed."
        },
        "ihuman: Instant animatable digital humans from monocular videos": {
          "authors": [
            "P Paudel",
            "A Khanal",
            "DP Paudel",
            "J Tandukar"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73226-3_18",
          "ref_texts": "58. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 9054\u20139063. Computer Vision Foundation / IEEE (2021).https://doi.org/10.1109/CVPR46437.",
          "ref_ids": [
            "58"
          ],
          "2": "NeRF [45]), several methods [8\u201310,13,14,16,18,18, 21\u201323,32,32,37,40,42,48,57,58,61, 64,65,69,71,73,75,76,79,82,84,87], have been developed to capture high-fidelity humans from multiple frames of videos.",
          "3": "Anim-NeRF [8] and other similar methods [9,10,16,18,19,21, 23,37,40,56,58,69,71,73] extend NERF to dynamic scenes by using SMPL [43] guided deformations between the observed space and a static canonical space allowing for explicit control."
        },
        "Motion-oriented compositional neural radiance fields for monocular dynamic human modeling": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72933-1_27",
          "ref_texts": "14. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: CVPR (2023)",
          "ref_ids": [
            "14"
          ],
          "1": "b) The proposed MoCo-NeRF achieves state-of-the-art rendering quality and noteworthy efficiency in novel view synthesis, compared to leading competitors [14,21,73].",
          "4": "InstantNVR [14] achieves efficient training with part-based representation and 2D motion parameterization for learning non-rigid motions as 3D offsets.",
          "5": "In addition, a concurrent work GauHuman [21], which also employs spatial offset approach with Gaussian Splatting (GS) [29], further enhances the efficiency and performance upon [14] by leveraging SMPL for the density control of GS.",
          "7": "Ourapproachclearlyoutperformsthe best-performing competitors HumanNeRF [73], Instant-NVR [14], and GauHuman [21] and can further extend to support joint training of multiple subjects.",
          "9": "Despite pixel-wise metrics (PSNR, SSIM) tend to favor smooth renderings generated from [14,21], we achieve state-of-the-art performance while effectively modeling the photo-realistic non-rigid motions as illustrated in Fig."
        },
        "Generalizable neural human renderer": {
          "authors": [
            "M Masuda",
            "J Park",
            "S Iwase",
            "R Khirodkar"
          ],
          "url": "https://arxiv.org/abs/2404.14199",
          "ref_texts": "23. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: CVPR. pp. 8759\u20138770 (2023)",
          "ref_ids": [
            "23"
          ],
          "1": "[23] also presents a generalizable NeRF model to leverage learned prior to reducing the optimization time to minutes."
        },
        "Humannerf-se: A simple yet effective approach to animate humannerf with diverse poses": {
          "authors": [
            "Caoyuan Ma",
            "Lun Liu",
            "Zhixiang Wang",
            "Wu Liu",
            "Xinchen Liu",
            "Zheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Ma_HumanNeRF-SE_A_Simple_yet_Effective_Approach_to_Animate_HumanNeRF_with_CVPR_2024_paper.html",
          "ref_texts": "[11] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes.arXiv preprint arXiv:2302.12237, 2023. 1, 2, 3",
          "ref_ids": [
            "11"
          ],
          "1": "Deformable NeRFs endow implicit fields with the capability to express dynamic objects [37, 38, 42, 53, 59] or even humans [11, 16, 17, 56, 62].",
          "3": "Existing monocular-based methods [11, 16, 17, 56, 62] usually decompose the human implicit field into rigid and non-rigid components, reducing the ill-posedness in joint optimization.",
          "4": "Inspired by [37, 38, 42], which maps rays in dynamic scenes to canonical space, [11, 16, 17, 56, 62] introduce priors to regularize the deformation."
        },
        "Expressive Gaussian Human Avatars from Monocular RGB Video": {
          "authors": [
            "H Hu",
            "Z Fan",
            "T Wu",
            "Y Xi",
            "S Lee",
            "G Pavlakos"
          ],
          "url": "https://arxiv.org/abs/2407.03204",
          "ref_texts": "[10] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, pages 8759\u20138770, 2023.",
          "ref_ids": [
            "10"
          ],
          "1": "To tackle the issue of slow computation in implicit models, some works have proposed various techniques to reduce training [10, 14] or inference time [34, 27, 5]."
        },
        "Generalizable neural voxels for fast human radiance fields": {
          "authors": [
            "T Yi",
            "J Fang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://arxiv.org/abs/2303.15387",
          "ref_texts": "[24] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In arXiv:2302.12237, 2023. 2",
          "ref_ids": [
            "24"
          ],
          "1": "Some other methods [83, 40] share similar ideas, which are not limited to the human body, but also apply to other dynamic scenes [61, 55, 54, 18, 32, 24]."
        },
        "PGAHum: prior-guided geometry and appearance learning for high-fidelity animatable human reconstruction": {
          "authors": [
            "H Wang",
            "Q Xu",
            "H Chen",
            "R Ma"
          ],
          "url": "https://arxiv.org/abs/2404.13862",
          "ref_texts": "[6] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. 2023. Learning Neural Volumetric Representations of Dynamic Humans in Minutes. In CVPR.",
          "ref_ids": [
            "6"
          ],
          "1": "Also, methods [6, 16\u201318, 42, 45] that support learning neural radiance field (NeRF) from dynamic animation sequences are also developed so that human in novel unseen poses can be rendered.",
          "5": "It\u2019s worthy noting that when comparing with the InstantNVR [6] method, we directly use their official code and train on the original ZJU-MoCap dataset [35] for a fair comparison."
        },
        "Explicifying neural implicit fields for efficient dynamic human avatar modeling via a neural explicit surface": {
          "authors": [
            "R Zhang",
            "J Chen",
            "Q Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611707",
          "ref_texts": "[8] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. 2023. Learning Neural Volumetric Representations of Dynamic Humans in Minutes. arXiv preprint arXiv:2302.12237 (2023).",
          "ref_ids": [
            "8"
          ],
          "1": "While other methods [8, 16, 28, 39] establish dense correspondences between video frames through deformation fields for joint learning of a shared canonical model, they can neglect variations in lighting or wrinkle shadows, which can decrease fidelity.",
          "2": "All the compared methods and other SOTA methods [8, 16, 38] model the human body as implicit fields, which requires dense sample points during inference."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[34] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d in CVPR, 2023.",
          "ref_ids": [
            "34"
          ],
          "1": "Unlike prior methods [29], [34], [35] that depend on complex backward skinning [36], [37] for transformations to a canonical space [38], Gaussian primitives for clothed human avatars are easily and efficiently animated via forward skinning, similar to the template mesh [39].",
          "2": "In addition to the textured mesh, NeRFs [16] also became a useful representation for photo-realistic clothed avatar reconstruction [34], [35], [38], [77], [78], [79], [80], [81], [82], [83], [84] from monocular or multi-view videos.",
          "3": "3 PBR properties estimation results on the real-world datasets with complex motions We follow the setting in the InstantNVR [34] and choose view 4 from ZJU-MoCap as the training view."
        },
        "Efficient Integration of Neural Representations for Dynamic Humans": {
          "authors": [
            "W Li",
            "L Zeng",
            "C Gao",
            "N Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10666828/",
          "ref_texts": "[14] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 8759\u20138770.",
          "ref_ids": [
            "14"
          ],
          "2": "Remarkably, Instant-Nvr [14] has emerged as a significant breakthrough, accomplishing optimization within approximately 5 minutes.",
          "4": "In contrast, Instant-Nvr [14] introduces a part-based representation to more efficiently allocate network capacity among different body parts.",
          "10": "To improve optimization efficiency, Instant-Nvr [14] proposes a part-based human representation and a 2D motion parameterization scheme for effective modeling of canonical humans and deformation fields."
        },
        "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
          "authors": [
            "H Wang",
            "W Zhang",
            "S Liu",
            "X Zhou",
            "J Li",
            "Z Tang"
          ],
          "url": "https://arxiv.org/abs/2405.12477",
          "ref_texts": "[5] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8759\u20138770, 2023. 6, 7",
          "ref_ids": [
            "5"
          ],
          "3": "Quantitative Results To verify the effectiveness of our method in solving the geometric distortion problem in reconstructing the human body, we compare our method with NeuralBody [35], HumanNeRF [43] AnimateNeRF [34], InstantNVR [5] InstantAvatar [15] GauHuman [11] on the ZJU dataset and the Monocap dataset, as shown in Table 1."
        },
        "MV2MP: Segmentation Free Performance Capture of Humans in Direct Physical Contact from Sparse Multi-Cam Setups": {
          "authors": [
            "Sergei Eliseev",
            "Leonid Shtanko",
            "Rasim Akhunzianov",
            "Yaroslav Romanenko",
            "Anatoly Starostin"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2024/html/Eliseev_MV2MP_Segmentation_Free_Performance_Capture_of_Humans_in_Direct_Physical_ACCV_2024_paper.html",
          "ref_texts": "7. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8759\u20138770 (2023)",
          "ref_ids": [
            "7"
          ],
          "1": "Fasterversion[7]ofneuralbodysignificantlyacceleratestheoptimizationprocess for creating neural volumetric representations of dynamic humans, achieving a 100x speedup by efficient distribution of the network\u2019s representational power across different human body parts and models the 3D human deformation in a 2D domain by projecting near-surface points to neighboring regions on a parametric human model (e."
        },
        "MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering": {
          "authors": [
            "A AlMughrabi",
            "R Marques",
            "P Radeva"
          ],
          "url": "https://arxiv.org/abs/2407.02668",
          "ref_texts": "[14] Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759\u20138770 (2023)",
          "ref_ids": [
            "14"
          ],
          "1": "The groundbreaking work in this area is Neural Radiance Fields (NeRF) [2], which has been extensively researched and applied in various domains, such as novel view synthesis [2\u20135], 3D generation [4, 6\u201310], deformation [11\u201313], neural dynamic [14\u201316], depth-supervised methods [17\u201319], fast neural rendering [14, 20\u201323], scene understanding [24\u201328], controllable scene synthesis [29\u201332], one-shot rendering [33\u201336], zero-shot rendering [4, 10, 37\u201339], and more."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "175. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: CVPR, pp. 8759\u20138770 (2023).https://doi.org/10.1109/ CVPR52729.2023.00846",
          "ref_ids": [
            "175"
          ],
          "1": "Some methodologies [157, 174, 175] adopt a human model based on InstantNGP [158] to reduce training costs (Fig."
        },
        "AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation": {
          "authors": [
            "M Li",
            "S Yao",
            "C Kai",
            "Z Xie",
            "K Chen",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2502.19441",
          "ref_texts": "[80] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d in CVPR, 2023.",
          "ref_ids": [
            "80"
          ],
          "1": "[80] C."
        },
        "Bundle Adjusted Gaussian Avatars Deblurring": {
          "authors": [
            "M Niu",
            "Y Zhan",
            "Q Zhu",
            "Z Li",
            "W Wang",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2411.16758",
          "ref_texts": "[8] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8759\u20138770, 2023. 6",
          "ref_ids": [
            "8"
          ],
          "1": "Consistent with previous works [8, 11, 52, 67], we select six human subjects (IDs: 377, 386, 387, 392, 393, 394) for our experiments."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[9] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8759\u2013",
          "ref_ids": [
            "9"
          ],
          "1": "3 [9] C."
        },
        "WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction": {
          "authors": [
            "Z Wang",
            "Z Dou",
            "Y Liu",
            "C Lin",
            "X Dong",
            "Y Guo"
          ],
          "url": "https://arxiv.org/abs/2502.01045",
          "ref_texts": "[15] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d in CVPR, 2023.",
          "ref_ids": [
            "15"
          ],
          "1": "To reconstruct from monocular videos, other recent methods [10], [11], [12], [13], [14], [15], [16] reconstruct dynamic avatars by animating them within a canonical space derived from observation spaces using video frames.",
          "2": "Compared to state-of-the-art methods [10], [15], [24], [25], [26], WonderHuman produces higher-quality photorealistic renderings of reconstructed human avatars, particularly in rendering visually plausible content for previously unseen parts of the human body.",
          "3": "But other works [14], [15], [50], [51] additionally introduced explicit representations like meshes [14], [50], and points [51] to improve its efficiency.",
          "4": "We train and test using this dataset following Instant-NVR [15].",
          "5": "3 Comparisons with Video-based Methods We conduct comparisons of our method with HumanNeRF [10], Instant-NVR [15], SplattingAvatar [24], ExAvatar [25], GaussianAvatar [16], and GuessTheUnseen [26].",
          "6": "For a fair comparison, Instant-NVR [15] is trained on the revised version of the ZJU-Mocap dataset, which offers refined camera parameters, SMPL fittings, and more accurate instance masks with body-part segmentation, crucial for the execution of their method.",
          "7": "We compare the novel view synthesis quality with HumanNeRF [10], Instant-NVR [15], SplattingAvatar [24], ExAvatar [25] and GaussianAvatar [16].",
          "8": "While our approach generally yields more realistic results, similar to many existing methods [10], [15], [16], it still faces challenges in accurately modeling loose attire, such as dresses, underscoring areas for potential improvement in future iterations.",
          "9": "[15] C."
        },
        "Neural rendering of humans in novel view and pose from monocular video": {
          "authors": [
            "T Wang",
            "N Sarafianos",
            "MH Yang",
            "T Tung"
          ],
          "url": "https://arxiv.org/abs/2204.01218",
          "ref_texts": "[12] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In CVPR, 2023. 2",
          "ref_ids": [
            "12"
          ],
          "1": "To model the temporal cues across frames, previous works [7, 20, 22, 48, 53] combine motion information and introduce animatable avatar approaches [11, 12, 16, 21, 27, 28, 42, 43, 45, 46, 49, 55, 56]."
        },
        "NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images": {
          "authors": [
            "J Yu",
            "D Nandi",
            "R Seidel",
            "G Hirtz"
          ],
          "url": "https://arxiv.org/abs/2402.18196",
          "ref_texts": "16. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8759\u20138770",
          "ref_ids": [
            "16"
          ],
          "1": "Recent advances focus on reducing temporal cost [5,16,26,74], reducing supervision [4,17,22,56] and enabling multiple usage of a single trained model [44,62]."
        },
        "RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians": {
          "authors": [
            "S Peng",
            "W Xie",
            "Z Wang",
            "X Guo",
            "Z Chen",
            "B Yang"
          ],
          "url": "https://arxiv.org/abs/2501.07104",
          "ref_texts": "[11] Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X., 2023. Learning neural volumetric representations of dynamic humans in minutes, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759\u20138770.",
          "ref_ids": [
            "11"
          ],
          "1": "Recentmethods[11,18,40,56]basedonimplicitneural fields [34, 37, 38] usually learn a canonical avatar representation by mapping camera rays from observation space to canonical space.",
          "2": "To address the expensive computation of NeRF-based animatablehumanavatar,effortshasbeenmadeinaccelerated data structures for speeding up training and inference of NeRFs [51, 69, 17, 11, 24].",
          "3": "InstantNVR[11]useiNGP[36]astheunderlyingrepresentationfor articulatedNeRFs,andmodellingnon-rigiddeformationsin theUVspaceforfasttraining."
        },
        "SAGA: Surface-Aligned Gaussian Avatar": {
          "authors": [
            "R Chen",
            "Y Cong",
            "J Liu"
          ],
          "url": "https://arxiv.org/abs/2412.00845",
          "ref_texts": "[18] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, \u201cLearning neural volumetric representations of dynamic humans in minutes,\u201d inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 8759\u20138770.",
          "ref_ids": [
            "18"
          ],
          "2": "To enable efficient training and rendering of dynamic human video, recent methods [18], [19] have applied InstantNGP [12] as the human representation.",
          "3": "InstantNVR [18] designs specific hash embedders for each human part to adjust representational power based on part complexity thus accelerating the convergence.",
          "10": "Moreover, SAGA also achieves third highest training efficiency of \u223c12 minutes and second highest real-time rendering speed at 60 FPS, which is comparable to the most efficient methods InstantNVR [18] and GauHuman [22]."
        },
        "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video": {
          "authors": [
            "H Wang",
            "X Cai",
            "X Sun",
            "J Yue",
            "Z Tang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2405.12806",
          "ref_texts": "[10] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8759\u20138770, 2023. 7, 17",
          "ref_ids": [
            "10"
          ],
          "1": "2 Comparison methods Compare MOSS with two categories of SOTA methods: Human NeRF-based methods, such as NeuralBody [39], HumanNeRF [50] AnimateNeRF [38], InstantNVR [10] InstantAvatar [17].",
          "6": "Method PSNR \u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 MonoCap Lan Marc InstantNVR [10] 32.",
          "7": "69 Olek Vlad InstantNVR [10] 34.",
          "8": "35 ZJU-MoCap 377 386 InstantNVR [10] 31.",
          "10": "393 394 InstantNVR [10] 29."
        },
        "Accelerating Human Avatar Creation: Pose-dependent Hybrid Representations for Efficient Rendering of Clothed Human Avatars": {
          "authors": [
            "Z Qian"
          ],
          "url": "https://www.research-collection.ethz.ch/handle/20.500.11850/610316",
          "ref_texts": "[21] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. arXiv preprint arXiv:2302.12237, 2023.",
          "ref_ids": [
            "21"
          ],
          "1": "ArXiv Pre-prints: A rich set of concurrent works have explored explicit tri-plane[18, 7, 80] and grid-based [41, 98, 29, 111, 21] methods for dynamic reconstruction."
        },
        "TAGA: Self-supervised Learning for Template-free Animatable Gaussian Avatars": {
          "authors": [
            "Z Zhai",
            "G Chen",
            "W Wang",
            "D Zheng",
            "J Xiao"
          ],
          "url": "https://openreview.net/forum?id=47wXbygsvp",
          "ref_texts": "[13] Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric representations of dynamic humans in minutes. In: CVPR (2023) 3, 7, 8, 17",
          "ref_ids": [
            "13"
          ],
          "5": "As described in Table 2, TAGA provides notable performances over all template-free methods across all metrics, including PSNR, SSIM, and LPIPS, as well as state-of-the-art SMPL-based NeRF methods like InstantAvatar [12] and InstantNVR [13]."
        },
        "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos\u2014Supplementary Material": {
          "authors": [
            "S Hu",
            "T Hu",
            "Z Liu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_GauHuman_Articulated_Gaussian_CVPR_2024_supplemental.pdf",
          "ref_texts": "[1] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8759\u20138770, 2023. 1",
          "ref_ids": [
            "1"
          ],
          "1": "InstantNVR [1] and InstantAvatar [4] propose to use multi-hashing encoding for fast training of 3D humans."
        }
      }
    },
    {
      "title": "neural 3d scene reconstruction with the manhattan-world assumption",
      "id": 8,
      "valid_pdf_number": "128/143",
      "matched_pdf_number": "105/128",
      "matched_rate": 0.8203125,
      "citations": {
        "Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction": {
          "authors": [
            "Z Yu",
            "S Peng",
            "M Niemeyer",
            "T Sattler"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/9f0b1220028dfa2ee82ca0a0e0fc52d1-Abstract-Conference.html",
          "ref_texts": "[18] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 7, 8, 9",
          "ref_ids": [
            "18"
          ],
          "1": "Manhattan world assumptions [18]",
          "2": "Manhattan-SDF [18] uses dense MVS depth maps from COLMAP [57] as supervision and adopts Manhattan world priors [10] to handle low-textured planar regions corresponding to walls, floors, etc.",
          "4": "We compare against a) state-of-the-art neural implicit surfaces methods: UNISURF [43], V olSDF [74], NeuS [69], and Manhattan-SDF [18].",
          "5": "For Replica and ScanNet, following [18, 37, 47, 48, 62, 82], we report the Chamfer Distance, the F-score with a threshold of 5cm, as well as a Normal Consistency measure.",
          "6": "On ScanNet, we use the test split from [18] and also follow their evaluation protocol in which depth maps are rendered from input camera poses and then re-fused using TSDF Fusion [11] to evaluate only observed areas."
        },
        "Benchmarking neural radiance fields for autonomous robots: An overview": {
          "authors": [
            "Y Ming",
            "X Yang",
            "W Wang",
            "Z Chen",
            "J Feng"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0952197624018438",
          "ref_texts": "[51] H.Guo,S.Peng,H.Lin,Q.Wang,G.Zhang,H.Bao,X.Zhou, Neural 3d scene reconstruction with the manhattan-world assumption, in: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR),2022,pp.5501\u20135510.doi: 10.1109/CVPR52688.",
          "ref_ids": [
            "51"
          ],
          "1": "ManhattanSDF [51] focuses on boosting reconstruction performance in planar regions.",
          "2": "To best compare the performance, we choose the most followed scene selections used in ManhattanSDF[51](scene0050,0084,0580,0616)."
        },
        "Towards better gradient consistency for neural signed distance functions via level set alignment": {
          "authors": [
            "Baorui Ma",
            "Junsheng Zhou",
            "Shen Liu",
            "Zhizhong Han"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ma_Towards_Better_Gradient_Consistency_for_Neural_Signed_Distance_Functions_via_CVPR_2023_paper.html",
          "ref_texts": "[24] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InIEEE Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 3, 7",
          "ref_ids": [
            "24"
          ],
          "1": "Using gradient descent, we can train neural networks by adjusting parameters to minimize errors to either signed distance ground truth [9, 31, 45, 51, 52] or signed distances inferred from 3D point clouds [1,2,11,22,38,60,77] or multiview images [19, 24, 66\u201369, 73, 74, 76].",
          "2": "Following methods improve accuracy of implicit functions using multi-view consistency [10, 20, 27, 68, 69, 76] or additional priors including depth [3, 76, 80], normals [24, 67, 76].",
          "3": "Without signed distance ground truth, current methods infer SDFs by mining supervision from 3D point clouds with normals [1,23,60], 3D point clouds without normals [11, 38, 54], or multi-view images [19,24, 66\u201369, 73, 74, 76]."
        },
        "Streetsurf: Extending multi-view implicit surface reconstruction to street views": {
          "authors": [
            "J Guo",
            "N Deng",
            "X Li",
            "Y Bai",
            "B Shi",
            "C Wang"
          ],
          "url": "https://arxiv.org/abs/2306.04988",
          "ref_texts": "[16] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5511\u20135520, 2022.",
          "ref_ids": [
            "16"
          ],
          "2": "Multi-view 3D reconstruction is typically an underconstrained problem, driving researchers to introduce different priors or regularizations, such as multi-view stereo prior [11], geometric priors [16, 31, 57, 67], shadow information [23] and sparse regularization [68]."
        },
        "Object-compositional neural implicit surfaces": {
          "authors": [
            "Q Wu",
            "X Liu",
            "Y Chen",
            "K Li",
            "C Zheng",
            "J Cai"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19812-0_12",
          "ref_texts": "7. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5511\u2013",
          "ref_ids": [
            "7"
          ],
          "1": "The emerging neural implicit representation rendering approaches provide promising results in novel view synthesis [20] and 3D reconstruction [24,39,36,7]."
        },
        "Dn-splatter: Depth and normal priors for gaussian splatting and meshing": {
          "authors": [
            "M Turkulainen",
            "X Ren",
            "I Melekhov",
            "O Seiskari"
          ],
          "url": "https://arxiv.org/abs/2403.17822",
          "ref_texts": "[12] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 2",
          "ref_ids": [
            "12"
          ],
          "1": "For SDF-based models, ManhattanSDF [12] uses planar constraints on walls and flat surfaces to improve indoor reconstruction, and MonoSDF [59] uses depth and normal monocular estimates for scene geometry regularization."
        },
        "Objectsdf++: Improved object-compositional neural implicit surfaces": {
          "authors": [
            "Qianyi Wu",
            "Kaisiyuan Wang",
            "Kejie Li",
            "Jianmin Zheng",
            "Jianfei Cai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wu_ObjectSDF_Improved_Object-Compositional_Neural_Implicit_Surfaces_ICCV_2023_paper.html",
          "ref_texts": "[12] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022.6, 7, 8",
          "ref_ids": [
            "12"
          ],
          "1": "(11) Moreover, recent approaches [57, 44, 12] have proven that surface reconstruction could be significantly improved by incorporating geometry guidance such as depth and surface normal.",
          "2": "Following the setting [12, 57, 60], we select 4 scenes from ScanNet for experiments and report the accuracy, completeness, Chamfer Distance, precision, recall and F-score."
        },
        "Learning a more continuous zero level set in unsigned distance fields through level set projection": {
          "authors": [
            "Junsheng Zhou",
            "Baorui Ma",
            "Shujuan Li",
            "Shen Liu",
            "Zhizhong Han"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Learning_a_More_Continuous_Zero_Level_Set_in_Unsigned_Distance_ICCV_2023_paper.html",
          "ref_texts": "[28] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "28"
          ],
          "1": "ManhattanSDF [28] leverages the manhattan assumption for representing indoor scenes."
        },
        "I2-sdf: Intrinsic indoor scene reconstruction and editing via raytracing in neural sdfs": {
          "authors": [
            "Jingsen Zhu",
            "Yuchi Huo",
            "Qi Ye",
            "Fujun Luan",
            "Jifan Li",
            "Dianbing Xi",
            "Lisha Wang",
            "Rui Tang",
            "Wei Hua",
            "Hujun Bao",
            "Rui Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 2",
          "ref_ids": [
            "8"
          ],
          "1": "To tackle with texture-less regions, additional priors are exploited to guide the network optimization, including semantic priors [8], normal priors [34, 42] and depth priors [42]."
        },
        "Real acoustic fields: An audio-visual room acoustics dataset and benchmark": {
          "authors": [
            "Ziyang Chen",
            "Israel D. Gebru",
            "Christian Richardt",
            "Anurag Kumar",
            "William Laney",
            "Andrew Owens",
            "Alexander Richard"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_Real_Acoustic_Fields_An_Audio-Visual_Room_Acoustics_Dataset_and_Benchmark_CVPR_2024_paper.html",
          "ref_texts": "[20] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3D scene reconstruction with the Manhattan-world assumption. In CVPR, 2022. 3",
          "ref_ids": [
            "20"
          ],
          "1": "Many approaches that focus on 3D scene reconstruction use representations such as (truncated) signed distance fields to combine multiple observations from RGB-D sensors [43, 44, 58, 69] or standard color videos [20, 25, 41, 59]."
        },
        "Neural-PBIR reconstruction of shape, material, and illumination": {
          "authors": [
            "Cheng Sun",
            "Guangyan Cai",
            "Zhengqin Li",
            "Kai Yan",
            "Cheng Zhang",
            "Carl Marshall",
            "Bin Huang",
            "Shuang Zhao",
            "Zhao Dong"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Sun_Neural-PBIR_Reconstruction_of_Shape_Material_and_Illumination_ICCV_2023_paper.html",
          "ref_texts": "[10] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 2",
          "ref_ids": [
            "10"
          ],
          "1": "Subsequent works improve quality by introducing regularization losses to encourage surface smoothness [39], multiview consistency [8], and manhattan alignment [10]."
        },
        "Total-Decom: decomposed 3D scene reconstruction with minimal interaction": {
          "authors": [
            "Xiaoyang Lyu",
            "Chirui Chang",
            "Peng Dai",
            "Tian Sun",
            "Xiaojuan Qi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lyu_Total-Decom_Decomposed_3D_Scene_Reconstruction_with_Minimal_Interaction_CVPR_2024_paper.html",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 5",
          "ref_ids": [
            "8"
          ],
          "1": "To regularize the reconstruction of these areas, we follow the Manhattan world assumption [8, 42], i."
        },
        "Mirror-nerf: Learning neural radiance fields for mirrors with whitted-style ray tracing": {
          "authors": [
            "J Zeng",
            "C Bao",
            "R Chen",
            "Z Dong",
            "G Zhang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611857",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. 2022. Neural 3d scene reconstruction with the manhattanworld assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5511\u20135520.",
          "ref_ids": [
            "8"
          ],
          "1": "Several extensions and improvements have been proposed to apply NeRF to more challenging problems, such as scene reconstruction [1, 8, 13, 29, 30, 32, 36, 38, 39, 44, 48], generalization [24, 33], novel view extrapolation [35, 45], scene manipulation [2, 28, 40\u201342], SLAM [23, 54], segmentation [20, 53], human body [18, 31] and so on."
        },
        "Learning neural implicit through volume rendering with attentive depth fusion priors": {
          "authors": [
            "P Hu",
            "Z Han"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/68637ee6b30276f900bc67320466b69f-Abstract-Conference.html",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.",
          "ref_ids": [
            "16"
          ],
          "1": "Using volume rendering, we can learn neural implicit functions by comparing their 2D renderings with multi-view ground truth including color [88, 97], depth [88, 3, 97] or normal [88, 76, 16] maps.",
          "2": "2 Related Work Learning 3D implicit functions using neural networks has made huge progress [61, 62, 46, 51, 88, 78, 73, 76, 16, 56, 34, 25, 11, 93, 42, 44, 9, 43, 4, 33, 5, 19, 26, 47, 20, 70, 12, 48, 55, 17, 27, 79, 35, 32, 21, 7, 58, 86, 52].",
          "3": "We can learn neural implicit functions from 3D ground truth [23, 8, 54, 45, 68, 39, 69], 3D point clouds [94, 41, 15, 1, 92, 2, 10] or multi-view images [46, 14, 51, 77, 88, 78, 73, 76, 16].",
          "4": "improve accuracy of implicit functions using additional priors or losses related to depth [88, 3, 97], normals [88, 76, 16], and multi-view consistency [14]."
        },
        "Helixsurf: A robust and efficient neural implicit surface learning of indoor scenes with iterative intertwined regularization": {
          "authors": [
            "Zhihao Liang",
            "Zhangjin Huang",
            "Changxing Ding",
            "Kui Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Liang_HelixSurf_A_Robust_and_Efficient_Neural_Implicit_Surface_Learning_of_CVPR_2023_paper.html",
          "ref_texts": "[14] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5511\u20135520, 2022. 2, ",
          "ref_ids": [
            "14"
          ],
          "1": "Most recent works [14, 40, 50] try to get rid of this dilemma by incorporating geometric cues provided by models pre-trained on auxiliary data.",
          "2": "For the ScanNet, we follow ManhattanSDF [14] and select 4 scenes to conduct our experiments.",
          "4": "We compare our method with the state-of-the-art neural implicit surface learning methods [14, 28, 40, 41, 48, 50] and PatchMatch based multi-view stereo methods (PM-MVS) [37, 44]."
        },
        "Dg-recon: Depth-guided neural 3d scene reconstruction": {
          "authors": [
            "Jihong Ju",
            "Ching Wei",
            "Oleksandr Bailo",
            "Georgi Dikov",
            "Mohsen Ghafoorian"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ju_DG-Recon_Depth-Guided_Neural_3D_Scene_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[20] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 3",
          "ref_ids": [
            "20"
          ],
          "1": "ManhattanSDF [20] further enables scene reconstruction of an entire room by jointly optimizing the rendered geometry and semantics."
        },
        "Learning unsigned distance functions from multi-view images with volume rendering priors": {
          "authors": [
            "W Zhang",
            "K Shi",
            "YS Liu",
            "Z Han"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_22",
          "ref_texts": "18. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3D SceneReconstructionwiththeManhattan-worldAssumption.In:IEEEConference on Computer Vision and Pattern Recognition (2022) 4",
          "ref_ids": [
            "18"
          ],
          "1": "Some methods modify the rendering equations to use more 2D supervisions like normal maps [44], detected planes [18], and segmentation maps [25] to pursue higher reconstruction efficiency."
        },
        "Nerf in robotics: A survey": {
          "authors": [
            "G Wang",
            "L Pan",
            "S Peng",
            "S Liu",
            "C Xu",
            "Y Miao"
          ],
          "url": "https://arxiv.org/abs/2405.01333",
          "ref_texts": "[35] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in CVPR, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "35"
          ],
          "1": "[35] improve the SDF reconstruction quality of low-texture areas in indoor scenes by leveraging the Manhattan assumption and semantic guidance."
        },
        "Spacetime surface regularization for neural dynamic scene reconstruction": {
          "authors": [
            "Jaesung Choe",
            "Christopher Choy",
            "Jaesik Park",
            "In So",
            "Anima Anandkumar"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[26] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "26"
          ],
          "1": "For accurate 3D reconstruction, the Signed Distance Function (SDF) started to gain traction for its high fidelity 3D surface reconstruction [26, 47, 70, 76, 81]."
        },
        "Hi-slam: Monocular real-time dense mapping with hybrid implicit fields": {
          "authors": [
            "W Zhang",
            "T Sun",
            "S Wang",
            "Q Cheng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10374214/",
          "ref_texts": "[35] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "35"
          ],
          "1": "Method Pose Acc \u00d3 Comp\u00d3 Prec\u00d2 Recall\u00d2 F-score\u00d2 Time[h]\u00d3 ManhattanSDF [35] GT 0.",
          "2": "Given the challenging nature of the ScanNet dataset, previous methods [27], [35], [36] have relied on GT poses and offline pipelines to circumvent the problem."
        },
        "Learning a room with the occ-sdf hybrid: Signed distance function mingled with occupancy aids scene representation": {
          "authors": [
            "Xiaoyang Lyu",
            "Peng Dai",
            "Zizhang Li",
            "Dongyu Yan",
            "Yi Lin",
            "Yifan Peng",
            "Xiaojuan Qi"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lyu_Learning_a_Room_with_the_Occ-SDF_Hybrid_Signed_Distance_Function_ICCV_2023_paper.html",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2, 3, 7",
          "ref_ids": [
            "8"
          ],
          "2": "Manhattan-SDF [8] follows semantic-NeRF[38] to estimate the volume density and semantic label at the same time, and then uses ManhattanWorld assumption to regularize the geometry in floor and wall regions.",
          "3": "(3) Manhattan-SDF [8] is an SDF-based method that adds a semantic branch and uses the Manhattan constraint to regularize the geometry in floor and wall regions.",
          "6": "We conducted a comparative analysis of our proposed approach against existing implicit reconstruction methods, including ManhattanSDF [8], NeuRIS [31] and MonoSDF [36] using the ScanNet dataset."
        },
        "Surface reconstruction from 3d gaussian splatting via local structural hints": {
          "authors": [
            "Q Wu",
            "J Zheng",
            "J Cai"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72627-9_25",
          "ref_texts": "3. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: CVPR (2022) 4, 5",
          "ref_ids": [
            "3"
          ],
          "1": "Our approach achieves a comparable performance with Manhattan-SDF [3] and MonoSDF (Grids) [12] in all metrics, which demonstrates a strong potential ability of 3DGS to produce a high-quality surface mesh."
        },
        "Debsdf: Delving into the details and bias of neural indoor scene reconstruction": {
          "authors": [
            "Y Xiao",
            "J Xu",
            "Z Yu",
            "S Gao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10559771/",
          "ref_texts": "[6] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201dNeural 3d scene reconstruction with the Manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5511\u20135520, 2022. 1, 3, 10",
          "ref_ids": [
            "6"
          ],
          "1": "For example, by assuming a Manhattan world [35] where the wall and floor regions are orthogonal, Manhattan-SDF [6] achieves better performance than methods that directly optimize SDF from multi-view images.",
          "2": "Manhattan-SDF [6] regularize the surface normal at the wall and floor regions under the Manhattan world assumption [35].",
          "3": "Previous works [6], [7], [8] have shown that regularizing the optimization with geometry priors can significantly improve the reconstruction quality at texture-less areas such JOURNAL OF LATEX CLASS FILES, VOL.",
          "4": "We select 4 scenes from ScanNet for performance evaluation by following the setting of Manhattan-SDF [6].",
          "5": "We compare our method with the following methods: (i) Neural volume rendering methods with prior, including MonoSDF [7], NeuRIS [8], Manhattan-SDF [6]; (ii) Neural volume rendering methods without prior, including VolSDF [10], NeuS [11], and Unisurf [23]; and (iii) Classical MVS reconstruction method: COLMAP [9].",
          "7": "The Manhattan-SDF [6], NeurRIS [8], and MonoSDF [7] utilize the auxiliary data from pre-trained models."
        },
        "Self-supervised super-plane for neural 3D reconstruction": {
          "authors": [
            "Botao Ye",
            "Sifei Liu",
            "Xueting Li",
            "Hsuan Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.html",
          "ref_texts": "[9] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 1, 2, 5, 6, 7",
          "ref_ids": [
            "9"
          ],
          "1": "For instance, ManhattanSDF [9] introduces the Manhattan assumption on the floor and wall regions, which are predicted by a semantic segmentation model.",
          "6": "4) Neural volume rendering methods with explicit supervision: ManhattanSDF [9], NeuRIS [37], and MonoSDF [48].",
          "7": "In addition, our model performs better than the method requiring a segmentation network trained on annotated 2D datasets (ManhattanSDF [9]) and on par with the scheme requiring a normal network trained on 3D datasets (NeuRIS [37]).",
          "8": "ManhattenSDF [9] achieves compelling results by introducing an assistive segmentation network to find floors and walls, then applying Manhattan as21421 COLMAP VolSDF ManhattanSDF Ours Ground Truth Figure 7."
        },
        "NeTO: neural reconstruction of transparent objects with self-occlusion aware refraction-tracing": {
          "authors": [
            "Zongcheng Li",
            "Xiaoxiao Long",
            "Yusen Wang",
            "Tuo Cao",
            "Wenping Wang",
            "Fei Luo",
            "Chunxia Xiao"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_NeTONeural_Reconstruction_of_Transparent_Objects_with_Self-Occlusion_Aware_Refraction-Tracing_ICCV_2023_paper.html",
          "ref_texts": "[11] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 3",
          "ref_ids": [
            "11"
          ],
          "1": "Recently, implicit neural representations have been applied to a variety of applications, including novel view synthesis [25, 58], camera pose estimation [18, 49], human [19, 33] and multi-view 3D reconstruction [11, 20, 21, 28, 29, 43, 47, 48, 55, 56], and achieved impressive successes."
        },
        "Gaussianroom: Improving 3d gaussian splatting with sdf guidance and monocular cues for indoor scene reconstruction": {
          "authors": [
            "H Xiang",
            "X Li",
            "X Lai",
            "W Zhang",
            "Z Liao",
            "K Cheng"
          ],
          "url": "https://arxiv.org/abs/2405.19671",
          "ref_texts": "[19] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "19"
          ],
          "1": "To further improve reconstruction quality, [15, 64] propose to regularize optimization using SfM points, and [19, 62] leverage priors such as Manhattan world assumption and pseudo depth supervision."
        },
        "Fast monocular scene reconstruction with global-sparse local-dense grids": {
          "authors": [
            "Wei Dong",
            "Christopher Choy",
            "Charles Loop",
            "Or Litany",
            "Yuke Zhu",
            "Anima Anandkumar"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Dong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023_paper.html",
          "ref_texts": "[14] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, pages 5511\u20135520, 2022. 1, 2, 3, 7, 8",
          "ref_ids": [
            "14"
          ],
          "3": "Experiments show that our method is 10\u00d7 faster in training, 100\u00d7 faster in inference, and has comparable accuracy measured by F-scores against state-of-the-art implicit reconstruction systems [14, 49].",
          "4": "To enable large-scale indoor scene reconstruction, ManhattanSDF [14] and MonoSDF [49] incorporate monocular geometric priors and achieve state-of-the-art results.",
          "5": "Similarly, monocular priors are used to enhance SDF-based neural reconstruction [14, 49] with remarkable performance."
        },
        "Nerfvs: Neural radiance fields for free view synthesis via geometry scaffolds": {
          "authors": [
            "Chen Yang",
            "Peihao Li",
            "Zanwei Zhou",
            "Shanxin Yuan",
            "Bingbing Liu",
            "Xiaokang Yang",
            "Weichao Qiu",
            "Wei Shen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.html",
          "ref_texts": "[9] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2, 3, 4, 5, 8",
          "ref_ids": [
            "9"
          ],
          "3": "To construct this scaffold, we use the geometry from neural geometry reconstruction methods [9, 21, 37] as they can reconstruct a complete and smooth global mesh containing holistic priors.",
          "4": "Specifically, we apply the geometry produced by [9] which incorporates Manhattan world assumptions on the structure of the scene into the optimization process.",
          "5": "[9] achieves smooth and coherent geometry reconstruction with semantic information, especially in planar regions."
        },
        "Rico: Regularizing the unobservable for indoor compositional reconstruction": {
          "authors": [
            "Zizhang Li",
            "Xiaoyang Lyu",
            "Yuanyuan Ding",
            "Mengmeng Wang",
            "Yiyi Liao",
            "Yong Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_RICO_Regularizing_the_Unobservable_for_Indoor_Compositional_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 6, 8",
          "ref_ids": [
            "8"
          ],
          "1": "[8] further applies the planar regularization for scene-level reconstruction.",
          "2": "As for surface reconstruction, [8] proposes a manhattan-world assumption for the planar regions, [43] utilizes the normal and depth prediction from off-the-shelf model [5] as prior to regularize the texture-less regions."
        },
        "Phyrecon: Physically plausible neural scene reconstruction": {
          "authors": [
            "J Ni",
            "Y Chen",
            "B Jing",
            "N Jiang",
            "B Wang",
            "B Dai"
          ],
          "url": "https://arxiv.org/abs/2404.16666",
          "ref_texts": "[18] Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2022)",
          "ref_ids": [
            "18"
          ],
          "2": "Incorporating Priors into Neural Surface Reconstruction Various priors, such as Manhattanworld assumptions [20, 18], monocular geometric priors (i."
        },
        "Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors": {
          "authors": [
            "ZX Zou",
            "SS Huang",
            "YP Cao",
            "TJ Mu",
            "Y Shan"
          ],
          "url": "https://arxiv.org/abs/2209.15153",
          "ref_texts": "[28] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in IEEE CVPR, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "28"
          ],
          "1": "Although subsequent works achieve even better fine-grained surface reconstruction with the aid of geometry regularization (NeuS [25], MonoSDF [26], NeuRIS [27]) or Manhattan-world assumption [28], the time-consuming geometry learning of implicit scene representations keeps them away from online surface reconstruction for monocular videos.",
          "2": "[28] and [26] additionally use semantic or geometric cues to improve reconstruction quality in indoor scenes.",
          "3": "Although some methods based on volume rendering optimization achieve impressive and fine-grained surface reconstruction [25], [26], [27], [28], they require a time-consuming optimization from scratch and is hard to extend to an online version without a good initial prediction."
        },
        "Neural 3D reconstruction from sparse views using geometric priors": {
          "authors": [
            "TJ Mu",
            "HX Chen",
            "JX Cai",
            "N Guo"
          ],
          "url": "https://link.springer.com/article/10.1007/s41095-023-0337-5",
          "ref_texts": "[39] Guo, H. Y.; Peng, S. D.; Lin, H. T.; Wang, Q. Q.; Zhang, G. F.; Bao, H. J.; Zhou, X. Neural 3D scene reconstruction with the Manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5501\u20135510, 2022.",
          "ref_ids": [
            "39"
          ],
          "1": "Some recent reconstruction methods consider depths [38] or normals [39, 40] as geometric priors to help reconstruction; however, to obtain finely detailed geometry, these methods usually require a large number of views to be input to perform perscene optimization, leading to difficulties to generalize to new scenes."
        },
        "MonoSelfRecon: Purely self-supervised explicit generalizable 3D reconstruction of indoor scenes from monocular RGB views": {
          "authors": [
            "Runfa Li",
            "Upal Mahbub",
            "Vasudev Bhaskaran",
            "Truong Nguyen"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/3DMV/html/Li_MonoSelfRecon_Purely_Self-Supervised_Explicit_Generalizable_3D_Reconstruction_of_Indoor_Scenes_CVPRW_2024_paper.html",
          "ref_texts": "[10] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3D scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 1, 3",
          "ref_ids": [
            "10"
          ],
          "2": "The standard NeRF for implicit 2D view synthesis has been successfully extended to explicit 3D mesh representation [3, 10, 17, 29, 30, 40, 42, 45, 49, 53]."
        },
        "Tmo: Textured mesh acquisition of objects with a mobile device by using differentiable rendering": {
          "authors": [
            "Jaehoon Choi",
            "Dongki Jung",
            "Taejae Lee",
            "Sangwook Kim",
            "Youngdong Jung",
            "Dinesh Manocha",
            "Donghwan Lee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Choi_TMO_Textured_Mesh_Acquisition_of_Objects_With_a_Mobile_Device_CVPR_2023_paper.html",
          "ref_texts": "[19] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "19"
          ],
          "1": "Their follow-up works [19, 34, 54, 67] import prior information from the learning-based network."
        },
        "Improving Neural Surface Reconstruction with Feature Priors from Multi-view Images": {
          "authors": [
            "X Ren",
            "C Cao",
            "Y Fu",
            "X Xue"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73636-0_26",
          "ref_texts": "14. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5511\u2013",
          "ref_ids": [
            "14"
          ],
          "1": "Since the emergence of volume rendering techniques pioneered by NeRF [28], numerous NSR methods have seen a dramatic rise [3,6,9,14,32, 42\u201344,51,54].",
          "2": "Prior works [6,9,14] have incorporated patch-wise photometric consistency directly computed from raw images or pixelwise feature similarity to enhance NSR training.",
          "3": "NeuralWarp [14] introduces a patch-wise warping loss along with a visibility map that masks incorrect warps arising from occlusion.",
          "4": "While NeuralWarp [14] has introduced a patching-based warping method to transform source view patches to the reference view, it posits that every sampled point along the ray could be projected to other views to obtain the warped colors, leading to the final warped colors through volume rendering."
        },
        "Neural 3D Scene Reconstruction With Indoor Planar Priors": {
          "authors": [
            "X Zhou",
            "H Guo",
            "S Peng",
            "Y Xiao",
            "H Lin"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10476755/",
          "ref_texts": "[78] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison, \u201cIn-place scene labelling and understanding with implicit scene representation,\u201d inProc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 15818\u201315827. Xiaowei Zhou received the PhD degree from The Hong Kong University of Science and Technology, after which he was a postdoctoral researcher with the GRASP Lab, University of Pennsylvania. He is a tenured associate professor of computer science with Zhejiang University, China. His research interests include 3D reconstruction and scene understanding. Haoyu Guo received the bachelor\u2019s degree in computer science from the University of Science and Technology of China, in 2020. He is currently working toward the PhD degree in computer science with Zhejiang University, supervised by Prof. Xiaowei Zhou. His research focuses on 3D scene reconstruction and understanding. Sida Peng received the PhD degree from the College of Computer Science and Technology, Zhejiang University, in 2023. He is a research professor with the School of Software, Zhejiang University, China. His research interests include 3D reconstruction, rendering, and 3D generation. Yuxi Xiao is currently working toward the PhD degree with the State Key Laboratory of CAD&CG, Zhejiang University, supervised by Prof. Xiaowei Zhou. His interests lies in 3D vision, graphics and robotics. Haotong Lin received the bachelor\u2019s degree in computer science from Zhejiang University, in 2021. He is currently working toward the PhD degree in computer science with Zhejiang University, advised by Prof. Xiaowei Zhou. His research focuses on 3D/4D reconstruction and object pose estimation. Qianqian Wang received the bachelor\u2019s degree from Zhejiang University, where she worked with Prof. Xiaowei Zhou, and the PhD degree in computer science from Cornell Tech, Cornell University with her advisors Prof. Noah Snavely and Prof. Bharath Hariharan. She is a postdoctoral researcher with UC Berkeley, working with Prof. Angjoo Kanazawa and Prof. Alyosha Efros. Additionally, she is a visiting researcher with Google Research. Guofeng Zhang received the BS and PhD degrees in computer science from Zhejiang University, in 2003 and 2009, respectively. He is a professor with the State Key Lab of CAD&CG, Zhejiang University. His research interests include structure-from-motion, SLAM, 3D reconstruction, augmented reality, video segmentation and editing. Hujun Bao received the BSc degree in mathematics from Zhejiang University, in 1987, and the PhD degree in applied mathematics from Zhejiang University, in 1993. He is currently a professor with the State Key Laboratory of CAD&CG and College of Computer Science and Technology, Zhejiang University. Authorized licensed use limited to: Zhejiang University. Downloaded on December 29,2024 at 15:06:12 UTC from IEEE Xplore. Restrictions apply. ",
          "ref_ids": [
            "78"
          ],
          "1": "More recently,[78] proposes to extend NeRF to encode semantics with radiance fields.",
          "2": "Inspired by[78], we augment the neural scene representation by additionally predicting semantic logits for each point in 3D space.",
          "3": "Note that learning 3D semantics withLs naturally utilizes the multi-view consensus to improve the accuracy of semantic scene segmentation, as shown in [78]."
        },
        "Pmvc: Promoting multi-view consistency for 3d scene reconstruction": {
          "authors": [
            "Chushan Zhang",
            "Jinguang Tong",
            "Tao Jun",
            "Chuong Nguyen",
            "Hongdong Li"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PMVC_Promoting_Multi-View_Consistency_for_3D_Scene_Reconstruction_WACV_2024_paper.html",
          "ref_texts": "[11] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 3, 7, 12, 14",
          "ref_ids": [
            "11"
          ],
          "1": "For instance, [3, 29, 49, 57] integrates feature information to derive more implicit priors, while studies such as [7,11,52,59] combine semantic information in jointly trained models to enhance performance.",
          "2": "For implicit representation models, we compare ours with other deep-learning approaches including UNISURF [30], V olSDF [55], Manhattan-SDF [11], NeuS [48], as well as the latest NeuRIS [47] and MonoSDF [56], which integrate monocular constraints within training phrase.",
          "3": "Evaluation Metrics: Following previous work [11,15,25, 27], we utilize several metrics to evaluate our results.",
          "5": "These include comparing with the Manhattan fine-tuned model [11] (as presented in Table S6) and integrating our pipeline into the NeuRIS [47] framework (Table S7)."
        },
        "Painting 3d nature in 2d: View synthesis of natural scenes from a single semantic mask": {
          "authors": [
            "Shangzhan Zhang",
            "Sida Peng",
            "Tianrun Chen",
            "Linzhan Mou",
            "Haotong Lin",
            "Kaicheng Yu",
            "Yiyi Liao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Painting_3D_Nature_in_2D_View_Synthesis_of_Natural_Scenes_CVPR_2023_paper.html",
          "ref_texts": "[14] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 4",
          "ref_ids": [
            "14"
          ],
          "1": "We adopt a continuous neural field to represent the semantics and geometry of a 3D scene, similar to [14]."
        },
        "Rdfc-gan: Rgb-depth fusion cyclegan for indoor depth completion": {
          "authors": [
            "H Wang",
            "Z Che",
            "Y Yang",
            "M Wang",
            "Z Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10497905/",
          "ref_texts": "[26] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "26"
          ],
          "1": "This domain knowledge, usually referred to as Manhattan world assumption [23], can help people easily tell invalid and unreasonable depth estimation results and has been properly used in SLAM [24], monocular depth estimation [25], and 3D reconstruction [26].",
          "2": "An increasing number of methods [26], [62], [63] leverage it for indoor vision tasks.",
          "3": "In 3D scene understanding and reconstruction, Manhattan world assumption has been integrated as connections between 3D scenes and 2D images [67] or as additional constraints [26], [68], [69]."
        },
        "Towards Energy-Efficiency by Navigating the Trilemma of Energy, Latency, and Accuracy": {
          "authors": [
            "B Tian",
            "Y Pang",
            "M Huzaifa",
            "S Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10765385/",
          "ref_texts": "[25] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5511\u20135520, June 2022. 3",
          "ref_ids": [
            "25"
          ],
          "1": "Finally, for accuracy, we use the F-score [25, 39] to quantify the quality of the final reconstructed mesh against the ground-truth meshes from the dataset."
        },
        "Parf: Primitive-aware radiance fusion for indoor scene novel view synthesis": {
          "authors": [
            "Haiyang Ying",
            "Baowei Jiang",
            "Jinzhi Zhang",
            "Di Xu",
            "Tao Yu",
            "Qionghai Dai",
            "Lu Fang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ying_PARF_Primitive-Aware_Radiance_Fusion_for_Indoor_Scene_Novel_View_Synthesis_ICCV_2023_paper.html",
          "ref_texts": "[10] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "10"
          ],
          "1": "Signed Distance Field is also an implicit representation that is beneficial to model a continuous geometric surface [25,2, 44, 10, 39].",
          "2": "Primitive based Rendering and Fusion Structural scene prior has been proven to be beneficial in neural rendering and fusion [26, 16, 10, 4, 36, 38]."
        },
        "Real-time neural dense elevation mapping for urban terrain with uncertainty estimations": {
          "authors": [
            "B Yang",
            "Q Zhang",
            "R Geng",
            "L Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9992063/",
          "ref_texts": "[2] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5511\u20135520, 2022.",
          "ref_ids": [
            "2"
          ],
          "1": "Neural Radiance Fields (NeRFs) are constructed using multi-view images [12]\u2013[14] to represent large-scale urban environments [1], [2]."
        },
        "Neuroswarm: Multi-agent neural 3d scene reconstruction and segmentation with uav for optimal navigation of quadruped robot": {
          "authors": [
            "I Zhura",
            "D Davletshin",
            "NDW Mudalige"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10394221/",
          "ref_texts": "[6] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "6"
          ],
          "1": "Neural rendering techniques received a high surge of interest in the field of computer vision [5], [6]."
        },
        "A comprehensive benchmark for neural human radiance fields": {
          "authors": [
            "K Liu",
            "D Jin",
            "A Zeng",
            "X Han"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6e566c91d381bd7a45647d9a90838817-Abstract-Datasets_and_Benchmarks.html",
          "ref_texts": "[17] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "17"
          ],
          "1": "There is a series of variants that improve the vanilla NeRF in several aspects, including improving rendering quality [2, 3, 43], reducing train views [35, 23, 42, 58, 53], acceleration [52, 34, 40, 41, 14, 6], oneshot training [46, 7, 30, 22, 9], mesh reconstruction [51, 17, 13, 48, 56, 45, 36] and so on."
        },
        "InfoNorm: Mutual Information Shaping of Normals for Sparse-View Reconstruction": {
          "authors": [
            "X Wang",
            "S Dong",
            "Y Zheng",
            "Y Yang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72897-6_14",
          "ref_texts": "10. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5511\u2013",
          "ref_ids": [
            "10"
          ],
          "1": "For instance, ManhattanSDF [10] works under the assumption that all captured scenes adhere to the Manhattan-world concept, thus, predicting semantic segmentation to identify walls and floors."
        },
        "Psdf: Prior-driven neural implicit surface learning for multi-view reconstruction": {
          "authors": [
            "W Su",
            "C Zhang",
            "Q Xu",
            "W Tao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10637766/",
          "ref_texts": "[32] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "32"
          ],
          "1": "Manhattan-SDF [32] incorporates the Manhattan-world assumption with semantic constraints in the optimization process, and depth priors obtained from MVS are further utilized as supervision."
        },
        "Tsdf-sampling: Efficient sampling for neural surface field using truncated signed distance field": {
          "authors": [
            "C Min",
            "S Cha",
            "C Won",
            "J Lim"
          ],
          "url": "https://arxiv.org/abs/2311.17878",
          "ref_texts": "[18] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "18"
          ],
          "2": "Second, our proposed method is model-agnostic, and it can be readily applied to improve the speed of any other neural surface fields methods [18, 27, 29, 37, 40, 44, 47, 48, 53, 54] without having to train them again, as long as they use volume rendering [21] for their inference of the 3D representation."
        },
        "NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation": {
          "authors": [
            "Ziyi Chen",
            "Xiaolong Wu",
            "Yu Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_NC-SDF_Enhancing_Indoor_Scene_Reconstruction_Using_Neural_SDFs_with_View-Dependent_CVPR_2024_paper.html",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 1, 2, 5, 6, 7",
          "ref_ids": [
            "16"
          ],
          "1": "Recent advancements have mitigated this problem by incorporating additional priors for supervision, including sensor depths [3, 30, 35, 43, 54, 56], semantic priors [16], depth priors from MVS methods [22, 34, 47] and monocular geometric priors [11, 44, 52, 55].",
          "2": "ManhattanSDF [16] assumes that the normals of walls and floors adhere to the Manhattan-world assumption.",
          "3": "We compare our method with the following baselines: (1) Traditional MVS method COLMAP [36]; (2) Neural implicit representation methods without additional supervision, including V olSDF [50] and NeuS [45]; (3) Neural implicit representation methods with additional supervision, including ManhattanSDF [16], HelixSurf [22], NeuRIS [44], and MonoSDF (both MLP and voxel grids version) [52].",
          "5": "The methods with additional priors, such as ManhattanSDF [16], HelixSurf [22], and MonoSDF (MLP) [52] do improve the reconstruction quality; however, their reconstructions exhibit noisy or missing surfaces due to the limitations in the quality of priors and the expressiveness of the model."
        },
        "Ray-Distance Volume Rendering for Neural Scene Reconstruction": {
          "authors": [
            "R Yin",
            "Y Chen",
            "S Karaoglu",
            "T Gevers"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72630-9_22",
          "ref_texts": "13. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5511\u2013",
          "ref_ids": [
            "13"
          ],
          "1": "Some extensions [13,40,43,50,51] introduce extra priors to achieve superior outcomes.",
          "2": "For instance, Manhattan-SDF [13] employs planar constraints based on Manhattan-world assumption [4].",
          "4": "Except for the baseline MonoSDF, our method is also compared to (1) the classic MVS method: COLMAP [32], (2) neural implicit reconstruction methods: UNISURF [27], NeuS [42], VolSDF [46], Manhattan-SDF [13], NeuRIS [40], S3P [47], HelixSurf [18], and Occ_SDF_Hybrid [21]."
        },
        "Sketchanimar: Sketch-based 3d animal fine-grained retrieval": {
          "authors": [
            "TN Le",
            "TV Nguyen",
            "MQ Le",
            "TT Nguyen",
            "VT Huynh"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0097849323001644",
          "ref_texts": "[4] Guo, H, Peng, S, Lin, H, Wang, Q, Zhang, G, Bao, H, et al. Neural 3D scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE /CVF Conference on Computer Vision and Pattern Recognition. 2022, p. 5511\u20135520.",
          "ref_ids": [
            "4"
          ],
          "1": "Due to the innate intuitive appeal of freehand drawings, sketch-based 3D object retrieval has drawn a significant amount of attention and is being utilized in numerous critical applications such as 3D scene reconstruction [4, 5, 6], 3D geometry video retrieval [7, 8, 9], and 3D augmented/virtual reality entertainment [10, 11]."
        },
        "Pcrdiffusion: Diffusion probabilistic models for point cloud registration": {
          "authors": [
            "Y Wu",
            "Y Yuan",
            "X Fan",
            "X Huang",
            "M Gong"
          ],
          "url": "https://arxiv.org/abs/2312.06063",
          "ref_texts": "[3] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "3"
          ],
          "1": "\u2726 1 I NTRODUCTION W ITH the rapid development of 3D data acquisition technology [1], [2], point cloud registration, as a fundamental visual recognition task plays an essential role in the 3D vision field, and has been widely applied in various vision tasks, such as 3D scene reconstruction [3], [4], object pose estimation [5], [6], and simultaneous localization and mapping (SLAM) [7], [8]."
        },
        "Fine-detailed Neural Indoor Scene Reconstruction using multi-level importance sampling and multi-view consistency": {
          "authors": [
            "X Li",
            "Y Ji",
            "X Lai",
            "W Zhang",
            "L Zeng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10648179/",
          "ref_texts": "[8] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in CVPR, June 2022, pp. 5511\u20135520. 1, 4, 5",
          "ref_ids": [
            "8"
          ],
          "1": "To address the challenge, [8, 9] leverage priors about the indoor scenes, such as Manhattan world assumption [8] and pseudo depth supervision [9].",
          "4": "Baselines We compare against: (1) classic MVS method: COLMAP [19], (2) TSDF based method: NeuralRecon [4], (3) neural volume rendering methods: NeRF [5], NeuS [6], Manhattan-SDF [8], MonoSDF [9], NeuRIS [10] and HelixSurf [20]."
        },
        "A review of deep learning-powered mesh reconstruction methods": {
          "authors": [
            "Z Chen"
          ],
          "url": "https://arxiv.org/abs/2303.02879",
          "ref_texts": "[52] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "52"
          ],
          "1": "ManhattanSDF [52] targets indoor scene reconstruction and follows VolSDF [177]."
        },
        "OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation": {
          "authors": [
            "H Jiang",
            "Y Xu",
            "Y Zeng",
            "H Xu",
            "W Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801860/",
          "ref_texts": "[29] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in CVPR, 2022.",
          "ref_ids": [
            "29"
          ],
          "3": "III, we conducted a comparison of our proposed approach against existing implicit reconstruction methods, including Manhattan-SDF [29], MonoSDF [24], Go-Surf [26] and OccSDF [30]."
        },
        "Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid Representation and Normal Prior Enhancement": {
          "authors": [
            "S Ye",
            "Y Hu",
            "M Lin",
            "YH Wen",
            "W Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10637492/",
          "ref_texts": "[6] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 . IEEE, 2022, pp. 5501\u20135510.",
          "ref_ids": [
            "6"
          ],
          "2": "ManhattanSDF [6] utilizes extra 2D semantic segmentation to detect wall and floor regions, and applies geometry regularization based on Manhattan-world assumption.",
          "5": "2 Compared Methods We compare our method with the following baselines: (1) Traditional MVS method COLMAP [10]; (2) State-of-the-art neural implicit surface reconstruction methods, including NeuS [4] and NeuralAngelo [26]; (3) State-of-the-art neural indoor scene reconstruction methods, including ManhattanSDF [6], NeuRIS [8], MonoSDF [7], and HelixSurf [35].",
          "7": "Aided by extra priors, methods like ManhattanSDF [6], NeuRIS [8], and MonoSDF [7] achieve performance improvement."
        },
        "VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction": {
          "authors": [
            "A Gassol Puigjaner",
            "E Mello Rella",
            "E Sandstr\u00f6m"
          ],
          "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/713602/1/vf_nerf_3dv.pdf",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. CVPR, 2022. 1, 2, 3, 5, 6, 7, 8, 11",
          "ref_ids": [
            "16"
          ],
          "4": "We rigorously evaluate our method against leading benchmarks for indoor scenes, including ManhattanSDF [16], MonoSDF [59], and Neuralangelo [26], on indoor datasets such as Replica [47] and ScanNet [9], showing superior performance on both reconstruction and novel view rendering.",
          "5": "For instance, [16] suggests incorporating dense depth maps from COLMAP [43] to facilitate the learning of 3D geometry and employs Manhattan world [8] priors to address the challenges posed by low-textured planar surfaces.",
          "6": "To address this challenge, similarly to prior works [16, 26, 34, 50, 51, 57, 59], we propose a hierarchical sampling strategy to allocate samples selectively in regions likely to contain surfaces."
        },
        "AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes": {
          "authors": [
            "J Jang",
            "I Lee",
            "M Kim",
            "K Joo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10463130/",
          "ref_texts": "[36] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3D Scene Reconstruction with the Manhattan-world Assumption,\u201d in CVPR, 2022.",
          "ref_ids": [
            "36"
          ],
          "1": "Several research works [34], [35], [36] pay attention to enhancing the quality of neural scene reconstructions by combining geometric priors (e.",
          "2": "[36] propose a neural 3D scene reconstruction method with a strict structural assumption of indoor scenes."
        },
        "MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors": {
          "authors": [
            "Z Du",
            "B Xu",
            "H Zhang",
            "K Huo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10685078/",
          "ref_texts": "[11] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2022.",
          "ref_ids": [
            "11"
          ],
          "1": "Built upon the work of [4] and [7], Manhattan-SDF [11] learns a joint representation of scene geometry and semantics motivated by Manhattan-world assumption within indoor scenes.",
          "3": "Adopting the surface representation of V olSDF [7], Manhattan-SDF [11] simultaneously learns scene geometry and semantics, enabling monocular 3D semantic mapping of indoor scenes.",
          "5": "Hyperparameters of MLPs are similar to [9] and [11] for a fair comparison.",
          "6": "Following previous works [9], [11], we select 8 representative scenes and use provided camera poses to perform experiments.",
          "7": "For geometry reconstruction, we follow the evaluation procedure of [11], utilizing six standard metrics: accuracy (Acc), completeness (Comp), precision (Prec), recall, F-score and chamfer distance (CD).",
          "8": "(2) Manhattan-SDF and Manhattan-SDF*: Since ManhattanSDF [11] mainly considers three coarse semantic classes: floor, wall and others, we extend it to learn NYU-40 classes and denote it as Manhattan-SDF* while keeping all other components intact.",
          "9": "We compare our surface reconstruction results to vanilla Manhattan-SDF [11], ManhattanSDF*, and NeuRIS* (same geometry to NeuRIS [9])."
        },
        "PSDF for Neural Indoor Scene Reconstruction": {
          "authors": [
            "J Li",
            "J Yu",
            "R Wang",
            "Z Li",
            "Z Zhang",
            "L Cao"
          ],
          "url": "https://arxiv.org/abs/2303.00236",
          "ref_texts": "[5] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "5"
          ],
          "1": "To improve the reconstruction for low-textured regions for indoor scenes, Manhattan-SDF [5] proposes to use a pretrained semantic segmentation network to find the floor and wall regions, then regularize the geometry of these regions.",
          "2": "Manhattan-SDF [5] regularizes the normal directions of pixels in the floor and wall regions under the Manhattan world assumption [6].",
          "4": "We compare our method with the classical MVS and the neural volume rendering methods on the four indoor scenes from Manhattan-SDF [5] in each dataset.",
          "7": "Following [5], [31], [55], we use RGB-D fusion results as 3D reconstruction ground truth and evaluate our method using five standard metrics defined in [30]: accuracy, completeness, precision, recall, and F-score with a threshold of 5cm.",
          "8": "To overcome the effect of unobserved regions in evaluation, suggested by Manhattan-SDF [5], we render depth maps from predicted mesh and re-fuse them with TSDF fusion [54] following [30].",
          "10": "to reconstruct mesh from extracted point clouds; Neural volume rendering methods: NeRF [1], NeuS [3] and VolSDF [4]; Volume rendering with additional pre-trained segmentation model: Manhattan-SDF [5].",
          "25": "709 Though our method achieves comparable performance on the four Manhattan scenes in ScanNet dataset from Manhattan-SDF [5] and performs better on other scenes, we combine our method with Manhattan-SDF to achieve better performance."
        },
        "TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection": {
          "authors": [
            "M Li",
            "K Liu",
            "H Chen",
            "J Bu",
            "H Wang",
            "H Wang"
          ],
          "url": "https://arxiv.org/abs/2411.11641",
          "ref_texts": "[12] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. 2022. Neural 3d scene reconstruction with the manhattanworld assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5511\u20135520.",
          "ref_ids": [
            "12"
          ],
          "1": "INR learns continuous representations and has been widely applied in numerous scenarios, such as 2D image generation [40, 52, 75] and 3D scene reconstruction [12, 18, 44], physicsinformed problems [39, 43] and video representation [6, 30, 31]."
        },
        "Surface normal clustering for implicit representation of manhattan scenes": {
          "authors": [
            "Nikola Popovic",
            "Danda Pani",
            "Luc Van"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Popovic_Surface_Normal_Clustering_for_Implicit_Representation_of_Manhattan_Scenes_ICCV_2023_paper.html",
          "ref_texts": "[14] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian W ang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 5511\u20135520, 2022.",
          "ref_ids": [
            "14"
          ],
          "2": "In this context, a notable recent work ManhattanSDF [14] demonstrates the benefit of exploiting the high-level geometric prior for structured scenes.",
          "3": "More precisely , ManhattanSDF [14] uses the known semantic regions to impose the planar geometry prior of floors and walls under the Manhattan scene assumption.",
          "6": "Auxiliary supervision methods:In addition to the images, other inputs such as depth [33, 39, 9], semantics [58, 15, 17, 41], normal [52, 53], and their combinations [56, 14, 21] are shown to be beneficial on improving the neural radiance field representation.",
          "7": "One notable work ManhattanSDF [14] exploits the Manhattan prior without requiring any SfM reconstruction.",
          "8": "ManhattanDF [14]:This method exploits the Manhattan prior by supervising the explicit normals of the floors to align with the knownnz, as well as by supervising normals of the walls to align with two learned orthogonal axes which are also orthogonal tonz."
        },
        "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction": {
          "authors": [
            "W Zhang",
            "H Xiang",
            "Z Liao",
            "X Lai",
            "X Li"
          ],
          "url": "https://arxiv.org/abs/2412.03428",
          "ref_texts": "[46] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "46"
          ],
          "1": "To further enhance reconstruction quality, [44, 45] suggest regularizing optimization with SfM points, while [6, 46] incorporate priors like the Manhattan world assumption and pseudo depth supervision."
        },
        "Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering": {
          "authors": [
            "Z Wang",
            "H Zhou",
            "MB Blaschko",
            "T Tuytelaars"
          ],
          "url": "https://arxiv.org/abs/2409.07098",
          "ref_texts": "[5] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 2",
          "ref_ids": [
            "5"
          ],
          "1": "Another line of work addresses geometric difficulties by incorporating prior information, such as depth [26, 28, 31], normal prior [22, 26, 28, 31], or semantic prior [5]."
        },
        "Artificial Intelligence in Creative Industries: Advances Prior to 2025": {
          "authors": [
            "N Anantrasirichai",
            "F Zhang",
            "D Bull"
          ],
          "url": "https://arxiv.org/abs/2501.02725",
          "ref_texts": "[193] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3D scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2022, pp. 5511\u2013",
          "ref_ids": [
            "193"
          ],
          "1": "1), there have been integrations utilizing semantic segmentation to enhance 3D representation [193]."
        },
        "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors": {
          "authors": [
            "J Yin",
            "W Yin",
            "H Chen",
            "X Ren",
            "Z Ma",
            "J Guo"
          ],
          "url": "https://arxiv.org/abs/2311.15171",
          "ref_texts": "[10] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 5511\u2013",
          "ref_ids": [
            "10"
          ],
          "1": "Recent studies employ depth priors [2], sparse point clouds [34], semantic labels [10], and Manhattan-world Assumption [8, 10] for novel view synthesis on neural scene representations."
        },
        "Scalable MAV Indoor Reconstruction with Neural Implicit Surfaces": {
          "authors": [
            "Haoda Li",
            "Puyuan Yi",
            "Yunhao Liu",
            "Avideh Zakhor"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/Li_Scalable_MAV_Indoor_Reconstruction_with_Neural_Implicit_Surfaces_ICCVW_2023_paper.html",
          "ref_texts": "[9] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, 2022. 1, 2, 3, 4",
          "ref_ids": [
            "9"
          ],
          "1": "We leverage both traditional Structure from Motion (SfM) [19] and neural surface reconstruction [9] methods.",
          "2": "Thereafter, we employ a divide-and-conquer strategy, perform neural surface reconstruction while embedding the Manhattan-world assumption [9], and finally merge the block-wise reconstructions through depth refinement.",
          "3": "Recently, novel approaches specifically tackle indoor scene reconstruction by introducing additional priors such as depth [25, 32, 9], geometric consistency [5], and planar region assumptions [9, 24].",
          "4": "Each block is then reconstructed with a modified ManhattanSDF [9] method with 2D depth maps and segmentation supervision.",
          "5": "Thus, we apply [9] as our backbone.",
          "6": "The semantic logits are accumulated similar to the color accumulation, and is optimized against the estimated semantic segmentation map using the joint optimization loss described in [9]."
        },
        "Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning": {
          "authors": [
            "Z Xie",
            "R Xie",
            "R Li",
            "K Huang",
            "P Qiao",
            "J Zhu",
            "X Yin"
          ],
          "url": "https://arxiv.org/abs/2311.11825",
          "ref_texts": "3. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: CVPR (2022)",
          "ref_ids": [
            "3"
          ],
          "1": "Manhattan-SDF [3] adopt semantic priors from Manhattan world assumptions, while MonoSDF [37] exploit monocular depth and normal predictions to supervise the reconstruction of scene geometry."
        },
        "FAWN: Floor-and-Walls Normal Regularization for Direct Neural TSDF Reconstruction": {
          "authors": [
            "A Sokolova",
            "A Vorontsova",
            "B Gabdullin"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10647694/",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in CVPR, 2022.",
          "ref_ids": [
            "16"
          ],
          "1": "Implicit Manhattan-SDF [16] also combines surface normals and semantics for 3D reconstruction, forcing floor and walls normals to be collinear with three dominant directions."
        },
        "VolumeNeRF: CT Volume Reconstruction from a Single Projection View": {
          "authors": [
            "J Liu",
            "X Bai"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72104-5_71",
          "ref_texts": "8. Guo, H., Peng, S., Lin, H., Wang, Q., Zhang, G., Bao, H., Zhou, X.: Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5511\u2013",
          "ref_ids": [
            "8"
          ],
          "1": "However, generating such representations generally requires multiple images from various viewpoints [16,4,8,22]."
        },
        "Clusterfusion: Real-time relative positioning and dense reconstruction for UAV cluster": {
          "authors": [
            "Y Dong",
            "S Bu",
            "K Li",
            "L Chen",
            "Z Xia",
            "Y Wang"
          ],
          "url": "https://arxiv.org/abs/2304.04943",
          "ref_texts": "[25] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "25"
          ],
          "1": "There has been a lot of work on scene reconstruction based on NeRF with good results, for example Manhattan-SDF [25].",
          "2": "[25] H."
        },
        "Residual Learning for Image Point Descriptors": {
          "authors": [
            "R Shrestha",
            "A Chhatkuli",
            "M Kanakis"
          ],
          "url": "https://arxiv.org/abs/2312.15471",
          "ref_texts": "[16] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "16"
          ],
          "1": "As an example, SfM has played no small part in the optimization of Neural Radiance Field (NERF) models by offering accurate camera poses [15] and sparse 3D initialization [16]."
        },
        "Optimizing NeRF-based SLAM with Trajectory Smoothness Constraints": {
          "authors": [
            "Y He",
            "G Chen",
            "H Zhang"
          ],
          "url": "https://arxiv.org/abs/2410.08780",
          "ref_texts": "[22] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstruction with the manhattan-world assumption,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "22"
          ],
          "1": "These methods propose new forms of geometric representations [22], [23] and incorporate depth images for supervision [24], [25].",
          "2": "[22] H."
        },
        "Robo-vision! 3D mesh generation of a scene for a robot for planar and non-planar complex objects": {
          "authors": [
            "Swapna Agarwal"
          ],
          "url": "https://link.springer.com/article/10.1007/s11042-023-15111-8",
          "ref_texts": "31. Guo H, Peng S, Lin H, Wang Q, Zhang G, Bao H, Zhou X (2022) Neural 3d scene reconstruction with the manhattan-world assumption. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp 5511\u20135520",
          "ref_ids": [
            "31"
          ],
          "1": "Many follow-up papers [6,18, 31, 83, 85] incorporate depth with RGB and demonstrate very good reconstruction."
        },
        "Neural 3D Scene Reconstruction from Multiple 2D Images without 3D Supervision": {
          "authors": [
            "Y Guo",
            "C Sun",
            "Y Jia",
            "Y Wu"
          ],
          "url": "https://arxiv.org/abs/2306.17643",
          "ref_texts": "[10] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, pages 5511\u20135520, 2022. 1, 2, 3, 6, 7, 8",
          "ref_ids": [
            "10"
          ],
          "4": "Manhattan-SDF [10] uses dense depth maps predicted by COLMAP [29] as supervision and uses Manhattan assumption to handle the walls and floor regions.",
          "8": "(4) State-of-the-art neural scene reconstruction methods: ManhattanSDF [10], NeuRIS [38].",
          "11": "Manhattan-SDF [10] performs well, but it is limited by the Manhattan-world assumption."
        },
        "Deep Learning Methods for Point Matching, Visual Localization and 3D Reconstruction": {
          "authors": [
            "Shuzhe Wang"
          ],
          "url": "https://aaltodoc.aalto.fi/items/becab958-d8b1-40d1-9146-0fe3248171fc",
          "ref_texts": "[66] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "66"
          ],
          "1": "To manage large-scale environments, recent methods [66, 216, 223, 224] incorporate geometric priors into implicit models, significantly enhancing reconstruction detail."
        },
        "A Qualitative Analysis Strategy Towards AI-Enabled 3D City Reconstruction": {
          "authors": [
            "A Christodoulides"
          ],
          "url": "https://www.swansea.ac.uk/media/Andreas-Christodoulides---Thesis.pdf",
          "ref_texts": "[64] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3d scene reconstructionwiththemanhattan-worldassumption,\u201din ProceedingsoftheIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5511\u20135520.",
          "ref_ids": [
            "64"
          ],
          "1": "ManhattanSDF [64] through a single MLP can predict SDFs, colour fields and semantic logits, improving reconstruction accuracy by enforcing the Manhattan-world assumption [75].",
          "6": "In ManhattanSDF [64], they enhance the scene representation by incorporating semantic logits through DeepLabv3+ [86].",
          "7": "4itcanalsobeseenthatonlythreeapproachescombinescenereconstructions with semantic capabilities [11,53,64].",
          "9": "4: Semantic and Scene Capabilities as seen by the literature Author Papers Semantic Capabilities Scene Capabilities MTBR-Net [11] Yes Yes BUOL [53] Yes Yes ManhattanSDF [64] Yes Yes AutoSDF [45] Yes No SDFusion [46] Yes No ShapeClipper [47] Yes No AutoRecon [54] Yes No Neuralangelo [51] No Yes ALTO [52] No Yes Behind the Scenes [57] No Yes NeuDA [60] No Yes ShadowNeuS [62] No Yes NKSR [4] No Yes City3D [3] No Yes POCO [63] No Yes HEAT [9] No Yes Predictable Context [66] No Yes Direct Voxel Grid Optimisation [67] No Yes Vis2Mesh [68] No Yes PSR [69] No Yes Search and Evaluate [10] No Yes RetrievalFuse [71] No Yes Learning SDF for Multiview Surface Reconstruction [72] No Yes Inaddition,variationsinthescaleofreconstructionsareevident.",
          "10": "Survey Mapping on Affinity Diagram Focus on 3D Representations Instead of Rendering: ++ (32 papers) Preferably, PhotogrammetryBased Point Clouds Fusion with LiDAR Data: None Semantic Scene Understanding: [11, 53, 64] Semantic Feature Understanding (e."
        },
        "Neural Mesh Reconstruction": {
          "authors": [
            "Z Chen"
          ],
          "url": "https://diglib.eg.org/bitstream/handle/10.2312/3543919/PhD_thesis__Zhiqin_Chen__Neural_Mesh_Reconstruction.pdf?sequence=1&isAllowed=y",
          "ref_texts": "[89] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022.",
          "ref_ids": [
            "89"
          ],
          "1": "ManhattanSDF [89] incorporates planer constraints to regularize the geometry in floor and wall regions."
        },
        "NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds Supplementary Material": {
          "authors": [
            "C Yang",
            "P Li",
            "Z Zhou",
            "S Yuan",
            "B Liu",
            "X Yang",
            "W Qiu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/supplemental/Yang_NeRFVS_Neural_Radiance_CVPR_2023_supplemental.pdf",
          "ref_texts": "[3] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5511\u20135520, 2022. 1",
          "ref_ids": [
            "3"
          ],
          "1": "For training image selection, we first uniformly sample 10% views from the raw image sequence for each scene following the setting of [3]."
        },
        "Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense Grids Supplementary Material": {
          "authors": [
            "W Dong",
            "C Choy",
            "C Loop",
            "O Litany",
            "Y Zhu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/supplemental/Dong_Fast_Monocular_Scene_CVPR_2023_supplemental.pdf",
          "ref_texts": "[4] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In CVPR, pages 5511\u20135520, 2022. 1, 2, 3, 4",
          "ref_ids": [
            "4"
          ],
          "1": "Metrics We follow the evaluation protocols defined by ManhattanSDF [4], where the metrics between predicted point set 1 0050 0084 0580 0616 Figure 1.",
          "2": "Sparse reconstruction and covisibility matrix of ScanNet scenes selected by ManhattanSDF [4].",
          "3": "Generation of P and P\u2217 We follow previous works [4, 17] that applied TSDF refusion to generateP for evaluation: use Marching Cubes [5] to generate a global mesh; render depth map from mesh at selected viewpoints to crop points out of viewports; apply TSDF fusion [18] to obtain the final mesh and point cloud P.",
          "4": "Scene-wise statistics on ScanNet We use reconstructed mesh provided by ManhattanSDF [4], and report scene-wise statistics in Table 2."
        },
        "TerrainMesh: Metric-Semantic Terrain Reconstruction From Aerial Images Using Joint 2-D-3-D Learning": {
          "authors": [
            "Q Feng",
            "N Atanasov"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10388460/",
          "ref_texts": "[63] H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou, \u201cNeural 3D Scene Reconstruction with the Manhattan-world Assumption,\u201d in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 5501\u20135510.",
          "ref_ids": [
            "63"
          ],
          "1": "[63] jointly optimize the geometry and semantics by predicting the implicit neural representations of the signed distance, color and semantic field."
        }
      }
    },
    {
      "title": "animatable neural implicit surfaces for creating avatars from videos",
      "id": 20,
      "valid_pdf_number": "42/47",
      "matched_pdf_number": "39/42",
      "matched_rate": 0.9285714285714286,
      "citations": {
        "Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling": {
          "authors": [
            "Zhe Li",
            "Zerong Zheng",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[59] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 2",
          "ref_ids": [
            "59"
          ],
          "1": "In contrast, NeRF containing a density and color field is widely used in textured avatar modeling [14, 18, 28, 29, 57, 59, 73, 78, 85] because of its good differentiable property."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[38] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. ArXiv, abs/2203.08133, 2022. 1, 2, 3, 5",
          "ref_ids": [
            "38"
          ],
          "1": "Recent advances in implicit neural fields [27, 30, 32, 36, 47, 50, 51, 53, 55, 65, 66] have enabled high-quality reconstruction of geometry [8, 38, 57, 61] and appearance [13, 20, 22, 31, 35, 37, 42, 58, 70] of clothed human bodies from sparse multi-view or monocular videos.",
          "2": "To achieve state-of-the-art rendering quality, existing methods rely on training a neural radiance field (NeRF) [27] combined with either explicit body articulation [8, 12, 13, 20, 35, 38, 57, 58, 70] or conditioning the NeRF on human body related encodings [31, 37, 48, 61].",
          "3": "The majority of the works focus on either learning a NeRF conditioned on human body related encodings [31, 48, 61], or learning a canonical NeRF representation and warp camera rays from the observation space to the canonical space to query radiance and density values from the canonical NeRF [8, 12, 13, 20, 35, 38, 57, 58, 70].",
          "4": "To model human articulations, a widely adopted paradigm is to represent geometry and appearance in a shared canonical space [8, 12, 13, 20, 35, 38, 57, 58] and use Linear Blend Skinning (LBS) [2, 9, 24, 33, 34, 60] to deform the parametric human body under arbitrary poses."
        },
        "Gauhuman: Articulated gaussian splatting from monocular human videos": {
          "authors": [
            "Shoukang Hu",
            "Tao Hu",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.html",
          "ref_texts": "[83] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 4(5), 2022. 1, 3, 6",
          "ref_ids": [
            "83"
          ],
          "2": "Recent methods show 3D human avatar can be learned from sparse-view videos [12, 48, 50, 80, 82, 83, 103, 114, 116, 120] or even a single image [8, 43, 65, 117] with NeRF-based implicit representation [76].",
          "3": "Another line of works reconstruct 3D human performers with implicit neural representations from sparse-view videos [12, 48, 50, 66, 68, 80, 82, 83, 83, 103, 114, 116, 120] or even a single image [8, 43, 65, 117].",
          "9": "Thanks to the LBS weight field and deformation field learned in HumanNeRF [116] and AS [83], they achieve comparable visualization results as our GauHuman for 600x more time for training and 472x more time for rendering."
        },
        "Gaussianavatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians": {
          "authors": [
            "Liangxiao Hu",
            "Hongwen Zhang",
            "Yuxiang Zhang",
            "Boyao Zhou",
            "Boning Liu",
            "Shengping Zhang",
            "Liqiang Nie"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GaussianAvatar_Towards_Realistic_Human_Avatar_Modeling_from_a_Single_Video_CVPR_2024_paper.html",
          "ref_texts": "[32] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. In ICCV, 2021. 3",
          "ref_ids": [
            "32"
          ],
          "1": "Due to the high-quality rendering of neural radiance field [29], various efforts [20, 21, 24, 32, 33, 47, 67, 68] have been made to reconstruct the dynamic appearance of moving people."
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
          "authors": [
            "Chen Geng",
            "Sida Peng",
            "Zhen Xu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.html",
          "ref_texts": "[57] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 5, 6, 7, 8",
          "ref_ids": [
            "57"
          ],
          "1": "MonoCap datasetcontains four multi-view videos collected by [57] from the DeepCap dataset [26] and the DynaCap dataset [25].",
          "2": "[57] additionally estimate the SMPL parameters for each image.",
          "3": "We adopt the setting of training and test camera views in [57].",
          "4": "[57] extend [56] with a signed distance field and pose-dependent deformation field to better model the residual deformation and geometric 8763.",
          "5": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
          "6": "[57, 93] exhibit better results than [56, 58].",
          "7": "52 AS [57] \u02dc10 h 30.",
          "8": "[57, 93] demonstrate similar visual quality and show that representing the human motion with the LBS model and residual deformation works particularly well, but their models require 100x more time to optimize.",
          "9": "(2) XYZ-Code: hash encoded x and a per-frame learnable latent code [57]."
        },
        "Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation": {
          "authors": [
            "M Hu",
            "W Yin",
            "C Zhang",
            "Z Cai",
            "X Long"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10638254/",
          "ref_texts": "[39] S. Peng, S. Zhang, Z. Xu, C. Geng, B. Jiang, H. Bao, and X. Zhou, \u201cAnimatable neural implicit surfaces for creating avatars from videos,\u201d arXiv: Comp. Res. Repository, p. 2203.08133, 2022. 3",
          "ref_ids": [
            "39"
          ],
          "1": "propose a canonical camera transformation method in training, inspired by the canonical pose space from human body reconstruction methods [39].",
          "3": "We show the ground-truth trajectory and Droid-SLAM [39] predicted trajectory and their dense mapping."
        },
        "Cagenerf: Cage-based neural radiance field for generalized 3d deformation and animation": {
          "authors": [
            "Y Peng",
            "Y Yan",
            "S Liu",
            "Y Cheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb78e6b5246b03e0b82b4acc8b11cc21-Abstract-Conference.html",
          "ref_texts": "[33] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. CoRR, abs/2203.08133, 2022.",
          "ref_ids": [
            "33"
          ],
          "1": "Referring to the widely concerned digital humans, a vast range of methods are proposed for the deformation of the implicit face [12, 57, 14] and body [34, 8, 36, 33]."
        },
        "Fast-snarf: A fast deformer for articulated neural fields": {
          "authors": [
            "X Chen",
            "T Jiang",
            "J Song",
            "M Rietmann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10112633/",
          "ref_texts": "[47] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "47"
          ],
          "1": "These methods serve as a foundation for many tasks such as generative modeling of articulated objects or humans [3, 8, 12, 20, 40, 63], and reconstructing animatable avatars from scans [9, 13, 27, 34, 35, 51, 56], depth [14, 42, 57], videos [7, 24, 25, 26, 29, 39, 45, 47, 58, 59, 64] or a single image [18, 21, 60]."
        },
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[48] S. Peng, S. Zhang, Z. Xu, C. Geng, B. Jiang, H. Bao, and X. Zhou, \u201cAnimatable neural implicit surfaces for creating avatars from videos,\u201d arXiv preprint arXiv:2203.08133, 2022. 2",
          "ref_ids": [
            "48"
          ],
          "1": "In contrast, NeRF containing a density and color field is widely used in textured avatar modeling [48]\u2013[56] because of its good differentiable property."
        },
        "One-shot implicit animatable avatars with model-based priors": {
          "authors": [
            "Yangyi Huang",
            "Hongwei Yi",
            "Weiyang Liu",
            "Haofan Wang",
            "Boxi Wu",
            "Wenxiao Wang",
            "Binbin Lin",
            "Debing Zhang",
            "Deng Cai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 2, 7",
          "ref_ids": [
            "38"
          ],
          "1": "Among which [39] learns structured latent codes on SMPL [29] mesh vertices, other methods construct the representation in a canonical space by modeling pose-driven deformation [57, 62, 52, 71, 38, 37].",
          "2": "For Ani-NeRF, we use the pose-dependent displacement field model proposed in [38], which reports their best results."
        },
        "Neca: Neural customizable human avatar": {
          "authors": [
            "Junjin Xiao",
            "Qing Zhang",
            "Zhan Xu",
            "Shi Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xiao_NECA_Neural_Customizable_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[43] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 4, 5",
          "ref_ids": [
            "43"
          ],
          "1": "Moreover, we can calculate normal vector by normalizing the gradient of SDF d with respect to xc [43, 60]: nc = \u2202d \u2202xc /||\u2202d \u2202xc ||2.",
          "2": "We employ the binary cross-entropy loss to reduce discrepancies in determining whether each ray intersects the subject or not [43, 65]."
        },
        "Human101: Training 100+ fps human gaussians in 100s from 1 view": {
          "authors": [
            "M Li",
            "J Tao",
            "Z Yang",
            "Y Yang"
          ],
          "url": "https://arxiv.org/abs/2312.15258",
          "ref_texts": "[45] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 1, 2, 4, 6, 7, 12, 13",
          "ref_ids": [
            "45"
          ],
          "1": "Different from the usual inverse skinning used by [15, 22, 43, 45] this forward skinning deformation method avoids searching for the corresponding canonical points of the target pose points but directly deform the canonical points into observation space.",
          "5": "AnimatableNeRF(AnimNeRF) [43] and AnimatableSDF(AnimSDF) [45] use SMPL deformation and posedependent neural blend weight field to model dynamic humans.",
          "6": "1 presents a comprehensive quantitative comparison between our method and other prominent techniques like InstantNvr [15], InstantAvatar [22], 3D GS [24], HumanNeRF [62], AnimSDF [45], NeuralBody [44], and AnimNeRF [43]."
        },
        "Meshavatar: Learning high-quality triangular human avatars from multi-view videos": {
          "authors": [
            "Y Chen",
            "Z Zheng",
            "Z Li",
            "C Xu",
            "Y Liu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73113-6_15.pdf",
          "ref_texts": "72. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133 (2022)",
          "ref_ids": [
            "72"
          ],
          "1": "To capture the avatar appearance, recent works leverage NeRF as the underlying representation [16,19,23,37,38,50,51,70,72,77,82,86,90,93,106,107] for its impressive learning capability."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[40] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "40"
          ],
          "1": "To improve the geometry, [53, 40] represent the human geometry as the signed distance field and use the volume rendering to learn the representation from images.",
          "2": "1 Multi-part human model in the canonical space Similar to [53, 40], the human geometry and appearance are represented as signed distance fields Fs and color fields Fc given by MLP networks.",
          "4": "2 Comparison with the baselines Since most previous methods only focus on body modeling, we extend the state-of-the-art methods AniNeRF [39] and AniSDF [40] with hands to compare with our method."
        },
        "Intrinsicavatar: Physically based inverse rendering of dynamic humans from monocular videos via explicit ray tracing": {
          "authors": [
            "Shaofei Wang",
            "Bozidar Antic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_IntrinsicAvatar_Physically_Based_Inverse_Rendering_of_Dynamic_Humans_from_Monocular_CVPR_2024_paper.html",
          "ref_texts": "[56] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 2024.",
          "ref_ids": [
            "56"
          ],
          "1": "Recently, reconstruction of clothed humans from monocular videos has also been explored [23, 56, 73, 76].",
          "2": "Some methods use SDFs to represent the geometry of humans and achieve impressive results in both geometry reconstruction and photorealistic rendering [23, 56, 73, 77].",
          "3": "\u2022 SyntheticHuman-Relit To additionally evaluate relighting on more complex training poses of continuous videos, we create a synthetic dataset by rendering two subjects from the SyntheticHuman dataset [56] with Blender under different illumination conditions."
        },
        "Livehand: Real-time and photorealistic neural hand rendering": {
          "authors": [
            "Akshay Mundra",
            "Mallikarjun B",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos.arXiv preprint arXiv:2203.08133, 2022. 3",
          "ref_ids": [
            "29"
          ],
          "1": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_1",
          "ref_texts": "59. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133 (2022) 4",
          "ref_ids": [
            "59"
          ],
          "1": "Concurrent Works: Several concurrent works extend NeRF-based articulated models to improve novel view synthesis, geometry reconstruction, or animation quality [10,24,27,32,46,59,71,79,84,92]."
        },
        "MetaCap: Meta-learning Priors from Multi-view Imagery for Sparse-View Human Performance Capture and Rendering": {
          "authors": [
            "G Sun",
            "R Dabral",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72952-2_20",
          "ref_texts": "60. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133 (2022)",
          "ref_ids": [
            "60"
          ],
          "1": "While many works have recently focused on animatable implicit radiance fields for humans [21,41,60,84,86,100], only few [19,42,52,93] have explored priors for optimizing or fitting a neural implicit field given sparse imagery."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
          "authors": [
            "X Zhou",
            "S Peng",
            "Z Xu",
            "J Dong",
            "Q Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10401886/",
          "ref_texts": "[20] S. Peng, S. Zhang, Z. Xu, C. Geng, B. Jiang, H. Bao, and X. Zhou, \u201cAnimatable neural implict surfaces for creating avatars from videos,\u201d arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "20"
          ],
          "2": "In addition, we demonstrate that using signed distance field significantly improve the reconstruction performance on SyntheticHuman [20].",
          "3": "MonoCap is created by [20], which consists of two videos from DeepCap dataset [18] and two videos from DynaCap dataset [19], which are captured by dense camera views and provide the human masks and 3D human poses.",
          "4": "SyntheticHuman is a synthetic dataset created by [20], which contains 7 animated 3D characters."
        },
        "Selfnerf: Fast training nerf for human from monocular self-rotating video": {
          "authors": [
            "B Peng",
            "J Hu",
            "J Zhou",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2210.01651",
          "ref_texts": "[24] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 1, 2, 6, 8",
          "ref_ids": [
            "24"
          ],
          "1": "NeuralBody [25], AnimatableNeRF [24], H-NeRF [38] and other works [15, 16, 30, 41] are able to synthesize high-quality rendering images and extract rough body geometry from 1 arXiv:2210.",
          "2": "AnimatableNeRF [24] introduces deformation fields based on neural blend weight fields to generate observation-to-canonical correspondences.",
          "3": "Evaluation We compare with state-of-the-art implicit human novel view synthesis methods that based on NeRF: 1) AnimatableNeRF [24] utilizes deformation fields based on neural blend weight fields to aggregate per-frame information to reconstruct the canonical space\u2019s neural radiance field."
        },
        "Dynamic multi-view scene reconstruction using neural implicit surface": {
          "authors": [
            "D Chen",
            "H Lu",
            "I Feldmann",
            "O Schreer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10096704/",
          "ref_texts": "[14] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou, \u201cAnimatable neural implict surfaces for creating avatars from videos,\u201d arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "14"
          ],
          "1": "Although these methods [13, 14, 15, 16, 17, 18] have demonstrated impressive efficacy in view synthesis and reconstruction in terms of the human body, the requirement of human priors th view MLP Hyper-coordinates Network SE3 Deformation Network SDF Network MLP Radiance Network."
        },
        "Generalizable neural voxels for fast human radiance fields": {
          "authors": [
            "T Yi",
            "J Fang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://arxiv.org/abs/2303.15387",
          "ref_texts": "[59] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv:2203.08133, 2022. 1, 2, 4",
          "ref_ids": [
            "59"
          ],
          "2": "To solve this problem, Animatable NeRF [58, 59] maps human poses from the observation space to the predefined canonical space."
        },
        "Replay: Multi-modal multi-view acted videos for casual holography": {
          "authors": [
            "Roman Shapovalov",
            "Yanir Kleiman",
            "Ignacio Rocco",
            "David Novotny",
            "Andrea Vedaldi",
            "Changan Chen",
            "Filippos Kokkinos",
            "Ben Graham",
            "Natalia Neverova"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Shapovalov_Replay_Multi-modal_Multi-view_Acted_Videos_for_Casual_Holography_ICCV_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Shang-Wei Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv, abs/2203.08133, 2022.3",
          "ref_ids": [
            "45"
          ],
          "1": "Much work has focused on modelling articulated human bodies, including Neural volumes [36], Relightables [20], Articulated Neural Rendering [48], A-NeRF [56], Neural Actor [35], H-NeRF [67], Neural Performer [28], Deep Dynamic Character [22], Human Re-rendering [50], Pixel Aligned Avatars [49], HumanNeRF [65], HiFi Human Avatar [72], Generative Neural Articulated RFs [2], Animatable NeIS [45]."
        },
        "Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering": {
          "authors": [
            "Chuanyue Shen",
            "Letian Zhang",
            "Zhangsihao Yang",
            "Masood Mortazavi",
            "Xiyun Song",
            "Liang Peng",
            "Heather Yu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 4, 5",
          "ref_ids": [
            "25"
          ],
          "1": "Inspired by previous work [9,15,25,32,38], we represent human motion field M as addition of skeleton-driven motion field Mskel and a residual non-rigid motion field Mres, M = Mskel + Mres.",
          "2": "Similar to [9, 15, 17, 25, 31, 32, 38], we model the skeleton motion volume based on an inverse linear blend skinning algorithm that wraps the points in observation space to canonical space (equivalent to warping an observed pose\u03b8o to a predefined canonical pose \u03b8c) in a form as follows: Mskel(x, \u03b8o) = KX i=1 wo i (x)Gi(x), (7) where wo i is the blend weight for the i-th bone in the observation space and Gi is the skeleton motion basis for thei-th bone.",
          "3": "In light of previous works [25, 32], we model the residual motion as a pose-dependent deformation field."
        },
        "Xhand: Real-time expressive hand avatar": {
          "authors": [
            "Q Gan",
            "Z Zhou",
            "J Zhu"
          ],
          "url": "https://arxiv.org/abs/2407.21002",
          "ref_texts": "[39] S. Peng, S. Zhang, Z. Xu, C. Geng, B. Jiang, H. Bao, and X. Zhou, \u201cAnimatable neural implicit surfaces for creating avatars from videos,\u201d CoRR, vol. abs/2203.08133, 2022.",
          "ref_ids": [
            "39"
          ],
          "1": "For instance, previous works [22], [39], [40] have focused on establishing correspondences between pose space and standard space through techniques such as linear blend skinning and inverse skinning weights.",
          "2": "Previous works [22], [39], [40] have explored to establish the correspondences between pose space and template space through linear blend skinning and inverse skinning weights."
        },
        "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
          "authors": [
            "H Wang",
            "W Zhang",
            "S Liu",
            "X Zhou",
            "J Li",
            "Z Tang"
          ],
          "url": "https://arxiv.org/abs/2405.12477",
          "ref_texts": "[36] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos.arXiv preprint arXiv:2203.08133, 4(5), 2022. 6",
          "ref_ids": [
            "36"
          ],
          "1": "The MonoCap Dataset includes four multi-view videos from DeepCap [8] and DynaCap [9] datasets, collected by [36].",
          "2": "As in [36], we use one camera view for training and selected ten uniformly distributed cameras for testing."
        },
        "HumanGif: Single-View Human Diffusion with Generative Prior": {
          "authors": [
            "S Hu",
            "T Narihira",
            "K Fukuda",
            "R Sawata"
          ],
          "url": "https://arxiv.org/abs/2502.12080",
          "ref_texts": "[90] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 4(5), 2022. 1, 2",
          "ref_ids": [
            "90"
          ],
          "1": "Recent methods enable the novel view and pose synthesis of 3D human avatars from sparse-view human videos [19, 21, 45, 46, 48, 50, 59, 61, 85, 88, 90, 109, 114, 119, 125], with Neural Radiance Field [82] or Gaussian Splatting-based [66] representation.",
          "2": "While efforts to create high-quality multi-view human image datasets [4, 11, 22, 36\u201338, 41, 55\u201357, 73, 80, 89, 90, 110, 128, 131] have accelerated in recent years, the number of human subjects in these datasets remains significantly smaller compared to multi-view object and scene image datasets [26, 27, 111, 122, 133, 142]."
        },
        "Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos": {
          "authors": [
            "X Luo",
            "J Peng",
            "Z Cai",
            "L Yang",
            "F Yang",
            "Z Cao"
          ],
          "url": "https://arxiv.org/abs/2501.13335",
          "ref_texts": "[57] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 4(5), 2022. 1, 2",
          "ref_ids": [
            "57"
          ],
          "1": "To tackle the challenge, implicit neural radiance field (NeRF) [38, 51, 69, 83] is introduced to combine with body articulation [10, 12, 20, 35, 39, 57, 77] or condition on body-related encodings [69, 83].",
          "2": "To reconstruct dynamic humans, body model encoding [69, 83] is proposed to enhance generalization, and several works learn deformation field in canonical space [10, 12, 20, 35, 39, 57, 77] combined with parametric models [42, 54] to model pose and shape, where a latent code [56] for SMPL [42] vertex is proposed to model appearance, which can be posed by a coordinate-based neural skinning field [4, 16, 45]."
        },
        "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors": {
          "authors": [
            "J Yin",
            "W Yin",
            "H Chen",
            "X Ren",
            "Z Ma",
            "J Guo"
          ],
          "url": "https://arxiv.org/abs/2311.15171",
          "ref_texts": "[31] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "31"
          ],
          "1": "Some works [30, 31] use the human pose and skinning weights form SMPL as prior knowledge to facilitate the learning of deformation field.",
          "2": "We choose Ani-NeRF [30] as a default baseline model, and Ani-SDF [31] as an additional one.",
          "3": "We adopt Ani-NeRF [30] and Ani-SDF [31] as our baseline model and present the reconstruction results of our method in comparison with the two baseline models.",
          "4": "Specifically, compared with Ani-NeRF [30] and Ani-SDF [31], our approach not only reconstructs smoother geometry results with high-quality but also captures more geometry details (e.",
          "5": "compared with Ani-NeRF [30] and Ani-SDF [31], our approach not only reconstructs smoother geometry results with high-quality but also captures more geometry details (e."
        },
        "Accelerating Human Avatar Creation: Pose-dependent Hybrid Representations for Efficient Rendering of Clothed Human Avatars": {
          "authors": [
            "Z Qian"
          ],
          "url": "https://www.research-collection.ethz.ch/handle/20.500.11850/610316",
          "ref_texts": "[65] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022.",
          "ref_ids": [
            "65"
          ],
          "1": "With recent advances in implicit neural rendering [52, 82, 86, 85, 91] and implicit surface reconstruction [56, 105, 104, 93, 59] from RGB images, drastic improvements have been made regarding rendering fidelity [66, 64, 57, 39, 71, 35, 30] and geometric quality [101, 65, 97, 23] for clothed human avatars created using sparse multi-view or monocular acquisition setups.",
          "2": "For learning avatars directly from RGB inputs, volumetric rendering with implicit representations has been combined with human priors to achieve sparse multi-view reconstruction and rendering by a number of recent works [84, 101, 66, 64, 65, 57, 110, 99, 28, 97, 35, 49, 33, 109, 39].",
          "3": "Another line of work is articulation-based and tackles the problem by first warping camera rays from observation space to a pre-defined canonical space and then querying a canonical NeRF model [64, 65, 99, 28, 97, 35, 20, 110, 39].",
          "4": "For articulation [28, 97, 35, 99] use learned skinning networks while [64, 65, 20, 110, 39] adopt the SMPL [44] skinning weights.",
          "5": "For geometry estimation, [97, 65, 28] achieve detailed geometry reconstruction by using an SDF field while other methods [35, 99, 64, 20, 110, 39] that rely on density fields produce noisy surface."
        },
        "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos\u2014Supplementary Material": {
          "authors": [
            "S Hu",
            "T Hu",
            "Z Liu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_GauHuman_Articulated_Gaussian_CVPR_2024_supplemental.pdf",
          "ref_texts": "[13] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implict surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 4(5), 2022. 1, 2, 3",
          "ref_ids": [
            "13"
          ],
          "1": "AS[13] further extends [11] by learning a signed distance field and a posedependent deformation field for residual information and geometric details of dynamic 3D humans.",
          "2": "For each subject in ZJU_MoCap [12] and MonoCap [2, 3, 13], we collect 20 frames for novel pose synthesis by sampling 1 frame every 10 frames."
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors* Supplemental Material": {
          "authors": [
            "Y Huang",
            "H Yi",
            "W Liu",
            "H Wang",
            "B Wu",
            "W Wang",
            "B Lin"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Huang_One-shot_Implicit_Animatable_ICCV_2023_supplemental.pdf",
          "ref_texts": "[12] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 3",
          "ref_ids": [
            "12"
          ],
          "1": "5, we replaced the HumanNeRF model used in ELICIT with an SDF-based model from Animatable NeRF[12, 11].",
          "2": "The SDF-based model from Ani-NeRF[12] reduces floating artifacts (marked with red rectangles), which are commonly present in our HumanNeRF-based model, leading to better surface geometry."
        },
        "Free-Viewpoint Video in the Wild Using a Flying Camera": {
          "authors": [
            "Z Hong",
            "W Shen"
          ],
          "url": "https://openreview.net/forum?id=AsPHD00Wer",
          "ref_texts": "33. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133 (2022)",
          "ref_ids": [
            "33"
          ],
          "1": "On the one hand, for human reconstruction, existing backbones [9,13,16,25,33,34,42] highly rely on full observations of humans (including the person\u2019s back and side) to render a human in high-fidelity and intricate details.",
          "2": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 42, 45] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity."
        },
        "Free-Viewpoint Video of Outdoor Sports Using a Drone": {
          "authors": [
            "Z Hong"
          ],
          "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02178.pdf",
          "ref_texts": "33. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133 (2022)",
          "ref_ids": [
            "33"
          ],
          "1": "On the one hand, for human reconstruction, existing backbones [9,13,16,25,33,34,41] highly rely on full observations of humans (including the person\u2019s back and side) to render a human in high-fidelity and intricate details.",
          "2": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 41, 44] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity."
        },
        "Learning Neural Volumetric Representations of Dynamic Humans in Minutes Supplemental Material": {
          "authors": [
            "C Geng",
            "S Peng",
            "Z Xu",
            "H Bao",
            "X Zhou"
          ],
          "url": "https://chen-geng.com/instant_nvr/files/supp.pdf",
          "ref_texts": "[10] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable neural implicit surfaces for creating avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 3",
          "ref_ids": [
            "10"
          ],
          "1": "Details of baselines Neural Body [11], Ani-SDF [10], HumanNeRF [14] and Ani-NeRF [9] We use the released code and conduct experiments on a single NVIDIA RTX 3090 GPU."
        }
      }
    },
    {
      "title": "neural body: implicit neural representations with structured latent codes for novel view synthesis of dynamic humans",
      "id": 3,
      "valid_pdf_number": "569/703",
      "matched_pdf_number": "458/569",
      "matched_rate": 0.804920913884007,
      "citations": {
        "Nerf: Neural radiance field in 3d vision, a comprehensive review": {
          "authors": [
            "K Gao",
            "Y Gao",
            "H He",
            "D Lu",
            "L Xu",
            "J Li"
          ],
          "url": "https://arxiv.org/abs/2210.00379",
          "ref_texts": "[60] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "60"
          ],
          "1": "The latter focuses on topological changes, and includes scenes such as a human subject opening and closing their eyes and mouth, peeling a banana, 3D printing a chicken toy, and a broom deforming The ZJU-MOCap LightStage dataset [60] is a multiview (20+ cameras) motion capture dataset consisting of 9 dynamic human sequences consisting of exercise-like motions.",
          "3": "Neural Body [60] (Dec 2020) applied NeRF volume rendering to rendering humans with moving poses from videos."
        },
        "Hexplane: A fast representation for dynamic scenes": {
          "authors": [
            "Ang Cao",
            "Justin Johnson"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.html",
          "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "52"
          ],
          "1": "Representing dynamic scenes by neural radiance fields is an essential extension of NeRF, enabling numerous real-world applications [27, 47, 52, 65, 78, 84, 91]."
        },
        "Freenerf: Improving few-shot neural rendering with free frequency regularization": {
          "authors": [
            "Jiawei Yang",
            "Marco Pavone",
            "Yue Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "24"
          ],
          "1": "The seminal work, Neural Radiance Fields (NeRF) [21], has been widely studied and advanced in a variety of applications [2, 3, 13, 19, 23, 25, 32], including novel view synthesis [18, 21], 3D generation [10, 25], deformation [23,26,28], video [7,14,15,24,35]."
        },
        "Ibrnet: Learning multi-view image-based rendering": {
          "authors": [
            "Qianqian Wang",
            "Zhicheng Wang",
            "Kyle Genova",
            "Pratul P. Srinivasan",
            "Howard Zhou",
            "Jonathan T. Barron",
            "Ricardo Martin",
            "Noah Snavely",
            "Thomas Funkhouser"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Wang_IBRNet_Learning_Multi-View_Image-Based_Rendering_CVPR_2021_paper.html",
          "ref_texts": "[45] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "While NeRF opens up many new research opportunities [30, 36, 43, 45, 55, 58], it must be optimized for each new scene, taking hours or days to converge."
        },
        "Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction": {
          "authors": [
            "Michael Oechsle",
            "Songyou Peng",
            "Andreas Geiger"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Oechsle_UNISURF_Unifying_Neural_Implicit_Surfaces_and_Radiance_Fields_for_Multi-View_ICCV_2021_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 3",
          "ref_ids": [
            "44"
          ],
          "1": "NeRF [34] and follow-ups [6,28,35,36,44,45,49,53,63] use volume rendering by learning alpha-compositing of a radiance field along rays.",
          "2": "Several follow-up works (Neural Body [44] D-NeRF [45] and NeRD [6]) extract meshes using the volume density from NeRF, but none of them considers optimizing surfaces directly."
        },
        "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps": {
          "authors": [
            "Christian Reiser",
            "Songyou Peng",
            "Yiyi Liao",
            "Andreas Geiger"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Reiser_KiloNeRF_Speeding_Up_Neural_Radiance_Fields_With_Thousands_of_Tiny_ICCV_2021_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Finally, [10, 12, 21, 22, 34, 36, 39, 41, 62, 67, 70] extend NeRF to videos."
        },
        "Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction": {
          "authors": [
            "Yiming Wang",
            "Qin Han",
            "Marc Habermann",
            "Kostas Daniilidis",
            "Christian Theobalt",
            "Lingjie Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 1(1):9054\u20139063, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "Some works in human performance modeling [54, 31, 43, 7, 63, 38, 19, 44, 24, 60] can model large movements by introducing a deformable template as a prior."
        },
        "Humannerf: Free-viewpoint rendering of moving people from monocular video": {
          "authors": [
            "Yi Weng",
            "Brian Curless",
            "Pratul P. Srinivasan",
            "Jonathan T. Barron",
            "Ira Kemelmacher"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.html",
          "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021.",
          "ref_ids": [
            "48"
          ],
          "3": "[48] explored the use of learned structured latent codes embedded for point clouds (from MVS [53]) or reposed mesh vertices (from SMPL [33]) and learn an accompanying UNetor NeRF-based neural renderer.",
          "4": "We then define \u03c4 as a function of the optimization iteration: \u03c4(t) = Lmax(0, t\u2212 Ts) Te \u2212 Ts , (15)",
          "5": "58 Subject 392 Subject 393 Subject 394 PSNR \u2191 SSIM \u2191 LPIPS* \u2193 PSNR \u2191 SSIM \u2191 LPIPS* \u2193 PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [48] 30.",
          "6": "Evaluation dataset We evaluate our method on the ZJU-MoCap dataset [48], self-captured data (rugby, hoodie ), and YouTube videos downloaded from Internet (story3, way2sexy4, invisible5).",
          "7": "We compare our method with Neural Body [48] (typically used with multiple cameras) and HyperNeRF [46] (single moving camera around the subject), state-of-the-art methods for modeling humans and general scenes for novel view synthesis.",
          "8": "PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [48] 29."
        },
        "Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation": {
          "authors": [
            "Titas Anciukevicius",
            "Zexiang Xu",
            "Matthew Fisher",
            "Paul Henderson",
            "Hakan Bilen",
            "Niloy J. Mitra",
            "Paul Guerrero"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.html",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "There has been exponential progress in the computer vision community on representing 3D scenes as neural fields [4,11,37,39,59,64], allowing for high-fidelity rendering in various reconstruction and image synthesis tasks [2, 33, 45, 47, 66]."
        },
        "A survey on 3d gaussian splatting": {
          "authors": [
            "G Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2401.03890",
          "ref_texts": "[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9054\u2013",
          "ref_ids": [
            "292"
          ],
          "2": "\u2022 Dataset: ZJU-MoCap [292] is a prevalent benchmark in human modeling from videos, captured with 23 synchronized cameras at a 1024 \u00d71024 resolution.",
          "3": "\u2022 Benchmarking Algorithms: For performance comparison, we involve three recent papers which model human avatar with 3D GS [145], [146], [249], as well as six human rendering approaches [292], [305]\u2013[309].",
          "4": "4) on ZJU-MoCap [292] (avatar), in terms of PSNR, SSIM, and LPIPS*.",
          "5": "Method GS PSNR\u2191 SSIM\u2191 LPIPS*\u2193 NeuralBody [292][CVPR21] 29."
        },
        "Animatable neural radiance fields for modeling dynamic human bodies": {
          "authors": [
            "Sida Peng",
            "Junting Dong",
            "Qianqian Wang",
            "Shangzhan Zhang",
            "Qing Shuai",
            "Xiaowei Zhou",
            "Hujun Bao"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.html?ref=https://githubhelp.com",
          "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 6, 7",
          "ref_ids": [
            "49"
          ],
          "3": "[49] combines NeRF with the SMPL model, allowing it to handle dynamic humans and synthesize photorealistic novel views from very sparse camera views.",
          "7": "3) Neural body [49] represents the human body with an implicit field conditioned on the latent codes anchored on the vertices of SMPL and renders the images using volume rendering.",
          "8": "Moreover, the proposed method achieves comparable results with the most recent state-of-the-art approach [49] as shown in Table 2, despite not being specifically designed for the novel view synthesis task."
        },
        "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering": {
          "authors": [
            "Ruizhi Shao",
            "Zerong Zheng",
            "Hanzhang Tu",
            "Boning Liu",
            "Hongwen Zhang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Using parametric body templates as the semantic prior, methods like Neural Body [39] and HumanNeRF [60] enable photo-realistic novel view synthesis of complex human performance."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of CVPR, 2021. 1, 2, 5, 6, 7, 8",
          "ref_ids": [
            "37"
          ],
          "5": "Experiments In this section, we first compare the proposed approach with recent state-of-the-art methods [7, 12, 37, 57, 58], demonstrating that our proposed approach achieves superior rendering quality in terms of LPIPS, which is more informative under monocular setting, while achieving fast training and real-time rendering speed, respectively 400x and 250x faster than the most competitive baseline [58]."
        },
        "Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis": {
          "authors": [
            "Shunyuan Zheng",
            "Boyao Zhou",
            "Ruizhi Shao",
            "Boning Liu",
            "Shengping Zhang",
            "Liqiang Nie",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_GPS-Gaussian_Generalizable_Pixel-wise_3D_Gaussian_Splatting_for_Real-time_Human_Novel_CVPR_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2, 3",
          "ref_ids": [
            "40"
          ],
          "2": "The following efforts [40, 49] in human free-view rendering immensely ease the burden of viewpoint quantities while maintaining high qualities."
        },
        "Gauhuman: Articulated gaussian splatting from monocular human videos": {
          "authors": [
            "Shoukang Hu",
            "Tao Hu",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.html",
          "ref_texts": "[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 3, 6",
          "ref_ids": [
            "82"
          ],
          "7": "Although NB [82] and NHP [60] show impressive results with 4-view videos, they do not perform well with monocular videos."
        },
        "Gaussianavatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians": {
          "authors": [
            "Liangxiao Hu",
            "Hongwen Zhang",
            "Yuxiang Zhang",
            "Boyao Zhou",
            "Boning Liu",
            "Shengping Zhang",
            "Liqiang Nie"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GaussianAvatar_Towards_Realistic_Human_Avatar_Modeling_from_a_Single_Video_CVPR_2024_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "33"
          ],
          "1": "Due to the high-quality rendering of neural radiance field [29], various efforts [20, 21, 24, 32, 33, 47, 67, 68] have been made to reconstruct the dynamic appearance of moving people.",
          "2": "Neural Body [33] associates a latent code to each SMPL [26] vertex to encode the appearance, which is transformed into observation space based on the human pose."
        },
        "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields": {
          "authors": [
            "L Song",
            "A Chen",
            "Z Li",
            "Z Chen",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10049689/",
          "ref_texts": "[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "62"
          ],
          "1": "The scene representation in NeRF inspired a number of works focusing on 3D modeling, such as human face and body capture [24, 42, 55, 61, 62, 72], relighting [4, 5, 71] and 3D content generation [9, 10, 22, 25, 31, 69, 78]."
        },
        "Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models": {
          "authors": [
            "Yukang Cao",
            "Pei Cao",
            "Kai Han",
            "Ying Shan",
            "Yee K. Wong"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Cao_DreamAvatar_Text-and-Shape_Guided_3D_Human_Avatar_Generation_via_Diffusion_Models_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "In recent years, various methods [9, 30, 42, 45, 59] have been proposed to utilize NeRF and train on 2D human videos for novel view synthesis."
        },
        "Ad-nerf: Audio driven neural radiance fields for talking head synthesis": {
          "authors": [
            "Yudong Guo",
            "Keyu Chen",
            "Sen Liang",
            "Jin Liu",
            "Hujun Bao",
            "Juyong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021. 2",
          "ref_ids": [
            "33"
          ],
          "1": "[33] integrate observations across video frames to enable novel view synthesis for human body from a sparse multi-view video."
        },
        "Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting": {
          "authors": [
            "Zhijing Shao",
            "Zhaolong Wang",
            "Zhuang Li",
            "Duotun Wang",
            "Xiangru Lin",
            "Yu Zhang",
            "Mingming Fan",
            "Zeyu Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[35] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR, 2021. 2, 3",
          "ref_ids": [
            "35"
          ],
          "1": "Recent advances in the field have seen a shift towards using Neural Radiance Fields (NeRF) [32], especially for capturing high-frequency details in 3D avatar modeling [3, 18, 21, 26, 27, 34, 35, 53].",
          "2": "This is done by tracing ray samples backward from their posed positions to their canonical origins [21, 34, 35, 53].",
          "3": "T o achieve convincing rendering beyond the limitation of triangle mesh, especially on the hair, glasses, and clothes, some recent works [3, 16, 21, 26, 34, 35, 44, 47, 53] focus on constructing NeRF in the canonical space (usually T -pose of SMPL [28] or neutral expression of FLAME [25]) and conduct volume rendering at the posed space."
        },
        "Gart: Gaussian articulated template models": {
          "authors": [
            "Jiahui Lei",
            "Yufu Wang",
            "Georgios Pavlakos",
            "Lingjie Liu",
            "Kostas Daniilidis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html",
          "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 6, 7",
          "ref_ids": [
            "52"
          ],
          "1": "To reconstruct dynamic humans, neural representations are combined with parametric models to disentangle pose and shape [40, 52, 67].",
          "2": "Appearance can be modeled in the canonical space and then posed by the articulated template [8, 9, 52] or the deformation field [73].",
          "4": "Comparison of view synthesis on ZJU-MoCap [52].",
          "5": "We conduct comparisons on three datasets: ZJU-MoCap [52], People-Snapshot [2], and UBCFashion [83].",
          "6": "ZJU-MoCap [52] We compare with Instant-NVR [13] and other human rendering methods on the ZJU-MoCap dataset [52] with the same setup as [13].",
          "8": "Comparison on ZJU-MoCap [52].",
          "9": "11, where the input to the MLP is the SMPL Pose; GART-T-Table 19881 male-3-casual male-4-casual female-3-casual female-4-casual PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Neural Body [52] (\u223c14h) 24."
        },
        "Structured local radiance fields for human avatar modeling": {
          "authors": [
            "Zerong Zheng",
            "Han Huang",
            "Tao Yu",
            "Hongwen Zhang",
            "Yandong Guo",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.html",
          "ref_texts": "[55] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3, 5, 6, 7",
          "ref_ids": [
            "55"
          ],
          "1": "Human motions are usually much more challenging to learn using neural networks, and several works [30,49,55] incorporated prior from a statistical body template to tackle this difficulty.",
          "2": "Recently, neural scene representations and rendering techniques are adopted for higher-fidelity results [35,54, 55].",
          "4": "Comparison We mainly compare our method with Animatable NeRF [54] and Neural Body [55].",
          "5": "To conduct a fair comparison with Neural Body [55], we use their dataset and follow the same protocal in their paper.",
          "6": "In this comparison, we train our network using only 300 image frames from four views, as done in [55].",
          "7": "Quantitative comparison with Neural Body [55] and Animatable NeRF [54] on ZJU-MoCap dataset.",
          "9": "Comparison against Neural Body[55] in terms of both novel view synthesis and pose generation.",
          "10": "2 shows that our model achieves higher accuracy than [55] in both metrics."
        },
        "Headnerf: A real-time nerf-based parametric head model": {
          "authors": [
            "Yang Hong",
            "Bo Peng",
            "Haiyao Xiao",
            "Ligang Liu",
            "Juyong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.html",
          "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Benefiting from these advantages, NeRF has been widely used in many fields, such as 3D modeling [53, 58], human face/body digitization [12, 17, 42, 43, 48, 55], generating 4D free-view video [31,39], etc."
        },
        "Neuman: Neural human radiance field from a single video": {
          "authors": [
            "W Jiang",
            "KM Yi",
            "G Samei",
            "O Tuzel",
            "A Ranjan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_24",
          "ref_texts": "33. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 1, 3, 10, 20 NeuMan 17",
          "ref_ids": [
            "33"
          ],
          "1": "Recent efforts also focus on animation of these radiance field models [19,33,32,12,40] of human, with the aid of large controlled datasets, further extending the application domain of radiance-field-based modeling to enable augmented reality experiences.",
          "4": "While these methods have shown interesting and exciting results, they often require separate training of editable instances [9] or careful curation of training data [33].",
          "6": "Neural Body [33] associates a latent code to each SMPL vertex, and use sparse convolution to diffuse the latent code into the volume in observation space."
        },
        "Gram: Generative radiance manifolds for 3d-aware image generation": {
          "authors": [
            "Yu Deng",
            "Jiaolong Yang",
            "Jianfeng Xiang",
            "Xin Tong"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.html",
          "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "52"
          ],
          "1": "Most of the NeRF-based methods [32,35,46,48,52] focus on scene-specific learning tasks where a network is trained to fit a set of posed images of a certain scene."
        },
        "Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition": {
          "authors": [
            "Chen Guo",
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.html",
          "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Fitting neural implicit surfaces to videos has recently been demonstrated [23, 25, 42, 49, 50, 55, 71].",
          "2": "Recent works fit implicit neural fields to videos via neural rendering to obtain articulated human models [23\u201325, 42, 49, 50, 55, 71]."
        },
        "Humannerf: Efficiently generated human radiance field from sparse inputs": {
          "authors": [
            "Fuqiang Zhao",
            "Wei Yang",
            "Jiakai Zhang",
            "Pei Lin",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 6",
          "ref_ids": [
            "33"
          ],
          "1": "Remarkably, NeRF [29] and its dynamic extensions [24,33,35,50,53,64] enable photo-realistic novel view synthesis for dynamic scenes without heavy reliance on the reconstruction accuracy.",
          "2": "Comparison We first compare our HumanNeRF method with perscene optimization approaches including Neural Body [33], Neural V olumes [27] and ST-NeRF [64] both qualitatively and quantitatively.",
          "3": "Results from our approach exhibit much better textures and the geometries are complete and accurate both for the \u201cTaichi\u201d from public ZJUMoCap [33] and the \u201cBatman\u201d data collected by ourselves."
        },
        "Monohuman: Animatable human neural field from monocular video": {
          "authors": [
            "Zhengming Yu",
            "Wei Cheng",
            "Xian Liu",
            "Wayne Wu",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021.",
          "ref_ids": [
            "33"
          ],
          "2": "Previous rendering methods [33] can synthesize realistic novel view images of the human body, but hard to animate the avatar in unseen poses.",
          "4": "We compare our method with (1) NeuralBody [33], which uses structured latent code to represent the human body; (2) HumanNeRF [49], which achieves state-of-the-art image synthesis performance of digital human by learning a motion field mapping network from monocular video.",
          "5": "16948 PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [33] 28.",
          "6": "NeuMan [16] only PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [33] 28.",
          "7": "PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [33] 28.",
          "8": "PSNR \u2191 SSIM \u2191 LPIPS* \u2193 Neural Body [33] 28."
        },
        "Infonerf: Ray entropy minimization for few-shot neural volume rendering": {
          "authors": [
            "Mijeong Kim",
            "Seonguk Seo",
            "Bohyung Han"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Kim_InfoNeRF_Ray_Entropy_Minimization_for_Few-Shot_Neural_Volume_Rendering_CVPR_2022_paper.html",
          "ref_texts": "[23] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 5, 6, 7",
          "ref_ids": [
            "23"
          ],
          "5": "2 ZJU-MoCap For the ZJU-MoCap dataset, InfoNeRF is evaluated in comparison with NeRF [20], Neural V olume (NV) [17], and Neural Body (NB) [23], where all algorithms are trained with 4 images.",
          "6": "NB [23] has the geometric prior by exploiting the pretrained human body model (SMPL).",
          "7": "Method Prior PSNR \u2191 SSIM\u2191 LPIPS\u2193 NB [23] \u2713 24."
        },
        "Instantavatar: Learning avatars from monocular video in 60 seconds": {
          "authors": [
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.html",
          "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2, 5, 7",
          "ref_ids": [
            "49"
          ],
          "4": "16925 male-3-casual male-4-casual female-3-casual female-4-casual PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Neural Body [49] (\u223c14 hours) 24.",
          "5": "We report PSNR, SSIM and LPIPS [68] between real images and the images generated by our method and two SoTA methods, Neural Body [49] and Anim-NeRF [6].",
          "6": "Neural Body [49] This baseline learns a set of latent codes anchored to a deformable SMPL mesh.",
          "7": "When training all methods to convergence, our generated images are significantly better than Neural Body [49] and achieve on-par quality as SoTA method Anim-NeRF [6], as indicated by the image quality metrics in Tab."
        },
        "Neural head avatars from monocular rgb videos": {
          "authors": [
            "William Grassal",
            "Malte Prinzler",
            "Titus Leistner",
            "Carsten Rother",
            "Matthias Niessner",
            "Justus Thies"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.html",
          "ref_texts": "[57] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer V ision and 18662",
          "ref_ids": [
            "57"
          ],
          "1": "Motivated by their recent success in 3D scene reconstruction [67], neural radiance fields (NeRF) in combination with volumetric rendering [49] have been used to replace the discrete feature voxel grids [5, 29, 39, 44, 52\u201354, 56, 57, 60, 65, 80]."
        },
        "Human gaussian splatting: Real-time rendering of animatable avatars": {
          "authors": [
            "Arthur Moreau",
            "Jifei Song",
            "Helisa Dhamo",
            "Richard Shaw",
            "Yiren Zhou",
            "Eduardo Perez"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Moreau_Human_Gaussian_Splatting_Real-time_Rendering_of_Animatable_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 6",
          "ref_ids": [
            "44"
          ],
          "1": "Neural Body [44] anchors latent codes to SMPL vertices which are then decoded to density and colors by a CNN.",
          "2": "ZJU-Mocap [44] is obtained using 23 hardware-synchronized cameras, with 1024\u00d71024 resolution.",
          "3": "3 Evaluation on ZJU-MoCap dataset To benchmark HuGS on ZJU-MoCap [44], we follow the setting of NeuralBody [44], i."
        },
        "Neural rays for occlusion-aware image-based rendering": {
          "authors": [
            "Yuan Liu",
            "Sida Peng",
            "Lingjie Liu",
            "Qianqian Wang",
            "Peng Wang",
            "Christian Theobalt",
            "Xiaowei Zhou",
            "Wenping Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "To further improve the rendering resolution, some methods [20, 23, 27, 31, 34, 36, 49] resort to pure neural fields encoded by neural networks to represent 3D scenes."
        },
        "Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing": {
          "authors": [
            "B Yang",
            "C Bao",
            "J Zeng",
            "H Bao",
            "Y Zhang",
            "Z Cui"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_34",
          "ref_texts": "36. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 4",
          "ref_ids": [
            "36"
          ],
          "1": "NeRF [27] takes advantages of volume rendering to boost rendering quality, which inspires a lot of works, including surface reconstruction [32,55,65], human modeling [21,36], pose estimation [67], scene understanding [63] and relighting [48,71,2,72], etc."
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
          "authors": [
            "Chen Geng",
            "Sida Peng",
            "Zhen Xu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.html",
          "ref_texts": "[58] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "58"
          ],
          "1": "Given a monocular video of a human performer, our model can be learned in \u223c5 minutes to produce photorealistic novel view rendering, which is 100 times faster than Neural Body [58].",
          "2": "Recently, some methods [58, 93] have shown that high-quality volumetric videos can be recovered from sparse multi-view videos by representing dynamic humans with neural scene representations.",
          "4": "Our method is implemented purely with the PyTorch framework [53] to demonstrate the effectiveness of our representation It also enables us to fairly compare with baseline methods [34, 56, 58] implemented in PyTorch.",
          "5": "Datasets ZJU-MoCap [58] datasetis a widely-used benchmark for human modeling from videos.",
          "6": "Neural Body (NB) [58] anchors a set of latent codes to the SMPL mesh and regresses the radiance field from the posed latent codes.",
          "7": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
          "10": "[58] implicitly aggregates the temporal information using structured latent 8764"
        },
        "Fourier plenoctrees for dynamic radiance field rendering in real-time": {
          "authors": [
            "Liao Wang",
            "Jiakai Zhang",
            "Xinhang Liu",
            "Fuqiang Zhao",
            "Yanshun Zhang",
            "Yingliang Zhang",
            "Minye Wu",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 6",
          "ref_ids": [
            "41"
          ],
          "2": "To demonstrate the overall performance of our approach, we compare to the existing free-viewpoint video methods based on neural rendering, including the voxel-based method Neural Volumes [28], and implicit methods iButter [58], ST-NeRF [71] and Neural Body [41] based on neural radiance field."
        },
        "Selfrecon: Self reconstruction your digital avatar from monocular video": {
          "authors": [
            "Boyi Jiang",
            "Yang Hong",
            "Hujun Bao",
            "Juyong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.html",
          "ref_texts": "[39] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021.",
          "ref_ids": [
            "39"
          ],
          "1": "NeuralBody [39] reconstructs per frame\u2019s NeRF [34] field conditioned at body structured latent codes and utilizes the NeRF field to synthesize new images.",
          "2": "Qualitative Evaluation We also qualitatively compare SelfRecon with multiframe prediction algorithm PaMIR [55], optimization method VideoAvatar [2] and NeRF [34] based neural rendering method NeuralBody [39] on several sequences of PeopleSnapshot dataset.",
          "3": "Comparison results with methods that use video or multi-frame images, including PaMIR [55], NeuralBody [39] and VideoAvatar [2]."
        },
        "H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion": {
          "authors": [
            "H Xu",
            "T Alldieck"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/7d62a275027741d98073d42b8f735c68-Abstract.html",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.",
          "ref_ids": [
            "36"
          ],
          "1": "Most related, Neural Body [36] attaches learnable features to the vertices of a SMPL body model [24].",
          "2": "Model Dataset PSNR \u2191 SSIM \u2191 LPIPS \u2193 Ch \u00d710\u22123 \u2193 NC \u2191 IoU \u2191 NeuralBody [36] RenderPeople 27.",
          "3": "We compare H-NeRF against NeuralBody [36]"
        },
        "Gomavatar: Efficient animatable human modeling from monocular video using gaussians-on-mesh": {
          "authors": [
            "Jing Wen",
            "Xiaoming Zhao",
            "Zhongzheng Ren",
            "Alexander G. Schwing",
            "Shenlong Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.html",
          "ref_texts": "[54] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 5, 6",
          "ref_ids": [
            "54"
          ],
          "5": "We validate our approach on six 2063 Novel view synthesis Novel pose synthesis Inference time (ms)# Memory (MB)#PSNR\" SSIM\" LPIPS*# PSNR\" SSIM\" LPIPS*# Neural Body [54] 28.",
          "6": "(best, second best) CD# NC\" Neural Body [54] 5.",
          "7": "We compare with state-of-the-art approaches for single-video articulated human capturing algorithms, including NeuralBody [54], HumanNeRF [70], NeuMan [27], MonoHuman [81], Anim-NeRF [8] and InstantAvatar [26]."
        },
        "Sherf: Generalizable human nerf from a single image": {
          "authors": [
            "Shoukang Hu",
            "Fangzhou Hong",
            "Liang Pan",
            "Haiyi Mei",
            "Lei Yang",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.html",
          "ref_texts": "[58] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "58"
          ],
          "3": "Neural Body [58] applies sparse convolutions to model the radiance volume, while others model human NeRF in the canonical space [13, 57, 78, 54, 75, 68, 30, 31, 73] using SMPL LBS weights or optimizing LBS weights with appearance."
        },
        "4k4d: Real-time 4d view synthesis at 4k resolution": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Haotong Lin",
            "Guangzhao He",
            "Jiaming Sun",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html",
          "ref_texts": "[48] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2, 6",
          "ref_ids": [
            "48"
          ],
          "3": "Even when compared with concurrent work [48], our method still achieves 13x speedup and produces consistently higher quality images.",
          "4": "Other image-based methods [48, 49, 90] produce high-quality appearance."
        },
        "Tava: Template-free animatable volumetric actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_25",
          "ref_texts": "35. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "In our experiments, we demonstrate that the proposed approach outperforms state-of-the art approaches for animating and rendering human actors on the ZJU motion capture dataset [35].",
          "13": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [34,35,45]."
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
          "authors": [
            "Y Kwon",
            "D Kim",
            "D Ceylan"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/cf866614b6b18cda13fe699a3a65661b-Abstract.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "33"
          ],
          "1": "Recently, neural radiance fields (NeRF) [26, 11, 17, 30, 33, 35, 36, 47, 50, 52, 53] have shown photo-realistic novel view synthesis results in per-scene optimization settings.",
          "5": "To regularize the training, Neural Body [33] combines NeRF with a deformable human body model (e.",
          "9": "We perform comparisons with the state-of-the-art Neural Body (NB) [33] that combines the body model prior SMPL and NeRF in a per-subject optimization setting.",
          "13": "Even when we train a single model of our method for all the source sobjects (\u2018ours\u2019), ours achieves comparable results with the state-of-the-art per-subject based methods [33].",
          "14": "1c shows that in this case, our model (\u2018ours per-subject\u2019) achieves a significant improvement over the best-performing baseline [33] by +3 PSNR and +1.",
          "15": "The visualizations show that our 3D reconstructions align well with the input images, and is more accurate than even the per-subject method [33] (e."
        },
        "Doublefield: Bridging the neural surface and radiance fields for high-fidelity human reconstruction and rendering": {
          "authors": [
            "Ruizhi Shao",
            "Hongwen Zhang",
            "He Zhang",
            "Mingjia Chen",
            "Pei Cao",
            "Tao Yu",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Shao_DoubleField_Bridging_the_Neural_Surface_and_Radiance_Fields_for_High-Fidelity_CVPR_2022_paper.html",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 6, 7, 8",
          "ref_ids": [
            "37"
          ],
          "4": "In comparison with existing approaches [37,39,64] built upon surface and radiance fields, DoubleField not only improves the reconstruction quality of both geometry and appearance but also has the capability to eliminate the prerequisite SMPL fitting in previous methods [37] and even handle loose clothing (e.",
          "14": "5, unlike NeuralBody [37], the surface reconstructed by our method is more consistent and contains more details.",
          "16": "Moreover, our method does not rely on human shape prior SMPL [28] compared with NeuralBody [37] and achieves photo-realistic rendering even under challenging scenarios like swinging skirt, topological changes and loose cloth, which demonstrates the strong generalization capacity of our method to real world data."
        },
        "Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering": {
          "authors": [
            "Wei Cheng",
            "Ruixiang Chen",
            "Siming Fan",
            "Wanqi Yin",
            "Keyu Chen",
            "Zhongang Cai",
            "Jingbo Wang",
            "Yang Gao",
            "Zhengming Yu",
            "Zhengyu Lin",
            "Daxuan Ren",
            "Lei Yang",
            "Ziwei Liu",
            "Chen Change",
            "Chen Qian",
            "Wayne Wu",
            "Dahua Lin",
            "Bo Dai",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "44"
          ],
          "5": "NeuralBody [44] learns a neural radiance field of dynamic humans conditioned by body structure and temporal latent code from sparse multi-view videos.",
          "12": "(2) Among case-specific dynamic methods, A-NeRF [53] achieves the best PSNR, and NeuralBody [44] and HumanNeRF [61] gets the best SSIM and LPIPS respectively.",
          "13": "5, NeuralBody [44] and A-NeRF [53] could render novel view image with fewer background artifacts than other methods, while HumanNeRF [61] can better preserve high fidelity textures, especially in high-frequency texture regions."
        },
        "Neural residual radiance fields for streamably free-viewpoint videos": {
          "authors": [
            "Liao Wang",
            "Qiang Hu",
            "Qihan He",
            "Ziyu Wang",
            "Jingyi Yu",
            "Tinne Tuytelaars",
            "Lan Xu",
            "Minye Wu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.html",
          "ref_texts": "[50] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes 85",
          "ref_ids": [
            "50"
          ],
          "1": "V arious schemes to compensate for temporal motions have been proposed by employing implicit matching [18, 38, 48, 49, 62] or data-driven priors such as depth [73], Fourier features [67], optical flow [17, 37], or skeletal/facial motion priors [28,50,69,82]."
        },
        "Representing volumetric videos as dynamic mlp maps": {
          "authors": [
            "Sida Peng",
            "Yunzhi Yan",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.html",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 7",
          "ref_ids": [
            "46"
          ],
          "1": "To model high-resolution scenes, [14, 16, 31, 42, 44, 46, 47, 71] extend NeRF to represent dynamic scenes.",
          "2": "Datasets To evaluate the performance of our approach, we conduct experiments of the ZJU-MoCap [46] and NHR [67] datasets."
        },
        "Ag3d: Learning to generate 3d avatars from 2d image collections": {
          "authors": [
            "Zijian Dong",
            "Xu Chen",
            "Jinlong Yang",
            "Michael J. Black",
            "Otmar Hilliges",
            "Andreas Geiger"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Dong_AG3D_Learning_to_Generate_3D_Avatars_from_2D_Image_Collections_ICCV_2023_paper.html",
          "ref_texts": "[53] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "53"
          ],
          "1": "Several methods [13, 17, 27, 32, 53, 62, 63, 73] combine NeRF with human priors to enable 3D human reconstruction from multi-view data or even monocular videos."
        },
        "Local-to-global registration for bundle-adjusting neural radiance fields": {
          "authors": [
            "Yue Chen",
            "Xingyu Chen",
            "Xuan Wang",
            "Qi Zhang",
            "Yu Guo",
            "Ying Shan",
            "Fei Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[35] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "35"
          ],
          "1": "They have seen widespread success in problems such as image synthesis [4, 7, 40], 3D shape [9, 27, 34], view-dependent appearance [6, 18, 29, 33], and animation of humans [8, 35, 45]."
        },
        "Deforming radiance fields with cages": {
          "authors": [
            "T Xu",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_10",
          "ref_texts": "38. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "38"
          ],
          "1": "Harada For some specific object categories, such as the human body or articulated objects, recent studies [24,32,33,37,38,41,46] enable the generation of the unseen scene by controlling the body shape or bone pose.",
          "2": "For the specific task of human body modeling, various works proposed to combine NeRF with a parametric human model to enable human body reposing [37, 38], shape control [24] or even clothing changes [46]."
        },
        "High-fidelity clothed avatar reconstruction from a single image": {
          "authors": [
            "Tingting Liao",
            "Xiaomei Zhang",
            "Yuliang Xiu",
            "Hongwei Yi",
            "Xudong Liu",
            "Jun Qi",
            "Yong Zhang",
            "Xuan Wang",
            "Xiangyu Zhu",
            "Zhen Lei"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Liao_High-Fidelity_Clothed_Avatar_Reconstruction_From_a_Single_Image_CVPR_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "33"
          ],
          "1": "NeRFbased methods [33, 52, 57] optimize a goal using conditions on articulated cues."
        },
        "A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose": {
          "authors": [
            "SY Su",
            "F Yu",
            "M Zollh\u00f6fer"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html",
          "ref_texts": "[46] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "46"
          ],
          "1": "Even more similar is the recent NeuralBody [46] representation, which combines a NeRF with a surface body model and underlying skeleton.",
          "2": "Following concurrent works [33, 46], we also optimize an appearance code for each image to handle dynamic light effects.",
          "3": "While NeuralBody [46] can also learn photo-realistic human models from monocular images, their model anchors its representation on the SMPL 3D."
        },
        "Sifu: Side-view conditioned implicit function for real-world usable clothed human reconstruction": {
          "authors": [
            "Zechuan Zhang",
            "Zongxin Yang",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.html",
          "ref_texts": "[63] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "63"
          ],
          "1": "The rise of Neural Radiance Fields (NeRF) has seen methods [18, 20, 33, 34, 56, 59, 62, 63, 78, 88] using videos or multi-view images to optimize NeRF for human form capture."
        },
        "Deliffas: Deformable light fields for fast avatar synthesis": {
          "authors": [
            "Y Kwon",
            "L Liu",
            "H Fuchs"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/805c06617d2b643278936daadfde4280-Abstract-Conference.html",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 1(1):9054\u20139063, 2021.",
          "ref_ids": [
            "43"
          ],
          "2": "Neural Actor (NA) [32], Neural Body (NB) [43], and A-NeRF [51] are hybrid methods that combine the human prior, i."
        },
        "Discoscene: Spatially disentangled generative radiance fields for controllable 3d-aware scene synthesis": {
          "authors": [
            "Yinghao Xu",
            "Menglei Chai",
            "Zifan Shi",
            "Sida Peng",
            "Ivan Skorokhodov",
            "Aliaksandr Siarohin",
            "Ceyuan Yang",
            "Yujun Shen",
            "Ying Lee",
            "Bolei Zhou",
            "Sergey Tulyakov"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9054\u20139063, 2021. 1",
          "ref_ids": [
            "38"
          ],
          "1": "Recent approaches like GRAF [40] and Pi-GAN [5] introduce 3D inductive bias by taking neural radiance fields [1, 28, 29, 36, 38] as the underlying representation, gaining the capability of geometry modeling and explicit camera control."
        },
        "3d-aware neural body fitting for occlusion robust 3d human pose estimation": {
          "authors": [
            "Yi Zhang",
            "Pengliang Ji",
            "Angtian Wang",
            "Jieru Mei",
            "Adam Kortylewski",
            "Alan Yuille"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_3D-Aware_Neural_Body_Fitting_for_Occlusion_Robust_3D_Human_Pose_ICCV_2023_paper.html",
          "ref_texts": "[54] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "54"
          ],
          "1": "Recently, implicit volume representation has become increasingly popular [21, 37, 48, 50, 54, 62, 67, 75, 78] as they can achieve highly realistic human reconstruction."
        },
        "Physavatar: Learning the physics of dressed 3d avatars from visual observations": {
          "authors": [
            "Y Zheng",
            "Q Zhao",
            "G Yang",
            "W Yifan",
            "D Xiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72913-3_15",
          "ref_texts": "87. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 4",
          "ref_ids": [
            "87"
          ],
          "1": "Several different types of representations have been explored for clothed avatars, including meshes [68] with dynamic textures [4,33,131], neural surface [16,90,107] and radiance fields [14,18,24,27,44\u201346, 86,87,99,133], point sets [66,67,69], and 3D Gaussians [34,58,82,134]."
        },
        "Learning locally editable virtual humans": {
          "authors": [
            "I Ho",
            "Lixin Xue",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ho_Learning_Locally_Editable_Virtual_Humans_CVPR_2023_paper.html",
          "ref_texts": "[50] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "50"
          ],
          "1": "With the advances in neural rendering [63, 64] and the availability of large-scale human datasets [18,28,29,32,47,67,73,74], numerous approaches have been proposed to reconstruct [4, 56, 57] and explicitly control [10, 30, 50] human avatars in a data-driven manner.",
          "2": "Other methods learn to model challenging pose-dependent deformations on avatars either by predicting LBS weights [8, 10, 14, 58, 59, 76] or improving the capabilities of body models [23, 30, 34, 35, 37, 50, 53] with the power of implicit neural fields."
        },
        "Mobrecon: Mobile-friendly hand mesh reconstruction from monocular image": {
          "authors": [
            "Xingyu Chen",
            "Yufeng Liu",
            "Yajiao Dong",
            "Xiong Zhang",
            "Chongyang Ma",
            "Yanmin Xiong",
            "Yuan Zhang",
            "Xiaoyan Guo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_MobRecon_Mobile-Friendly_Hand_Mesh_Reconstruction_From_Monocular_Image_CVPR_2022_paper.html",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "57"
          ],
          "1": "The implicit function [50] has merits of continuity and high resolution, which is recently used for digitizing articulated human [51, 34, 3, 59, 32, 57, 38, 39]."
        },
        "The power of points for modeling humans in clothing": {
          "authors": [
            "Qianli Ma",
            "Jinlong Yang",
            "Siyu Tang",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.html",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, June 2021. 3",
          "ref_ids": [
            "47"
          ],
          "1": "Neural implicit surfaces [11, 38, 43], on the other hand, do not require any pre-defined template, are flexible with surface topology, and have recently become a promising choice for reconstructing [7, 24, 25, 47, 53, 54, 64, 65] and modeling [10, 12, 14, 39, 41, 55] shapes of 3D humans."
        },
        "Mofanerf: Morphable facial neural radiance field": {
          "authors": [
            "Y Zhuang",
            "H Zhu",
            "X Sun",
            "X Cao"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_16",
          "ref_texts": "43. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "43"
          ],
          "1": "SMPL)[39,7,34,43] or skeleton[42] as prior to build NeRF for human body."
        },
        "Lisa: Learning implicit shape and appearance of hands": {
          "authors": [
            "Enric Corona",
            "Tomas Hodan",
            "Minh Vo",
            "Francesc Moreno",
            "Chris Sweeney",
            "Richard Newcombe",
            "Lingni Ma"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Corona_LISA_Learning_Implicit_Shape_and_Appearance_of_Hands_CVPR_2022_paper.html",
          "ref_texts": "[51] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition (CVPR), pages 9054\u20139063, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "For modeling dynamic human bodies, Neural Body [51] attaches learnable vertex features to SMPL, and diffuses the features with sparse convolution for volumetric rendering."
        },
        "Cagenerf: Cage-based neural radiance field for generalized 3d deformation and animation": {
          "authors": [
            "Y Peng",
            "Y Yan",
            "S Liu",
            "Y Cheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb78e6b5246b03e0b82b4acc8b11cc21-Abstract-Conference.html",
          "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "34"
          ],
          "1": "To address this issue, when modeling dynamic objects, most works optimize inside a deformation-invariant canonical space with data-dependent priors, such as 3DMM [4] for face deformation [1, 14], and SMPL [26] for human animation [34, 32].",
          "2": "Referring to the widely concerned digital humans, a vast range of methods are proposed for the deformation of the implicit face [12, 57, 14] and body [34, 8, 36, 33].",
          "3": "Following the majority of previous literature [29, 32, 34, 45], we use PSNR/SSIM [48] and LPIPS [54] as evaluation metrics."
        },
        "Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction": {
          "authors": [
            "Wenjia Wang",
            "Yongtao Ge",
            "Haiyi Mei",
            "Zhongang Cai",
            "Qingping Sun",
            "Yanjun Wang",
            "Chunhua Shen",
            "Lei Yang",
            "Taku Komura"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9054\u20139063, 2021. 9",
          "ref_ids": [
            "37"
          ],
          "1": "Our results show significant value for human mesh reconstruction in perspective-distorted images and can empower many downstream tasks, such as monocular clothed human reconstruction [43, 37, 46] and human motion reconstruction in live shows, vlogs, and selfie videos."
        },
        "Synbody: Synthetic dataset with layered human models for 3d human perception and modeling": {
          "authors": [
            "Zhitao Yang",
            "Zhongang Cai",
            "Haiyi Mei",
            "Shuai Liu",
            "Zhaoxi Chen",
            "Weiye Xiao",
            "Yukun Wei",
            "Zhongfei Qing",
            "Chen Wei",
            "Bo Dai",
            "Wayne Wu",
            "Chen Qian",
            "Dahua Lin",
            "Ziwei Liu",
            "Lei Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 9",
          "ref_ids": [
            "41"
          ],
          "3": "NeuralBody [41] incorporates prior from a statistical body template to learn dynamic sequence, while Animatable NeRF [40] proposes to reconstruct an animatable human model that generalizes to new poses.",
          "6": "We benchmark five methods in total, including the vanilla NeRF [34], NeuralBody [41] and HumanNeRF [50] for novel view synthesis, AnimNeRF [40] for novel pose synthesis, NHP [26] for generalizable human NeRF (novel identity synthesis)."
        },
        "Avatargen: a 3d generative model for animatable human avatars": {
          "authors": [
            "J Zhang",
            "Z Jiang",
            "D Yang",
            "H Xu",
            "Y Shi",
            "G Song"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25066-8_39",
          "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "51"
          ],
          "1": "Some methods augment NeRF with human body priors to enable 3D human reconstruction from sparse multi-view data [5, 51, 57, 64]."
        },
        "Back to optimization: Diffusion-based zero-shot 3d human pose estimation": {
          "authors": [
            "Zhongyu Jiang",
            "Zhuoran Zhou",
            "Lei Li",
            "Wenhao Chai",
            "Yen Yang",
            "Neng Hwang"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Jiang_Back_to_Optimization_Diffusion-Based_Zero-Shot_3D_Human_Pose_Estimation_WACV_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1",
          "ref_ids": [
            "40"
          ],
          "1": "[40] utilize 3D human poses with Neural Radiance Fields (NeRF) for 3D Human Reconstruction."
        },
        "Unsupervised learning of efficient geometry-aware neural articulated representations": {
          "authors": [
            "A Noguchi",
            "X Sun",
            "S Lin",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_36",
          "ref_texts": "57. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "57"
          ],
          "1": "We demonstrate this approach by modeling the pose prior as a skeletal distribution [51,63], while noting that other models like meshes [57,56] may bring potential performance benefits.",
          "2": "They have achieved the state-of-the art in learning 3D shape [12,43,53], static [62,44,4] and dynamic scenes [58,37,54], articulated objects [57,14,7,65,11,56,51,63,2,69,39], and image synthesis [60,10].",
          "3": "Recently, articulated representations based on NeRF have been proposed [57,51,63,56,69,70,39]."
        },
        "A-sdf: Learning disentangled signed distance functions for articulated shape representation": {
          "authors": [
            "Jiteng Mu",
            "Weichao Qiu",
            "Adam Kortylewski",
            "Alan Yuille",
            "Nuno Vasconcelos",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Mu_A-SDF_Learning_Disentangled_Signed_Distance_Functions_for_Articulated_Shape_Representation_ICCV_2021_paper.html",
          "ref_texts": "[60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.arXiv preprint arXiv: 2012.15838, 2021.2",
          "ref_ids": [
            "60"
          ],
          "1": "One line of work leverages parametric mesh models [43, 38, 92, 5] to estimate shape and articulation for faces [74, 62, 66], hands[16], humans bodies [60, 4, 86, 34, 28, 56, 85, 79], and animals [91, 29, 36, 90] by directly inferring shape and articulation parameters."
        },
        "Metaavatar: Learning animatable clothed human models from few depth images": {
          "authors": [
            "S Wang",
            "M Mihajlovic",
            "Q Ma"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/1680829293f2a8541efa2647a0290f88-Abstract.html",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of CVPR, 2021. 3",
          "ref_ids": [
            "57"
          ],
          "1": "Neural Implicit Representations:Neural implicit representations [11, 45, 46, 52, 55] have been used to tackle both image-based [27, 35, 36, 57, 59, 63, 64, 87] and point cloud-based [7, 12] clothed human reconstruction."
        },
        "Deepmulticap: Performance capture of multiple characters using sparse multiview cameras": {
          "authors": [
            "Yang Zheng",
            "Ruizhi Shao",
            "Yuxiang Zhang",
            "Tao Yu",
            "Zerong Zheng",
            "Qionghai Dai",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zheng_DeepMultiCap_Performance_Capture_of_Multiple_Characters_Using_Sparse_Multiview_Cameras_ICCV_2021_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 6",
          "ref_ids": [
            "44"
          ],
          "1": "Performance on Real World DataWe evaluate our method on ZJU-MoCap dataset [44], a multi-view real world dataset, with comparison to DeepVisualHull [25], a volumetric performance capture from sparse multi-view, Neural Body [44], a differentiable rendering method directly Figure 5: Performance on ZJU-Mocap dataset [44].",
          "2": "Our method outperforms state-of-the-art approaches including DeepVisualHull [25], PIFuHD [48] and Neural Body [44]."
        },
        "Anifacegan: Animatable 3d-aware face image generation for video avatars": {
          "authors": [
            "Y Wu",
            "Y Deng",
            "J Yang",
            "F Wei"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eae78bf2712f222f101bd7d12f875a57-Abstract-Conference.html",
          "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u2013",
          "ref_ids": [
            "49"
          ],
          "1": "The original NeRF and most of its successors [43, 33, 45, 49, 48, 40, 32, 73] focus on learning scene-specific representation using a set of posed images or a video sequence of a static or dynamic scene."
        },
        "E-nerv: Expedite neural video representation with disentangled spatial-temporal context": {
          "authors": [
            "Z Li",
            "M Wang",
            "H Pi",
            "K Xu",
            "J Mei",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19833-5_16",
          "ref_texts": "37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "37"
          ],
          "1": "Unlike regular grid-wise representations, the compact INRs are proved to be suitable for complex scenes [30] and arbitrary scale sampling [6], as well as in lots of 3D tasks [30,24,37,34] and image representations [43,6,27,59,57,40,64]."
        },
        "Gm-nerf: Learning generalizable model-based neural radiance fields from multi-view images": {
          "authors": [
            "Jianchuan Chen",
            "Wentao Yi",
            "Liqian Ma",
            "Xu Jia",
            "Huchuan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "31"
          ],
          "1": "To better generalize to unseen poses, NeuralBody [31] introduces a statistical body model SMPL [23] into neural radiance fields which can reconstruct vivid digital humans from a sparse multi-view video.",
          "2": "To alleviate this limitation, some works [6, 30, 31, 41, 48, 51] combine neural radiance fields [27] with SMPL [23] to represent the human body, which can be rendered to 2D images by differentiable rendering.",
          "3": "NeuralBody [31] optimizes a set of structured latent codes from scratch on vertices of the SMPL model for each specific identity.",
          "4": "Similar to NeuralBody [31], we use SparseConvNet [13] D to diffuse the structured latent codes {zi}N i=1 into the nearby space to form a 3D feature volume G.",
          "5": "0 [55] ZJUMocap [31] GeneBody [8] Model PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 IBRNet [47] 28.",
          "6": "Qualitative results of novel pose synthesis on ZJUMocap [31] datasets.",
          "7": "Novel View Synthesis Novel Pose Synthesis Model PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 NB [31] 28.",
          "8": "0 [55] and Multi-garment [4] and real-world datasets Genebody [8] and ZJUMocap [31] for the generalizable scene task.",
          "9": "Following the evaluation protocols used in NB [31], we select 4 fixed view videos for training.",
          "10": "The SMPL parameters are obtained using EasyMocap [31].",
          "11": "Specially, we also usem = 3 views as input on ZJUMocap [31] dataset following the evaluation protocol used in KeypointNeRF.",
          "12": "We also compare with per-scene optimization methods NB [31], Ani-NeRF [30], A-NeRF [41], ARAH [48]."
        },
        "Masked space-time hash encoding for efficient dynamic scene reconstruction": {
          "authors": [
            "F Wang",
            "Z Chen",
            "G Wang",
            "Y Song"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/df31126302921ca9351fab73923a172f-Abstract-Conference.html",
          "ref_texts": "[66] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "66"
          ],
          "1": "Extending the neural radiance field to express dynamic scenes is a natural yet challenging task that has been proven crucial to many downstream applications [39, 66, 77, 31]."
        },
        "Animatable neural radiance fields from monocular rgb videos": {
          "authors": [
            "J Chen",
            "Y Zhang",
            "D Kang",
            "X Zhe",
            "L Bao",
            "X Jia"
          ],
          "url": "https://arxiv.org/abs/2106.13629",
          "ref_texts": "[30] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021. 2, 5, 6, 8",
          "ref_ids": [
            "30"
          ],
          "3": "Compared with NeuralBody[30] and SMPLpix[47], our approach can produce realistic images with well preserved identity and cloth details.",
          "6": "NeuralBody[30] is the most similar work to ours in the sense that it also combines NeRF with SMPL."
        },
        "Hosnerf: Dynamic human-object-scene neural radiance fields from a single video": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Tianyuan Yang",
            "Zhongcong Xu",
            "Jussi Keppo",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "32"
          ],
          "3": "Among them, Neural Actor [23] and Neural Body [32] pioneer in combining NeRF [27] with SMPL deformable meshes to represent human bodies with complex motions."
        },
        "Datid-3d: Diversity-preserved domain adaptation using text-to-image diffusion for 3d generative model": {
          "authors": [
            "Gwanghyun Kim",
            "Se Young"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/html/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.html",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "These 3D generative models can be trained with single-view images and then can sample infinite 3D images in real-time, while 3D scene representation as neural implicit fields using NeRF [38] and its variants [3, 4, 8, 10, 14, 17, 20, 32\u201334, 36, 45, 47, 50, 53, 54, 64, 66, 70\u201373] require multi-view images and training for each scene.",
          "2": "Such 3D generative models can be trained using single-view images and then can sample infinite 3D images in real-time whereas 3D scene representation as neural implicit fields using Neural Radiance Field (NeRF) [38] and its variants [3, 4, 8, 10, 14, 17, 20, 32\u201334, 36, 45, 47, 50, 53, 54, 64, 66, 70\u201373] requires multi-view images and training time for each scene."
        },
        "Surmo: surface-based 4D motion modeling for dynamic human rendering": {
          "authors": [
            "Tao Hu",
            "Fangzhou Hong",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_SurMo_Surface-based_4D_Motion_Modeling_for_Dynamic_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Recent neural rendering methods [25, 43, 54, 61] have made great progress in generating free-viewpoint videos of humans from several sparse multi-view videos, which are simple yet effective compared with traditional graphics approaches [2, 3, 62].",
          "4": "In addition, quantitative and qualitative experiments are performed on three datasets with a total of 9 subject sequences, including ZJU-MoCap [43], AIST++ [24], and MPII-RDDC [13], which validate the effectiveness of SurMo in different scenarios.",
          "7": "ZJU-S1-6 LPIPS \u2193 FID \u2193 SSIM \u2191 PSNR \u2191 Neural Body [43].",
          "8": "We compare our method against SOTA methods including Neural Body [43], HumanNeRF [61], InstantNVR [9], ARAH [58], DV A [47] and HVTR++ [18]."
        },
        "Pina: Learning a personalized implicit neural avatar from a single rgb-d video sequence": {
          "authors": [
            "Zijian Dong",
            "Chen Guo",
            "Jie Song",
            "Xu Chen",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.html",
          "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "48"
          ],
          "1": "Implicit Human Models from 3D ScansImplicit neural representations [14, 37, 43] can handle topological changes better [10,44] and have been used to reconstruct clothed human shapes [12,26,27,30,48,49,51,52,59]."
        },
        "HandNeRF: Neural radiance fields for animatable interacting hands": {
          "authors": [
            "Zhiyang Guo",
            "Wengang Zhou",
            "Min Wang",
            "Li Li",
            "Houqiang Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 1, 2",
          "ref_ids": [
            "26"
          ],
          "2": "Neural Body [26] signifies a breakthrough in low-cost human rendering by combining NeRF with the mesh-based SMPL model [15]."
        },
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[109] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021, pp. 9054\u20139063. 3",
          "ref_ids": [
            "109"
          ],
          "1": "Relighting4D [104] employs NeuralBody [109] as the dynamic human model and utilizes neural inverse rendering to decompose it into a 3D reflectance field, enabling the estimation of materials and lighting properties."
        },
        "Neural parametric gaussians for monocular non-rigid object reconstruction": {
          "authors": [
            "D Das",
            "C Wewer",
            "R Yunus",
            "E Ilg"
          ],
          "url": "https://arxiv.org/abs/2312.01196",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "43"
          ],
          "1": "A few approaches utilize either skeletons [22, 29, 35, 73] or surface templates [14, 43, 68, 77] from parametric models like SMPL [30] to guide the motion field and learn the canonical radiance field on top of such templates."
        },
        "Tela: Text to layer-wise 3d clothed human generation": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Z Huang",
            "X Xu",
            "J Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_2",
          "ref_texts": "33. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 4",
          "ref_ids": [
            "33"
          ],
          "1": "Benefiting from advancements in implicit functions [26,27,30], recent methods [7,10,32,33,44,45,47,49,52] have presented impressive clothed human reconstruction or generation from images and 3D scans."
        },
        "Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training": {
          "authors": [
            "Junfan Lin",
            "Jianlong Chang",
            "Lingbo Liu",
            "Guanbin Li",
            "Liang Lin",
            "Qi Tian",
            "Wen Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "29"
          ],
          "1": "This powerful representation ability of foundation model has led to the emergence of zero-shot text-driven applications [9, 14, 26,29], including 3D meshes generation [16,17,23,28,39]."
        },
        "Gaussianbody: Clothed human reconstruction via 3d gaussian splatting": {
          "authors": [
            "M Li",
            "S Yao",
            "Z Xie",
            "K Chen"
          ],
          "url": "https://arxiv.org/abs/2401.09720",
          "ref_texts": "[15] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 5, 6",
          "ref_ids": [
            "15"
          ],
          "1": "We compare our method with original 3D-GS [24], Neural body[15], Anim-NeRF [16] and InstantAvatar [17].",
          "2": "Neural body[15] adopts the SMPL vertexes as the set of latent code to record the local feature and reconstruct humans in NeRF."
        },
        "Reacto: Reconstructing articulated objects from a single video": {
          "authors": [
            "Chaoyue Song",
            "Jiacheng Wei",
            "Chuan Sheng",
            "Guosheng Lin",
            "Fayao Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Song_REACTO_Reconstructing_Articulated_Objects_from_a_Single_Video_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.3",
          "ref_ids": [
            "45"
          ],
          "1": "Several studies [3, 16, 21, 22, 28, 37, 45, 55, 56, 78] have explored reconstructing shape and appearance from images or videos relied on neural radiance fields (NeRF) [33].",
          "2": "To address these challenges, several works [16, 37, 44, 61] employ the parametric 3D human models, such as SMPL [29], while other methods [28, 44, 45] utilize synchronized multi-view video inputs."
        },
        "An arbitrary scale super-resolution approach for 3d mr images via implicit neural representation": {
          "authors": [
            "Q Wu",
            "Y Li",
            "Y Sun",
            "Y Zhou",
            "H Wei",
            "J Yu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9954892/",
          "ref_texts": "[38] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9054\u20139063, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Since then, a number of INRbased models [33]\u2013[38] for 3D shape and surface have been proposed."
        },
        "One-shot implicit animatable avatars with model-based priors": {
          "authors": [
            "Yangyi Huang",
            "Hongwei Yi",
            "Weiyang Liu",
            "Haofan Wang",
            "Boxi Wu",
            "Wenxiao Wang",
            "Binbin Lin",
            "Debing Zhang",
            "Deng Cai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 2, 6, 7, 8",
          "ref_ids": [
            "39"
          ],
          "5": "We selected three state-of-the-art methods as baselines: Neural Body [39] (NB) and Animatable NeRF [37] (AniNeRF) from per-subject optimization methods, and Neural Human Performer [22] (NHP) from generalizable methods.",
          "6": "Compared with state-of-the-art NeRF based methods[39, 22, 37] on novel view synthesis and novel pose synthesis, ELICIT generates human 3D renderings with more consistent appearance and realistic details from a single image.",
          "7": "As in previous works [39, 22, 27], we evaluated our results using two standard metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).",
          "8": "We followed the evaluation protocol of [39] and calculated the metrics only on the bounding box region, rather than the entire image.",
          "9": "For per-subject optimization methods NB [39] and Ani-NeRF [37], we optimized one model for each frame.",
          "10": "6m Subjects Methods PSNR \u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS \u2193 Sall NB[39] 20.",
          "11": "143 Stest NB[39] 19.",
          "12": "6m PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS \u2193 NeuralBody[39] 20."
        },
        "Mps-nerf: Generalizable 3d human rendering from multiview images": {
          "authors": [
            "X Gao",
            "J Yang",
            "J Kim",
            "S Peng",
            "Z Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9888037/",
          "ref_texts": "[5] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063. 1, 2, 3, 5, 6, 7, 10",
          "ref_ids": [
            "5"
          ],
          "2": "In particular, promising results have been shown by methods [5], [6], [7] that are based on the neural radiance field (NeRF) [8] representation.",
          "4": "NeuralBody [5] adopts SMPL [15] and uses per-vertex latent code which is used to generate a continuous latent code volume.",
          "5": "To render the target image, we follow recent works [5], [6], [7] and base our rendering scheme on NeRF [8], which is a compact yet powerful representation for neural rendering.",
          "6": "Instead of using the sampling strategy in original NeRF [8] which samples points in the whole volume, we follow [5] to sample points within a 3D bounding box derived based on the SMPL model.",
          "7": "Instead of directly calculating PSNR and SSIM for the whole image, we follow AniNeRF [6] and NeuralBody [5] to project the 3D bounding box of a body onto image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "9": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera parameters to render a novel view of these trained subjects.",
          "10": "Hence, for reference purpose, we compare our method with recent person-specific models NeuralBody (NB) [5] and Animatable NeRF (AniNeRF) [6].",
          "11": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera and pose parameters to render these trained subjects.",
          "15": "For a reference, we also present the visual results of NeuralBody [5]."
        },
        "Control4d: Efficient 4d portrait editing with text": {
          "authors": [
            "Ruizhi Shao",
            "Jingxiang Sun",
            "Cheng Peng",
            "Zerong Zheng",
            "Boyao Zhou",
            "Hongwen Zhang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Shao_Control4D_Efficient_4D_Portrait_Editing_with_Text_CVPR_2024_paper.html",
          "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "48"
          ],
          "1": "Additionally, methods including Neuralbody [48] and [37, 73, 83, 84] leverage parametric body templates as semantic priors to achieve photo-realistic novel view synthesis of complex human performances."
        },
        "Humangen: Generating human radiance fields with explicit priors": {
          "authors": [
            "Suyi Jiang",
            "Haoran Jiang",
            "Ziyu Wang",
            "Haimin Luo",
            "Wenzheng Chen",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.html",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021. 3",
          "ref_ids": [
            "57"
          ],
          "1": "Embracing the developing of NeRF techniques [8,9, 40\u201342, 44, 45, 48, 69, 71, 73, 82, 86], the human shape prior augmented NeRFs achieve modeling realistic human bodies [32, 37, 50, 57, 89], learning animatable avatars [34, 56, 74] and generalizing across different persons [32, 70, 89] from temporal data."
        },
        "Actorsnerf: Animatable few-shot human rendering with generalizable nerfs": {
          "authors": [
            "Jiteng Mu",
            "Shen Sang",
            "Nuno Vasconcelos",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 1, 2, 5",
          "ref_ids": [
            "33"
          ],
          "3": "Recently, NeRF-based human representations have shown promise for high-quality view synthesis [33, 32, 50, 29, 19, 23, 42, 48, 55, 15, 10, 20, 45, 40, 41].",
          "8": "NeuralBody [33] is a representative method for rendering from observation space."
        },
        "Stochastic neural radiance fields: Quantifying uncertainty in implicit 3d representations": {
          "authors": [
            "J Shen",
            "A Ruiz",
            "A Agudo"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665942/",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: 9 Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "Other recent works [33, 8, 34, 19, 37, 32] have extended NeRF to cases with dynamic objects in the scene."
        },
        "Transhuman: A transformer-based human representation for generalizable neural human rendering": {
          "authors": [
            "Xiao Pan",
            "Zongxin Yang",
            "Jianxin Ma",
            "Chang Zhou",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.1, 2, 5, 6, 7, 14, 15",
          "ref_ids": [
            "33"
          ],
          "3": "Extensive experiments on ZJU-MoCap [33] and H36M [16] demonstrate the superior generalization ability and high efficiency of TransHuman which attains a new state-of-the-art performance and outperforms previous methods by significant margins, e.",
          "9": "We achieve a significantly new sate-of-the-art performance compared with both generalizable [35, 51, 6, 19, 28] and per-subject methods [25, 39, 45, 33].",
          "12": "For the more challenging cross-dataset generalization setting, we also outperform the baseline methods remarkably albeit these two datasets [33, 16] have significantly different distributions, which proves the superior generalization ability of our TransHuman."
        },
        "Npc: Neural point characters from video": {
          "authors": [
            "Yang Su",
            "Timur Bagautdinov",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 4, 6, 7, 8",
          "ref_ids": [
            "35"
          ],
          "6": "Experiments We quantify the improvements our NPC brings over the most recent surface-free approach TA V A [19], DANBO [43] and A-NeRF [44], as well as most recent and established approaches that leverage template or scan-based prior, including ARAH [50], Anim-NeRF [34] and NeuralBody [35].",
          "8": "Compared to Neural Body [35], which anchors the body representation on template body mesh and diffuses the feature using 3D CNN, our NPC produces crispier details like motion capture body trackers and wrinkles, and maintains better texture consistency like the clearer stripes."
        },
        "ULNeF: Untangled layered neural fields for mix-and-match virtual try-on": {
          "authors": [
            "I Santesteban",
            "M Otaduy"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/4ee3ac2cd119023c79b0d21c4a464dc7-Abstract-Conference.html",
          "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "NeuralBody [51] appends learnable features to the vertices of a surface body model, enabling free-viewpoint rendering of animatable humans."
        },
        "Structured 3d features for reconstructing controllable avatars": {
          "authors": [
            "Enric Corona",
            "Mihai Zanfir",
            "Thiemo Alldieck",
            "Eduard Gabriel",
            "Andrei Zanfir",
            "Cristian Sminchisescu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.html",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 3, 7",
          "ref_ids": [
            "46"
          ],
          "1": "Recently, research on implicit representations [6, 16, 22, 52, 53] and neural fields [24, 46, 63, 70] has made significant progress in improving the realism of avatars."
        },
        "Pref: Predictability regularized neural motion fields": {
          "authors": [
            "L Song",
            "X Gong",
            "B Planche",
            "M Zheng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_38",
          "ref_texts": "40. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "40"
          ],
          "1": "NeRF led to a series of breakthroughs in the fields of 3D scene understanding and rendering, such as relighting [2,46,3], human face and body capture [14,34,40,39,49,24], and city-scale reconstruction [50,58,53,43]."
        },
        "Seas: Shape-aligned supervision for person re-identification": {
          "authors": [
            "Haidong Zhu",
            "Pranav Budhwant",
            "Zhaoheng Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_SEAS_ShapE-Aligned_Supervision_for_Person_Re-Identification_CVPR_2024_paper.html",
          "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "51"
          ],
          "1": "However, recent research trends are shifting towards more advanced implicit representations, such as implicit functions [14, 48, 51, 55, 56, 61] and Neural Radiance Fields (NeRF) [29, 50, 69, 80, 87]."
        },
        "Neca: Neural customizable human avatar": {
          "authors": [
            "Junjin Xiao",
            "Qing Zhang",
            "Zhan Xu",
            "Shi Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xiao_NECA_Neural_Customizable_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "42"
          ],
          "1": "Method Pose Lighting Shadow Shape Texture Neural Body [42] % % % % % Neural Actor [31] ! % % ! ! Ani-NeRF [41] ! % % % % DS-NeRF [75] ! % % ! ! SA-NeRF [63] ! % % ! ! HumanNeRF [59] ! % % % % ARAH [56] ! % % % % TA V A [29] ! % % % ! Relighting4D [9] % ! % % % MonoHuman [68] ! % % % % CustomHumans [20] ! % % ! ! UV V olumes [8] ! % % ! ! PoseV ocab [30] ! % % % % Sun et al.",
          "3": "We evaluate our method on an indoor dataset ZJU-MoCap [42], an outdoor dataset NeuMan [23].",
          "4": "We train and evaluate our method on all 9 sequences, and follow the training and testing view and pose split introduced in [42].",
          "5": "Akin to [42], we employ a bounding box surrounding the human as mask for metric calculation.",
          "6": "on novel pose synthesis: Neural Body (NB) [42], Animatable NeRF (AN) [41], Dual-Space NeRF (DS) [75], ARAH [56], and PoseV ocab (PV) [30]."
        },
        "Surface-aligned neural radiance fields for controllable 3d human synthesis": {
          "authors": [
            "Tianhan Xu",
            "Yasuhiro Fujita",
            "Eiichi Matsumoto"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "36"
          ],
          "5": "Neural Body [36] uses the structured latent code anchored at the SMPL vertices to encode the pose information.",
          "6": "Specifically, NARF [30] transforms the position information of spatial points into each bone coordinate and uses all of them as input; Neural Body [36] uses a neural network to compute a latent code representing the position information relative to the body mesh and uses it as input.",
          "10": "For the ZJU-Mocap dataset, we compare with Neural Body [36], which uses structured latent code to model the appearance of the human body.",
          "11": "For the ZJU-MoCap dataset, our approach outperforms [30] and shows a slightly better performance compared to [36], although these studies use per-frame optimization for better modeling of the training pose.",
          "12": "For both datasets, the performance of our approach almost consistently outperforms [36], [35], and [30].",
          "13": "Also, while we do not explicitly model the time-varying deformation components (such as using per-frame embedding in [35,36]), neural networks could implicitly model such components by inferring time from the skeleton pose information."
        },
        "Relightable and animatable neural avatar from sparse-view video": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Chen Geng",
            "Linzhan Mou",
            "Zihan Yan",
            "Jiaming Sun",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_Relightable_and_Animatable_Neural_Avatar_from_Sparse-View_Video_CVPR_2024_paper.html",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2, 3, 7",
          "ref_ids": [
            "46"
          ],
          "3": "For example, Neural Body [46] represents a dynamic human model by combining SMPL model [39] with neural radiance field (NeRF) [41].",
          "4": "Following [46], the SSIM and LPIPS [68] metrics are computed in the bounding box of the human region, while the degree of normal and PSNR metrics are computed within the foreground mask.",
          "5": "Relighting4D [16] passes structured latent codes [46] to NeRFactor\u2019s MLPs, enabling it to relight a dynamic video of human performance."
        },
        "Generalizable human gaussians for sparse view synthesis": {
          "authors": [
            "Y Kwon",
            "B Fang",
            "Y Lu",
            "H Dong",
            "C Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73229-4_26",
          "ref_texts": "36. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 4",
          "ref_ids": [
            "36"
          ],
          "1": "While utilizing 3D human prior has proven its effectiveness in the human rendering task [11,25,35,36,43], representing the geometry gap between human template and the real geometry (e."
        },
        "Ohta: One-shot hand avatar via data-driven implicit priors": {
          "authors": [
            "Xiaozheng Zheng",
            "Chao Wen",
            "Zhuo Su",
            "Zeran Xu",
            "Zhaohu Li",
            "Yang Zhao",
            "Zhou Xue"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_OHTA_One-shot_Hand_Avatar_via_Data-driven_Implicit_Priors_CVPR_2024_paper.html",
          "ref_texts": "[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "56"
          ],
          "1": "Mesh information also aids implicit texture modeling [4, 9, 43, 56, 72, 74, 80].",
          "2": "Recent implicit human body [56, 80] and face [4, 15] modeling also rely on mesh information for guidance."
        },
        "Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos": {
          "authors": [
            "R Jena",
            "GS Iyer",
            "S Choudhary",
            "B Smith"
          ],
          "url": "https://arxiv.org/abs/2311.10812",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 4, 6, 7",
          "ref_ids": [
            "29"
          ],
          "2": "Neuralbody [29] uses structured latent codes based on the posed SMPL [22] mesh to produce a perframe NeRF.",
          "4": "For peoplesnapshot, we consider SMPLPix [30], NeuralBody [29], AnimNeRF [7] and SelfRecon [14] as state-of-the-art baselines."
        },
        "Relighting4d: Neural relightable human from videos": {
          "authors": [
            "Z Chen",
            "Z Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19781-9_35",
          "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. arXiv:2012.15838 [cs] (Mar 2021), http:// arxiv.org/abs/2012.15838 2, 3, 4, 5, 9, 10, 11, 12",
          "ref_ids": [
            "34"
          ],
          "3": "To model dynamic humans, Neural Body [34] proposes to attach a set of latent codes to a deformable human body model (i.",
          "5": "NeuralBody [34] employs a similar strategy on human representations.",
          "11": "Moreover, to demonstrate the importance of physically based rendering, we implement two variants on top of NeuralBody (NB) [34] which succeeds in novel view synthesis of dynamic humans but fails to incorporate lighting and reflectance.",
          "12": "Two variants of NeuralBody [34], NB+A and NB+LE, are good at reconstructing appearance but fail to incorporate novel illuminations in a perceptually salient way."
        },
        "D-miso: Editing dynamic 3d scenes using multi-gaussians soup": {
          "authors": [
            "J Waczynska",
            "P Borycki",
            "J Kaleta"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/c32319f4868da7613d78af9993100e42-Abstract-Conference.html",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "57"
          ],
          "1": "For instance, [57] introduces a human body representation where neural features across frames share a latent code anchored to a deformable mesh, facilitating cross-frame integration and efficient 3D representation learning in sparse monocular video scenarios."
        },
        "Differentiable physics simulation of dynamics-augmented neural objects": {
          "authors": [
            "S Le Cleac'h",
            "HX Yu",
            "M Guo",
            "T Howell"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10070844/",
          "ref_texts": "[19] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9054\u20139063, 2021.",
          "ref_ids": [
            "19"
          ],
          "1": "To allow modeling dynamics in addition to appearance, there have been various extensions to NeRF for deformable objects [11], [12], [13], [14], [15] and general motions [16], [17], [18], [19], [20]."
        },
        "A survey on big data technologies and their applications to the metaverse: past, current and future": {
          "authors": [
            "Haolan Zhang",
            "Sanghyuk Lee",
            "Yifan Lu",
            "Xin Yu",
            "Huanda Lu"
          ],
          "url": "https://www.mdpi.com/2227-7390/11/1/96",
          "ref_texts": "132. Peng, S.; Zhang, Y.; Xu, Y.; Wang, Q.; Shuai, Q.; Bao, H.; Zhou, X. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp. 9050\u20139059. [CrossRef]",
          "ref_ids": [
            "132"
          ],
          "1": "Meanwhile, similar techniques have also facilitated downstream tasks, such as clothed human reconstruction [129\u2013131], volume rendering [132], virtual try-on [133], the computer-assistant system [134] and many more Metaverse applications.",
          "2": "Meanwhile, similar techniques have also facilitated downstream tasks, such as clothed human reconstruction [129\u2013131], volume rendering [132], virtual try-on [133], the computer-assistant system [134] and many more Metaverse applications."
        },
        "Human101: Training 100+ fps human gaussians in 100s from 1 view": {
          "authors": [
            "M Li",
            "J Tao",
            "Z Yang",
            "Y Yang"
          ],
          "url": "https://arxiv.org/abs/2312.15258",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "44"
          ],
          "5": "ZJU-Mocap [44] is a prominent benchmark in human modeling from videos, supplying foreground human masks and SMPL parameters.",
          "6": "NeuralBody [44] utilizes structured SMPL data together with per-frame latent codes to optimize neural human radiance fields.",
          "7": "1 presents a comprehensive quantitative comparison between our method and other prominent techniques like InstantNvr [15], InstantAvatar [22], 3D GS [24], HumanNeRF [62], AnimSDF [45], NeuralBody [44], and AnimNeRF [43]."
        },
        "Uv volumes for real-time rendering of editable free-view human performance": {
          "authors": [
            "Yue Chen",
            "Xuan Wang",
            "Xingyu Chen",
            "Qi Zhang",
            "Xiaoyu Li",
            "Yu Guo",
            "Jue Wang",
            "Fei Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3, 5, 6, 8",
          "ref_ids": [
            "39"
          ],
          "2": "Particularly, DyNeRF [25] takes the latent code as the condition for time-varying scenes, while NeuralBody [39] employs structured latent codes anchored to a posed human model.",
          "5": "To validate our method, we compare it against several state-of-the-art free-view video synthesis techniques: 1) DN: DyNeRF [25], which takes time-varying latent codes as the conditions for dynamic scenes; and 2) NB: NeuralBody [39], which takes as input the posed human model with structured time-invariant latent codes and generates a pose-conditioned neural radiance field; 3) AN: Animatable-NeRF [38], which uses neural blend weight fields to generate correspondences between observation and canonical space."
        },
        "Instant-nvr: Instant neural volumetric rendering for human-object interactions from monocular rgbd stream": {
          "authors": [
            "Yuheng Jiang",
            "Kaixin Yao",
            "Zhuo Su",
            "Zhehao Shen",
            "Haimin Luo",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 5, 6, 7",
          "ref_ids": [
            "37"
          ],
          "1": "For dynamic human, in contrast to recent approaches [35, 36, 38, 50] which can\u2019t handle long sequences via pure MLP and human NeRFs [37, 55, 69] that heavily rely on SMPL [27] which do not align well with the surface and easily cause artifacts, we introduce a hybrid deformation module to efficiently leverage the motion priors.",
          "2": "Comparison We compare Instant-NVR against the fusion-based methods RobustFusion [44], NeuralHOFusion [17] and NeRF-based methods NeuralBody [37], HumanNerf [68], both in efficiency and rendering quality."
        },
        "Mirror-nerf: Learning neural radiance fields for mirrors with whitted-style ray tracing": {
          "authors": [
            "J Zeng",
            "C Bao",
            "R Chen",
            "Z Dong",
            "G Zhang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611857",
          "ref_texts": "[18] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "18"
          ],
          "1": "Several extensions and improvements have been proposed to apply NeRF to more challenging problems, such as scene reconstruction [1, 8, 13, 29, 30, 32, 36, 38, 39, 44, 48], generalization [24, 33], novel view extrapolation [35, 45], scene manipulation [2, 28, 40\u201342], SLAM [23, 54], segmentation [20, 53], human body [18, 31] and so on."
        },
        "Multiply: Reconstruction of multiple people from monocular video in the wild": {
          "authors": [
            "Zeren Jiang",
            "Chen Guo",
            "Manuel Kaufmann",
            "Tianjian Jiang",
            "Julien Valentin",
            "Otmar Hilliges",
            "Jie Song"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_MultiPly_Reconstruction_of_Multiple_People_from_Monocular_Video_in_the_CVPR_2024_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "33"
          ],
          "1": "Recent works employ neural rendering to train neural fields based on videos to obtain articulated human model [5, 6, 11, 17, 18, 20, 33, 37, 40]."
        },
        "Total-recon: Deformable scene reconstruction for embodied view synthesis": {
          "authors": [
            "Chonghyuk Song",
            "Gengshan Yang",
            "Kangle Deng",
            "Yan Zhu",
            "Deva Ramanan"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3",
          "ref_ids": [
            "39"
          ],
          "1": "One group of work leverages human-specific priors [38, 53, 32, 39, 24, 16, 19, 37] such as human body models (e."
        },
        "Rendering humans from object-occluded monocular videos": {
          "authors": [
            "Tiange Xiang",
            "Adam Sun",
            "Jiajun Wu",
            "Ehsan Adeli",
            "Li Fei"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Rendering_Humans_from_Object-Occluded_Monocular_Videos_ICCV_2023_paper.html",
          "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "51"
          ],
          "2": "Since the emergence of Neural Radiance Fields (NeRF) [43], different extensions have been recently developed to enable highquality rendering of static scenes [22, 57, 2, 3, 63, 61, 58, 44], moving objects [18, 36, 47, 48, 52, 46], and dynamic humans [51, 4, 7, 11, 9, 13, 14, 17, 20, 21, 26, 27, 35, 37, 45, 50, 62, 64, 68, 49, 28, 30]."
        },
        "Harp: Personalized hand reconstruction from a monocular rgb video": {
          "authors": [
            "Korrawe Karunratanakul",
            "Sergey Prokudin",
            "Otmar Hilliges",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.html",
          "ref_texts": "[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "56"
          ],
          "1": "Numerous methods have been proposed to learn and estimate appearance from images or videos for bodies with clothing [22,33,51,56,58,65] and faces [2, 15, 16, 19, 29, 53], including those that use NeRF [42] to implicitly represent appearance [53, 54]."
        },
        "Generalizable neural performer: Learning robust radiance fields for human novel view synthesis": {
          "authors": [
            "W Cheng",
            "S Xu",
            "J Piao",
            "C Qian",
            "W Wu",
            "KY Lin"
          ],
          "url": "https://arxiv.org/abs/2204.11798",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. arXiv preprint arXiv:2012.15838, 2020. 2, 3, 5, 7, 8, 9, 16, 17",
          "ref_ids": [
            "33"
          ],
          "1": "Recent works adopt neural networks to learn 3D geometry and appearance from data [21,33,36,54].",
          "3": "Some current cutting-edge approaches rely on temporal coherence of the same subject [21,33] which requires geometry fitting or motion tracking across canonical models.",
          "4": "Methods Generalizable Render Latent Prior Occlusion Supervision NeRF [28] \u0017 \u0013 2D pixelNerf [49] \u0013 \u0013 2D 2D IBRNet [42] \u0013 \u0013 2D 2D PIFu [36] \u0013 \u0017 2D 3D PaMIR [54] \u0013 \u0017 2D+3D SMPL (Depth) 3D NB [33] \u0017 \u0013 2D+3D SMPL (Vertices) 2D GNR \u0013 \u0013 2D+3D SMPL (SDF&Depth) \u0013 2D(3D \u2217) Table 1.",
          "10": "We evaluate two categories of baseline methods: (1) generalization methods, pixelNerf [49] and IBRNet [42]; (2) case-specific methods, NeuralBody (NB) [33], NeuralTexture (NT) [39], NHR [46] and NeuralV olumes (NV) [24].",
          "13": "We compare our methods with case-specific methods NV [24], NT [39], NHR [46] and NB [33] on this dataset.",
          "14": "Specifically, the first setting (denoted as \u2020in Table S2) is case-specific training but testing on unseen poses , where generalizable methods such as IBRNet [42] and ours are trained following same protocol as case-specific methods such as [24, 33, 39, 46].",
          "16": "Our method achieve comparable visual result on total unseen pose with optimized NB [33] model.",
          "17": "More Quantitative and Qualitative Results On V-Sense We compare our method to case-specific methods such as [24, 33, 46] on V-Sense dataset, and provide quantitative comparison in this subsection.",
          "19": "We compare our methods with case-specific methods NV [24], NHR [46], NB [33]."
        },
        "Neuraldome: A neural modeling pipeline on multi-view human-object interactions": {
          "authors": [
            "Juze Zhang",
            "Haimin Luo",
            "Hongdi Yang",
            "Xinru Xu",
            "Qianyang Wu",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 6, 7",
          "ref_ids": [
            "45"
          ],
          "1": "Most notably, the variants of Neural Radiance Field (NeRF) [37] achieve compelling novel view synthesis, which can enable realtime rendering performance [38, 59, 67] even for dynamic scenes [45, 63, 77], and can be extended to the generative setting without per-scene training [23, 69, 80].",
          "2": "Existing works equip NeRF with pose-embeddings [23, 27, 40, 45, 80], learnable skinning weights [25, 44, 70] and even generalization across individuals [23, 66, 80].",
          "3": ", ST-NeRF [77], NeuralBody(NB) [45], in human-object interaction scenario.",
          "4": "We show ground truth and synthesized images of novel view for NeuralBody [45], and ST-NeRF [77] and our layered human-object representation."
        },
        "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints": {
          "authors": [
            "M Mihajlovic",
            "A Bansal",
            "M Zollhoefer",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19784-0_11",
          "ref_texts": "45. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 8, 9, 13, 14, 3",
          "ref_ids": [
            "45"
          ],
          "2": "Recent approaches have incorporated priors specific to human faces [8, 9, 15, 17, 50, 61, 72] and human bodies [44,45,64,66,67,71,73,74] to reduce the dependence on multi-view captures.",
          "3": "For the experiments on human heads, we use a sphere with a radius of 30 centimeters centered around the keypoints, while for the human bodies we follow the prior work [29, 45] and use a 3D bounding box.",
          "4": "KeypointNeRF 9 6 Experiments In this section, we validate our method on three different reconstruction tasks and datasets: 1) reconstruction of human heads from images captured in a multicamera studio, 2) reconstruction of human heads from in-the-wild images taken with the iPhone\u2019s camera, and 3) reconstruction of human bodies on the public ZJU-MoCap dataset [45].",
          "5": "We use the public ZJU [45] dataset in order to follow the experimental setup used in [29], so that we could closely compare our method\u2019s ability to reconstruct human bodies to the current state-of-the-art method without changing any experimental variables.",
          "6": "Comparison of NHP [29] and our method on unseen identities from the ZJU-MoCap dataset [45].",
          "7": "We re-implemented PVA [48] since their code is not public and we directly used the public results of NHP [29] for the experiments on the ZJU-MoCap dataset [45]."
        },
        "Lightweight multi-person total motion capture using sparse multi-view cameras": {
          "authors": [
            "Yuxiang Zhang",
            "Zhe Li",
            "Liang An",
            "Mengcheng Li",
            "Tao Yu",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Lightweight_Multi-Person_Total_Motion_Capture_Using_Sparse_Multi-View_Cameras_ICCV_2021_paper.html",
          "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Total Motion Capture Total motion capture methods, which aim at markerless multi-scale human behaviour capture (including body motion, facial expressions and hand gestures), have shown great potentials in human 4D reconstruction and highfidelity neural rendering [42, 49, 28, 63]."
        },
        "Learning implicit templates for point-based clothed human modeling": {
          "authors": [
            "S Lin",
            "H Zhang",
            "Z Zheng",
            "R Shao",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_13",
          "ref_texts": "50. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "50"
          ],
          "1": "Recently, based on neural radiance fields (NeRF) [43], attempts have been made to bypass the underlying geometry and synthesize rendered images of clothed humans directly [49,50,62,67]."
        },
        "Reconstructing 3d human pose by watching humans in the mirror": {
          "authors": [
            "Qi Fang",
            "Qing Shuai",
            "Junting Dong",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.html",
          "ref_texts": "[44] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "Some other works use temporal constraints [24, 55, 44] or geometric self-consistency [8]."
        },
        "Point2pix: Photo-realistic point cloud rendering via neural radiance fields": {
          "authors": [
            "Tao Hu",
            "Xiaogang Xu",
            "Shu Liu",
            "Jiaya Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hu_Point2Pix_Photo-Realistic_Point_Cloud_Rendering_via_Neural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "35"
          ],
          "1": "For human reconstruction and synthesis, many methods, such as Neural-Body [35], Neural-Actor [25], and Anim-NeRF [6], introduce the parameterized human model SMPL [26] as a strong 3D prior and achieve impressive performance."
        },
        "Deep review and analysis of recent nerfs": {
          "authors": [
            "Fang Zhu",
            "Shuai Guo",
            "Li Song",
            "Ke Xu",
            "Jiayu Hu"
          ],
          "url": "https://www.nowpublishers.com/article/OpenAccessDownload/SIP-2022-0062",
          "ref_texts": "[66] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, 9054\u201363.",
          "ref_ids": [
            "66"
          ],
          "1": "In particular, implicit representation based on ML has become a hot topic of current research and has been widely discussed, such as in these works [48, 66, 73]."
        },
        "Caphy: Capturing physical properties for animatable human avatars": {
          "authors": [
            "Zhaoqi Su",
            "Liangxiao Hu",
            "Siyou Lin",
            "Hongwen Zhang",
            "Shengping Zhang",
            "Justus Thies",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_CaPhy_Capturing_Physical_Properties_for_Animatable_Human_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "32"
          ],
          "1": "Some methods use neural texture or neural voxel representations [37, 32] to perform neural rendering-based avatars from single or multiview images."
        },
        "Gaussiandiffusion: 3d gaussian splatting for denoising diffusion probabilistic models with structured noise": {
          "authors": [
            "X Li",
            "H Wang",
            "KK Tseng"
          ],
          "url": "https://arxiv.org/abs/2311.11221",
          "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "25"
          ],
          "1": "Thanks to its versatile representation, it has already sparked significant advancements in various directions within NeRF-based methodologies [1, 4, 12, 21, 22, 25, 27, 28, 41, 47, 48, 52]."
        },
        "PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling": {
          "authors": [
            "Xiaoyun Zheng",
            "Liwei Liao",
            "Xufeng Li",
            "Jianbo Jiao",
            "Rongjie Wang",
            "Feng Gao",
            "Shiqi Wang",
            "Ronggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_PKU-DyMVHumans_A_Multi-View_Video_Benchmark_for_High-Fidelity_Dynamic_Human_Modeling_CVPR_2024_paper.html",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 3",
          "ref_ids": [
            "30"
          ],
          "1": "This approach has also shown promising results in human modeling [18, 30, 52], although it still relies on relatively dense multiview videos as input.",
          "4": "In the field of human rendering, several approaches [21, 30, 43] utilize human priors to model human motions and achieve free-viewpoint rendering of dynamic human."
        },
        "Cla-nerf: Category-level articulated neural radiance field": {
          "authors": [
            "WC Tseng",
            "HJ Liao",
            "L Yen-Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9812272/",
          "ref_texts": "[17] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021.",
          "ref_ids": [
            "17"
          ],
          "1": "Leveraging the abundant prior knowledge of human bodies, efficient techniques [14], [15], [16], [17], [18], [19], [20], [21], [22], [23]1 have been developed to model the deformation of a wide variety of body shapes."
        },
        "Xagen: 3d expressive human avatars generation": {
          "authors": [
            "Z Xu",
            "J Zhang",
            "JH Liew",
            "J Feng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6d6f9908ea35313dd7566f5ce8c6e815-Abstract-Conference.html",
          "ref_texts": "[48] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "48"
          ],
          "1": "On the other hand, leveraging the remarkable advances in implicit neural representations [41, 42], another line of research has proposed to either rely purely on implicit representations [53] or combine it with statistical models [61, 48, 8] to reconstruct expressive 3D human bodies."
        },
        "Learning visibility field for detailed 3d human reconstruction and relighting": {
          "authors": [
            "Ruichen Zheng",
            "Peng Li",
            "Haoqian Wang",
            "Tao Yu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.html",
          "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "40"
          ],
          "1": "Human templates such as SMPL [29] can serve as effective guidance [2,40,56,64], but introduce additional template alignment errors and therefore cannot guarantee complete occlusion awareness."
        },
        "Nvfi: Neural velocity fields for 3d physics learning from dynamic videos": {
          "authors": [
            "J Li",
            "Z Song",
            "B Yang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html",
          "ref_texts": "[45] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. CVPR, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "In parallel, there are also a number of domain-specific NeRFs to model particular dynamic objects such as human bodies [44, 50, 45, 70, 68] and faces [4, 20, 66, 67]."
        },
        "Danbo: Disentangled articulated neural body representations via graph neural networks": {
          "authors": [
            "SY Su",
            "T Bagautdinov",
            "H Rhodin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_7",
          "ref_texts": "41. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "41"
          ],
          "1": "1 Introduction Animating real-life objects in the digital world is a long-pursued goal in computer vision and graphics, and recent advances already enable 3D free-viewpoint video, animation, and human performance retargeting [17,41,53].",
          "4": "4 Experiments In the following, we evaluate the improvements upon the most recent surface-free neural body model A-NeRF [46], and compare against recent model-based solutions NeuralBody [41] and Anim-NeRF [40]."
        },
        "High-fidelity facial avatar reconstruction from monocular video with generative priors": {
          "authors": [
            "Yunpeng Bai",
            "Yanbo Fan",
            "Xuan Wang",
            "Yong Zhang",
            "Jingxiang Sun",
            "Chun Yuan",
            "Ying Shan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "Recently, Neural Radiance Field (NeRF) [7, 8, 11, 12, 26\u201330, 36, 38, 44, 49, 51] obtains impressive performance for novel view synthesis of complex scenes."
        },
        "Learned vertex descent: A new direction for 3d human model fitting": {
          "authors": [
            "E Corona",
            "G Pons-Moll",
            "G Alenya"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_9",
          "ref_texts": "61. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "61"
          ],
          "1": "Recent works have already explored possible integrations between implicit and parametric representations for the tasks of 3D reconstruction [33,84], clothed human modeling [73,46,47], or human rendering [61]."
        },
        "Instruct 4d-to-4d: Editing 4d scenes as pseudo-3d scenes using 2d diffusion": {
          "authors": [
            "Linzhan Mou",
            "Kun Chen",
            "Xiong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Mou_Instruct_4D-to-4D_Editing_4D_Scenes_as_Pseudo-3D_Scenes_Using_2D_CVPR_2024_paper.html",
          "ref_texts": "[23] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "23"
          ],
          "1": "Additionally, NeuralBody [23] and [35, 38, 40] focus on acquiring precise dynamic human body motion information, building upon the SMPL [17] model."
        },
        "Deformable 3d gaussian splatting for animatable human avatars": {
          "authors": [
            "HJ Jung",
            "N Brasch",
            "J Song",
            "E Perez-Pellitero"
          ],
          "url": "https://arxiv.org/abs/2312.15059",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 3, 6, 7, 2",
          "ref_ids": [
            "47"
          ],
          "1": "While being free of ground truth mask for training, it generalizes well to novel human poses as shown in the above reposed results on the individuals from ZJU-MoCap [47] and THUman4.",
          "8": "Ours is limited on generating exact shading (ZJU [47] 313, 315), but produces more realistic and detailed humans in novel poses."
        },
        "Rana: Relightable articulated neural avatars": {
          "authors": [
            "Umar Iqbal",
            "Akin Caliskan",
            "Koki Nagano",
            "Sameh Khamis",
            "Pavlo Molchanov",
            "Jan Kautz"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 9",
          "ref_ids": [
            "46"
          ],
          "2": "Hence, the generated images may not be the true representation of the 23143 novel viewnovel posegeneralizablerelightableMethod \u2713 NeuralBody [46], SelfRecon [31] \u2713 \u2713 AnimatableNeRf [45, 14], NeuMan [32] \u2713 \u2713 \u2713 ANR [47], TNA [52], StylePeople [22] \u2713 \u2713 Relighting4D [16] \u2713 \u2713 \u2713 \u2713 RANA (Ours) Table 1.",
          "4": "Thanks to the design of RANA, we can pretrain on as many subjects as available, which is not possible with most of the state-of-the-art methods for human synthesis [45, 46, 58, 31]."
        },
        "Gait recognition using 3-d human body shape inference": {
          "authors": [
            "Haidong Zhu",
            "Zhaoheng Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Zhu_Gait_Recognition_Using_3-D_Human_Body_Shape_Inference_WACV_2023_paper.html",
          "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "28"
          ],
          "1": "With the introduction of NeRF [25], researchers also introduce Animatable NeRF [28] and Neural Body [28] for reconstructing the human body shape in the video sequence with SMPL priors."
        },
        "Learning personalized high quality volumetric head avatars from monocular rgb videos": {
          "authors": [
            "Ziqian Bai",
            "Feitong Tan",
            "Zeng Huang",
            "Kripasindhu Sarkar",
            "Danhang Tang",
            "Di Qiu",
            "Abhimitra Meka",
            "Ruofei Du",
            "Mingsong Dou",
            "Sergio Orts",
            "Rohit Pandey",
            "Ping Tan",
            "Thabo Beeler",
            "Sean Fanello",
            "Yinda Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Bai_Learning_Personalized_High_Quality_Volumetric_Head_Avatars_From_Monocular_RGB_CVPR_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. pages 9054\u20139063, 2021. 3, 4",
          "ref_ids": [
            "34"
          ],
          "1": "Sparse local feature embedding attached on geometry has been demonstrated to be effective in improving the rendering quality of neural radiance field [26\u201328, 34, 54].",
          "2": "Inspired by local feature based neural radiance field [34, 47], we attach feature vectors zj on each 3DMM vertexvj i to encode the local radiance fields that can be decoded with MLPs, where idenotes frame index and j denotes vertex index."
        },
        "ANIM: Accurate neural implicit model for human reconstruction from a single rgb-d image": {
          "authors": [
            "Marco Pesavento",
            "Yuanlu Xu",
            "Nikolaos Sarafianos",
            "Robert Maier",
            "Ziyan Wang",
            "Han Yao",
            "Marco Volino",
            "Edmond Boyer",
            "Adrian Hilton",
            "Tony Tung"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Pesavento_ANIM_Accurate_Neural_Implicit_Model_for_Human_Reconstruction_from_a_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InIEEE Conference on Computer V ision and P attern Recognition , 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "Instead of using random embedding like previous works [44], performance improves when features extracted from the input image are used (see Sec."
        },
        "PersonNeRF: Personalized reconstruction from photo collections": {
          "authors": [
            "Yi Weng",
            "Pratul P. Srinivasan",
            "Brian Curless",
            "Ira Kemelmacher"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Weng_PersonNeRF_Personalized_Reconstruction_From_Photo_Collections_CVPR_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "34"
          ],
          "1": "Methods have reconstructed neural field representations of humans from a variety of different inputs, including 3D scans [4, 23, 31, 35, 45], multi-view RGB observations [18, 21, 34], RGB-D sequences [8], or monocular videos [13, 46]."
        },
        "Chord: Category-level hand-held object reconstruction via shape deformation": {
          "authors": [
            "Kailin Li",
            "Lixin Yang",
            "Haoyu Zhen",
            "Zenan Lin",
            "Xinyu Zhan",
            "Licheng Zhong",
            "Jian Xu",
            "Kejian Wu",
            "Cewu Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_CHORD_Category-level_Hand-held_Object_Reconstruction_via_Shape_Deformation_ICCV_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Computer Vision and Pattern Recognition (CVPR), 2020. 4",
          "ref_ids": [
            "41"
          ],
          "1": "We draw inspiration from the Neural Body [41], which employs the structured latent codes anchored on the SMPL body vertices (inner surface) for reconstructing the clothed human body at the outer surface."
        },
        "Mvhumannet: A large-scale dataset of multi-view daily dressing human captures": {
          "authors": [
            "Zhangyang Xiong",
            "Chenghong Li",
            "Kenkun Liu",
            "Hongjie Liao",
            "Jianqiao Hu",
            "Junyi Zhu",
            "Shuliang Ning",
            "Lingteng Qiu",
            "Chongjie Wang",
            "Shijie Wang",
            "Shuguang Cui",
            "Xiaoguang Han"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xiong_MVHumanNet_A_Large-scale_Dataset_of_Multi-view_Daily_Dressing_Human_Captures_CVPR_2024_paper.html",
          "ref_texts": "[55] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 4",
          "ref_ids": [
            "55"
          ],
          "1": "Previous multi-view human datasets [31, 41, 55] involve only a few dozen human subjects.",
          "2": "Compared with the existing multi-view human datasets [12, 30, 31, 55], MVHumanNet provides a significantly larger number of human subjects and outfits than previously available.",
          "4": "These successes inspire subsequent works [20, 40, 55, 60] to extend reconstruction and generation tasks to high-fidelity clothed full-body humans.",
          "5": "With the recent progress of neural rendering techniques, NHR [71], ZJU-Mocap [55], Neural Actor [24, 25, 44] and THuman4."
        },
        "Geometry-guided progressive nerf for generalizable and efficient neural human rendering": {
          "authors": [
            "M Chen",
            "J Zhang",
            "X Xu",
            "L Liu",
            "Y Cai",
            "J Feng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20050-2_14",
          "ref_texts": "28. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 4, 7, 8, 9, 10",
          "ref_ids": [
            "28"
          ],
          "6": "NB [28] combines NeRF with a parametric human body model SMPL [20] to regularize the training process.",
          "7": "4 Geometry-guided Progressive Rendering We render the human body in the target view through the volumetric rendering following previous NeRF-based methods [22,28,12].",
          "15": "We also achieve competitive fitting performance on the training frames, even comparable to the per-scene optimization methods [37,39,28]."
        },
        "Deep learning for 3d human pose estimation and mesh recovery: A survey": {
          "authors": [
            "Y Liu",
            "C Qiu",
            "Z Zhang"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0925231224008208",
          "ref_texts": "[199] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, X. Zhou, Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "199"
          ],
          "2": "[199] developed a novel human body representation."
        },
        "High-fidelity human avatars from a single rgb camera": {
          "authors": [
            "Hao Zhao",
            "Jinsong Zhang",
            "Kun Lai",
            "Zerong Zheng",
            "Yingdi Xie",
            "Yebin Liu",
            "Kun Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_High-Fidelity_Human_Avatars_From_a_Single_RGB_Camera_CVPR_2022_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 3, 7, 8",
          "ref_ids": [
            "33"
          ],
          "1": "Neural Body [33] proposed a representation where the learned latent codes are anchored to a deformable mesh to provide the network with geometric guidance.",
          "3": "We compare our method with three state-of-the-art methods Neural Body [33], HF-NHMT [15] and StylePeople [12]."
        },
        "Watch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects": {
          "authors": [
            "Atsuhiro Noguchi",
            "Umar Iqbal",
            "Jonathan Tremblay",
            "Tatsuya Harada",
            "Orazio Gallo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2, 6, 7",
          "ref_ids": [
            "41"
          ],
          "4": "We use the ZJU-MoCap dataset [41] for our experiments.",
          "5": "Since the ZJU-MoCap dataset [41] has ground truth SMPL annotations, we can use this mapping to re-pose our model to target frames not observed in training, as shown in Figure 7."
        },
        "Gaussianhair: Hair modeling and rendering with light-aware gaussians": {
          "authors": [
            "H Luo",
            "M Ouyang",
            "Z Zhao",
            "S Jiang",
            "L Zhang"
          ],
          "url": "https://arxiv.org/abs/2402.10483",
          "ref_texts": "[60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 4",
          "ref_ids": [
            "60"
          ],
          "1": "Potentially, subsequent variants of NeRF [4, 10, 41, 49, 69] can be deployed to conduct re-lighting [5, 7, 82, 87, 91] or even handle dynamic scenes [8, 57, 60]."
        },
        "Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Jay Zhangjie",
            "Weijia Mao",
            "Yuchao Gu",
            "Rui Zhao",
            "Jussi Keppo",
            "Ying Shan",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_DynVideo-E_Harnessing_Dynamic_NeRF_for_Large-Scale_Motion-_and_View-Change_Human-Centric_CVPR_2024_paper.html",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "37"
          ],
          "1": "Another series of work focus on human modelling and leverage estimated human pose priors [37, 54] to reconstruct dynamic humans with complex motions."
        },
        "Novel-view synthesis and pose estimation for hand-object interaction from sparse views": {
          "authors": [
            "Wentian Qu",
            "Zhaopeng Cui",
            "Yinda Zhang",
            "Chenyu Meng",
            "Cuixia Ma",
            "Xiaoming Deng",
            "Hongan Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "39"
          ],
          "2": "Prior arts [9, 30, 6, 23, 39, 38, 35] use implicitly shape representation to reconstruct articulated human body shape.",
          "3": "In order to learn generative novel view synthesis, several methods [39, 38, 35, 45, 51, 64, 50, 8] integrate bone transformation [45] and linear blend skinning [38, 64, 50, 8] with neural radiance fields."
        },
        "Reloo: Reconstructing humans dressed in loose garments from monocular video in the wild": {
          "authors": [
            "C Guo",
            "T Jiang",
            "M Kaufmann",
            "C Zheng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72673-6_2",
          "ref_texts": "38. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "38"
          ],
          "1": "More recently, methods based on neural implicit functions have emerged as a promising remedy for the disadvantages of template-based methods [14,17,19,21,38,42,44,50,52,61\u201363].",
          "2": "Recent works employ neural rendering to fit neural fields to videos to obtain an articulated human model [9,14,19,21,22,29,37,38,40,48,50]."
        },
        "Seal-3d: Interactive pixel-level editing for neural radiance fields": {
          "authors": [
            "Xiangyu Wang",
            "Jingsen Zhu",
            "Qi Ye",
            "Yuchi Huo",
            "Yunlong Ran",
            "Zhihua Zhong",
            "Jiming Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_Seal-3D_Interactive_Pixel-Level_Editing_for_Neural_Radiance_Fields_ICCV_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "30"
          ],
          "1": "Typically, Neural radiance field (NeRF) [24] uses a single MLP to implicitly encode a scene into a volumetric field of density and color, and takes advantage of volume rendering to achieve impressive rendering results with view-dependent effects, which inspires a lot of follow-up works on human [30, 42], deformable objects [28, 29], pose estimations [18], autonomous system [32, 49], surface reconstruction [45, 41], indoor scenes [47], city [38, 43], etc."
        },
        "Diner: Depth-aware image-based neural radiance fields": {
          "authors": [
            "Malte Prinzler",
            "Otmar Hilliges",
            "Justus Thies"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Prinzler_DINER_Depth-Aware_Image-Based_NEural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021.",
          "ref_ids": [
            "32"
          ],
          "1": "Especially with the introduction of neural rendering and neural scene representations [43, 44], we see 3D digital humans that can be rendered under novel views while being controlled via face and body tracking [3, 11, 14, 15, 24, 32, 37, 47, 50, 51, 58]."
        },
        "Guess the unseen: Dynamic 3d scene reconstruction from partial 2d glimpses": {
          "authors": [
            "Inhee Lee",
            "Byungjun Kim",
            "Hanbyul Joo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lee_Guess_The_Unseen_Dynamic_3D_Scene_Reconstruction_from_Partial_2D_CVPR_2024_paper.html",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "36"
          ],
          "3": "It models each person, object, and background with a NeuralBody [36] and NeRF [31] respectively and renders them compositionally through ST-NeRF [59] pipeline.",
          "4": "Single Human Benchmarks We also conduct experiments on an existing single human benchmark ZJU-Mocap [36] dataset, to check the performance of our model on the single human reconstruction task with sufficient observations.",
          "5": "It represents each person with NeuralBody [36] and background with NeRF [31] and renders them together by using the compositional rendering pipeline of ST-NeRF [59].",
          "6": "Quantitative results on 6 subjects in ZJU-Mocap dataset [36].",
          "7": "Evaluations on Single Person Reconstruction We show the quantitative comparisons on ZJUMocap [36] dataset.",
          "8": "Ablation Studies with lower-body occluded ZJUMocap [36] 377 subject.",
          "9": "As a way for the quantitative evaluations, we use ZJU-Mocap [36] dataset and simulate a challenging scenario by (1) completely occluding the lower body, and (2) using a few frames only for the input (10% of frames from the ZJU-Mocap 377 subject).",
          "10": "We compared novel pose rendering speed between InstantAvatar [16] and ours with ZJUMocap [36] 377 subject."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "41"
          ],
          "1": "Recently, optimizing a network to represent a person-specific model shows impressive results [14, 46, 10, 41, 39].",
          "2": "Inspired by NeRF [33], Neural Body [41] optimizes the radiance field conditioned on the structured latent codes with only images as supervision.",
          "3": "Head Hands Total P2S\u2193 CD\u2193 P2S\u2193 CD\u2193 P2S\u2193 CD\u2193 NeuralBody [41] 1.",
          "4": "We also compare with NeuralBody [41].",
          "5": "Head Hands Total PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 NeuralBody [41] 18."
        },
        "Vidu4d: Single generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels": {
          "authors": [
            "Y Wang",
            "X Wang",
            "Z Chen",
            "Z Wang",
            "F Sun"
          ],
          "url": "https://arxiv.org/abs/2405.16822",
          "ref_texts": "[59] Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR",
          "ref_ids": [
            "59"
          ],
          "1": "The dynamic reconstruction of scenes from video captures presents a more complex challenge than static reconstruction, necessitating the capture of non-rigid motion and deformation over time [30, 37, 59, 75, 87]."
        },
        "Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation": {
          "authors": [
            "X Wang",
            "Y Wang",
            "J Ye",
            "F Sun",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_19",
          "ref_texts": "23. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 3",
          "ref_ids": [
            "23"
          ],
          "1": "Furthermore, exploring category-specific or articulate priors offers promising avenues for reconstructing non-rigid objects [6,23,40,49\u201351]."
        },
        "Interactive nerf geometry editing with shape priors": {
          "authors": [
            "YJ Yuan",
            "YT Sun",
            "YK Lai",
            "Y Ma",
            "R Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10252034/",
          "ref_texts": "[45] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "45"
          ],
          "1": "However, NeRF still has shortcomings and plenty of work has extended the original NeRF, including better synthesis effects [2], [43], [44], applicable to dynamic scenes [8], [14], [18], [19], [45], [46], [47], [48], [49], faster training and rendering speed [3], [4], [50], [51], [52], generalization to different scenes [53], [54], relighting [6], [7], [55], [56], and various kinds of editing [20], [57], [58], [59], [60], [61]."
        },
        "Humanliff: Layer-wise 3d human generation with diffusion model": {
          "authors": [
            "S Hu",
            "F Hong",
            "T Hu",
            "L Pan",
            "H Mei",
            "W Xiao"
          ],
          "url": "https://arxiv.org/abs/2308.09712",
          "ref_texts": "[50] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.",
          "ref_ids": [
            "50"
          ],
          "1": "For stable view synthesis, some recent papers [50], [51], [52], [53], [54], [55], [56] propose to unify geometry reconstruction with view synthesis by volume rendering [57], which, however, is generally computationally heavy."
        },
        "Gaussian shadow casting for neural characters": {
          "authors": [
            "Luis Bolanos",
            "Yang Su",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Bolanos_Gaussian_Shadow_Casting_for_Neural_Characters_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 6",
          "ref_ids": [
            "28"
          ],
          "1": "The most recent body models [17,20,28,36,37,40] which are based on neural radiance fields (NeRFs) [26], approximate the light transport by casting primary rays between the camera and the scene, sampling the underlying neural network dozens of times along each ray to obtain the density and color."
        },
        "Livehand: Real-time and photorealistic neural hand rendering": {
          "authors": [
            "Akshay Mundra",
            "Mallikarjun B",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 3",
          "ref_ids": [
            "30"
          ],
          "1": "Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9].",
          "2": "The literature contains works for modeling other animatable objects such as the human face [20, 4, 44, 8, 7, 22], human body [10, 41, 3, 18, 42, 30, 38], and animals [21].",
          "3": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Exocentric-to-egocentric video generation": {
          "authors": [
            "JW Liu",
            "W Mao",
            "Z Xu",
            "J Keppo"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/f5a8b5e5d007e66c929b971c2bc21d76-Abstract-Conference.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "41"
          ],
          "1": "While initially limited to reconstructing static 3D scenes, NeRF has been extended to modelling dynamic scenes [42, 38, 39, 50, 27, 12], dynamic humans [41, 53, 22, 32]."
        },
        "3d-aware semantic-guided generative model for human synthesis": {
          "authors": [
            "J Zhang",
            "E Sangineto",
            "H Tang",
            "A Siarohin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19784-0_20",
          "ref_texts": "61. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2",
          "ref_ids": [
            "61"
          ],
          "1": "An important class of implicit 3D representations are the Neural Radiance Fields (NeRFs), which can generate high-quality unseen views of complex scenes [50, 27, 12, 61, 60, 62, 7]."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_1",
          "ref_texts": "60. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proc. of CVPR (2021) 2, 3, 4, 9, 10, 13, 21, 25, 27",
          "ref_ids": [
            "60"
          ],
          "7": "Neural Body [60] proposes to diffuse latent per-vertex codes associated with SMPL meshes in observation space and condition NeRF [49] on such latent codes.",
          "10": "Datasets: We use the ZJU-MoCap [60] dataset as our primary testbed because its setup includes 23 cameras which allows us to extract pseudo-ground-truth geometry to evaluate our model.",
          "11": "We use the training/testing splits from Neural Body [60] for both the cameras and the poses.",
          "13": "Baselines: We compare against three major baselines: Neural Body [60](NB), Ani-NeRF [58](AniN), and A-NeRF [72](AN).",
          "14": "We report LPIPS [91] on synthesized images under unseen poses from the testset of the ZJU-MoCap dataset [60] (i.",
          "15": "We report L2 Chamfer Distance (CD) and Normal Consistency (NC) on the training poses of the ZJUMoCap dataset [60].",
          "16": "We report PSNR, SSIM, and LPIPS [91] for novel views of training poses of the ZJU-MoCap dataset [60].",
          "18": "We use the resulting model as the initialization for our per-subject optimization on the ZJU-MoCap [60] dataset.",
          "19": "For inference, we follow [58, 60] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
          "20": "For unseen test poses we follow the practice of [58, 60] and use the latent code Z of the last training frame as the input.",
          "21": "Neural Body [60], Ani-NeRF [58], and A-NeRF [72].",
          "23": "G Additional Quantitative Results We present complete evaluation metrics including PSNR, SSIM, LPIPS on the test poses of the ZJU-MoCap [60] dataset in Table G."
        },
        "Learning 3d-aware image synthesis with unknown pose distribution": {
          "authors": [
            "Zifan Shi",
            "Yujun Shen",
            "Yinghao Xu",
            "Sida Peng",
            "Yiyi Liao",
            "Sheng Guo",
            "Qifeng Chen",
            "Yan Yeung"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Shi_Learning_3D-Aware_Image_Synthesis_With_Unknown_Pose_Distribution_CVPR_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9054\u20139063, 2021. 1",
          "ref_ids": [
            "25"
          ],
          "1": "Compared with 2D synthesis, 3D-aware image synthesis requires the understanding of the geometry underlying 2D images, which is commonly achieved by incorporating 3D representations, such as neural radiance fields (NeRF) [2, 16, 17, 24, 25, 50], into generative models like generative adversarial networks (GANs) [8]."
        },
        "En3d: An enhanced generative model for sculpting 3d humans from 2d synthetic data": {
          "authors": [
            "Yifang Men",
            "Biwen Lei",
            "Yuan Yao",
            "Miaomiao Cui",
            "Zhouhui Lian",
            "Xuansong Xie"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Men_En3D_An_Enhanced_Generative_Model_for_Sculpting_3D_Humans_from_CVPR_2024_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2",
          "ref_ids": [
            "41"
          ],
          "1": "Realistic human modeling is an essential task, and many valuable efforts have been made by leveraging neural implicit fields to learn high-quality articulated avatars [8, 10, 41, 48].",
          "2": "With the explosion of NeRF, valuable efforts have been made towards combining NeRF models with explicit human models [8, 10, 28, 41, 48].",
          "3": "Neural body [41] anchors a set of latent codes to the vertices of the SMPL model [29] and transforms the spatial locations of the codes to the volume in the observation space."
        },
        "Uv gaussians: Joint learning of mesh deformation and gaussian textures for human avatar modeling": {
          "authors": [
            "Y Jiang",
            "Q Liao",
            "X Li",
            "L Ma",
            "Q Zhang",
            "C Zhang"
          ],
          "url": "https://arxiv.org/abs/2403.11589",
          "ref_texts": "40. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 2, 4, 7, 10, 12",
          "ref_ids": [
            "40"
          ],
          "3": "Amongtheseworks, Neural Body [40] introduces an implicit neural representation that is conditioned with structured latent codes anchored to the mesh vertices.",
          "5": "Subject S1 S2 S3 S4 S5Metric PSNR \u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 NeuralBody [40]26.",
          "6": "Subject S1 S2 S3 S4 S5Metric PSNR \u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 Neural Body [40]23.",
          "7": "2 Comparison We compare our algorithm with several baselines: Neural Body [40], AnimNeRF [39], UV Volumes [9], and a recent state-of-the-art human avatar methods using GS with source code available, 3DGS-Avatar [41]."
        },
        "Neural capture of animatable 3d human from monocular video": {
          "authors": [
            "G Te",
            "X Li",
            "X Li",
            "J Wang",
            "W Hu",
            "Y Lu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_16",
          "ref_texts": "25. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 5, 9, 12",
          "ref_ids": [
            "25"
          ],
          "5": "We also refer to the supplemental material for a comparsion to NeuralBody [25], the precursor method of AniNeRF."
        },
        "Recovery of continuous 3D refractive index maps from discrete intensity-only measurements using neural fields": {
          "authors": [
            "R Liu",
            "Y Sun",
            "J Zhu",
            "L Tian",
            "US Kamilov"
          ],
          "url": "https://www.nature.com/articles/s42256-022-00530-3",
          "ref_texts": "[38] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), DOI:10.1109/CVPR46437.2021.00894, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Prior applications of NF include novel view synthesis [28, 31, 60], dynamic scene representation [24,35,38], object lightning [45,57], and computed tomography [40]."
        },
        "Veri3d: generative vertex-based radiance fields for 3d controllable human image synthesis": {
          "authors": [
            "Xinya Chen",
            "Jiaxin Huang",
            "Yanrui Bin",
            "Lu Yu",
            "Yiyi Liao"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Chen_VeRi3D_Generative_Vertex-based_Radiance_Fields_for_3D_Controllable_Human_Image_ICCV_2023_paper.html",
          "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 3",
          "ref_ids": [
            "49"
          ],
          "1": "There are many attempts to leverage implicit neural representations for human reconstruction [49, 48, 59, 69, 7, 61, 35, 30, 27, 41].",
          "2": "Among these methods, earlier methods [49, 35] model humans in observation space, increasing the variation to be learned."
        },
        "MetaCap: Meta-learning Priors from Multi-view Imagery for Sparse-View Human Performance Capture and Rendering": {
          "authors": [
            "G Sun",
            "R Dabral",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72952-2_20",
          "ref_texts": "61. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "61"
          ],
          "1": "Several volumetric rendering-based methods focus on learning the human geometry and appearance in the canonical space [29,54,59,61,63,69,80,81], which learns shared features across different poses."
        },
        "Relightablehands: Efficient neural relighting of articulated hand models": {
          "authors": [
            "Shun Iwase",
            "Shunsuke Saito",
            "Tomas Simon",
            "Stephen Lombardi",
            "Timur Bagautdinov",
            "Rohan Joshi",
            "Fabian Prada",
            "Takaaki Shiratori",
            "Yaser Sheikh",
            "Jason Saragih"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "44"
          ],
          "1": "Neural rendering approaches based on volumetric representation have been extended to articulation modeling, compensating for inaccurate geometry by using view-dependent appearance [40, 44]."
        },
        "Hvtr: Hybrid volumetric-textural rendering for human avatars": {
          "authors": [
            "T Hu",
            "T Yu",
            "Z Zheng",
            "H Zhang",
            "Y Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044408/",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "57"
          ],
          "1": "Recent neural rendering methods [29, 34, 57, 61, 72, 80, 86] have made great progress in generating realistic images of humans, which are simple yet effective compared with traditional graphics pipelines [2, 4, 82].",
          "3": "Inspired by the recent neural scene representations [29, 44, 53, 57, 60], we model articulated humans with an implicit volumetric representation by constructing a dynamic pose-conditioned neural radiance field.",
          "7": "For stable view synthesis, recent papers [7, 29, 48, 56, 57, 71, 83] propose to unify geometry reconstruction with view 2 Figure 1: We illustrate the differences between (left) GAN-based methods (DNR), (middle) our hybrid approach, and (right) NeRF methods (Neural Body [57]).",
          "11": "We compare our method with GAN-based methods (DNR[73], SMPLpix[58], ANR[61]), and V olume Rendering method Neural Body [57].",
          "13": ", [58, 61, 73]) and become more obvious for dynamic humans due to the uncertainties of 7 Figure 7: Comparisons with GAN-based methods (DNR[73], SMPLpix [58], ANR [61]), and Neural Body [57] on R2-4, Z1, and Z3.",
          "14": "We evaluate the results of image synthesis on novel views while the test poses are the same as training poses and follow the same protocol as Neural Body [57] on Z1 and Z3 sequences from ZJU MoCap [57].",
          "15": "Comparisons against SMPLpix [58] and Neural Body [57] on shape editing are shown in Fig.",
          "17": "FP is based on Pix2PixHD [78] architecture with Encoder blocks of [Conv2d, BatchNorm, ReLU], ResNet [17] blocks, and 11 Z1 (this example) LPIPS \u2193 FID \u2193 SSIM\u2191 PSNR\u2191 Neural Body [57] .",
          "18": "75 Z1 (mean) LPIPS \u2193 FID \u2193 SSIM\u2191 PSNR\u2191 Neural Body [57] .",
          "19": "16 Z3 (this example) LPIPS FID SSIM PSNR Neural Body [57] .",
          "20": "09 Z3 (mean) LPIPS FID SSIM PSNR Neural Body [57] ."
        },
        "NeTO: neural reconstruction of transparent objects with self-occlusion aware refraction-tracing": {
          "authors": [
            "Zongcheng Li",
            "Xiaoxiao Long",
            "Yusen Wang",
            "Tuo Cao",
            "Wenping Wang",
            "Fei Luo",
            "Chunxia Xiao"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_NeTONeural_Reconstruction_of_Transparent_Objects_with_Self-Occlusion_Aware_Refraction-Tracing_ICCV_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "33"
          ],
          "1": "Recently, implicit neural representations have been applied to a variety of applications, including novel view synthesis [25, 58], camera pose estimation [18, 49], human [19, 33] and multi-view 3D reconstruction [11, 20, 21, 28, 29, 43, 47, 48, 55, 56], and achieved impressive successes."
        },
        "Modiff: Action-conditioned 3d motion generation with denoising diffusion probabilistic models": {
          "authors": [
            "M Zhao",
            "M Liu",
            "B Ren",
            "S Dai",
            "N Sebe"
          ],
          "url": "https://arxiv.org/abs/2301.03949",
          "ref_texts": "[23] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "23"
          ],
          "2": "[23] presented an implicit neural representation for dynamic humans for the new-view RGB video synthesis and 3D human model reconstruction."
        },
        "Entity-nerf: Detecting and removing moving entities in urban scenes": {
          "authors": [
            "Takashi Otonari",
            "Satoshi Ikehata",
            "Kiyoharu Aizawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Otonari_Entity-NeRF_Detecting_and_Removing_Moving_Entities_in_Urban_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "26"
          ],
          "1": "To address this issue, subsequent research has proposed methods that either explicitly learn scene dynamics by category-specific methods [6, 8, 14, 15, 25, 26, 30, 34, 37, 46, 50, 57], detection [10, 22], deformation [18, 23, 24, 27, 41, 44, 48, 54, 56], flow [4, 5, 7, 12, 55], multiple synchronized videos [11, 47, 58], depth-based approaches [52], or treat moving objects as outliers in a robust approach [33]."
        },
        "Occfusion: Rendering occluded humans with generative diffusion priors": {
          "authors": [
            "A Sun",
            "T Xiang",
            "S Delp",
            "FF Li"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/a7bfdee9544cea324cf183ac03c7d5c0-Abstract-Conference.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "Methods ZJU-MoCap [44] OcMotion [15] PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2217\u2191 SSIM\u2217\u2191 LPIPS\u2217\u2193 HumanNeRF [57] 20."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "30. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 9054\u20139063. Computer Vision Foundation / IEEE (2021).https://doi.org/10.1109/CVPR46437.",
          "ref_ids": [
            "30"
          ],
          "1": "existing datasets mitigate the issue of appearance variations caused by dynamic contexts by reducing the magnitude of motion (DNA-Rendering Dataset [7]) or wearing tight-fitting garments (ZJU-Mocap Dataset [30], ENeRF-Outdoor Dataset [21] and NHR Dataset [41]).",
          "2": "ZJU-MoCap [30], PeopleSnapshot [3], only focus on tightly-clothed figures with slow and gradual motion."
        },
        "Ghnerf: Learning generalizable human features with efficient neural radiance fields": {
          "authors": [
            "Arnab Dey",
            "Di Yang",
            "Rohith Agaram",
            "Antitza Dantcheva",
            "Andrew I. Comport",
            "Srinath Sridhar",
            "Jean Martinet"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NRI/html/Dey_GHNeRF_Learning_Generalizable_Human_Features_with_Efficient_Neural_Radiance_Fields_CVPRW_2024_paper.html",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 5",
          "ref_ids": [
            "32"
          ],
          "1": "Experimental Setting Datasets: We trained our model to be applicable to various types of human image using two different datasets, namely ZJU MoCap [32] and RenderPeople [14]."
        },
        "Neural reconstruction of relightable human model from monocular video": {
          "authors": [
            "Wenzhang Sun",
            "Yunlong Che",
            "Han Huang",
            "Yandong Guo"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Sun_Neural_Reconstruction_of_Relightable_Human_Model_from_Monocular_Video_ICCV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 7",
          "ref_ids": [
            "34"
          ],
          "1": "Datasets: We validate our method on the ZJU-Mocap dataset [34] qualitatively."
        },
        "Clothed human performance capture with a double-layer neural radiance fields": {
          "authors": [
            "Kangkan Wang",
            "Guofeng Zhang",
            "Suxu Cong",
            "Jian Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021. 2, 3, 4",
          "ref_ids": [
            "25"
          ],
          "1": "Clothed humans can be reconstructed through implicit representation-based methods such as voxel representation [33, 42], implicit function [27, 28], or neural radiance fields (NeRFs) [20, 24, 25].",
          "2": "Neuralbody [25] and AniNeRF [24] extend NeRFs for a dynamic human with deformation.",
          "3": "Double-layer NeRFs for Dynamic Humans Unlike a single NeRFs for clothed humans [17,24,25,34, 41], we represent the clothing with an independent NeRFs on the body, which forms a double-layer NeRFs.",
          "4": "The color network Fc in canonical frame is formulated as, ci(x)= Fc(\u03b3x(x),\u03d5i), (3) where \u03d5i is an appearance latent code [24, 25] for framei."
        },
        "Neural radiance fields from sparse rgb-d images for high-quality view synthesis": {
          "authors": [
            "YJ Yuan",
            "YK Lai",
            "YH Huang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9999509/",
          "ref_texts": "[61] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "61"
          ],
          "1": "In addition to the above extensions, NeRF has been extended for dynamic scenes [37], [38], better rendering effects [39], [40], generalization on multiple scenes [41], [42], [43], [44], [45], faster training or inference speed [46], [47], [48], [49], [50], [51], [52], re-lighting rendering [53], [54], [55], geometry or appearance editing [56], [57], [58], [59], [60] and specifically for processing human bodies [61], [62], [63] and faces [64], [65]."
        },
        "Joint2human: High-quality 3d human generation via compact spherical embedding of 3d joints": {
          "authors": [
            "Muxin Zhang",
            "Qiao Feng",
            "Zhuo Su",
            "Chao Wen",
            "Zhou Xue",
            "Kun Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Joint2Human_High-Quality_3D_Human_Generation_via_Compact_Spherical_Embedding_of_CVPR_2024_paper.html",
          "ref_texts": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2",
          "ref_ids": [
            "38"
          ],
          "1": "3D Human Generation with 2D Generators Many approaches [12, 19, 20, 22, 35, 46] try to learn the 3D shape from 2D images via various NeRF representations [25, 28, 31, 33, 38, 39] and differentiable volume rendering [32, 45, 51]."
        },
        "Morphable diffusion: 3D-consistent diffusion for single-image avatar creation": {
          "authors": [
            "Xiyi Chen",
            "Marko Mihajlovic",
            "Shaofei Wang",
            "Sergey Prokudin",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chen_Morphable_Diffusion_3D-Consistent_Diffusion_for_Single-image_Avatar_Creation_CVPR_2024_paper.html",
          "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 4",
          "ref_ids": [
            "57"
          ],
          "1": "Recently, the task of generating photorealistic avatars [57] gained significant attention from the research community due to its potential to revolutionize our ways of digital communication.",
          "2": "Several recent works have successfully combined neural rendering with articulated human models [46, 57, 86] to enable high-quality photorealistic avatar reconstruction.",
          "3": "The output vertex features VF \u2208 Rnv\u00d7d are then processed via a sparse 3D ConvNet [19, 57] f\u03b8 and trilinearly interpolated to create a 3D morphable-model-aware feature volume: FV = f\u03b8(VF ) \u2208 Rx\u00d7y\u00d7z\u00d7fV , (8) where x, y, z, and fV denote the dimensionality and number of channels of the interpolated 3DMM-aware feature volume FV ."
        },
        "TexVocab: texture vocabulary-conditioned human avatars": {
          "authors": [
            "Yuxiao Liu",
            "Zhe Li",
            "Yebin Liu",
            "Haoqian Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_TexVocab_Texture_Vocabulary-conditioned_Human_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "45"
          ],
          "1": "AniNeRF [44], ARAH [64], NeuralBody [45] TotalSelfScan [8] and PoseV ocab [26] auto-decode [40] latent embeddings to encode the dynamic appearances with perframe latent code or joint-structured feature lines."
        },
        "Neural radiance fields: Past, present, and future": {
          "authors": [
            "A Mittal"
          ],
          "url": "https://arxiv.org/abs/2304.10050",
          "ref_texts": "172. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "172"
          ],
          "1": "\u2022 Neural Body [172] paper proposes a method for representing and synthesizing novel views of dynamic humans using structured latent codes and implicit neural representations."
        },
        "ihuman: Instant animatable digital humans from monocular videos": {
          "authors": [
            "P Paudel",
            "A Khanal",
            "DP Paudel",
            "J Tandukar"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73226-3_18",
          "ref_texts": "58. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 9054\u20139063. Computer Vision Foundation / IEEE (2021).https://doi.org/10.1109/CVPR46437.",
          "ref_ids": [
            "58"
          ],
          "1": "In contrast, the existing methods Anim-NeRF [58] and GART [33] provide lower quality mesh and rendered images, even after using more training time and compute.",
          "2": "NeRF [45]), several methods [8\u201310,13,14,16,18,18, 21\u201323,32,32,37,40,42,48,57,58,61, 64,65,69,71,73,75,76,79,82,84,87], have been developed to capture highfidelity humans from multiple frames of videos.",
          "3": "Anim-NeRF [8] and other similar methods [9,10,16,18,19,21, 23,37,40,56,58,69,71,73] extend NERF to dynamic scenes by using SMPL [43] guided deformations between the observed space and a static canonical space allowing for explicit control."
        },
        "Slimmerf: Slimmable radiance fields": {
          "authors": [
            "S Yuan",
            "H Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550817/",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "46"
          ],
          "1": "Scene Representation with Radiance Fields Recently, models based on Neural Radiance Fields (NeRF) [38] have become popular in areas such as generative modelling [2, 3, 13, 29, 33\u201335, 41, 47, 48, 53, 61], pose estimation [8, 21, 30, 52, 57, 60, 64, 73], human body modelling [9,22,28,45,46,54,69,72], mobile 3D rendering [6,12, 25, 42, 49, 62, 63], and so on."
        },
        "Munerf: Robust makeup transfer in neural radiance fields": {
          "authors": [
            "YJ Yuan",
            "X Han",
            "Y He",
            "FL Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10443587/",
          "ref_texts": "[36] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "36"
          ],
          "1": "Incorporated with the prior models [34], [35], dynamic human face and body modeled by NeRF [6], [7], [36], [37] have been fully studied."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
          "authors": [
            "X Zhou",
            "S Peng",
            "Z Xu",
            "J Dong",
            "Q Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10401886/",
          "ref_texts": "[17] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "17"
          ],
          "4": "[17] combines neural radiance field with the SMPL model, allowing it to handle dynamic humans and synthesize photorealistic novel views from very sparse camera views.",
          "10": "[17] anchors a set of latent codes to the SMPL model and regresses a neural radiance field from these latent codes.",
          "28": "2) Neural body [17] anchors a set of latent codes on the vertices of SMPL and uses a network to regress neural radiance fields from the latent codes, which are then rendered into images using volume rendering."
        },
        "Fvor: Robust joint shape and pose optimization for few-view object reconstruction": {
          "authors": [
            "Zhenpei Yang",
            "Zhile Ren",
            "Miguel Angel",
            "Zaiwei Zhang",
            "Qi Shan",
            "Qixing Huang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yang_FvOR_Robust_Joint_Shape_and_Pose_Optimization_for_Few-View_Object_CVPR_2022_paper.html",
          "ref_texts": "[43] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian W ang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.Proceedings of the IEEE Conference on Computer V ision and P attern Recognition (CVPR), 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Many of these approach assume ground truth camera poses as input [2, 29, 41, 43, 57, 64, 65, 68]."
        },
        "SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image": {
          "authors": [
            "Yunhao Li",
            "Xiaodong Wang",
            "Ping Wang",
            "Xin Yuan",
            "Peidong Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_SCINeRF_Neural_Radiance_Fields_from_a_Snapshot_Compressive_Image_CVPR_2024_paper.html",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "29"
          ],
          "1": "Others focus on developing NeRF which can deal with non-rigid object reconstruction [1, 9, 28, 29]."
        },
        "Smpler: Taming transformers for monocular 3d human shape and pose estimation": {
          "authors": [
            "X Xu",
            "L Liu",
            "S Yan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10354384/",
          "ref_texts": "[23] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021. 1",
          "ref_ids": [
            "23"
          ],
          "1": "It has been widely used in many applications including visual tracking [8], [9], virtual/augmented reality [10], [11], [12], [13], [14], [15], [16], [17], [18], motion generation [19], [20], image manipulation [21], [22], and neural radiance field [23], [24], [25]."
        },
        "Flexnerf: Photorealistic free-viewpoint rendering of moving humans from sparse views": {
          "authors": [
            "Vinoj Jayasundara",
            "Amit Agrawal",
            "Nicolas Heron",
            "Abhinav Shrivastava",
            "Larry S. Davis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.html",
          "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "28"
          ],
          "1": "Hence, most methods [27, 28, 41] begin with assuming SMPL template as a prior [18].",
          "2": "This helps to eliminate the halo effects [28,40] and provide sharper boundaries.",
          "3": "9003 Full Neural Body [28] 57.",
          "4": "9043 ZJU-MoCap [7, 28] HumanNeRF [40] 36.",
          "5": "9685 Full Neural Body [28] 52.",
          "6": "9765 Neural Body [28] 48.",
          "7": "Benchmark Datasets and Metrics We evaluate the proposed method on two public datasets: ZJU-MoCap [7, 28] and People Snapshot [2], and one SelfCaptured Fashion (SCF) dataset.",
          "8": "Results and Analysis Quantitative Results: Table 1 compares our method against HumanNeRF [40] and Neural-Body [28] across the three datasets."
        },
        "Motion-oriented compositional neural radiance fields for monocular dynamic human modeling": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72933-1_27",
          "ref_texts": "52. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "52"
          ],
          "1": "We present our extensive results on ZJU-MoCap [52] and MonoCap [17,18,50] to demonstrate the effectiveness of the proposed MoCo-NeRF in learning photo-realistic representation and modeling complex non-rigid motions from monocular videos.",
          "2": "In recent days, there have been many works learning an implicit human representation for novel pose synthesis [22,23, 25,27,34,50,56,64,71,87\u201389] or for a novel-view synthesis [27,28,31,37,51,52, 64,73,74,78,85,86].",
          "3": "2 as: Lo = \u03bbLr + (1\u2212 \u03bb)Lf (10) Lr = Lr LPIPS + Lr MSE, Lf = Lf LPIPS + Lf MSE + Lf SSIM (11)",
          "4": "4 Experiments We conduct extensive experiments on ZJU-MoCap [52] and MonoCap [17,18,50] to verify the effectiveness of our proposed approach on both singleand multisubject settings.",
          "5": "Single-subject ZJU-MoCap [52] MonoCap [50] methods PSNR\u2191 SSIM\u2191 LPIPS\u2217\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2217\u2193 HumanNeRF [73] 30."
        },
        "Neural free-viewpoint performance rendering under complex human-object interactions": {
          "authors": [
            "G Sun",
            "X Chen",
            "Y Chen",
            "A Pang",
            "P Lin"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3474085.3475442",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR.",
          "ref_ids": [
            "47"
          ],
          "1": "For photorealistic human performance rendering, various data representation have been explored, such as point-clouds [42, 64], voxels [30], implicit representations [36, 47, 48, 62] or hybrid neural texturing [57].",
          "2": "More recently, [26, 44, 47, 48, 61, 65, 74] extend neural radiance field (NeRF) [36] into the dynamic setting.",
          "3": "Recent approaches [47] and [57] adopt a sparse set of camera views to synthesize photo-realistic novel views of a performer."
        },
        "Dual-space nerf: Learning animatable avatars and scene lighting in separate spaces": {
          "authors": [
            "Y Zhi",
            "S Qian",
            "X Yan",
            "S Gao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044388/",
          "ref_texts": "[20] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 5, 6, 7, 11, 12",
          "ref_ids": [
            "20"
          ],
          "2": "Neural Body [20] binds features onto SMPL\u2019s vertices and diffuses them into the space before volumetric rendering.",
          "18": "4, where Neural Body [20] exhibits superiority on PSNR and SSIM, and achieves comparable LPIPS to our method.",
          "21": "Our method outperforms AniNeRF [19] and is comparable to Neural Body [20] on the perceptual metric.",
          "25": "The results of Neural Body [20] and our method exhibit fewer artifacts compared with AniNeRF [19]."
        },
        "Mononhr: Monocular neural human renderer": {
          "authors": [
            "H Choi",
            "G Moon",
            "M Armando",
            "V Leroy"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044444/",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 4, 5, 6, 11, 12, 13, 14",
          "ref_ids": [
            "36"
          ],
          "13": "The Figure 3 and Table 2 show that MonoNHR produces the best results on both ZJUMoCap [36] and AIST [23, 48].",
          "19": "The initial learning rate is set to5\u00d710\u22124 and decays exponentially to 5\u00d710\u22125 following Neural Body [36]."
        },
        "Generalizable neural human renderer": {
          "authors": [
            "M Masuda",
            "J Park",
            "S Iwase",
            "R Khirodkar"
          ],
          "url": "https://arxiv.org/abs/2404.14199",
          "ref_texts": "75. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR. pp. 9054\u20139063 (2021) 2, 3, 7, 9, 10",
          "ref_ids": [
            "75"
          ],
          "3": "[75] used learned structured latent codes embodied for reposed mesh vertices from the SMPL model [60] and introduced a NeRF-based neural renderer."
        },
        "Ghunerf: Generalizable human nerf from a monocular video": {
          "authors": [
            "C Li",
            "J Lin",
            "GH Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550750/",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "30"
          ],
          "2": "We demonstrate the effectiveness of our approach on the widely used ZJU-MoCap dataset [30], where we achieve comparable performance with existing multi-view video based approaches.",
          "4": "NeuralBody [30] attaches a set of latent codes to the vertices of the SMPL model, which is able to aggregate information across different video frames.",
          "9": "Moreover, we also achieve comparable results with the optimization based approach Neuralbody [30]."
        },
        "MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors": {
          "authors": [
            "He Zhang",
            "Shenghao Ren",
            "Haolei Yuan",
            "Jianhui Zhao",
            "Fan Li",
            "Shuangpeng Sun",
            "Zhenghao Liang",
            "Tao Yu",
            "Qiu Shen",
            "Xun Cao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_MMVP_A_Multimodal_MoCap_Dataset_with_Vision_and_Pressure_Sensors_CVPR_2024_paper.html",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Multimodal MoCap Datasets The Human datasets [1, 3, 19, 33, 39, 55] have greatly promoted the development of pose detection."
        },
        "Panoptic compositional feature field for editable scene rendering with network-inferred labels via metric learning": {
          "authors": [
            "Xinhua Cheng",
            "Yanmin Wu",
            "Mengxi Jia",
            "Qian Wang",
            "Jian Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.html",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "35"
          ],
          "1": "The following works extend NeRF to address its limitations in various aspects, including efficient rendering [2, 9, 25, 38, 41, 48], better generalization [3, 16, 44, 49], dynamic synthesis [10,23,35,36,45], appearance and shape editing [17, 26, 42], and scene stylization [7, 11, 15, 51]."
        },
        "3dfaceshop: Explicitly controllable 3d-aware portrait generation": {
          "authors": [
            "J Tang",
            "B Zhang",
            "B Yang",
            "T Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10287169/",
          "ref_texts": "[98] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063. 3",
          "ref_ids": [
            "98"
          ],
          "1": "Implicit representations [27], [28], [29], [92] in particular neural radiance field (NeRF) [33], [34], [36], [42], [93], [94] have been widely used in many areas such as 3D modeling [95], [96] and face/body digitization [66], [97], [98], [99], [100], [101], [102]."
        },
        "3dmm-rf: Convolutional radiance fields for 3d face modeling": {
          "authors": [
            "Stathis Galanakis",
            "Baris Gecer",
            "Alexandros Lattas",
            "Stefanos Zafeiriou"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.html",
          "ref_texts": "[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "62"
          ],
          "1": "Other approaches focus on 3D shape reconstruction [89, 55, 83, 3], human bodies registration [62, 87, 61], dealing with non-static scenes [63, 56], scene editing [49, 36] and scene relighting [72, 9]."
        },
        "Common pets in 3d: Dynamic new-view synthesis of real-life deformable categories": {
          "authors": [
            "Samarth Sinha",
            "Roman Shapovalov",
            "Jeremy Reizenstein",
            "Ignacio Rocco",
            "Natalia Neverova",
            "Andrea Vedaldi",
            "David Novotny"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Sinha_Common_Pets_in_3D_Dynamic_New-View_Synthesis_of_Real-Life_Deformable_CVPR_2023_paper.html",
          "ref_texts": "[36] Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CoRR abs/2012.15838 (2020) 2",
          "ref_ids": [
            "36"
          ],
          "1": "4881 adopt parametric models of motion such as linear blend skinning [13,23,36,44,51,52], but these are difficult to generalise beyond a few object categories such as humans."
        },
        "SUPREYES: SUPer Resolutin for EYES Using Implicit Neural Representation Learning": {
          "authors": [
            "C Jiao",
            "Z Hu",
            "M B\u00e2ce",
            "A Bulling"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3586183.3606780",
          "ref_texts": "[59] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "59"
          ],
          "1": "Most importantly, recent work on neural radiance fields [56, 59, 60, 75, 79] and image representation learning [3, 9, 20, 41, 64] has demonstrated that implicit neural representation learning is highly effective at identifying and capturing structure in data and that the resulting representations can be used to synthesise new data that matches the statistics and is close to indistinguishable visually from real data."
        },
        "iVS-Net: learning human view synthesis from internet videos": {
          "authors": [
            "Junting Dong",
            "Qi Fang",
            "Tianshuo Yang",
            "Qing Shuai",
            "Chengyu Qiao",
            "Sida Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 5",
          "ref_ids": [
            "37"
          ],
          "3": "Conditioning the radiance field on the structured latent codes, Neural Body [37] achieves remarkable results only using four cameras."
        },
        "NDF: Neural deformable fields for dynamic human modelling": {
          "authors": [
            "R Zhang",
            "J Chen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_3",
          "ref_texts": "25. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "25"
          ],
          "2": "Neural Body [25] learns neural representations over the same set of latent codes anchored to the deformable human model SMPL [17], and naturally integrate observations across frames.",
          "10": "Neural Body [25] represents the dynamic scene with an implicit field conditioned on a shared set of latent codes anchored on the vertices of SMPL and renders the images using volume rendering."
        },
        "Avatarone: Monocular 3d human animation": {
          "authors": [
            "Akash Karthikeyan",
            "Robert Ren",
            "Yash Kant",
            "Igor Gilitschenski"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "Several studies have proposed controllable animatable NeRFs [29, 38, 44, 45, 56, 64], introducing an array of techniques such as pose-dependent radiance fields, latent codes anchored on deformable meshes, and transformation optimization between view and canonical space.",
          "2": "With the advent of NeRFs, new methods have proposed modeling human geometry as radiance fields [10, 21, 23, 41, 42, 44, 45, 64, 78] or distance functions [59, 67], offering more flexibility and improved rendering quality."
        },
        "Neural novel actor: Learning a generalized animatable neural representation for human actors": {
          "authors": [
            "Q Gao",
            "Y Wang",
            "L Liu",
            "L Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10221769/",
          "ref_texts": "[45] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.",
          "ref_ids": [
            "45"
          ],
          "4": "Additionally, we provide the results of a person-specific model, Neural Body (NB) [45], in the task of seen poses for seen subjects as an upper-bound baseline.",
          "10": "Our method achieves the best performance in two metrics, compared to three person-specific animatable human models, NeuralBody (NB) [45], AnimatbleNerf (AN) [44], and Neural Actor (NA) [31] and MPS-NeRF (MPS) [17]."
        },
        "UNIF: United neural implicit functions for clothed human reconstruction and animation": {
          "authors": [
            "S Qian",
            "J Xu",
            "Z Liu",
            "L Ma",
            "S Gao"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_8",
          "ref_texts": "[29] Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "29"
          ],
          "1": "2 Human Body Reconstruction and Animation As the most popular mesh-based human body model, SMPL [17] and its variations [12, 30, 27] dominate the area of human body reconstruction for its expressiveness and flexibility, supporting innumerable downstream task [15, 26, 2, 9, 29, 28, 14].",
          "2": "Besides the minimal-clothed human body, later works also use neural implicit functions to model clothed humans [31, 34, 24, 29, 28]."
        },
        "Handdgp: Camera-space hand mesh prediction with differentiable global positioning": {
          "authors": [
            "E Valassakis",
            "G Garcia-Hernando"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72920-1_27",
          "ref_texts": "43. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
          "ref_ids": [
            "43"
          ],
          "1": "Also typically constrained by efficiency but at the benefit of high resolution are implicit function methods [26,30,31], which inherit from the trend started by human body digitization [6,26,38,43,47]."
        },
        "Relightable neural actor with intrinsic decomposition and pose control": {
          "authors": [
            "D Carbonera Luvizon",
            "V Golyanik",
            "A Kortylewski"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73202-7_27.pdf",
          "ref_texts": "42. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicitneuralrepresentationswithstructuredlatentcodesfornovelviewsynthesis of dynamic humans. In: Computer Vision and Pattern Recognition (CVPR) (2021)",
          "ref_ids": [
            "42"
          ],
          "2": "Neural Body [42] optimizes latent vectors anchored in the vertices of the human mesh and Neural Actor [32] incorporates texture maps as a conditioning to break down the mapping from pose to dynamic effects (one-to-many mapping)."
        },
        "Camm: Building category-agnostic and animatable 3d models from monocular videos": {
          "authors": [
            "Tianshu Kuai",
            "Akash Karthikeyan",
            "Yash Kant",
            "Ashkan Mirzaei",
            "Igor Gilitschenski"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.html",
          "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2",
          "ref_ids": [
            "40"
          ],
          "1": "They serve as base templates for methods that focus on reconstructing and animating 3D humans and animals [7,21,26,36,39,40,55,58,60].",
          "2": "Many works [7,12,13,21, 26,36,39,40,55,58,60\u201362,65] utilize these template shapes or pose priors from shape models to recover 3D shapes and perform animations."
        },
        "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos": {
          "authors": [
            "F Lu",
            "Z Dong",
            "J Song",
            "O Hilliges"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73668-1_13.pdf",
          "ref_texts": "48. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "48"
          ],
          "1": "Many works [6,8,10,20,26,29,48,57,60,62] fit implicit neural fields to RGB or RGB-D videos by neural rendering to reconstruct the shape and appearance of a single human body."
        },
        "Hvh: Learning a hybrid neural volumetric representation for dynamic hair performance capture": {
          "authors": [
            "Ziyan Wang",
            "Giljoo Nam",
            "Tuur Stuyck",
            "Stephen Lombardi",
            "Michael Zollhofer",
            "Jessica Hodgins",
            "Christoph Lassner"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_HVH_Learning_a_Hybrid_Neural_Volumetric_Representation_for_Dynamic_Hair_CVPR_2022_paper.html",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 3",
          "ref_ids": [
            "44"
          ],
          "1": "Some recent works [26, 44, 53] have also shown their effectiveness in modeling appearance.",
          "2": "Neural body [44] incorporates SMPL [28] with Neural Volumes [26] for body modeling."
        },
        "Humannerf-se: A simple yet effective approach to animate humannerf with diverse poses": {
          "authors": [
            "Caoyuan Ma",
            "Lun Liu",
            "Zhixiang Wang",
            "Wu Liu",
            "Xinchen Liu",
            "Zheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Ma_HumanNeRF-SE_A_Simple_yet_Effective_Approach_to_Animate_HumanNeRF_with_CVPR_2024_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.1, 3, 4, 5",
          "ref_ids": [
            "41"
          ],
          "3": "NeuralBody [41] uses structured pose features generated from SMPL [29] vertices to anchor sampling points in any poses from sparse multi-camera videos, which inspired us to use spatial information of vertices.",
          "4": "We processed data similarly to NeuralBody [41] in this part.",
          "5": "Inspired by NeuralBody [41]\u2019s use of convolution to diffuse vertex occupancy, we use a simpler method to query the nearest neighbor SMPL weight.",
          "6": "To enable the network to learn nonrigid deformation from limited images as much as possible, some methods [41, 56, 62] introduce frame-level features to facilitate learning."
        },
        "Cat-nerf: Constancy-aware tx2former for dynamic body modeling": {
          "authors": [
            "Haidong Zhu",
            "Zhaoheng Zheng",
            "Wanrong Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2, 5, 8",
          "ref_ids": [
            "33"
          ],
          "6": "We also report SMPLpix [34] on H36M, along with NeuralBody [33] and HumanNeRF [49] on ZJUMoCap, since the authors did not provide numbers on the other datasets."
        },
        "4dynamic: Text-to-4d generation with hybrid priors": {
          "authors": [
            "YJ Yuan",
            "L Kobbelt",
            "J Liu",
            "Y Zhang",
            "P Wan"
          ],
          "url": "https://arxiv.org/abs/2407.12684",
          "ref_texts": "[28] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "28"
          ],
          "1": "Due to its fascinating results in novel view synthesis and 3D reconstruction [27], it has been further extended for digital human modeling [28], [29], [30], [31], [32], [33], better rendering effects [34], [35], generalization on different scenes [36], [37], faster training or inference speed [10], [38], [39], [40], [41], [42], geometry or appearance editing [43], [44], [45], [46], [47], etc."
        },
        "Selfnerf: Fast training nerf for human from monocular self-rotating video": {
          "authors": [
            "B Peng",
            "J Hu",
            "J Zhou",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2210.01651",
          "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2, 4, 6, 8",
          "ref_ids": [
            "25"
          ],
          "1": "NeuralBody [25], AnimatableNeRF [24], H-NeRF [38] and other works [15, 16, 30, 41] are able to synthesize high-quality rendering images and extract rough body geometry from 1 arXiv:2210.",
          "2": "Neuralbody [25] utilizes a set of latent codes anchored to a deformable mesh which is shared at different frames.",
          "3": "Following NeuralBody [25], an optimizable latent embedding \u2113t for each frame tis employed to encode the temporally-varying factors.",
          "6": "com/ashawkey/torch-ngp ZJUMocap Custom Data Method PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 NeuralBody [25] 23.",
          "7": "ZJUMocap Custom Data Method PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 NeuralBody [25] 23.",
          "8": "2) NeuralBody [25] reconstructs per frame\u2019s neural radiance field conditioned at body structured latent codes, which are diffused to the whole space by the SparseConvNet."
        },
        "Free-viewpoint rgb-d human performance capture and rendering": {
          "authors": [
            "P Nguyen-Ha",
            "N Sarafianos",
            "C Lassner"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_27",
          "ref_texts": "51. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 1, 3, 4, 11",
          "ref_ids": [
            "51"
          ],
          "3": "Given multi-view input frames or videos, recent works on rendering animate humans from novel views show impressive results [46,50,51,66]."
        },
        "Human 3d avatar modeling with implicit neural representation: A brief survey": {
          "authors": [
            "M Sun",
            "D Yang",
            "D Kou",
            "Y Jiang",
            "W Shan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10218567/",
          "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 3, 6",
          "ref_ids": [
            "40"
          ],
          "2": "[38] \u2713 \u2713 DD-NeRF [69] \u2713 \u2713 Neural Body [40] \u2713 \u2713 \u2713 Hand HALO [23] \u2713 \u2713 LISA [14] \u2713 \u2713 \u2713 Grasping Field [24] \u2713 \u2713 Head NeuralHDHair [58] \u2713 \u2713 ImFace [70] \u2713 \u2713 Yang et al.",
          "3": "Neural Body [40] attaches the latent code to the SMPL model as structured latent code."
        },
        "Lart: Neural correspondence learning with latent regularization transformer for 3d motion transfer": {
          "authors": [
            "H Chen",
            "H Tang",
            "R Timofte"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/88593e16b09104fb6010d370c081d7bc-Abstract-Conference.html",
          "ref_texts": "[24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "24"
          ],
          "1": "Inspired by NPT and NeuralBody [35, 24], the idea of using conditional normalization directly in the spatial sense and preserving the structured topology latent codes for detailed geometry capturing is very intuitive and proven effective."
        },
        "Efficient ray sampling for radiance fields reconstruction": {
          "authors": [
            "S Sun",
            "M Liu",
            "Z Fan",
            "Q Jiao",
            "Y Liu",
            "L Dong",
            "L Kong"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0097849323002868",
          "ref_texts": "[22] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "22"
          ],
          "1": "Improvements based on the problems of NeRF include accelerating the training [3, 6, 19, 31, 39, 40] and rendering [2, 15, 16, 41, 43] process, targeting on dynamic scenes [8,13,22,36], better generalization [4,15,35,37,42], training with fewer viewpoints [29, 33, 36]."
        },
        "A comprehensive benchmark for neural human radiance fields": {
          "authors": [
            "K Liu",
            "D Jin",
            "A Zeng",
            "X Han"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6e566c91d381bd7a45647d9a90838817-Abstract-Datasets_and_Benchmarks.html",
          "ref_texts": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9050\u20139059, 2020.",
          "ref_ids": [
            "38"
          ],
          "1": "The extended NeRFs for humans [38, 49, 12, 54, 10, 37, 32, 8, 24] follow similar pathways of development.",
          "2": "One of the representative works is NeuralBody [38], which requires four views and 100-300 frames for training.",
          "6": "Among these variants, NeRFs for human body rendering [38, 49, 12, 54, 10, 37, 32, 8, 24] have attracted a lot of attention due to their broad applications.",
          "7": "1 Scene-specific NeRFs for Human Scene-specific methods [38, 37, 47, 49, 54, 10, 20] for human body rendering require only sparse view videos for training as different video frames can be treated equivalent to dense view images by exploiting the human body prior.",
          "9": "2 Evaluating on Challenging Datasets The most commonly used dataset for neural human body rendering is ZJU-MoCap [38], which consists of 10 multi-view video sequences captured by 24 rounding synchronized cameras.",
          "13": "Reported in paper Unified evaluation Methods PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 Scene-specific NeuralBody [38] 28.",
          "14": "GT Mask Estimated by RobustVideoMatting Methods PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 Scene-specific NeuralBody [38] 27.",
          "16": "Novel view rendering ZJU-MoCap GeneBody normal GeneBody hard clothGeneBody hard poseMethods PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 NeuralBody [38]27.",
          "17": "1715 Novel pose rendering ZJU-MoCap GeneBody normal GeneBody hard clothGeneBody hard poseMethods PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 NeuralBody [38]23."
        },
        "Dance in the wild: Monocular human animation with neural dynamic appearance synthesis": {
          "authors": [
            "TY Wang",
            "D Ceylan",
            "KK Singh"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665964/",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 4322",
          "ref_ids": [
            "29"
          ],
          "1": "Another recent thread of work has explored the use of neural textures [39, 8] and neural rendering [38] to synthesize humans under different poses and viewpoints [35, 34, 32, 29]."
        },
        "Compositional 3d human-object neural animation": {
          "authors": [
            "Z Hou",
            "B Yu",
            "D Tao"
          ],
          "url": "https://arxiv.org/abs/2304.14070",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9050\u20139059, Nashville, TN, USA, June 2021. IEEE. 1, 3, 5",
          "ref_ids": [
            "45"
          ],
          "3": "[45] present to implicitly reconstruct human body from spare videos with neural radiance fields with a carefully designed skinning deformation."
        },
        "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters": {
          "authors": [
            "M Sun",
            "J Chen",
            "J Dong",
            "Y Chen",
            "X Jiang",
            "S Mao"
          ],
          "url": "https://arxiv.org/abs/2411.17423",
          "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "33"
          ],
          "1": "Notably, Neural body [33] first proposes using the SMPL to generate dynamic 3d human models automatically."
        },
        "FPO++: efficient encoding and rendering of dynamic neural radiance fields by analyzing and enhancing Fourier PlenOctrees": {
          "authors": [
            "Saskia Rabich"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03475-3",
          "ref_texts": "45. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
          "ref_ids": [
            "45"
          ],
          "1": "3 Dynamic scene representations Although novel views of scenes containing motions can be directly synthesized from the individual per-frame static models, significant effort has been spent into more efficient representations for neural rendering such as subdividing the scene into static and dynamic parts [30, 68], using point clouds [68], mixtures of volumetric primitives [35], deformable human models [45], or encoding the dynamics with encoder\u2013decoder architectures [34, 38]."
        },
        "Dialoguenerf: Towards realistic avatar face-to-face conversation video generation": {
          "authors": [
            "Yichao Yan"
          ],
          "url": "https://link.springer.com/article/10.1007/s44267-024-00057-8",
          "ref_texts": "43. Peng,S.,Zhang,Y.,Xu,Y.,Wang,Q.,Shuai,Q.,Bao,H.,etal.(2021).Neural body:implicitneuralrepresentationswithstructuredlatentcodesfor novelviewsynthesisofdynamichumans.In ProceedingsoftheIEEE/CVF conferenceoncomputervisionandpatternrecognition (pp.9054\u20139063). Piscataway:IEEE.",
          "ref_ids": [
            "43"
          ],
          "1": "[43] added human pose parameters as additional input to synthesize deformable human body."
        },
        "Expressive Gaussian Human Avatars from Monocular RGB Video": {
          "authors": [
            "H Hu",
            "Z Fan",
            "T Wu",
            "Y Xi",
            "S Lee",
            "G Pavlakos"
          ],
          "url": "https://arxiv.org/abs/2407.03204",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "32"
          ],
          "1": "Current studies [49, 32, 15, 23, 12] mainly focus on learning human avatar on the body region and have made remarkable progress.",
          "3": "Early works [32, 2, 37, 38, 3, 15, 49, 46, 39, 42, 6, 22, 41] mainly resort to the combination of implicit neural representations like NeRF [29] and parametric models to represent human avatar with high fidelity and flexibility."
        },
        "Dynamic NeRFs for soccer scenes": {
          "authors": [
            "S Lewin",
            "M Vandegar",
            "T Hoyoux",
            "O Barnich"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3606038.3616158",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
          "ref_ids": [
            "29"
          ],
          "1": "They often work by learning the motion of a skinned multi-person linear model (SMPL [22]) along with its appearance [28, 29, 42]."
        },
        "An implementation of multimodal fusion system for intelligent digital human generation": {
          "authors": [
            "Y Zhou",
            "Y Chen",
            "K Bi",
            "L Xiong",
            "H Liu"
          ],
          "url": "https://arxiv.org/abs/2310.20251",
          "ref_texts": "[28] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "28"
          ],
          "1": "6M [26], [27], ZJU-Mocap [28], and BEAT [29] in the area of 3D digital humans, VOCA [30] and MultiFace [31] in the area of speech-driven, and DHHQA [32], DDHQA [2], and SJTU-H3D [3] in the area of quality assessment of 3D digital humans."
        },
        "Noise-in, Bias-out: Balanced and Real-time MoCap Solving": {
          "authors": [
            "Georgios Albanis",
            "Nikolaos Zioulis",
            "Spyridon Thermos",
            "Anargyros Chatzitofis",
            "Kostas Kolomvatsos"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Albanis_Noise-in_Bias-out_Balanced_and_Real-Time_MoCap_Solving_ICCVW_2023_paper.html",
          "ref_texts": "[54] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 6",
          "ref_ids": [
            "54"
          ],
          "1": "Recent multi-view datasets [82, 14, 54] rely on markerless capturing technology to fit parametric body models to estimated keypoint observations."
        },
        "HSR: Holistic 3D Human-Scene Reconstruction from Monocular Videos": {
          "authors": [
            "L Xue",
            "C Guo",
            "C Zheng",
            "F Wang",
            "T Jiang",
            "HI Ho"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73220-1_25.pdf",
          "ref_texts": "47. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "47"
          ],
          "1": "More recently, implicit neural fields combined with neural rendering have emerged as a way to fit articulated human models to videos [21,33\u201335,39,47,54,55,63]."
        },
        "Progressively-connected light field network for efficient view synthesis": {
          "authors": [
            "P Wang",
            "Y Liu",
            "G Lin",
            "J Gu",
            "L Liu",
            "T Komura"
          ],
          "url": "https://arxiv.org/abs/2207.04465",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR.",
          "ref_ids": [
            "41"
          ],
          "1": "Recent works [1, 14, 22, 24, 28\u201334, 36, 40, 41, 48, 53, 54, 58, 61, 64] show that we are able to learn neural scene representations for the NVS task."
        },
        "Dynamic multi-view scene reconstruction using neural implicit surface": {
          "authors": [
            "D Chen",
            "H Lu",
            "I Feldmann",
            "O Schreer"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10096704/",
          "ref_texts": "[17] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "17"
          ],
          "1": "Although these methods [13, 14, 15, 16, 17, 18] have demonstrated impressive efficacy in view synthesis and reconstruction in terms of the human body, the requirement of human priors th view MLP Hyper-coordinates Network SE3 Deformation Network SDF Network MLP Radiance Network.",
          "2": "Comparison Since most of state-of-the-art multi-view dynamic reconstruction approaches are template-based, we have evaluated Neural Body [17], A-Nerf [13] and AniSDF [14] on both mentioned datasets as they are also human-driven dynamic scenes, following the official codes and instructions."
        },
        "Generalizable neural voxels for fast human radiance fields": {
          "authors": [
            "T Yi",
            "J Fang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://arxiv.org/abs/2303.15387",
          "ref_texts": "[60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 4, 5, 6, 7, 8, 12, 13",
          "ref_ids": [
            "60"
          ],
          "1": "Some works [60, 86, 72, 12, 59, 58, 15, 33, 89] successfully apply NeRF methods to human body rendering frameworks.",
          "3": "Representing Canonical Bodies with Neural Voxels Most previous NeRF-based methods for human bodies [60, 86, 72, 12, 59, 58, 15, 33, 89] adopt purely implicit representations.",
          "4": "Quantitative comparisons on the ZJU-MoCap dataset [60].",
          "5": "Subject 377 Subject 386 Subject 387 PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193 Neural Body [60] 29.",
          "6": "028 Subject 392 Subject 393 Subject 394 PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193PSNR \u2191SSIM \u2191LPIPS (\u00d710\u22122) \u2193 Neural Body [60] 30.",
          "7": "Comparisons about training cost and rendering quality on ZJU-MoCap dataset [60].",
          "8": "Method Pretrain Dataset Perscene Iterations Time PSNR \u2191 SSIM \u2191 LPIPS (\u00d710\u22122) \u2193 Neural Body [60] \u0017 \u2013 \u2013 29.",
          "12": "The comparison results of our method, Neural body [60], and HumanNeRF are shown in Tab.",
          "15": "Method Time PSNR \u2191 Neural Body [60] \u223c14 hours 24."
        },
        "Human image generation: A comprehensive survey": {
          "authors": [
            "Z Jia",
            "Z Zhang",
            "L Wang",
            "T Tan"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3665869",
          "ref_texts": "[128] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR. 9054\u20139063.",
          "ref_ids": [
            "128"
          ],
          "1": "Additionally, models like neural body [128], CustomHumans [61], ICON [184], and ECON [183] utilize SMPL for 3D human body modeling and MLP-modeled implicit functions for predicting color and density, enabling different view synthesis capabilities."
        },
        "MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition": {
          "authors": [
            "A Chatziagapi",
            "GG Chrysos",
            "D Samaras"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72691-0_22",
          "ref_texts": "69. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition 19",
          "ref_ids": [
            "69"
          ],
          "1": "They have been extended to dynamic scenes [44,45,71], making them a popular choice for modeling human bodies [16,27,31,47,59,68,69,93] and faces [15,64,65]."
        },
        "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale Environments": {
          "authors": [
            "Leif Van",
            "Patrick Stotko",
            "Stefan Krumpen",
            "Reinhard Klein",
            "Michael Weinmann"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Van_Holland_Efficient_3D_Reconstruction_Streaming_and_Visualization_of_Static_and_Dynamic_ICCVW_2023_paper.html",
          "ref_texts": "[119] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR.",
          "ref_ids": [
            "119"
          ],
          "1": "In particular, this includes implicit scene representations based on Neural Radiance Fields (NeRFs) [97] and respective extensions towards speeding up model training [125, 39, 16, 26, 10, 164, 147, 105, 37, 9, 11, 188, 179, 102] with training times of seconds, the adaptation to unconstrained image collections [94, 13, 63], deformable scenes [115, 123, 43, 158, 124, 111, 160, 118, 116, 12, 87, 58, 83, 37] and video inputs [82, 174, 30, 119, 44, 81, 151, 79], the refinement or complete estimation of camera pose parameters for the input images [181, 165, 146, 20, 192, 191, 130, 187, 95, 84, 57, 173, 90, 6, 17, 15, 14, 52, 148, 86], combining NeRFs with semantics regarding objects in the scene [163, 189, 40], incorporating depth cues [166, 26, 128, 126, 3] to guide the training and allow handling textureless regions, handling large-scale scenarios [150, 161, 96], and streamable representations [18, 149]."
        },
        "PGAHum: prior-guided geometry and appearance learning for high-fidelity animatable human reconstruction": {
          "authors": [
            "H Wang",
            "Q Xu",
            "H Chen",
            "R Ma"
          ],
          "url": "https://arxiv.org/abs/2404.13862",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR.",
          "ref_ids": [
            "35"
          ],
          "2": "Neural Body [35] anchors a set of structured latent codes to the vertices of the SMPL mesh to represent an implicit human body and learns the implicit geometry and radiance fields from sparse videos.",
          "15": "For the quantitative evaluation on the geometry reconstruction task in Table 4 of Neural Body [35] and ARAH [40], we use the official code and provided pretrained weights without modification to obtain the first view and first frame posed mesh.",
          "16": "It\u2019s worthy noting that when comparing with the InstantNVR [6] method, we directly use their official code and train on the original ZJU-MoCap dataset [35] for a fair comparison."
        },
        "You only train once: Multi-identity free-viewpoint neural human rendering from monocular videos": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://arxiv.org/abs/2303.05835",
          "ref_texts": "[24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 6",
          "ref_ids": [
            "24"
          ],
          "1": "We present our experimental results on ZJU-MoCap [24] and PeopleSnapshot [1] to demonstrate that YOTO can competently handle hard cases (e.",
          "2": "ZJU-MoCap [24]) to verify the effectiveness of the proposed approach for human rendering under free-viewpoints with monocular videos.",
          "3": "Datasets To thoroughly evaluate the performance of YOTO and to fairly compare against the baseline, we use ZJUMoCap [24] dataset for quantitative evaluation."
        },
        "Explicifying neural implicit fields for efficient dynamic human avatar modeling via a neural explicit surface": {
          "authors": [
            "R Zhang",
            "J Chen",
            "Q Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611707",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition . 9054\u20139063.",
          "ref_ids": [
            "30"
          ],
          "2": "3 THE PROPOSED METHOD To model dynamic humans from multi-view videos, we adopt the approach proposed in Neural Body [30], assuming that the cameras are pre-calibrated and synchronized.",
          "3": "Our proposed rasterization-based neural rendering approach has three key advantages: 1) our approach queries the texture color only once for rendering a pixel, which is a significant reduction from tens of sample points used in previous methods [30, 44].",
          "4": "To evaluate the effectiveness of our proposed method, we conduct experiments on the ZJU-MoCap dataset [30], which records multi-view videos with 23 synchronous cameras and collects the shape and pose parameters of SMPL.",
          "5": "Following the same protocol as [30], we randomly choose 4 cameras for training and use the remaining cameras for testing."
        },
        "Pixel2ISDF: implicit signed distance fields based human body model from multi-view and multi-pose images": {
          "authors": [
            "J Chen",
            "W Yi",
            "T Wang",
            "X Li",
            "L Ma",
            "Y Fan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-25072-9_24",
          "ref_texts": "19. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 4, 5",
          "ref_ids": [
            "19"
          ],
          "2": "NeuralBody [19] adopts the posed SMPLX mesh to construct the latent code volume that aims to extract the implicit pose code.",
          "3": "Different from the latent code in NeuralBody [19] which is initialized randomly for optimizing specific humans, our latent code is the feature vector learned by a network with the normal map as the input, which can generalize to humans unseen from training ones."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[38] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Unlike prior methods [29], [34], [35] that depend on complex backward skinning [36], [37] for transformations to a canonical space [38], Gaussian primitives for clothed human avatars are easily and efficiently animated via forward skinning, similar to the template mesh [39].",
          "2": "In addition to the textured mesh, NeRFs [16] also became a useful representation for photo-realistic clothed avatar reconstruction [34], [35], [38], [77], [78], [79], [80], [81], [82], [83], [84] from monocular or multi-view videos.",
          "3": "NeuralBody [38] assigned the latent code on the canonical space SMPL [39]vertices and applied the LBS [39] to transform the clothed human avatars to articulation space.",
          "4": "Relighing4D [25] first explored the MLPs representation based on NeuralBody [38] to jointly estimate the shape, lighting, and the albedo of dynamic humans from monocular or sparse videos under unknown illuminations by disentangling visibility via MLPs.",
          "5": "1 Evaluation datasets To validate the effectiveness of our proposed methods, we use a synthetic dataset (RANA [92]) and two real-world datasets (PeopleSnapshot [104], ZJU-MoCap [38], [105]) for performance evaluation.",
          "6": "ZJU-MoCap [38], [105] In ZJU-MoCap, the subjects captured in the video may conduct very complex motions.",
          "7": "Qualitative results on the ZJU-MoCap [38] dataset."
        },
        "Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering": {
          "authors": [
            "Chuanyue Shen",
            "Letian Zhang",
            "Zhangsihao Yang",
            "Masood Mortazavi",
            "Xiyun Song",
            "Liang Peng",
            "Heather Yu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021. 1, 2, 4, 5, 6",
          "ref_ids": [
            "26"
          ],
          "2": "It catalyzes a wave of human neural rendering methods that deliver high fidelity results [9,15,24,26,32,33].",
          "3": "Due to its high-quality performance while being simple and extendable, the use of NeRF as a core algorithm has been widely explored in a variety of scene representation and rendering tasks, such as pose estimation [24,26,29,32], lighting [1,2,37], scene labeling and understanding [30,39], and scene composition [23, 34].",
          "5": "Neural Body [26] achieves free-viewpoint synthesis from a sparse multi-view video by leveraging the arts from the NeRF model, a Skinned Multi-Person Linear (SMPL) model [18], and a latent variable model [16].",
          "8": "ZJU-MoCap dataset We use realistic dataset ZJU-MoCap [26] in our experiment for evaluating the system pipeline and algorithm performance in real-world conferencing scenarios."
        },
        "Phomoh: Implicit photorealistic 3d models of human heads": {
          "authors": [
            "M Zanfir",
            "T Alldieck"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550675/",
          "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 3",
          "ref_ids": [
            "51"
          ],
          "1": "NeRF-based models have been also used for personalization, either to represent the full person [40, 42, 51, 65, 69] or by focusing on the head [5, 25, 48]."
        },
        "Light field diffusion for single-view novel view synthesis": {
          "authors": [
            "Y Xiong",
            "H Ma",
            "S Sun",
            "K Han",
            "H Tang",
            "X Xie"
          ],
          "url": "https://arxiv.org/abs/2309.11525",
          "ref_texts": "35. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "Keywords: Novel View Synthesis\u00b7 Diffusion \u00b7 Light Field 1 Introduction Novel view synthesis (NVS), the inference of a 3D scene\u2019s appearance from novel viewpoints given several images of the scene [12,34,35,58,63,70], plays a fundamental role in many computer vision applications such as game studios, virtual reality, and augmented reality [7,30,55]."
        },
        "Neural kaleidoscopic space sculpting": {
          "authors": [
            "Byeongjoo Ahn",
            "Michael De",
            "Ioannis Gkioulekas",
            "Aswin C. Sankaranarayanan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.html",
          "ref_texts": "[27] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "27"
          ],
          "1": "The most successful among these work use class-specific priors for faces and the human body [17,26,27,32,35]."
        },
        "Hdhuman: High-quality human novel-view rendering from sparse views": {
          "authors": [
            "T Zhou",
            "J Huang",
            "T Yu",
            "R Shao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10168294/",
          "ref_texts": "[2] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d inProc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1, 1, 1, 4.2, 4.3, 4.4",
          "ref_ids": [
            "2"
          ],
          "1": "To render novel-view images of human performers from sparse views, neural body [2] integrates SMPL model [3] to the NeRF framework, in which they anchor a latent code on each vertex of SMPL model to integrate the observations over video frames.",
          "6": "3) Neural body [2] proposes a NeRF-based implicit neural representation for human novel-view rendering, which needs to train a separate network for each human.",
          "7": "Table 1 shows the quantitative reconstruction comparisons of our method with neural body [2]."
        },
        "Implicit epipolar geometric function based light field continuous angular representation": {
          "authors": [
            "Lin Zhong",
            "Bangcheng Zong",
            "Qiming Wang",
            "Junle Yu",
            "Wenhui Zhou"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/html/Zhong_Implicit_Epipolar_Geometric_Function_Based_Light_Field_Continuous_Angular_Representation_CVPRW_2023_paper.html",
          "ref_texts": "[20] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "20"
          ],
          "1": "[20] proposed a novel human body representation that assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh."
        },
        "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time rendering of temporally complex dynamic scenes": {
          "authors": [
            "J Yan",
            "R Peng",
            "L Tang",
            "R Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681463",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "41"
          ],
          "1": "The photo-realistic view synthesis capability of NeRF has inspired a series of works across various domains, including enhancing rendering quality[4\u20136, 8, 54, 55, 72, 74], sparse inputs[3, 34, 63], surface representation and segmentation[38, 59, 79], accelerating training and rendering[6, 14, 19, 20, 33, 43, 44, 49, 52, 69, 73], as well as human modeling[40, 41, 60, 71, 80], among others."
        },
        "Serf: Fine-grained interactive 3d segmentation and editing with radiance fields": {
          "authors": [
            "K Zhou",
            "L Hong",
            "E Xie",
            "Y Yang",
            "Z Li"
          ],
          "url": "https://arxiv.org/abs/2312.15856",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "43"
          ],
          "1": "NeRF, with its volume rendering technique, has achieved outstanding results, including view-dependent effects, and has inspired a wide range of applications such as human rendering [43, 63], pose estimation [32], surface reconstruction [60, 67], and indoor scene rendering [70]."
        },
        "Real-time volumetric rendering of dynamic humans": {
          "authors": [
            "I Rocco",
            "I Makarov",
            "F Kokkinos",
            "D Novotny"
          ],
          "url": "https://arxiv.org/abs/2303.11898",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProc. CVPR, 2021. 1, 2, 6, 7",
          "ref_ids": [
            "29"
          ],
          "2": "We show that adopting a factorized radiance field representation and a simple LBS-based deformation model allows for fast reconstruction (24 times faster than HumanNeRFUW) with comparable or better rendering quality on the ZJU-Mocap [29] scenes.",
          "4": "Neural Body [29] attaches neural codes to the SMPL vertices, poses them, and converts them to a full volumetric representation of the radiance field using a 3D sparse CNN [9, 13].",
          "8": "4 we present the quantitative and qualitative results of our method compared to NeuralBody [29] and HumanNeRF-UW [41]."
        },
        "FastHuman: Reconstructing High-Quality Clothed Human in Minutes": {
          "authors": [
            "L Lin",
            "S Peng",
            "Q Gan",
            "J Zhu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550690/",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2",
          "ref_ids": [
            "45"
          ],
          "1": "In order to model human avatars, some approaches [6, 43, 45, 60, 61] incorporate the estimated human skeleton and neural rendering to model animatable human avatars in an implicit Recovered Mesh Textured MeshReposed Mesh Input Video Figure 1.",
          "2": "[43, 45, 61] dynamically synthesize the human image."
        },
        "Self-nerf: A self-training pipeline for few-shot neural radiance fields": {
          "authors": [
            "J Bai",
            "L Huang",
            "W Gong",
            "J Guo",
            "Y Guo"
          ],
          "url": "https://arxiv.org/abs/2303.05775",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "35"
          ],
          "1": "[35] utilize the implicit spatial information in the local CNN features to construct the radiance fields."
        },
        "ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation": {
          "authors": [
            "H Li",
            "HX Yu",
            "J Li",
            "J Wu"
          ],
          "url": "https://arxiv.org/abs/2412.18600",
          "ref_texts": "[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
          "ref_ids": [
            "62"
          ],
          "1": "Neural rendering techniques have significantly advanced the synthesis of realistic human appearances [12, 36, 42, 44, 46, 54, 61, 62, 83].",
          "2": "NeuralBody [62] utilizes structured latent codes linked to SMPL [55] for novel view synthesis from sparse multi-view videos."
        },
        "SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering": {
          "authors": [
            "Mohammed Brahimi",
            "Bjoern Haefner",
            "Tarun Yenamandra",
            "Bastian Goldluecke",
            "Daniel Cremers"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.html",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "41"
          ],
          "1": "Neural Inverse Rendering and View Synthesis Neural approaches for inverse rendering [9,23,24,33,48, 51\u201353,63,74] and novel view synthesis [3,11,18,26,28,32, 41, 44, 50, 61] have gained a lot of attention over the recent years."
        },
        "DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras": {
          "authors": [
            "W Xie",
            "X Dong",
            "Y Yang",
            "Q Lin",
            "J Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10447270/",
          "ref_texts": "[10] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "10"
          ],
          "1": "Typically, dynamic NeRFs rely on video flow captured by multi-view cameras [10, 11] or one free-viewpoint camera [3, 4, 5, 6, 7] to get full view perception of dynamic scenes."
        },
        "Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior": {
          "authors": [
            "C Guo",
            "J Li",
            "Y Kant",
            "Y Sheikh",
            "S Saito"
          ],
          "url": "https://arxiv.org/abs/2503.01610",
          "ref_texts": "[60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "60"
          ],
          "1": "Highfidelity 3D avatar reconstruction has required calibrated multi-view systems [2, 9, 18, 23, 26, 44, 45, 48, 55, 59, 60, 62, 67, 69, 75, 79, 83, 87, 94, 95]."
        },
        "Efficient Integration of Neural Representations for Dynamic Humans": {
          "authors": [
            "W Li",
            "L Zeng",
            "C Gao",
            "N Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10666828/",
          "ref_texts": "[1] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. XX, XX 2024 12 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "1"
          ],
          "1": "Numerous studies [1], [2], [3], [4], [5], [6] have shown the potential for realistic rendering employing implicit Neural Radiance Fields (NeRF [7]).",
          "9": "NB [1] outperforms AS and Instant-NVR, and its performance in terms of PSNR and SSIM metrics is comparable to ours, attributed to its utilization of ground-truth vertices as a geometric prior.",
          "10": "Additionally, compared to [1], [35], [4], our reconstruction results exhibit fewer noisy points."
        },
        "Neural texture puppeteer: A framework for neural geometry and texture rendering of articulated shapes, enabling re-identification at interactive speed": {
          "authors": [
            "Urs Waldmann",
            "Ole Johannsen",
            "Bastian Goldluecke"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Waldmann_Neural_Texture_Puppeteer_A_Framework_for_Neural_Geometry_and_Texture_WACVW_2024_paper.html",
          "ref_texts": "[31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2",
          "ref_ids": [
            "31"
          ],
          "2": "Good results can be achieved with NeRF-based [26] approaches [30, 38], approaches based on implicit neural representations [31, 36] or approximate differentiable rendering [47]."
        },
        "AutoAvatar: Autoregressive neural fields for dynamic avatar modeling": {
          "authors": [
            "Z Bai",
            "T Bagautdinov",
            "J Romero",
            "M Zollh\u00f6fer"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20068-7_13",
          "ref_texts": "37. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054\u20139063",
          "ref_ids": [
            "37"
          ],
          "1": "Similarly, neural radiance fields [31] have been applied to body modeling to build animatable avatars from multi-view images [37,20].",
          "2": "The most exciting venue for future work is to extend the notion of dynamics to image-based avatars [37,20]."
        },
        "GAS: Generative Avatar Synthesis from a Single Image": {
          "authors": [
            "Y Lu",
            "J Dong",
            "Y Kwon",
            "Q Zhao",
            "B Dai"
          ],
          "url": "https://arxiv.org/abs/2502.06957",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "35"
          ],
          "1": "To address these challenges, some methods [8, 9, 21, 22, 34, 35, 59] use 3D human templates, such as the SMPL model [30], to anchor features accurately on the human form."
        },
        "Real-time distance field acceleration based free-viewpoint video synthesis for large sports fields": {
          "authors": [
            "Y Dai",
            "J Li",
            "Y Jiang",
            "H Qin",
            "B Liang",
            "S Hong"
          ],
          "url": "https://link.springer.com/article/10.1007/s41095-022-0323-3",
          "ref_texts": "[46] Peng, S. D.; Zhang, Y. Q.; Xu, Y. H.; Wang, Q. Q.; Shuai, Q.; Bao, H. J.; Zhou, X. W. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9050\u20139059, 2021.",
          "ref_ids": [
            "46"
          ],
          "1": "[46] proposed the Neural Body, a new human body representation."
        },
        "Crosshuman: learning cross-guidance from multi-frame images for human reconstruction": {
          "authors": [
            "L Chen",
            "J Li",
            "H Huang",
            "Y Guo"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3548351",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "41"
          ],
          "1": "For geometry feature extraction, inspired by [40, 41, 47], we choose the SparseConvNet [15] to process the SMPL model, divide the 3D bounding box of the SMPL into voxels and obtain a 352-channel geometry feature volume."
        },
        "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text": {
          "authors": [
            "G Shim",
            "S Lee",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2502.11642",
          "ref_texts": "[27] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "These studies draw inspiration from human deformation concepts derived from deformable neural representations [2, 26, 27, 42], which address how 3D coordinates on a human model are deformed across different poses."
        },
        "NeVRF: Neural Video-Based Radiance Fields for Long-Duration Sequences": {
          "authors": [
            "M Wu",
            "T Tuytelaars"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550605/",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "35"
          ],
          "1": "Previous work exploits multiview image features [23], a coarse proxy geometry [35, 50] paired with a differentiable rendering pipeline to handle dynamic scenes."
        },
        "Semantic-preserved point-based human avatar": {
          "authors": [
            "L Lin",
            "J Zhu"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S107731422500030X",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 3",
          "ref_ids": [
            "29"
          ],
          "1": "Also, there are some works that model dynamic humans from multiview videos [27,29]."
        },
        "DressRecon: Freeform 4D Human Reconstruction from Monocular Video": {
          "authors": [
            "J Tan",
            "D Xiang",
            "S Tulsiani",
            "D Ramanan"
          ],
          "url": "https://arxiv.org/abs/2409.20563",
          "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "46"
          ],
          "1": "With sufficient information as input, multi-view methods [9, 13, 26, 37, 41, 46, 59] can reconstruct human shape and appearance of very high fidelity, but the reliance on a dense capture studio limits their applicability at a consumer level."
        },
        "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
          "authors": [
            "H Wang",
            "W Zhang",
            "S Liu",
            "X Zhou",
            "J Li",
            "Z Tang"
          ],
          "url": "https://arxiv.org/abs/2405.12477",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 6",
          "ref_ids": [
            "35"
          ],
          "1": "The ZJU-Mocap dataset [35] is a prominent benchmark in human modeling from videos.",
          "2": "Quantitative Results To verify the effectiveness of our method in solving the geometric distortion problem in reconstructing the human body, we compare our method with NeuralBody [35], HumanNeRF [43] AnimateNeRF [34], InstantNVR [5] InstantAvatar [15] GauHuman [11] on the ZJU dataset and the Monocap dataset, as shown in Table 1."
        },
        "GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views": {
          "authors": [
            "B Zhou",
            "S Zheng",
            "H Tu",
            "R Shao",
            "B Liu",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2411.11363",
          "ref_texts": "[7] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 1, 2",
          "ref_ids": [
            "7"
          ],
          "1": "On the other hand, NeRF-like differentiable volumetric rendering techniques [6], [7], [8], [9] can synthesize novel views under sparse camera setting [10], but typically suffer from per-scene optimization [6], [7], [8], [9], slow rendering speed [6], [7] and overfitting to input views [11].",
          "2": "Neural implicit function has recently aroused a surge of interest to represent complicated scenes, in form of occupancy fields [31], [32], [33], [34], radiance fields [6], [7], [35], [36], [37], [38] and signed distance functions [10], [39], [40], [41], [42].",
          "3": "More recently, numerous methods have extended Neural Radiance Fields (NeRF) [6] to static human modeling [47], [48] and dynamic human modeling from sparse multi-view cameras [7], [10], [36] or a monocular camera [37], [38], [49]."
        },
        "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans": {
          "authors": [
            "A Chatziagapi",
            "B Chaudhuri",
            "A Kumar"
          ],
          "url": "https://arxiv.org/abs/2409.16666",
          "ref_texts": "51. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)",
          "ref_ids": [
            "51"
          ],
          "1": "Recent advances in dynamic neural radiance fields (NeRF) [41] have enabled significant progress in rendering and animating human bodies [51,64] and faces [15,21].",
          "2": "While many NeRF-based approaches require multiple views [31,42,46,47,51], recent works learn dynamic NeRFs from monocular videos [15,64,68].",
          "4": "Many NeRF-based approaches capture the 4D dynamics and appearance of humans from multi-view videos [16,27,29,37,44,46, 47,50,51,66], while recent works use monocular videos [15,21,28,42,60,62,64,68]."
        },
        "Diversity-Aware Sign Language Production through a Pose Encoding Variational Autoencoder": {
          "authors": [
            "MI Lakhal",
            "R Bowden"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10581951/",
          "ref_texts": "[31] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "Human-NeRF [48], [30], [31] uses a human 3D mesh template (usually SMPL [24]) which is deformed with a given body pose and rendered to the target viewpoint using Volume Rendering [23]."
        },
        "Survey on controlable image synthesis with deep learning": {
          "authors": [
            "S Zhang",
            "J Li",
            "L Yang"
          ],
          "url": "https://arxiv.org/abs/2307.10275",
          "ref_texts": "[90] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "90"
          ],
          "1": "9, SEPTEMBER 2020 6 TABLE I ENHANCEMENTS TO NERF Method Publication Image resolution Dataset no camera pose NeRF\u2013[73] arXiv2022 756x1008/1080x1920/520x780 [74]/[28]/[73] GNeRF[75] ICCV2021 400x400/500x400 [17]/[76] SCNeRF[77] ICCV2021 [74]/[78] NoPe-NeRF[79] CVPR2023 [76]/[74]/[80] SPARF[81] CVPR2023 960x540/648x484 [78]/[27] sparse data NeRS[82] NIPS2021 [82] MixNeRF[83] CVPR2023 [76]/[74]/[17] SceneRF[84] arXiv2023 1220x370 [85] GM-NeRF[86] CVPR2023 224x224 [87]/[88]/[89]/[90] SPARF[81] CVPR2023 960x540/648x484 [78]/[27] noisy data RawNeRF[91] CVPR2022 [91] Deblur-NeRF[92] CVPR2022 [92] HDR-NeRF[93] CVPR2022 400x400/804x534 [93] NAN[94] CVPR2022 [94] large-scale image synthesis Mip-NeRF 360[95] CVPR2022 [78] BungeeNeRF[96] ECCV2022 [97] Block-NeRF[98] arXiv2022 [98] GridNeRF[99] CVPR2023 [100]/[99] EgoNeRF[101] CVPR2023 [101] image synthesis speed PlenOctrees[102] ICCV2021 800x800/1920x1080 [17]/[78] DirectV oxGO[103] CVPR2022 800x800/800x800/768x576/1920x1080/512x512 [17]/[104]/[105]/[78]/[106] R2L[107] ECCV2022 [17]/[108] SqueezeNeRF[109] CVPR2022 [17]/[74] MobileNeRF[110] CVPR2023 800x800/1008x756/1256x828 [17]/[74]/[95] L2G-NeRF[111] CVPR2023 [74] rendering for both static and dynamic scenes while improving efficiency and robustness."
        },
        "HumanGif: Single-View Human Diffusion with Generative Prior": {
          "authors": [
            "S Hu",
            "T Narihira",
            "K Fukuda",
            "R Sawata"
          ],
          "url": "https://arxiv.org/abs/2502.12080",
          "ref_texts": "[88] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1",
          "ref_ids": [
            "88"
          ],
          "1": "Recent methods enable the novel view and pose synthesis of 3D human avatars from sparse-view human videos [19, 21, 45, 46, 48, 50, 59, 61, 85, 88, 90, 109, 114, 119, 125], with Neural Radiance Field [82] or Gaussian Splatting-based [66] representation."
        },
        "DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction": {
          "authors": [
            "B Kaye",
            "T Jakab",
            "S Wu",
            "C Rupprecht"
          ],
          "url": "https://arxiv.org/abs/2412.04464",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. CVPR, 2021. 3",
          "ref_ids": [
            "43"
          ],
          "1": "PhysGaussian [70], MD-Splatting [8], Gaussian Splashing [11] and VR-GS [17] consider reconstruction and generation of 4D scene which accounts for physical principles Works like Neural Human Video Rendering [37], AutoAvatar [1], Neural Body [43], Dynamic Facial RF [50], Relighting4D [4], Animate124 [84], Dynamic Gaussian Mesh [36] and IM4D [33] specialise in the reconstruction of articulated characters like humans and animals."
        },
        "Neural Radiance Fields with Torch Units": {
          "authors": [
            "B Ni",
            "H Wang",
            "D Bai",
            "M Weng",
            "D Qi",
            "W Qiu"
          ],
          "url": "https://arxiv.org/abs/2404.02617",
          "ref_texts": "[38] Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X., 2021b. Neural body: Implicit neural representations with structured 9 latentcodesfornovelviewsynthesisofdynamichumans,in:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9054\u20139063.",
          "ref_ids": [
            "38"
          ],
          "1": "Besides, applying NeRFs [17, 22, 38, 56] on video compressing, editing, and generation are also ongoing."
        },
        "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training": {
          "authors": [
            "R Yin",
            "V Yugay",
            "Y Li",
            "S Karaoglu",
            "T Gevers"
          ],
          "url": "https://arxiv.org/abs/2411.02229",
          "ref_texts": "[26] Peng, S., Xu, Y ., Wang, Q., Jiang, Q.S., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.",
          "ref_ids": [
            "26"
          ],
          "1": "Due to its capabilities, NeRF became widely adopted for 3D scene reconstruction [1, 18, 8, 34, 14], human body modeling [26, 25, 36, 16], robotics [41, 28], and medical imaging [7]."
        },
        "LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes": {
          "authors": [
            "Z Qu",
            "K Xu",
            "GP Hancke",
            "RWH Lau"
          ],
          "url": "https://arxiv.org/abs/2411.06757",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "37"
          ],
          "1": ", accelerating the training and rendering of NeRF [14, 18, 40, 41], handling dynamic scenes [21, 34, 39, 44, 61] and digital humans body [1, 34, 36, 37, 10] or human head [53, 68, 16] modeling, and the manipulation [4, 38, 28, 23] or generation [8, 15, 25, 33] of scene contents.",
          "2": "For example, [14, 18, 40, 41] are proposed to accelerate the NeRF training procedure, [21, 34, 39, 44, 61] are applied to render dynamic scenarios, [4, 38, 28, 23] are focused on the NeRF relighting methods, [1, 34, 36, 37, 53] are expanded to the non-rigid object rendering, [8, 15, 25, 33] are used for the generation models."
        },
        "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers": {
          "authors": [
            "L Prospero",
            "A Hamdi",
            "JF Henriques"
          ],
          "url": "https://arxiv.org/abs/2409.04196",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 5, 6, 7",
          "ref_ids": [
            "43"
          ],
          "1": "Similar to previous works [24], we utilize four comprehensive human datasets for evaluation: THuman [72], RenderPeople [1], ZJU MoCap [43], and HuMMan [4].",
          "2": "We compare GST on the ZJU_MoCap [43] and THuman [72] datasets on novel view synthesis.",
          "3": "Single Image NVS on 2 subjects of Zju-Mocap [43] compared to SHERF [24] (after being adapted with HMR2 to work with single image input only)."
        },
        "Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view Event Cameras": {
          "authors": [
            "V Rudnev",
            "G Fox",
            "M Elgharib",
            "C Theobalt"
          ],
          "url": "https://arxiv.org/abs/2412.06770",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "41"
          ],
          "1": "head avatars [44], full-body avatars [41, 22, 12], hands [31] and other domains."
        },
        "Representing Animatable Avatar via Factorized Neural Fields": {
          "authors": [
            "C Song",
            "Z Wu",
            "B Wandt",
            "L Sigal",
            "H Rhodin"
          ],
          "url": "https://arxiv.org/abs/2406.00637",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "Other methods [50, 44, 51, 52] aim to improve results with an image-to-image translation network and a per-frame latent code.",
          "2": "Following HumanNeRF and MonoHuman, we conduct evaluations across the eight sequences of the ZJU-Mocap dataset [44].",
          "3": "Table 1: Novel-view and novel-pose synthesis results, averaged over the ZJU-Mocap test set [44].",
          "4": "Table A: Novel-view synthesis comparisons on ZJU-Mocap [44].",
          "5": "Thus we visualize the front and back view of our reconstructed 3D avatars in 18 Table B: Novel-pose synthesis comparisons on ZJU-Mocap [44].",
          "6": "Table C: Geometry reconstruction comparisons on ZJU-Mocap [44].",
          "7": "All results are rendered on the ZJU-MoCap dataset [44].",
          "8": "19 Table D: Frame Consistency on ZJU-Mocap [44]."
        },
        "Pathway to future symbiotic creativity": {
          "authors": [
            "Y Guo",
            "Q Liu",
            "J Chen",
            "W Xue",
            "J Fu",
            "H Jensen"
          ],
          "url": "https://arxiv.org/abs/2209.02388",
          "ref_texts": "[116] Peng, Sida, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. \u201cNeural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans.\u201d arXiv preprint arXiv:2012.15838 (2020).",
          "ref_ids": [
            "116"
          ],
          "1": "3 Motion Capture Dataset There are several public mocap datasets, including the SFU Motion Capture Database [115], ZJUMoCap Dataset [116] and CMU Graphics Lab Motion Capture Database [117].",
          "2": "Recently, Neural Body [116] was introduced to integrate a parametric human model SMPL [107] and a canonical neural radiance field (NeRF) [124] for performance capture.",
          "3": "BodyFusion [123] leverage point clouds and fusion techniques, NeuralBody [116] and NDF [125] are based on the neural representation model NeRF and parametric 46 47 frameworks allow for highresolution rendering and work well with a dense multi-camera photogrammetry system."
        },
        "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features": {
          "authors": [
            "A Dey",
            "CY Lu",
            "AI Comport",
            "S Sridhar",
            "CT Lin"
          ],
          "url": "https://arxiv.org/abs/2411.03086",
          "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "6M [24] \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 SURREAL [56] \u2713 \u2713 \u2713 \u2713 \u2713 \u2717 3DPW [57] \u2713 \u2713 \u2713 \u2713 \u2717 \u2713 ZJU-MoCap [47] \u2713 \u2713 \u2713 \u2713 \u2717 \u2713 THuman2."
        },
        "Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects": {
          "authors": [
            "S Gopal",
            "R Dabral",
            "V Golyanik",
            "C Theobalt"
          ],
          "url": "https://arxiv.org/abs/2502.13968",
          "ref_texts": "[27] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "27"
          ],
          "1": "They have also been applied for human rendering [18, 27, 34, 41, 51], that extend to dynamic scenes as well as provide pose-conditioned animation capabilities.",
          "2": "This is akin to the canonicalised representation in several 3D human and non-rigid reconstruction works [18, 27, 28, 37]."
        },
        "Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos": {
          "authors": [
            "X Luo",
            "J Peng",
            "Z Cai",
            "L Yang",
            "F Yang",
            "Z Cao"
          ],
          "url": "https://arxiv.org/abs/2501.13335",
          "ref_texts": "[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 5, 6",
          "ref_ids": [
            "56"
          ],
          "1": "To reconstruct dynamic humans, body model encoding [69, 83] is proposed to enhance generalization, and several works learn deformation field in canonical space [10, 12, 20, 35, 39, 57, 77] combined with parametric models [42, 54] to model pose and shape, where a latent code [56] for SMPL [42] vertex is proposed to model appearance, which can be posed by a coordinate-based neural skinning field [4, 16, 45].",
          "2": "Datasets There are no available datasets for motion-blurred inputs in animatable human avatar tasks, so we curated two datasets: (1) Synthesized dataset from ZJU-MoCap [56] and (2) Real blur dataset from our capturing and internet videos.",
          "3": "To make the dataset more realistic, we use EasyMocap [7, 56] to re-calculate the human poses and the human masks of the synthesized blurred image sequences."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "143. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR, pp. 9054\u20139063 (2021).https://doi.org/10.",
          "ref_ids": [
            "143"
          ],
          "4": "Neural body [143] delineates a set of structural latent codes serving as the conditional feature of NeRF input predicated on the SMPL model, while SLRF [160] introduces several nodes to construct a scene embedding of query points within a local space on the SMPL model.",
          "5": "Multi-view Neural Human Rendering (NHR) [142], ZJUMoCap [142, 143], and Neural Actors [163] datasets are representative of a common multi-view human movement dataset in which actors perform complex motions with daily clothing.",
          "7": "6M [193] People-Snapshot [99] ZJU-MoCap [143] Train Infer Devices PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Neural Body [143] \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 28.",
          "8": "6M [193] ZJU-MoCap [143] Inference time \u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Animatable_NeRF [149] 22."
        },
        "AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation": {
          "authors": [
            "M Li",
            "S Yao",
            "C Kai",
            "Z Xie",
            "K Chen",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2502.19441",
          "ref_texts": "[14] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "14"
          ],
          "2": "Extensions of NeRF [38] into dynamic scenes [39]\u2013[41] and methods for animatable 3D human models in multi-view scenarios [20], [43]\u2013 [45], [58], [67] or monocular videos [8], [11], [14], [36] have shown promising results.",
          "4": "7: Visual comparison of different methods about novel view synthesis on ZJU-MoCap [14] .",
          "5": "8: Novel poses of ZJU-MoCap [14]."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3, 5",
          "ref_ids": [
            "43"
          ],
          "1": "NeRF-based approaches [2\u20135, 9, 12\u201314, 25, 43, 49] focus on reconstructing human avatars from monocular or multi-view synchronized videos.",
          "2": "Compared to previous datasets such as ZJU-MoCap [43], which are mainly collected under controlled speeds and tightfitting garments, I3D-Human contains scenes closer to daily life."
        },
        "PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing": {
          "authors": [
            "S Seth",
            "R Dabral",
            "D Luvizon",
            "M Habermann"
          ],
          "url": "https://arxiv.org/abs/2411.04249",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR, 2021. 1",
          "ref_ids": [
            "29"
          ],
          "1": "In recent years, there have been tremendous improvements in automatic human avatar creation methods that generate avatars from inputs ranging from multi-view images [16,29,46] to highly-detailed 3D scans [19,21]."
        },
        "A portable multiscopic camera for novel view and time synthesis in dynamic scenes": {
          "authors": [
            "T Zhang",
            "YF Lau",
            "Q Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9982040/",
          "ref_texts": "[28] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "28"
          ],
          "1": "Many subsequent works were used to improve NeRF\u2019s representation ability and expand the scope of application in terms of generalization ability [23], dynamic and deformable scenes synthesis [24], applications under extreme environments [25], improving training and rendering efficiency [26], reconstruction on specific shape domains [27], [28] and so on."
        },
        "Bundle Adjusted Gaussian Avatars Deblurring": {
          "authors": [
            "M Niu",
            "Y Zhan",
            "Q Zhu",
            "Z Li",
            "W Wang",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2411.16758",
          "ref_texts": "[50] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2, 6",
          "ref_ids": [
            "50"
          ],
          "1": "As there are currently no benchmarks for evaluating this task, we have developed a synthetic dataset based on the widely-used ZJU-MoCap dataset [50], complemented by a real-captured dataset obtained through a 360-degree hybridexposure camera system.",
          "2": "We create our synthetic dataset based on the ZJU-MoCap [50] dataset, a widely used dataset for evaluating 3D human avatar techniques.",
          "3": "We calibrate the SMPL parameters using the blurry video frames with EasyMoCap [1, 50], and utilize SegmentAnything [22] to obtain segmentation masks."
        },
        "Structured 3D features for reconstructing controllable avatars": {
          "authors": [
            "E Corona Puyane",
            "M Zanfir",
            "T Alldieck"
          ],
          "url": "https://upcommons.upc.edu/handle/2117/397279",
          "ref_texts": "[53] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 3, 7",
          "ref_ids": [
            "53"
          ],
          "1": "Recently, research on implicit representations [6, 17, 26, 61, 62] and neural fields [29, 53, 72, 81] has made significant progress in improving the realism of avatars.",
          "2": "NeRFs have been recently explored for novel human view synthesis [11, 15, 30, 52, 53, 64, 69, 70, 72]."
        },
        "Anipixel: Towards animatable pixel-aligned human avatar": {
          "authors": [
            "J Fan",
            "J Zhang",
            "Z Hou",
            "D Tao"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3612058",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InCVPR.",
          "ref_ids": [
            "30"
          ],
          "2": "With the recent success of neural radiance field (NeRF) representation [23], a line of works has tried to reconstruct volumetric avatars in radiance field [29, 30, 44].",
          "3": "CV] 17 Oct 2023 MM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada Jinlong Fan, Jing Zhang, Zhi Hou, & Dacheng Tao human reconstruction, skeleton motion is often taken as prior to constraining the deformation field [29, 30, 50].",
          "5": "To account for dynamic humans, deformation fields are devised to deform the posed body in target space to canonical space, where the density and color are predicted [29, 30].",
          "6": "To ensure stability during training, human priors are often introduced [44] or used to initialize the motion field [6, 30, 51].",
          "7": "One way is to assign the weight using its nearest neighbor (NN) on the SMPL mesh or through barycentric interpolation of Top-k closest vertices [6, 30, 43].",
          "8": "For each part, there are (a) the input three views and results of (b) AniNeRF [29], (c) NeuralBody [30], (d) MPS-NeRF [6], (e) our method, and (f) the ground truth.",
          "9": "893 Table 2: Comparison of our method with NeuralBody [30], AniNeRF [29], MPS-NeRF [6] on the Human3.",
          "10": "For each part, there are (a) the input three views and results of (b) NeuralBody [30], (c) KeypointNeRF [22], (d) our methods, and (e) the ground truth.",
          "11": "895 Table 3: Comparison of NeuralBody [30], AniNeRF [29], KeypointNeRF [22], and our method on ZJUMoCAP.",
          "12": "The second dataset is the ZJUMocap dataset [30], which provides video sequences of 10 subjects captured from 23 synchronized cameras.",
          "13": "for the whole image, we follow previous methods [6, 29, 30] to project the 3D bounding box of the fitted SMPL mesh onto the image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "14": "We compare our method with recent two animatable methods, NeuralBody [30] and AniNeRF [29], and two generalizable methods, KeypointNeRF [22] and MPS-NeRF [6]."
        },
        "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene": {
          "authors": [
            "S Biswas",
            "Q Wu",
            "B Banerjee",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2409.17459",
          "ref_texts": "[9] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "9"
          ],
          "1": "In NeRF-based dynamic scene reconstruction, the focus has predominantly been on human reconstruction [7, 8, 9, 10], utilizing template models such as SMPL [11], and CAPE [12].",
          "3": "For this purpose, we test performance on two datasets: the ZJU-MoCap dataset [9] for human reconstruction and a synthetic dataset for animal reconstruction from [28].",
          "4": "TA V A [28] learns the skinning 7 weight from scratch without using any template model, whereas AnimatableNeRF [9] initializes the skinning weight from SMPL and learns a residual weight to optimize the final skinning weight field.",
          "5": "Also, we evaluate our method for single-object reconstruction, including arbitrary deformable entities, Human reconstruction: We compare our human surface reconstruction results with TA V A [28] and AnimatableNeRF [7] on the ZJU-MoCap dataset [9] (Tab.",
          "6": "71 Table 5: Reconstruction on ZJU-Mocap (upper) [9, 46] and synthetic animal dataset [28] (lower).",
          "7": "Ours TA V A [28] AnimatableNeRF[7]I/P Figure 4: Qualitative comparison on ZJUMocap dataset [9]."
        },
        "DiHuR: Diffusion-Guided Generalizable Human Reconstruction": {
          "authors": [
            "J Chen",
            "C Li",
            "GH Lee"
          ],
          "url": "https://arxiv.org/abs/2411.11903",
          "ref_texts": "[18] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 5, 6, 7",
          "ref_ids": [
            "18"
          ],
          "1": "Although existing generalizable human NeRF [4, 5, 8, 18, 23, 26] have achieved impressive, they mainly focus on novel view synthesis.",
          "2": "NB [18] first combines NeRF with a parametric human body model SMPL [15] to regularize the optimization process.",
          "3": "We conduct experiments on the commonly used ZJU-MoCap dataset [18], THuman dataset [30] and HuMMan dataset [1].",
          "4": "On ZJU-MoCap dataset, we also compared with NeuS [24] and NB [18].",
          "5": "Our methods outperform existing methods 6 [4,5, 14,18, 24,29] with a large margin in terms of both CD and NC.",
          "6": "Methods [18] [24] [14] [4] [7] [29] Ours CD(\u2193) 1.",
          "7": "For ZJU-MoCap, we show results of novel view synthesis in 3 source views with minimal overlap FOV setting and compare with existing NeRF-based methods [4,11,18].",
          "8": "For THuman dataset, we follow [5] and compare with existing human NeRF methods [4, 5, 17, 18].",
          "9": "PSNR(\u2191) SSIM(\u2191) Seen people on seen frames NB [18] \u2713 \u2717 \u2717 28.",
          "10": "951 Seen people on unseen frames NB [18] \u2713 \u2717 \u2713 23.",
          "11": "941 Unseen people on unseen frames NB [18] \u2713 \u2713 \u2717 22.",
          "12": "Methods NB [18] NHP [11] MPS [5] GP-NeRF [4] Ours SSIM(\u2191) 0."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[36] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3, 6, 11, 12",
          "ref_ids": [
            "36"
          ],
          "4": "1, we quantitatively experiment on ZJUMocap [36] dataset to validate that our method is also effective in scenarios with tight-fitting garments."
        },
        "EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View": {
          "authors": [
            "Z Wang",
            "Y Kanamori",
            "Y Endo"
          ],
          "url": "https://arxiv.org/abs/2410.12242",
          "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, pages 9054\u20139063, 2021. 2, 3, 10, 11",
          "ref_ids": [
            "34"
          ],
          "1": "Using human body keypoints and statistical human models such as SMPL [28] and SMPL-X [33] as prior knowledge, the rendering quality is further improved, and the number of input views required is also reduced [34, 54].",
          "2": "Neural implicit representation is a promising alternative for representing 3D scenes [30, 37, 41, 49], including full-body human [6, 34, 39, 42, 54] because it enables high-quality rendering without explicit geometry reconstruction."
        },
        "WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction": {
          "authors": [
            "Z Wang",
            "Z Dou",
            "Y Liu",
            "C Lin",
            "X Dong",
            "Y Guo"
          ],
          "url": "https://arxiv.org/abs/2502.01045",
          "ref_texts": "[1] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021.",
          "ref_ids": [
            "1"
          ],
          "2": "Recent advancements in implicit neural radians fields [1], [3], [4], [5] and 3D Gaussian Splatting [6], [7], [8], [9] have explored the high-fidelity reconstruction of both geometry and appearance of dynamic human bodies from relatively sparse multi-view videos."
        },
        "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors": {
          "authors": [
            "J Yin",
            "W Yin",
            "H Chen",
            "X Ren",
            "Z Ma",
            "J Guo"
          ],
          "url": "https://arxiv.org/abs/2311.15171",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9054\u20139063, 2021.",
          "ref_ids": [
            "32"
          ],
          "6": "5, our method produces higher visual quality with fewer artifacts than existing methods [30,32,33,38,40], which also indicates a better correspondence across frames."
        },
        "Ma-nerf: Motion-assisted neural radiance fields for face synthesis from sparse images": {
          "authors": [
            "W Zhang",
            "X Zhou",
            "YK Cao",
            "WS Feng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10220034/",
          "ref_texts": "[13] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021.",
          "ref_ids": [
            "13"
          ],
          "1": "Inspired by [12], [13], we formulate a displacement volume by anchoring and diffusing the displacement of 3DMM with SparseConvNet [14].",
          "3": "We qualitatively and quantitatively compare our method with two other methods that perform closely related tasks: (1) Neural Body [13], using a set of latent codes anchored on the SMPL vertices to build local implicit representations."
        },
        "UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing": {
          "authors": [
            "J Fan",
            "J Zhang",
            "D Tao"
          ],
          "url": "https://arxiv.org/abs/2304.06969",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "32"
          ],
          "4": "Novel view and Novel pose synthesis For novel view synthesis and novel pose rendering, we compare our method against five existing approaches: 1) Neural Body (NB) [32] diffuses per-SMPL-vertex latent codes in observation space to condition the NeRF model and achieves high-quality novel view synthesis results on training poses; 2) Ani-NeRF [31] learns a backward LBS weight field and a canonical NeRF to reconstruct the human avatar;"
        },
        "Interactive Rendering of Relightable and Animatable Gaussian Avatars": {
          "authors": [
            "Y Zhan",
            "T Shao",
            "H Wang",
            "Y Yang",
            "K Zhou"
          ],
          "url": "https://arxiv.org/abs/2407.10707",
          "ref_texts": "[34] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "34"
          ],
          "1": "In recent years, many works [34], [35], [36], [37], [38], [38], [39], [40], [41], [42], [43] have used NeRF [7] to represent the human body by learning from multi-view videos, achieving pleasant rendering results."
        },
        "LokiTalk: Learning Fine-Grained and Generalizable Correspondences to Enhance NeRF-based Talking Head Synthesis": {
          "authors": [
            "T Li",
            "R Zheng",
            "B Li",
            "Z Zhang",
            "M Wang",
            "J Chen"
          ],
          "url": "https://arxiv.org/abs/2411.19525",
          "ref_texts": "[23] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "23"
          ],
          "1": "The inherent limitation of vanilla NeRF in modeling solely static scenes has motivated the development of diverse approaches [17, 18, 23, 24, 25, 26] to address the representation of dynamic scenes."
        },
        "NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View Mirror Scene Reconstruction with 3D Surface Primitives": {
          "authors": [
            "L Van Holland",
            "M Weinmann",
            "JU M\u00fcller"
          ],
          "url": "https://arxiv.org/abs/2501.04074",
          "ref_texts": "[73] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063. IEEE, 6 2021. 2",
          "ref_ids": [
            "73"
          ],
          "1": "Further extensions include streamable representations [19, 86], the handling of unbounded scenes [3, 115], the handling of image collections taken under in-the-wild conditions [14,38,57] or low-dynamic-range images with low or varying exposure [34, 62], video inputs [22, 28, 42, 44, 45, 73, 88, 108], and the refinement or complete estimation of camera pose parameters for the input images [7, 15, 16, 18, 20, 32, 35, 48, 49, 55, 60, 79, 83, 85, 103, 107, 111, 117, 123, 124] as well as the handling of deformable scenes [13, 24, 27, 36, 46, 51, 69\u201372, 74, 75, 91, 93] and large-scale scenarios [61, 87, 94]."
        },
        "Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB videos": {
          "authors": [
            "R Jena",
            "P Chaudhari",
            "J Gee",
            "G Iyer"
          ],
          "url": "https://arxiv.org/abs/2303.08808",
          "ref_texts": "[43] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Recently this approach has been extended to reconstruct clothed humans as well [43, 58, 12, 67, 24, 66, 30, 65, 23, 19, 62].",
          "5": "We choose baselines across a spectrum of representation choices: SMPLPix [45] (SP) which uses deferred rendering, VideoAvatar [5] (V A) which performs SMPL+D optimization, NeuralBody [43] (NB), HumanNeRF [67] (HN) and AnimNeRF [12] (AN) which are SOTA NeRF methods."
        },
        "Neural rendering of humans in novel view and pose from monocular video": {
          "authors": [
            "T Wang",
            "N Sarafianos",
            "MH Yang",
            "T Tung"
          ],
          "url": "https://arxiv.org/abs/2204.01218",
          "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "35"
          ],
          "2": "In addition, we reconstruct fine-level details such as cloth wrinkles, hand details at a resolution and fidelity that several prior top-performing methods such as NeuralBody [35] or HumanNeRF [46] fail to recover (Figure 1).",
          "5": "NeuralBody [35] proposes a set of latent codes shared across all frames anchored to a human body model in order to replay character motions from arbitrary view points under training poses.",
          "11": "NeuralBody [35] models dynamic scenes using latent codes anchored to the human pose as an extra input besides the coordinate and viewing direction.",
          "13": "The qualitative comparisons are provided in Fig 4 where our approach captures fine-level details on the body (1st,3rd rows) and head (2nd row) better than prior works [34, 35, 46]."
        },
        "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video": {
          "authors": [
            "Y Hong",
            "Y Wu",
            "Z Shen",
            "C Guo",
            "Y Jiang",
            "Y Zhang"
          ],
          "url": "https://arxiv.org/abs/2502.08297",
          "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 3",
          "ref_ids": [
            "49"
          ],
          "1": "Meanwhile, other approaches [22, 48, 49, 58, 66, 84, 85] leverage the human parametric model [41] to reconstruct the animatable avatar."
        },
        "NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images": {
          "authors": [
            "J Yu",
            "D Nandi",
            "R Seidel",
            "G Hirtz"
          ],
          "url": "https://arxiv.org/abs/2402.18196",
          "ref_texts": "51. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 5, 9",
          "ref_ids": [
            "51"
          ],
          "1": "The invention of NeRF [43] and the subsequent development of its human-centric variations [51,58,63] open up new possibilities for high quality semi-synthetic HPE data generation.",
          "3": "NeuralBody [51] uses SMPL instead of the simple skeleton as latent encoding of the human body.",
          "4": "ZJU-MoCap is a motion capture dataset with 9 sequences [51]."
        },
        "DEGAS: Detailed Expressions on Full-Body Gaussian Avatars": {
          "authors": [
            "Z Shao",
            "D Wang",
            "QY Tian",
            "YD Yang",
            "H Meng"
          ],
          "url": "https://arxiv.org/abs/2408.10588",
          "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3",
          "ref_ids": [
            "48"
          ],
          "1": "NeuralBody [48, 49] encodes learnable latent codes to SMPL mesh vertices."
        },
        "SAGA: Surface-Aligned Gaussian Avatar": {
          "authors": [
            "R Chen",
            "Y Cong",
            "J Liu"
          ],
          "url": "https://arxiv.org/abs/2412.00845",
          "ref_texts": "[7] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "7"
          ],
          "5": "NeuralBody [7] anchors latent codes to the vertices of a SMPL model, and diffuses to the whole observation space via sparse convolution.",
          "10": "For NeRF-based methods: NeuralBody [7] anchors latent codes on the SMPL mesh and diffuse in 3D space with 3D convolution networks for neural volume rendering; AnimatableNeRF [37] represents the scene with a large MLP, and learns a forward and backward blending weight MLP for animation; HumanNeRF [8] further incorporates a non-rigid deformation network and achieves SOTA performance; InstantAvatar [19] applies the efficient Instant-NGP [12] as the canonical representation; InstantNVR [18] designs multi-part hash encoder based on [12].",
          "12": "48 TABLE 1: Qualitative results of novel view synthesis on ZJU-MoCap [7].",
          "13": "Subjects male-3-casual male-4-casual female-3-casual female-4-casual Metrics Train\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 NeRF Neural Body [7] \u223c14h 24."
        },
        "HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos": {
          "authors": [
            "Q Chen",
            "R Xie",
            "K Huang",
            "Q Wang",
            "W Zheng"
          ],
          "url": "https://arxiv.org/abs/2405.11270",
          "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "37"
          ],
          "1": "For the implicit animatable human reconstruction, recent works [10, 27, 36, 37, 53] have solved the challenging task of multi-view reconstruction without the supervision of 3D information and present the inherent challenges of rendering non-rigid bodies and skins under dynamic motion.",
          "2": "There have been numerous efforts to reconstruct humans in motion using multiview videos [10, 16, 27, 37, 46, 51, 53].",
          "3": "We validate our method on two real-world datasets, including ZJU-MoCap [37] and People-Snapshot [4]."
        },
        "2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting": {
          "authors": [
            "Q Yan",
            "M Sun",
            "L Zhang"
          ],
          "url": "https://arxiv.org/abs/2503.02452",
          "ref_texts": "[2] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "2"
          ],
          "1": "Over the past few years, Neural Radiance Fields (NeRF) [1] has been employed by some studies, enabling to reconstruct avatars from videos [2]\u2013[5] and images [6], [7].",
          "2": "Learning Gaussian Parameters Directly Methods [9], [11] that directly learn Gaussian parameters typically follow a pipeline that is similar to NeRF-based approaches [2], [16], [17], where the avatar is represented in a canonical space and subsequently transformed into the posed space using LBS, after which the Gaussian primitives are rendered into images through the 3DGS rasterizer."
        },
        "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars": {
          "authors": [
            "S Sasaki",
            "J Wu",
            "K Nishino"
          ],
          "url": "https://arxiv.org/abs/2412.04433",
          "ref_texts": "[23] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2",
          "ref_ids": [
            "23"
          ],
          "1": "Following the release of NeRF, the earliest works in this area [7, 13, 22, 23, 27, 35] rely on the SMPL model [14] to skin an optimized NeRF of a human from a canonical space (unskinned) space to the observation space (skinned) and vice versa, which is key to both animation and dynamic reconstruction."
        },
        "Hm3d-abo: A photo-realistic dataset for object-centric multi-view 3d reconstruction": {
          "authors": [
            "Z Yang",
            "Z Zhang",
            "Q Huang"
          ],
          "url": "https://arxiv.org/abs/2206.12356",
          "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1",
          "ref_ids": [
            "28"
          ],
          "1": "Introduction Reconstructing 3D object has been studied for decades [3, 19, 21, 27, 28, 39, 43\u201345, 50] and has been receiving more and more interest due to the recent popularity AR/VR applications."
        },
        "One Shot, One Talk: Whole-body Talking Avatar from a Single Image": {
          "authors": [
            "J Xiang",
            "Y Guo",
            "L Hu",
            "B Guo",
            "Y Yuan"
          ],
          "url": "https://arxiv.org/abs/2412.01106",
          "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
          "ref_ids": [
            "41"
          ],
          "1": "With the help of these parametric geometric models, recent personalized human avatars learn to model dynamic geometry and appearance by integrating neural textures [11, 32], neural radiance fields [23, 41, 52, 67], and 3D Gaussians [16, 17, 27, 29, 43] for dynamic photo-realistic rendering."
        },
        "NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction": {
          "authors": [
            "Z Zhang",
            "J Song",
            "E P\u00e9rez-Pellitero"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550790/",
          "ref_texts": "[31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "NeRF has later been adapted to represent dynamic human body [20, 30, 31, 37, 38, 44], achieving compelling results on modeling a clothed human.",
          "3": "NeuralBody [31] proposes to integrate structured latent code diffused from SMPL meshes into the neural radiance field for dynamic human modeling."
        },
        "DRaCoN--Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars": {
          "authors": [
            "A Raj",
            "U Iqbal",
            "K Nagano",
            "S Khamis"
          ],
          "url": "https://arxiv.org/abs/2203.15798",
          "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "26"
          ],
          "1": "The state-of-the-art methods in this direction use implicit functions [31, 33] to model human avatars and use volumetric rendering for image generation [14,25,26,39].",
          "3": "NeuralBody [26] exploits the SMPL body model and learn latent codes corresponding to each vertex.",
          "12": "1) of the study indicate that our method is preferred 66 %of the time as compared to the the next highest rated method (NeuralBody [26]) at 24%."
        },
        "ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events": {
          "authors": [
            "K Chen",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2409.14103",
          "ref_texts": "[58] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProc. of CVPR, 2021.",
          "ref_ids": [
            "58"
          ],
          "1": "Recently, some pioneering works [58, 57, 15] are proposed to learn dynamic humans based on the body poses and observation frames by deforming the neural radiance fields (NeRF) [44] and 3D gausian splatting (3DGS) [26].",
          "2": "Some follow-up research efforts are then made to enhance the rendering quality [58, 78, 23] and optimize the rendering efficiency [55, 22, 27, 30].",
          "3": "06 dB PSNR improvement compared with the comparison methods on the ZJUMocap [58] dataset."
        },
        "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video": {
          "authors": [
            "H Wang",
            "X Cai",
            "X Sun",
            "J Yue",
            "Z Tang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2405.12806",
          "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 6, 7, 17",
          "ref_ids": [
            "39"
          ],
          "2": "2 Comparison methods Compare MOSS with two categories of SOTA methods: Human NeRF-based methods, such as NeuralBody [39], HumanNeRF [50] AnimateNeRF [38], InstantNVR [10] InstantAvatar [17]."
        },
        "TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering": {
          "authors": [
            "Sadia Mubashshira",
            "Kevin Desai"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2025W/ImageQuality/html/Mubashshira_TE-NeRF_Triplane-Enhanced_Neural_Radiance_Field_for_Artifact-Free_Human_Rendering_WACVW_2025_paper.html",
          "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 6, 7, 8",
          "ref_ids": [
            "29"
          ],
          "1": "Neural Body [29] enhances this by anchoring latent codes to deformable human model vertices, effectively integrating pose and appearance across frames for more robust representations.",
          "2": "Results Quantitative results on the ZJU-MoCap dataset (Table 1 and Table 2) present a comparative analysis of our method, TE-NeRF with two state-of-the-art models, Neural Body [29] and HumanNeRF [43].",
          "3": "243 Method Subject 377 Subject 386 Subject 387 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 Neural Body [29] 29.",
          "4": "87 Method Subject 392 Subject 393 Subject 394 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 Neural Body [29] 30.",
          "5": "244 Method PSNR (\u2191) SSIM (\u2191) LPIPS (\u2193) Neural Body [29] 29."
        },
        "Geometry Aware Field-to-field Transformations for 3D Semantic Segmentation": {
          "authors": [
            "D Hollidt",
            "C Wang",
            "P Golland",
            "M Pollefeys"
          ],
          "url": "https://arxiv.org/abs/2310.05133",
          "ref_texts": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9050\u20139059, Nashville, TN, USA, 2021. IEEE. 1",
          "ref_ids": [
            "38"
          ],
          "1": "NeRFs find many useful applications in graphics [22, 39, 41, 44], robotics [16, 28], and human body representation [38, 59]."
        },
        "ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding": {
          "authors": [
            "Y Cheng",
            "B Wang",
            "RT Tan"
          ],
          "url": "https://arxiv.org/abs/2309.12183",
          "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 3",
          "ref_ids": [
            "36"
          ],
          "1": "Recent NeRF-based novel view synthesis [20, 36, 48] has been shown to benefit the 3D human shape and pose model [43, 48, 44]."
        },
        "GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video": {
          "authors": [
            "J Chen"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3696409.3700241",
          "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
          "ref_ids": [
            "32"
          ],
          "3": "HumanNeRF[40] and Neural Body[32] leverage geometric priors to create implicit neural representations of dynamic humans, synthesizing realistic body details.",
          "6": "GGAvatar on the People Snapshot dataset, outperforming methods related to NeRF, such as Neural Body[32], InstantAvatar[15], and the baseline GART[20]."
        },
        "PixelHuman: Animatable Neural Radiance Fields from Few Images": {
          "authors": [
            "G Shim",
            "J Lee",
            "J Hyung",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2307.09070",
          "ref_texts": "[20] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 5",
          "ref_ids": [
            "20"
          ],
          "2": "By learning a set of latent codes structured to SMPL [16] vertices, NeuralBody [20] successfully renders a human body in a motion sequence given in training videos from any novel viewpoint."
        },
        "GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis": {
          "authors": [
            "Y Abdelkareem",
            "S Shehata",
            "F Karray"
          ],
          "url": "https://arxiv.org/abs/2309.11627",
          "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
          "ref_ids": [
            "25"
          ],
          "1": "NeuralBody [25] anchored NeRF with pre-fitted 3D human models to regularize the training producing more photo-realistic output.",
          "2": "[30] utilized a layered architecture by representing the human entities using NeuralBody [25] and weakly supervising the human instance segmentation.",
          "3": "NeuralBody [25] anchored NeRF with a deformable human model [18] to provide a prior over the human body shape and correctly render self-occluded regions.",
          "5": "[30] extended ST-NeRF by modeling the human subjects using NeuralBody [25] and predicted human segmentation masks as part of the network training.",
          "7": "We incorporate the SparseConvNet [7, 25] architecture which utilizes 3D sparse convolution to diffuse the vertex features into different nearby continuous spaces for every human and view.",
          "8": "Existing layered scene representations [30] follow the approach of NeuralBody [25] by encoding the vertices of human layers using learnable embeddings that are unique to each layer in each training scene.",
          "9": "Following NeuralBody [25], we use EasyMoCap [29] to fit the SMPL human models for all the subjects in the available frames."
        },
        "GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Human View Synthesis": {
          "authors": [
            "Y Abdelkareem",
            "S Shehata",
            "F Karray"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-54605-1_11",
          "ref_texts": "25. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 9050\u20139059 (2021)",
          "ref_ids": [
            "25"
          ],
          "1": "NeuralBody [25] anchored NeRF with pre-fitted 3D human models to regularize the training producing more photorealistic output.",
          "2": "[30] utilized a layered architecture by representing the human entities using NeuralBody [25] and weakly supervising the human instance segmentation.",
          "3": "NeuralBody [25] anchored NeRF with a deformable human model [18] to provide a prior over the human body shape and correctly render self-occluded regions.",
          "5": "[30] extended ST-NeRF by modeling the human subjects using NeuralBody [25] and predicted human segmentation masks as part of the network training.",
          "7": "We incorporate the SparseConvNet [7,25] architecture which utilizes 3D sparse convolution to diffuse the vertex features into different nearby continuous spaces for every human and view.",
          "8": "Existing layered scene representations [30] follow the approach of NeuralBody [25] by encoding the vertices of human layers using learnable embeddings that are unique to each layer in each training scene.",
          "9": "Following NeuralBody [25], we use EasyMoCap [29] to fit the SMPL human models for all the subjects in the available frames."
        },
        "Neural scene representations for 3D reconstruction and generative modeling": {
          "authors": [
            "M Niemeyer"
          ],
          "url": "https://tobias-lib.ub.uni-tuebingen.de/xmlui/handle/10900/148183",
          "ref_texts": "[220] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. \u201cNeural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans\u201d. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2021.",
          "ref_ids": [
            "220"
          ],
          "1": "2 Generative Modeling Development Since its introduction [129], V AEs have been applied to a wide variety of tasks, including semi-supervised classification [167], image inpainting [220], graph modeling [133], chemical design [84], dark energy modeling in astronomy [235], and more, mainly due to its simple design and easy optimization.",
          "2": "As a result, follow-up works proposed methods that allow for inferring neural fieldbased 4D representations from multi-view images [65, 75, 149, 213, 214, 220, 228] where most methods adapt our approach to split the task into 3D scene reconstruction and motion reconstruction."
        },
        "3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer Avenue": {
          "authors": [
            "B Cai",
            "Y Li",
            "Y Liang",
            "R Jia",
            "B Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10480280/",
          "ref_texts": "[42] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021, pp. 9054\u20139063.",
          "ref_ids": [
            "42"
          ],
          "1": "2 Neural Rendering Leveraging NeRFs Recent advances show neural fields representations are promising to describe scenes, and support rendering photorealistic images of the fitted scenes under desired viewpoints [6], [37], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52]."
        },
        "Human View Synthesis using a Single Sparse RGB-D Input": {
          "authors": [
            "P Nguyen",
            "N Sarafianos",
            "C Lassner",
            "J Heikkila"
          ],
          "url": "https://3dvar.com/Nguyen2021Human.pdf",
          "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR, 2021. 1, 2",
          "ref_ids": [
            "52"
          ],
          "2": "Given multi-view input frames or videos, recent works on rendering animatable humans from novel views show impressive results [49,51, 52, 71]."
        },
        "Modularizing deep learning for geometry-aware registration and reconstruction": {
          "authors": [
            "Wei Jiang"
          ],
          "url": "https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0427395",
          "ref_texts": "[196] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. \u2192pages 3, 80, 83, 93, 98, 99",
          "ref_ids": [
            "196"
          ],
          "3": "While these methods have shown interesting and exciting results, they often require separate training of editable instances [89] or careful curation of training data [196].",
          "5": "Neural Body [196] associates a latent code to each SMPL vertex, and use sparse convolution to diffuse the latent code into the volume in observation space."
        },
        "Accelerating Human Avatar Creation: Pose-dependent Hybrid Representations for Efficient Rendering of Clothed Human Avatars": {
          "authors": [
            "Z Qian"
          ],
          "url": "https://www.research-collection.ethz.ch/handle/20.500.11850/610316",
          "ref_texts": "[66] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of CVPR, 2021.",
          "ref_ids": [
            "66"
          ],
          "11": "With recent advances in implicit neural rendering [52, 82, 86, 85, 91] and implicit surface reconstruction [56, 105, 104, 93, 59] from RGB images, drastic improvements have been made regarding rendering fidelity [66, 64, 57, 39, 71, 35, 30] and geometric quality [101, 65, 97, 23] for clothed human avatars created using sparse multi-view or monocular acquisition setups.",
          "15": "2) with recent state-of-the-art methods [97, 66, 64, 99], showing that the proposed approach achieves comparable rendering and geometry reconstruction quality while providing 100 times faster rendering than the most competitive baseline [97]."
        },
        "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses Supplemental Material": {
          "authors": [
            "I Lee",
            "B Kim",
            "H Joo"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Guess_The_Unseen_CVPR_2024_supplemental.pdf",
          "ref_texts": "[11] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "11"
          ],
          "3": "For the Panoptic dataset [5] and Hi4D dataset [19], we model the background using a timeconditioned NeRF defined on the surface of the cylinder fully covering the scene and the human model with NeuralBody [11]."
        },
        "Scene Representations for Generalizable Novel View Synthesis": {
          "authors": [
            "Y Abdelkareem"
          ],
          "url": "https://uwspace.uwaterloo.ca/handle/10012/19247",
          "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9050\u20139059, 2021.",
          "ref_ids": [
            "45"
          ],
          "4": "NeuralBody [45] offered a solution by anchoring NeRF with a deformable human model [38] to provide a prior over the human body shape and correctly render selfoccluded regions.",
          "5": "Recently, [55] extended ST-NeRF by modeling the human subjects using NeuralBody [45] and predicted human segmentation masks as part of the network training.",
          "6": "Human-anchored Feature Generation Existing layered scene representations [55] follow the approach of NeuralBody [45] by encoding the vertices of human layers using learnable embeddings that are unique to each layer in each training scene."
        },
        "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos\u2014Supplementary Material": {
          "authors": [
            "S Hu",
            "T Hu",
            "Z Liu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_GauHuman_Articulated_Gaussian_CVPR_2024_supplemental.pdf",
          "ref_texts": "[12] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 1, 2",
          "ref_ids": [
            "12"
          ],
          "1": "Neural Body (NB) [12] encodes latent codes in SMPL vertex points and uses them to learn the neural radiance fields.",
          "2": "For each subject in ZJU_MoCap [12] and MonoCap [2, 3, 13], we collect 20 frames for novel pose synthesis by sampling 1 frame every 10 frames."
        },
        "MonoHuman: Animatable Human Neural Field from Monocular Video Supplementary Material": {
          "authors": [
            "Z Yu",
            "W Cheng",
            "X Liu",
            "W Wu",
            "KY Lin"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Yu_MonoHuman_Animatable_Human_CVPR_2023_supplemental.pdf",
          "ref_texts": "[3] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR, 2021. 3",
          "ref_ids": [
            "3"
          ],
          "1": "Subject377 Subject386 Subject387PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193Neural Body [3]29.",
          "2": "45Subject392 Subject393 Subject394PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193Neural Body [3]28.",
          "3": "Subject377 Subject386 Subject387PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193Neural Body [3]29.",
          "4": "06Subject392 Subject393 Subject394PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193 PSNR\u2191 SSIM\u2191 LPIPS*\u2193Neural Body [3]29.",
          "5": "And follow NeuralBody [3] to evaluate the subject in a 3d bounding box to avoid getting an inflated PSNR value."
        },
        "Template-free Articulated Neural Point Clouds for Reposable View Synthesis": {
          "authors": [
            "LUE Eisemann",
            "P Kellnhofer"
          ],
          "url": "https://graphics.tudelft.nl/Publications-new/2023/UEK23/paper.pdf",
          "ref_texts": "[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
          "ref_ids": [
            "82"
          ],
          "1": "Third, ZJU-MoCap dataset [82] is a common test suite for dynamic human reconstruction and we evaluate 5 of its multi-view sequences with the same 6 training views as used in Watch-It-Move [20]."
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors* Supplemental Material": {
          "authors": [
            "Y Huang",
            "H Yi",
            "W Liu",
            "H Wang",
            "B Wu",
            "W Wang",
            "B Lin"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Huang_One-shot_Implicit_Animatable_ICCV_2023_supplemental.pdf",
          "ref_texts": "[13] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021. 1",
          "ref_ids": [
            "13"
          ],
          "1": "1 Data splitting For per-subject optimization methods Animatable NeRF[11] (Ani-NeRF) and NeuralBody[13] (NB), we use all subjects of ZJU-MoCap data-set (313, 315, 377, 386, 387, 390, 392, 393, 394) and the \u201dPosing\u201d sequences of Human 3."
        },
        "Rendering Humans from Object-Occluded Monocular Videos Supplementary Materials": {
          "authors": [
            "T Xiang",
            "A Sun",
            "J Wu",
            "E Adeli",
            "L Fei-Fei"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Xiang_Rendering_Humans_from_ICCV_2023_supplemental.pdf",
          "ref_texts": "[6] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2, 4",
          "ref_ids": [
            "6"
          ],
          "1": "However, we are also interested in comparing with Neural Body, another standard benchmark [6].",
          "2": "ZJU-MoCap Subject 377 Subject 386 PSNRvis SSIMvis PSNRfull SSIMfull PSNRvis SSIMvis PSNRfull SSIMfull Neural Body [6] 7.",
          "3": "9639 ZJU-MoCap Subject 387 Subject 392 PSNRvis SSIMvis PSNRfull SSIMfull PSNRvis SSIMvis PSNRfull SSIMfull Neural Body [6] 9.",
          "4": "9575 ZJU-MoCap Subject 393 Subject 394 PSNRvis SSIMvis PSNRfull SSIMfull PSNRvis SSIMvis PSNRfull SSIMfull Neural Body [6] 9.",
          "5": "Quantitative comparison against Neural Body [6] on ZJU-MoCap."
        },
        "Free-Viewpoint Video in the Wild Using a Flying Camera": {
          "authors": [
            "Z Hong",
            "W Shen"
          ],
          "url": "https://openreview.net/forum?id=AsPHD00Wer",
          "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., W ang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "34"
          ],
          "1": "NeRF [29] and its follow-ups [28, 34, 48] have demonstrated promising performances in rendering both static scene and dynamic human.",
          "4": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 42, 45] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
          "authors": [
            "JA Rahim"
          ],
          "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
          "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProc. of Computer Vision and Pattern Recognition (CVPR), pages 9054\u20139063, 2021.",
          "ref_ids": [
            "30"
          ],
          "3": "Below we discuss three important works in this category: Animatable NeRF [29], Neural Body [30], and Structured Local Radiance Fields for Human Avatar Modeling [44].",
          "4": "Neural Body [30] also uses SMPL as the human body prior, but utilizes a very different approach overall.",
          "5": "A denser geometry feature volume \u02dcF is obtained by using a Sparse Convolution Network [12] similar to Neural Body [30].",
          "6": "Efforts have been made in adapting NeRF to rendering human subjects [30, 21, 18, 29, 42, 44, 6].",
          "8": "1 SMPL+D Optimization Inspired by previous Inspired by previous works [18, 30], we adopt the SMPL model [22] as a body prior to establish coarse correspondences across frames.",
          "10": "We compare against three methods; Structured Local Radiance Fields for Human Avatar Modeling (SLRF) [44], Animatable Nerf (AN) [29], Neural Body (NB) [30]."
        },
        "FlexNeRF: Photorealistic Free-Viewpoint Rendering of Moving Humans from Sparse Views (Supplementary Materials)": {
          "authors": [
            "V Jayasundara",
            "A Agrawal",
            "N Heron",
            "A Shrivastava"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_CVPR_2023_supplemental.pdf",
          "ref_texts": "[4] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9050\u20139059, 2021. 1, 5, 6",
          "ref_ids": [
            "4"
          ],
          "3": "Method LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 Neural Body [4] 57.",
          "4": "Method Subject 377 Subject 392 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 Neural Body [4] 40.",
          "5": "9769 Method Subject 386 Subject 393 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 Neural Body [4] 46.",
          "6": "9711 Method Subject 387 Subject 394 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u00d7103 \u2193 PSNR \u2191 SSIM \u2191 Neural Body [4] 59."
        },
        "Free-Viewpoint Video of Outdoor Sports Using a Drone": {
          "authors": [
            "Z Hong"
          ],
          "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02178.pdf",
          "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., W ang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "34"
          ],
          "1": "NeRF [29] and its follow-ups [28, 34, 47] have demonstrated promising performances in rendering both static scene and dynamic human.",
          "4": "Different from them, per-scene optimization methods like [9, 13, 14, 33, 34, 41, 44] use the 3D human poses as input priors and enable novel-viewpoint rendering with higher fidelity."
        },
        "Supplementary Materials for GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images": {
          "authors": [
            "J Chen",
            "W Yi",
            "L Ma",
            "X Jia",
            "H Lu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_GM-NeRF_Learning_Generalizable_CVPR_2023_supplemental.pdf",
          "ref_texts": "[8] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021. 2, 3",
          "ref_ids": [
            "8"
          ],
          "4": "we also train our model from scratch without any pretraining, Compared to per-scene optimization methods such as NT [10], NHR [12], NB [8], which often spend hours or even a day on training, our method can quickly produce more realistic and detailed images."
        },
        "Posed Neural Radiance Fields for Human Animation/submitted by Paul Knoll": {
          "authors": [
            "P Knoll"
          ],
          "url": "https://epub.jku.at/urn/urn:nbn:at:at-ubl:1-74368",
          "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.",
          "ref_ids": [
            "44"
          ],
          "2": "Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans Neural Body [44] introduces a neural representation that generates implicit fields at different frames from the same set of latent codes."
        },
        "Supplemental Materials for TAVA: Template-free Animatable Volumetric Actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall",
            "A Kanazawa"
          ],
          "url": "https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920417-supp.pdf",
          "ref_texts": "6. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
          "ref_ids": [
            "6"
          ],
          "2": "2, for the template-based baselines Animatable-NeRF [5] and NeuralBody [6], we use their official implementations.",
          "3": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [5,6,8]."
        },
        "Neural Articulated Radiance Field Supplemental Material": {
          "authors": [
            "ANXSS Lin",
            "T Harada"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2021/supplemental/Noguchi_Neural_Articulated_Radiance_ICCV_2021_supplemental.pdf",
          "ref_texts": "[5] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 5, 6",
          "ref_ids": [
            "5"
          ],
          "2": "This issue can be considered in future work, for example, by learning latent variables to account for both pose-dependent and pose-independent deformations similar to Neural Body [5]."
        },
        "\u59ff\u52e2\u63a8\u5b9a\u306b\u3088\u308b\u4eba\u9593\u62e1\u5f35\u306e\u305f\u3081\u306e\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u4eba\u5f71\u751f\u6210": {
          "authors": [
            "\u5409\u7530\u5320\u543e\uff0c \u8b1d\u6d69\u7136\uff0c \u5bae\u7530\u4e00\u4e58"
          ],
          "url": "https://dspace.jaist.ac.jp/dspace/handle/10119/17614",
          "ref_texts": "[13] Peng, S., Zhang, Y., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., & Zhou, X. (2021). Neural body: Impl icit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9054 -9063). ",
          "ref_ids": [
            "13"
          ],
          "1": "\u3053\u308c\u3089\u306e\u554f\u984c\u306f,\u6df1\u5ea6\u30bb\u30f3\u30b5\u304a \u3088\u3073\u9ad8\u6027\u80fd3D \u30b9\u30ad\u30e3\u30ca\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3084,Neural Body [13] \u306e\u624b\u6cd5\u3092\u5fdc\u7528\u3059\u308b\u3053\u3068\u3067\u89e3\u6c7a\u3067\u304d\u308b\u3068\u8003\u3048\u3089\u308c\u308b."
        }
      }
    },
    {
      "title": "representing volumetric videos as dynamic mlp maps",
      "id": 21,
      "valid_pdf_number": "35/43",
      "matched_pdf_number": "25/35",
      "matched_rate": 0.7142857142857143,
      "citations": {
        "4d gaussian splatting for real-time dynamic scene rendering": {
          "authors": [
            "Guanjun Wu",
            "Taoran Yi",
            "Jiemin Fang",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Qi Tian",
            "Xinggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[38] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4252\u2013",
          "ref_ids": [
            "38"
          ],
          "1": "[14, 27, 38, 50, 51, 53] are efficient methods to handle multi-view setups."
        },
        "Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction": {
          "authors": [
            "Ziyi Yang",
            "Xinyu Gao",
            "Wen Zhou",
            "Shaohui Jiao",
            "Yuqing Zhang",
            "Xiaogang Jin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Yang_Deformable_3D_Gaussians_for_High-Fidelity_Monocular_Dynamic_Scene_Reconstruction_CVPR_2024_paper.html",
          "ref_texts": "[39] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In CVPR, 2023. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Other methods seek to enhance the quality of dynamic neural rendering from various aspects, including segmenting static and dynamic objects in the scene [45, 48], incorporating depth information [1] to introduce geometric prior, introducing 2D CNN to encode scene priors [27, 39], and leveraging the redundant information in 20332."
        },
        "Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes": {
          "authors": [
            "Hua Huang",
            "Tian Sun",
            "Ziyi Yang",
            "Xiaoyang Lyu",
            "Pei Cao",
            "Xiaojuan Qi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Huang_SC-GS_Sparse-Controlled_Gaussian_Splatting_for_Editable_Dynamic_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[36] Sida Peng, Y unzhi Y an, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. InCVPR, 2023. 2",
          "ref_ids": [
            "36"
          ],
          "1": "Other methods used primitives [27], predicted MLP maps [36], or grid/planebased structures [1, 5, 38, 40, 47, 48] for speed and performance in various dynamic scenes."
        },
        "Spacetime gaussian feature splatting for real-time dynamic view synthesis": {
          "authors": [
            "Zhan Li",
            "Zhang Chen",
            "Zhong Li",
            "Yi Xu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Li_Spacetime_Gaussian_Feature_Splatting_for_Real-Time_Dynamic_View_Synthesis_CVPR_2024_paper.html",
          "ref_texts": "[71] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4252\u2013",
          "ref_ids": [
            "71"
          ],
          "1": "Extending static NeRF-related representations to dynamic scenes are also being actively explored [4, 12, 28, 38, 42, 47, 48, 56, 71, 77, 80, 87, 88, 90, 91]."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[39] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proc. of CVPR, 2023. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Inspired by these developments, several avatar reconstruction methods have been tailored to fast training [7, 12] or fast inference [6, 17, 39]."
        },
        "Human gaussian splatting: Real-time rendering of animatable avatars": {
          "authors": [
            "Arthur Moreau",
            "Jifei Song",
            "Helisa Dhamo",
            "Richard Shaw",
            "Yiren Zhou",
            "Eduardo Perez"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Moreau_Human_Gaussian_Splatting_Real-time_Rendering_of_Animatable_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In CVPR, 2023. 2",
          "ref_ids": [
            "45"
          ],
          "1": "These dynamic radiance fields can be used to train volumetric representation of humans in movement [10, 45] and to replay an existing video from a new camera viewpoint."
        },
        "4k4d: Real-time 4d view synthesis at 4k resolution": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Haotong Lin",
            "Guangzhao He",
            "Jiaming Sun",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html",
          "ref_texts": "[68] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4252\u20134262, 2023. 1, 2",
          "ref_ids": [
            "68"
          ],
          "2": "Inspired by static view synthesis approaches [20, 33, 97], some dynamic view synthesis methods [2, 49, 68, 89] increase the rendering speed by decreasing either the This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "5": "Inspired by their success, several approaches [2, 9, 48, 49, 53, 68, 75, 82, 82, 87, 88] have explored the possibility of real-time dynamic view synthesis."
        },
        "Masked space-time hash encoding for efficient dynamic scene reconstruction": {
          "authors": [
            "F Wang",
            "Z Chen",
            "G Wang",
            "Y Song"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/df31126302921ca9351fab73923a172f-Abstract-Conference.html",
          "ref_texts": "[67] S. Peng, Y . Yan, Q. Shuai, H. Bao, and X. Zhou. Representing volumetric videos as dynamic mlp maps. In CVPR, 2023.",
          "ref_ids": [
            "67"
          ],
          "1": "A more practical way of reconstructing dynamic scenes is by employing multi-view synchronized videos [33, 50, 104, 38, 1, 75, 10, 83, 85, 2, 67]."
        },
        "Gaussianbody: Clothed human reconstruction via 3d gaussian splatting": {
          "authors": [
            "M Li",
            "S Yao",
            "Z Xie",
            "K Chen"
          ],
          "url": "https://arxiv.org/abs/2401.09720",
          "ref_texts": "[48] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4252\u2013",
          "ref_ids": [
            "48"
          ],
          "1": "Some works [48, 49] aim to accelerate rendering in dynamic scenes, but they often require dense input images or additional geometry priors."
        },
        "Neca: Neural customizable human avatar": {
          "authors": [
            "Junjin Xiao",
            "Qing Zhang",
            "Zhan Xu",
            "Shi Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xiao_NECA_Neural_Customizable_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In CVPR, 2023. 3",
          "ref_ids": [
            "44"
          ],
          "1": "To better capture such intricate pose-dependent details, We adopt tri-plane representation [21, 44, 49] to learn pose-aware features."
        },
        "Tetrirf: Temporal tri-plane radiance fields for efficient free-viewpoint video": {
          "authors": [
            "Minye Wu",
            "Zehao Wang",
            "Georgios Kouros",
            "Tinne Tuytelaars"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_TeTriRF_Temporal_Tri-Plane_Radiance_Fields_for_Efficient_Free-Viewpoint_Video_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Y unzhi Y an, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pages 4252\u2013",
          "ref_ids": [
            "28"
          ],
          "1": "Most recently, several methods [28, 34, 40] have been developed to significantly improve the storage-performance trade-off.",
          "2": "Dynamic MLP Maps [28] and ReRF [40], on the other hand, This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "3": "Some approaches reconstruct dynamic scenes by conditioning an implicit representation on time [7, 10, 43] or time-varying latent codes [18, 28]."
        },
        "Videorf: Rendering dynamic radiance fields as 2d feature video streams": {
          "authors": [
            "Liao Wang",
            "Kaixin Yao",
            "Chengcheng Guo",
            "Zhirui Zhang",
            "Qiang Hu",
            "Jingyi Yu",
            "Lan Xu",
            "Minye Wu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wang_VideoRF_Rendering_Dynamic_Radiance_Fields_as_2D_Feature_Video_Streams_CVPR_2024_paper.html",
          "ref_texts": "[47] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. InCVPR, 2023. 2, 4",
          "ref_ids": [
            "47"
          ],
          "1": "Recently, several methods using 4D planes [3,13, 18, 53, 74], voxel grid [11], Fourier representation [64], residual layers [38] or dynamic MLP maps [47] to represent dynamic scene."
        },
        "Gear-NeRF: free-viewpoint rendering and tracking with motion-aware spatio-temporal sampling": {
          "authors": [
            "Xinhang Liu",
            "Wing Tai",
            "Keung Tang",
            "Pedro Miraldo",
            "Suhas Lohit",
            "Moitreya Chatterjee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_Gear-NeRF_Free-Viewpoint_Rendering_and_Tracking_with_Motion-aware_Spatio-Temporal_Sampling_CVPR_2024_paper.html",
          "ref_texts": "[63] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4252\u20134262, 2023.",
          "ref_ids": [
            "63"
          ],
          "1": "Neural Representations for Dynamic Scenes: NeRF-like representations have recently been extended to model dynamic scenes in high fidelity [17, 28, 33, 39, 46\u201348, 53, 63, 66, 84, 89, 90, 97]."
        },
        "Dynamic nerf: A review": {
          "authors": [
            "J Lin"
          ],
          "url": "https://arxiv.org/abs/2405.08609",
          "ref_texts": "[54] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2023. Representing Volumetric Videos as Dynamic MLP Maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4252\u20134262.",
          "ref_ids": [
            "54"
          ],
          "1": "[54] proposed a novel method to make a high efficient Dynamic NeRF, which is based on using a set of a shallow MLP network to represent the radiance field of each image frame of the input video data, and storing the parameters of the shallow MLP network in 2D grids."
        },
        "RoGUENeRF: a robust geometry-consistent universal enhancer for NeRF": {
          "authors": [
            "S Catley-Chandar",
            "R Shaw",
            "G Slabaugh"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73254-6_4",
          "ref_texts": "31. Peng, S., Yan, Y., Shuai, Q., Bao, H., Zhou, X.: Representing volumetric videos as dynamic mlp maps. In: CVPR (2023)",
          "ref_ids": [
            "31"
          ],
          "1": "The NeRF paradigm has been very popular in recent years [53], with active research in the field proposing new functionalities [27,30,31,48], applications [10,21,24] and also tackling some of the open challenges present in [23]."
        },
        "Compact neural volumetric video representations with dynamic codebooks": {
          "authors": [
            "H Guo",
            "S Peng",
            "Y Yan",
            "L Mou"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/ef63b00ad8475605b2eaf520747f61d4-Abstract-Conference.html",
          "ref_texts": "[30] S. Peng, Y . Yan, Q. Shuai, H. Bao, and X. Zhou. Representing volumetric videos as dynamic mlp maps. In IEEE Conf. Comput. Vis. Pattern Recog., 2023.",
          "ref_ids": [
            "30"
          ],
          "1": "Following NeRF\u2019s considerable success in modeling static scenes, some studies [19, 30, 17, 31, 35, 45] have sought to extend it to dynamic scenes.",
          "2": "NV[22] C-NeRF[47] D-NeRF[31] DyNeRF[17] DyMap[30] K-Planes[35] Ours PSNR\u2191 30.",
          "3": "Following the setting of DyMap [30], we select 100 frames from each video and use 90 percent of camera views for training and the rest for testing.",
          "4": "For experiments on NHR dataset, we compare with (1) Neural V olumes (NV) [22] (2) C-NeRF [47] (3) D-NeRF [31] (4) DyMap [30] (5) K-Planess [35].",
          "5": "For other methods, we directly adopt the results from [30] on NHR and [35, 45] on DyNeRF dataset."
        },
        "HPC: Hierarchical Progressive Coding Framework for Volumetric Video": {
          "authors": [
            "Z Zheng",
            "H Zhong",
            "Q Hu",
            "X Zhang",
            "L Song"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681107",
          "ref_texts": "[51] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2023. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4252\u20134262.",
          "ref_ids": [
            "51"
          ],
          "1": "Several methods [15, 51, 55, 59, 65, 66, 71] have been developed to compress explicit features of dynamic NeRF for efficiently storing and transmitting volumetric video.",
          "2": "Another category of methods[7, 17, 18, 22, 30, 51, 53, 57, 70, 76, 78] extends the radiance field to 4D spatialtemporal domains, where they model the time-varying radiance field in a higher-dimensional feature space for quicker training and rendering, though at the cost of increased storage needs.",
          "3": "Currently, there are some efforts[15, 28, 29, 51, 55] underway to apply compression techniques in the NeRF domain."
        },
        "Denser: 3d gaussians splatting for scene reconstruction of dynamic urban environments": {
          "authors": [
            "MA Mohamad",
            "G Elghazaly",
            "A Hubert"
          ],
          "url": "https://arxiv.org/abs/2409.10041",
          "ref_texts": "[23] S. Peng, Y. Yan, Q. Shuai, H. Bao, and X. Zhou, \u201cRepresenting volumetric videos as dynamic mlp maps,\u201d in CVPR, pp. 4252\u20134262, 2023.",
          "ref_ids": [
            "23"
          ],
          "1": "R ELATED WORK Dynamic scene representation has seen remarkable progress, especially in the domain of 4D neural scene representations focusing on scenes of single dynamic object, where time is considered as an additional dimension besides spatial ones [18], [19], [20], [21], [22], [23], [24]."
        },
        "AI-Driven Innovations in Volumetric Video Streaming: A Review": {
          "authors": [
            "E Entezami",
            "H Guan"
          ],
          "url": "https://arxiv.org/abs/2412.12208",
          "ref_texts": "[80] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4252\u2013",
          "ref_ids": [
            "80"
          ],
          "4": "[80] proposed using three 2D MLP maps for the spatial dimensions XY , XZ , and Y Zin each video frame."
        }
      }
    },
    {
      "title": "quickpose: real-time multi-view multi-person pose estimation in crowded scenes",
      "id": 34,
      "valid_pdf_number": "6/7",
      "matched_pdf_number": "2/6",
      "matched_rate": 0.3333333333333333,
      "citations": {
        "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos": {
          "authors": [
            "F Lu",
            "Z Dong",
            "J Song",
            "O Hilliges"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73668-1_13.pdf",
          "ref_texts": "76. Zhou,Z.,Shuai,Q.,Wang,Y.,Fang,Q.,Ji,X.,Li,F.,Bao,H.,Zhou,X.:Quickpose: Real-time multi-view multi-person pose estimation in crowded scenes. In: ACM SIGGRAPH 2022 Conference Proceedings. pp. 1\u20139 (2022)",
          "ref_ids": [
            "76"
          ],
          "1": "One straightforward idea of most methods [2,4,19,22,72,76] is to formulate the problem into cross-view matching and association problems."
        },
        "Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud": {
          "authors": [
            "J Jiang",
            "J Chen",
            "HY Au",
            "M Chen",
            "W Xue"
          ],
          "url": "https://arxiv.org/abs/2502.02936",
          "ref_texts": "[18] Z. Zhou, Q. Shuai, Y. Wang, Q. Fang, X. Ji, F. Li, H. Bao, and X. Zhou, \u201cQuickpose: Real-time multi-view multi-person pose estimation in crowded scenes,\u201d in In proceedings of SIGGRAPH , 2022, pp. 1\u20139.",
          "ref_ids": [
            "18"
          ],
          "1": "Current studies [12], [14], [15], [16], [17], [18] show that multi-view multi-person 3D HPE can be formulated as a multi-stage framework.",
          "2": "[18] associate joint at partand body-level.",
          "3": "4 MM\u201922 QuickPose [18] 99.",
          "4": "We compared our method with recent state-of-the-art 3D HPE methods which can be generally divided into two categories: (1) optimizationbased approaches [14], [15], [18], [51] and (2) learning-based approaches [21], [22], [23], [25], [28], [29], [30].",
          "5": "9 \u223c40 QuickPose [18] 98.",
          "6": "Table 5 presents the inference speed of three optimization-based approaches [14], [15], [18] and five learning-based approaches [21], [22], [23], [28], [30] (Our framework falls into the second category) as well as the performance of our model when the number of input views TABLE 7: Performance for JCSAT variants on the Shelf dataset.",
          "7": "We would like to clarify that, while QuickPose [18] appears to operate at a higher speed than our method with a small accuracy decrease of 0.",
          "8": "[18] Z."
        }
      }
    },
    {
      "title": "dyn-e: local appearance editing of dynamic neural radiance fields",
      "id": 39,
      "valid_pdf_number": "7/8",
      "matched_pdf_number": "6/7",
      "matched_rate": 0.8571428571428571,
      "citations": {
        "Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Jay Zhangjie",
            "Weijia Mao",
            "Yuchao Gu",
            "Rui Zhao",
            "Jussi Keppo",
            "Ying Shan",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_DynVideo-E_Harnessing_Dynamic_NeRF_for_Large-Scale_Motion-_and_View-Change_Human-Centric_CVPR_2024_paper.html",
          "ref_texts": "[64] Shangzhan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, and Xiaowei Zhou. Dyn-e: Local appearance editing of dynamic neural radiance fields. arXiv preprint arXiv:2307.12909, 2023. 3",
          "ref_ids": [
            "64"
          ],
          "1": "Subsequent works such as Control4D [49] and Dyn-E [64] propose to edit the contents of dynamic NeRFs."
        },
        "Heromaker: Human-centric video editing with motion priors": {
          "authors": [
            "S Liu",
            "Z Zhao",
            "Y Zhi",
            "Y Zhao",
            "B Huang",
            "S Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681147",
          "ref_texts": "[63] Shangzhan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, and Xiaowei Zhou. 2023. Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields. arXiv:2307.12909 [cs.CV]",
          "ref_ids": [
            "63",
            "cs\\.CV"
          ],
          "1": "Dyn-E [63] and Control4D [51] propose to edit the contents of dynamic NeRFs."
        }
      }
    },
    {
      "title": "you don't only look once: constructing spatial-temporal memory for integrated 3d object detection and tracking",
      "id": 35,
      "valid_pdf_number": "6/9",
      "matched_pdf_number": "6/6",
      "matched_rate": 1.0,
      "citations": {
        "Recent advances in embedding methods for multi-object tracking: A survey": {
          "authors": [
            "G Wang",
            "M Song",
            "JN Hwang"
          ],
          "url": "https://arxiv.org/abs/2205.10766",
          "ref_texts": "[238] J. Sun, Y. Xie, S. Zhang, L. Chen, G. Zhang, H. Bao, and X. Zhou, \u201cYou don\u2019t only look once: Constructing spatial-temporal mem20 ory for integrated 3d object detection and tracking,\u201d in ICCV, 2021.",
          "ref_ids": [
            "238"
          ],
          "1": "Since LiDAR has a good measure of depth, LiDAR-based trackers [199], [235], [236], [237], [238] have become popular in autonomous driving scenarios."
        },
        "A lightweight and detector-free 3d single object tracker on point clouds": {
          "authors": [
            "Y Xia",
            "Q Wu",
            "W Li",
            "AB Chan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10050385/",
          "ref_texts": "[35] Jiaming Sun, Yiming Xie, Siyu Zhang, Linghao Chen, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. You don\u2019t only look once: Constructing spatial-temporal memory for integrated 3d object detection and tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3185\u20133194, 2021. 2",
          "ref_ids": [
            "35"
          ],
          "1": ", motion-conditioned detection [15, 35, 48] for associating objects in consecutive frames and motionguided multiple proposal generation [24,36]."
        },
        "A survey of robust 3d object detection methods in point clouds": {
          "authors": [
            "W Zimmer",
            "E Ercelik",
            "X Zhou",
            "XJD Ortiz"
          ],
          "url": "https://arxiv.org/abs/2204.00106",
          "ref_texts": "[48] J. Sun, Y . Xie, S. Zhang, L. Chen, G. Zhang, H. Bao, and X. Zhou, \u201cYou don\u2019t only look once: Constructing spatial-temporal memory for integrated 3d object detection and tracking,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp.",
          "ref_ids": [
            "48"
          ],
          "1": "In [48], object proposals are generated based on temporal occupancy maps, odometry data, and Kalman filter-based motion estimations."
        },
        "Pushing point cloud compression to the edge": {
          "authors": [
            "Ziyu Ying"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9923794/",
          "ref_texts": "[78] J. Sun, Y . Xie, S. Zhang, G. Zhang, H. Bao, and X. Zhou, \u201cYou Don\u2019t Only Look Once: Constructing spatial-temporal memory for integrated 3d object detection and tracking,\u201d ICCV, 2021.",
          "ref_ids": [
            "78"
          ],
          "1": "2) Point Cloud Analysis:To analyze objects/scenes in PCs, 3D convolutional neural networks (CNNs) have been widely used in techniques like 3D shape classification [66], [67], [89], [91], object detection [38], [45], [65], tracking [22], [78], or segmentation [10], [66], [67], [89]."
        },
        "InScope: A New Real-world 3D Infrastructure-side Collaborative Perception Dataset for Open Traffic Scenarios": {
          "authors": [
            "X Zhang",
            "Y Li",
            "J Wang",
            "X Qin",
            "Y Shen",
            "Z Fan"
          ],
          "url": "https://arxiv.org/abs/2407.21581",
          "ref_texts": "[57] J. Sun, Y . Xie, S. Zhang, L. Chen, G. Zhang, H. Bao, X. Zhou, You don\u2019t only look once: Constructing spatial-temporal memory for integrated 3d object detection and tracking, in: Proc. IEEE Int. Conf. Comput. Vis. (ICCV), 2021, pp. 3185\u20133194.",
          "ref_ids": [
            "57"
          ],
          "1": "Baselines The 3D multiobject tracking methods can be divided into two paradigms [51, 52, 53, 54]: joint detection and tracking (JDT) [41, 50, 55] and tracking-by-detection (TBD) [56, 57, 58, 59, 60]."
        },
        "A Survey of Robust LiDAR-based 3D Object Detection Methods for Autonomous Driving": {
          "authors": [
            "W Zimmer",
            "E Er\u00e7elik",
            "X Zhou",
            "XJ Diaz Ortiz",
            "AC Knoll"
          ],
          "url": "https://mediatum.ub.tum.de/1713562",
          "ref_texts": "[48] J. Sun, Y . Xie, S. Zhang, L. Chen, G. Zhang, H. Bao, and X. Zhou, \u201cYou don\u2019t only look once: Constructing spatial-temporal memory for integrated 3d object detection and tracking,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp.",
          "ref_ids": [
            "48"
          ],
          "1": "In [48], object proposals are generated based on temporal occupancy maps, odometry data, and Kalman filter-based motion estimations.",
          "2": "[48] J."
        }
      }
    },
    {
      "title": "onepose++: keypoint-free one-shot object pose estimation without cad models",
      "id": 14,
      "valid_pdf_number": "56/65",
      "matched_pdf_number": "48/56",
      "matched_rate": 0.8571428571428571,
      "citations": {
        "Foundationpose: Unified 6d pose estimation and tracking of novel objects": {
          "authors": [
            "Bowen Wen",
            "Wei Yang",
            "Jan Kautz",
            "Stan Birchfield"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.html",
          "ref_texts": "[18] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. OnePose++: Keypoint-free oneshot object pose estimation without CAD models. Advances in Neural Information Processing Systems (NeurIPS) , 35: 35103\u201335115, 2022. 1, 2, 6, 7",
          "ref_ids": [
            "18"
          ],
          "3": "OnePose [53] and its extension OnePose++ [18] leverage structure-from-motion (SfM) for object modeling and pretrain 2D-3D matching networks to solve the pose from correspondences."
        },
        "Sam-6d: Segment anything model meets zero-shot 6d object pose estimation": {
          "authors": [
            "Jiehong Lin",
            "Lihua Liu",
            "Dekun Lu",
            "Kui Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lin_SAM-6D_Segment_Anything_Model_Meets_Zero-Shot_6D_Object_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[17] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. Advances in Neural Information Processing Systems, 35:35103\u201335115, 2022. 3",
          "ref_ids": [
            "17"
          ],
          "2": "OnePose [53] matches the pixel descriptors of proposals with the aggregated point descriptors of the point sets constructed by Structure from Motion (SfM) for 2D-3D correspondence, while OnePose++ [17] further improves it with a keypoint-free SfM and a sparse-to-dense 2D-3D matching model."
        },
        "Detector-free structure from motion": {
          "authors": [
            "Xingyi He",
            "Jiaming Sun",
            "Yifan Wang",
            "Sida Peng",
            "Qixing Huang",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/He_Detector-Free_Structure_from_Motion_CVPR_2024_paper.html",
          "ref_texts": "[19] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without CAD models. In NeurIPS, 2022. 2, 6",
          "ref_ids": [
            "19"
          ],
          "1": "[52] and OnePose++ [19] also eliminate feature detection by performing coarse grid-level matching first and then refining 2D points for subpixel accuracy.",
          "2": "2) Detector-free SfM baseline LoFTR [44] matches with PixSfM [26] and OnePose++ [19], where these methods are fed with LoFTR\u2019s quantized matches, same as our pipeline."
        },
        "Pope: 6-dof promptable pose estimation of any object in any scene with one reference": {
          "authors": [
            "Zhiwen Fan",
            "Panwang Pan",
            "Peihao Wang",
            "Yifan Jiang",
            "Dejia Xu",
            "Zhangyang Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Fan_POPE_6-DoF_Promptable_Pose_Estimation_of_Any_Object_in_Any_CVPRW_2024_paper.html",
          "ref_texts": "[20] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without cad models. arXiv preprint arXiv:2301.07673, 2023. 2, 3, 4, 6, 7",
          "ref_ids": [
            "20"
          ],
          "4": "OnePose [54] and OnePose++ [20] construct a sparse point cloud from the RGB sequences of all support viewpoints and then determine the object poses by matching the target view with the sparse point cloud.",
          "8": "The OnePose++ Dataset [20] supplements the original OnePose dataset with 40 household low-textured objects.",
          "9": "Qualitative results on the OnePose [54] and OnePose++ [20] datasets."
        },
        "Genflow: Generalizable recurrent flow for 6d pose refinement of novel objects": {
          "authors": [
            "Sungphill Moon",
            "Hyeontae Son",
            "Dongcheol Hur",
            "Sangwook Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Moon_GenFlow_Generalizable_Recurrent_Flow_for_6D_Pose_Refinement_of_Novel_CVPR_2024_paper.html",
          "ref_texts": "[23] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without CAD models. In Advances in Neural Information Processing Systems, 2022. 2",
          "ref_ids": [
            "23"
          ],
          "1": "A few works attempted the CAD modelfree pose estimation [23, 24, 45, 53, 58] to cover everyday objects, but the results were only evaluated on datasets without occlusion and cluttered scenes."
        },
        "Matchu: Matching unseen objects for 6d pose estimation from rgb-d images": {
          "authors": [
            "Junwen Huang",
            "Hao Yu",
            "Ting Yu",
            "Nassir Navab",
            "Slobodan Ilic",
            "Benjamin Busam"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Huang_MatchU_Matching_Unseen_Objects_for_6D_Pose_Estimation_from_RGB-D_CVPR_2024_paper.html",
          "ref_texts": "[16] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without CAD models. In NeurIPS, 2022. 3",
          "ref_ids": [
            "16"
          ],
          "1": "Recently, Gen6D [37], OnePose [47] and OnePose++[16], utilize SfM and feature matching techniques to align a posed set of images of a given object to a target view using refined nearest neighbor image retrieval [37] or 2D-3D image matching [46]."
        },
        "Learning to estimate 6dof pose from limited data: A few-shot, generalizable approach using rgb images": {
          "authors": [
            "P Pan",
            "Z Fan",
            "BY Feng",
            "P Wang",
            "C Li"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550594/",
          "ref_texts": "[34] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. arXiv preprint arXiv:2301.07673, 2023. 1, 2, 3, 4, 5, 6, 7",
          "ref_ids": [
            "34"
          ],
          "1": "While OnePose++ [34] (middle left) and Gen6D [58] (bottom left) achieve promising accuracy given sufficient support views, their performance significantly degrades when few views are available.",
          "2": "To overcome these limitations, generalizable (model-free) methods, OnePose [92] and OnePose++ [34] propose a one-shot object pose estimation method that can be used in arbitrary scenarios, which first reconstructs sparse object point clouds and then establishes 2D-3D correspondences between keypoints in the query image and the point cloud to estimate the object pose.",
          "4": "Other model-free methods such as Gen6D [58], OnePose [92] and OnePose++ [34], use only a set of reference/support images with annotated poses for the pose estimation of the object in the target image.",
          "5": "OnePose series [92, 34] first reconstruct the sparse point cloud from the RGB sequences of all support viewpoints, and match the target view with the sparse point cloud to determine the object poses.",
          "7": "Preliminary Generalizable 6DoF Estimation Framework In the model-free OnePose series [92, 34], object poses are determined by reconstructing a sparse point cloud from RGB sequences of support viewpoints and matching it with the 2D feature in the target view.",
          "16": "The first category is generalizable methods [58, 34] with the same training datasets as ours."
        },
        "Long-term visual localization with mobile sensors": {
          "authors": [
            "Shen Yan",
            "Yu Liu",
            "Long Wang",
            "Zehong Shen",
            "Zhen Peng",
            "Haomin Liu",
            "Maojun Zhang",
            "Guofeng Zhang",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yan_Long-Term_Visual_Localization_With_Mobile_Sensors_CVPR_2023_paper.html",
          "ref_texts": "[26] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without CAD models. In NeurIPS, 2022. 2, 4, 5",
          "ref_ids": [
            "26"
          ],
          "1": "In the second stage, inspired by the recent CADfree object pose estimation method, OnePose++ [26], we design a transformer-based network to directly match 3D points of the retrieved sub-map to dense 2D pixels of the query photo in a coarse-to-fine manner.",
          "2": "As for the time-consumption issue, inspired by recent 6-DoF pose estimation works [26,61], we aim to directly match 3D sub-map and 2D query image in one-shot with selfand cross-attention modules.",
          "3": "Similar to [26, 60], we first use positional encoding to augment the features {\u02dcF2D, \u02dcF3D}, resulting in {\u02dcFpe 2D, \u02dcFpe 3D}.",
          "4": "Following [26, 60], we supervise the network by a focal loss [38] for coarse matching and variance-weighted \u21132 loss for fine refinement."
        },
        "6dgs: 6d pose estimation from a single image and a 3d gaussian splatting model": {
          "authors": [
            "B Matteo",
            "T Tsesmelis",
            "S James",
            "F Poiesi"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-72943-0_24.pdf",
          "ref_texts": "15. He, X., Sun, J., Wang, Y., Huang, D., Bao, H., Zhou, X.: Onepose++: Keypointfree one-shot object pose estimation without cad models. In: NeurIPS (2022)",
          "ref_ids": [
            "15"
          ],
          "1": "OnePose++ [15] instead adopts a multi-modal approach matching a point cloud with an image."
        },
        "Dvmnet: Computing relative pose for unseen objects beyond hypotheses": {
          "authors": [
            "Chen Zhao",
            "Tong Zhang",
            "Zheng Dang",
            "Mathieu Salzmann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhao_DVMNet_Computing_Relative_Pose_for_Unseen_Objects_Beyond_Hypotheses_CVPR_2024_paper.html",
          "ref_texts": "[14] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models.Advances in Neural Information Processing Systems, 35:35103\u201335115, 2022. 1, 2",
          "ref_ids": [
            "14"
          ],
          "1": "Object pose estimation is then carried out through template matching [21, 29] or by establishing 2D-3D correspondences [14, 32].",
          "2": "Object pose estimation is then carried out by employing either a template matching strategy [21] or a 3D object reconstruction technique [14, 32]."
        },
        "Posematcher: One-shot 6d object pose estimation by deep feature matching": {
          "authors": [
            "Pedro Castro",
            "Kyun Kim"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Castro_PoseMatcher_One-Shot_6D_Object_Pose_Estimation_by_Deep_Feature_Matching_ICCVW_2023_paper.html",
          "ref_texts": "[12] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without CAD models. In Advances in Neural Information Processing Systems, 2022. 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "12"
          ],
          "1": "Particularly, OnePose [38] and OnePose++ [12] make use of existing state of the art pre-trained descriptor extractors on top of which a pose estimation pipeline is built.",
          "2": "Just by redesigning the training pipeline, we can improve OnePose++[12] without additional changes to its methodology.",
          "3": "We also introduce a new efficient image to object attention layer which we call IOLayer, which reduces both parameters when compared to the modules used by OnePose++ [12] and also separates the two input modalities, an image and a pointcloud, allowing the model to optimize weights specifically for either image or pointcloud keypoints.",
          "6": "In order to optimize through the matching task we apply the differentiable dual-softmax operator as proposed by LoFTR [37] and subsequently used by both OnePose and OnePose++ [38, 12].",
          "7": "For 2D keypoints we use the sinusoidal fixed version used by DETR [3] while for 3D positional encoding we use a simple 3-layer MLP as used by SurfEmb [10] and OnePose++ [12].",
          "8": "We flatten both feature maps and apply self and cross attention layers following other feature matching papers [34, 37, 38, 12, 44] to generate more easily separable features on each set.",
          "11": "Pose Refinement We add a fine level 2D-refinement post-processing step as described by LoFTR [37] and OnePose++[12].",
          "15": "Ablation Studies Our new training pipeline elevates the results from OnePose++ [12].",
          "16": "PoseMatcher almost achieves the same level of accuracy as OnePose++ [12] using only 75% of the available training images.",
          "17": "With this simple addition, we show that we can improve OnePose++ [12] without any additional changes."
        },
        "Zero123-6d: Zero-shot novel view synthesis for rgb category-level 6d pose estimation": {
          "authors": [
            "F Di Felice",
            "A Remus",
            "S Gasperini"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10802157/",
          "ref_texts": "[3] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without CAD models,\u201d in Advances in Neural Information Processing Systems, 2022.",
          "ref_ids": [
            "3"
          ],
          "2": "To overcome such problems, OnePose [2] and OnePose++ [3] deal with novel objects by acquiring a full RGB scan of the item followed by a structure from the motion pipeline."
        },
        "Open-vocabulary object 6D pose estimation": {
          "authors": [
            "Jaime Corsetti",
            "Davide Boscaini",
            "Changjae Oh",
            "Andrea Cavallaro",
            "Fabio Poiesi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Corsetti_Open-Vocabulary_Object_6D_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[13] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou. OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models. In NeurIPS, 2022. 1, 2, 3",
          "ref_ids": [
            "13"
          ],
          "2": "OnePose++ [13] extends OnePose to address pose estimation for low-textured objects, but maintains the same test-time requirements as OnePose."
        },
        "iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching": {
          "authors": [
            "Y Sun",
            "X Wang",
            "Y Zhang",
            "J Zhang",
            "C Jiang"
          ],
          "url": "https://arxiv.org/abs/2312.09031",
          "ref_texts": "13. He, X., Sun, J., Wang, Y., Huang, D., Bao, H., Zhou, X.: Onepose++: Keypointfree one-shot object pose estimation without cad models. NeurIPS35, 35103\u201335115",
          "ref_ids": [
            "13"
          ],
          "1": "Some approaches also use traditional Perspective-nPoint (PnP) algorithms, combining them with neural networks to match threedimensional and two-dimensional points [13,29,31,34], ultimately determining relative poses.",
          "2": "Moreover, there are methods that concentrate on matching between images and target point clouds or 3D models [5,10,12,13,34], achieving notable outcomes."
        },
        "Gs-pose: Category-level object pose estimation via geometric and semantic correspondence": {
          "authors": [
            "P Wang",
            "T Ikeda",
            "R Lee",
            "K Nishiwaki"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73383-3_7",
          "ref_texts": "[9] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. Advances in Neural Information Processing Systems, 35:35103\u201335115, 2022. 1, 2, 4",
          "ref_ids": [
            "9"
          ],
          "1": "Inspired by OnePose [9, 25], we utilize a transformer structure with multiple selfand cross-attention layers to fuse both semantic and geometric features, as shown under (2) in Fig."
        },
        "NeRF-feat: 6D object pose estimation using feature rendering": {
          "authors": [
            "SR Vutukur",
            "H Brock",
            "B Busam",
            "T Birdal"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550810/",
          "ref_texts": "[14] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without CAD models. In Advances in Neural Information Processing Systems, 2022. 2",
          "ref_ids": [
            "14"
          ],
          "1": "Recent Methods, Ove6D, OSOP, Latent Fusion, OnePose, OnePose++[5, 14, 33, 38, 42] propose approaches that can generalize to any object."
        },
        "Marrying nerf with feature matching for one-step pose estimation": {
          "authors": [
            "R Chen",
            "Y Cong",
            "Y Ren"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610766/",
          "ref_texts": "[13] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without CAD models,\u201d in Advances in Neural Information Processing Systems , 2022.",
          "ref_ids": [
            "13"
          ],
          "1": "To further avoid tedious retraining for each novel object, recent methods [12], [13] learn from the traditional pipeline of SfM (Struction-from-Motion) to estimate object poses via feature matching.",
          "2": "Moreover, comparing to former keypoint-based method [12], [13], this eases the difficulty of building 2D-3D correspondences in traditional SFMbased methods, which needs to find 2D matches between multiple input frames and the target image.",
          "3": "Onepose++ [13] later improves it with a keypoint-free reconstruction framework."
        },
        "Certifiable object pose estimation: Foundations, learning models, and self-training": {
          "authors": [
            "R Talak",
            "LR Peng",
            "L Carlone"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10137553/",
          "ref_texts": "[43] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without CAD models,\u201d in Advances in Neural Information Processing Systems (NIPS) , 2022.",
          "ref_ids": [
            "43"
          ],
          "1": "Recent work also attempts object pose estimation, without the availability of the object CAD model [43]."
        },
        "NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models": {
          "authors": [
            "F Milano",
            "JJ Chung",
            "H Blum"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801399/",
          "ref_texts": "[22] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,\u201d in NeurIPS, 2022. 1, 2, 5, 6, 7",
          "ref_ids": [
            "22"
          ],
          "1": "With this goal in mind, a number of model-free approaches for 6D pose estimation have been proposed, which typically construct a Structure-from-Motion (SfM)-based model of the object and later relocalize the camera with respect to it [21], [22].",
          "2": "CAD model-free object pose estimation SfM-based methods are the current state of the art for CAD model-free object pose estimation [21], [22].",
          "3": "These methods assume a set of reference images, which are used to construct a sparse [21] or semi-dense [22] point cloud, using SfM.",
          "4": "NeuS2-based object model and dataset Similarly to other CAD-model-free methods [21], [22], in our setting we assume to have available a small set of N images {Ii} (with N \u2248 100), captured at roughly uniformlydistributed viewpoints around the object.",
          "5": "However, in contrast to the sparse or semi-dense cloud of triangulated points that form the SfM-based object models in [21], [22], we learn a dense object model based on NeuS2 [1].",
          "6": "We compare our method to two recent state-of-the-art approaches, Gen6D [21] and OnePose++ [22], both of which, similarly to our method, do not require a CAD model of the objects of interest.",
          "9": "However, we select [21] and [22] for comparison in our real-world experiments because they represent the best viable option in a robotic scenario."
        },
        "High-resolution open-vocabulary object 6D pose estimation": {
          "authors": [
            "J Corsetti",
            "D Boscaini",
            "F Giuliari",
            "C Oh"
          ],
          "url": "https://arxiv.org/abs/2406.16384",
          "ref_texts": "[9] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,\u201d in NeurIPS, 2022.",
          "ref_ids": [
            "9"
          ],
          "5": "Exemplary methods include OnePose++ [9] and Gen6D [13].",
          "6": "This is often necessary to deal with highly occluded datasets [33], and is common both in the classic setting [4], [5] and in the unseen-object setting [9], [22]."
        },
        "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with 3D Gaussian Splatting": {
          "authors": [
            "D Cai",
            "J Heikkil\u00e4",
            "E Rahtu"
          ],
          "url": "https://arxiv.org/abs/2403.10683",
          "ref_texts": "[15] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. Advances in Neural Information Processing Systems, 35:35103\u201335115, 2022. 1, 2, 3, 6, 7, 8",
          "ref_ids": [
            "15"
          ],
          "1": "Alternatively, OnePose [47] and OnePose++ [15] explicitly reconstruct a 3D point cloud from the reference images via local feature matching.",
          "2": "Popular choices include 2D feature maps [28], 3D point clouds[15, 47], latent 3D models [37], and 3D CAD models[43], to name a few.",
          "3": "We evaluate the proposed framework, called GSPose, on the LINEMOD[16] and OnePose-LowTexture[15] datasets and obtain new state-of-the-art results on both benchmarks.",
          "4": "This type of work[1, 15, 28, 35, 43, 47, 48] removes the requirement of the object specific-training and can perform pose estimation for previously unseen objects during inference.",
          "5": "To avoid 3D CAD models, recent works [15, 28, 35, 47] resort to capturing object multi-view images with known poses as reference data for pose estimation.",
          "6": "OnePose series [15, 47] utilize the posed RGB images to reconstruct 3D object point clouds and establish explicit 2D-3D correspondences between 2D query images and the reconstructed 3D point clouds to solve the 6D pose.",
          "8": "We utilize the synthetic MegaPose dataset [20] for training and the real-world datasets LINEMOD [16] and OnePose-LowTexture [15] for evaluation.",
          "10": "OnePose-LowTexture [15] is a challenging dataset with low-texture or texture-less objects, from which eight scanned objects are utilized for evaluation.",
          "11": "We follow OnePose++ [15] and select the first video as the reference and the other as query data.",
          "12": "For comparison, we assess GS-Pose against several state-of-the-art methods: Gen6D [28], Cas6D [35], OnePose [47], OnePose++ [15], and MFOS [21].",
          "13": "6 OnePose++[15] \u2713 31.",
          "14": "1 OnePose++[15] \u2713 97.",
          "15": "7 OnePose++[15] \u2713 89.",
          "16": "Quantitative results on each object in OnePoseLowTexure [15] regarding the ADD(S)@0.",
          "18": "When using the 2D detection results predicted by YOLOv5 [49], as in OnePose [47] and OnePose++ [15], GS-Pose further improves the ADD(S)@0.",
          "20": "We further compare GS-Pose against the baselines [15, 28, 47] on OnePoseLowTexture [15].",
          "21": "To alleviate this, OnePose++ [15] employs the keypointfree LoFTR [46] for feature matching and significantly improves the result to 88.",
          "22": "Data Capture with ARKit We follow OnePose and OnePose++ [15, 47] and leverage the off-the-shelf ARKit1 to capture the RGB reference images using an iPhone.",
          "23": "We kindly refer the reader to OnePose/OnePose++ [15, 47] for more details."
        },
        "UNOPose: Unseen Object Pose Estimation with an Unposed RGB-D Reference Image": {
          "authors": [
            "X Liu",
            "G Wang",
            "R Zhang",
            "C Zhang",
            "F Tombari"
          ],
          "url": "https://arxiv.org/abs/2411.16106",
          "ref_texts": "[23] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without cad models. NeurIPS, 35: 35103\u201335115, 2022. 1, 3",
          "ref_ids": [
            "23"
          ],
          "3": "For example, OnePose [70] and OnePose++ [23] reconstruct the 3D point cloud of an unseen object to establish 2D-3D correspondences, whereas SAM-6D [46] builds 3D-3D correspondences."
        },
        "Novel Object 6D Pose Estimation with a Single Reference View": {
          "authors": [
            "J Liu",
            "W Sun",
            "K Zeng",
            "J Zheng",
            "H Yang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2503.05578",
          "ref_texts": "[25] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. InAdvances in Neural Information Processing Systems , pages 35103\u2013",
          "ref_ids": [
            "25"
          ],
          "5": "SinRef-6D outperforms OnePose [64] and OnePose++ [25], achieving comparable accuracy to LatentFusion [57] and FS6D [27]."
        },
        "GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation": {
          "authors": [
            "W Li",
            "H Xu",
            "J Huang",
            "H Jung",
            "PKT Yu",
            "N Navab"
          ],
          "url": "https://arxiv.org/abs/2502.04293",
          "ref_texts": "[20] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without CAD models. In NeurIPS, 2022. 2",
          "ref_ids": [
            "20"
          ],
          "1": "Methods like OnePose [57], OnePose++ [20], and CosyPose [31] employ Structure from Motion (SfM) to match features across views, while approaches such as NeRFPose [33], GS-Pose [2], and FoundationPose [71] utilize Neural Radiance Fields or 3D Gaussian Splatting [29] for flexible reconstruction."
        },
        "VAPO: Visibility-Aware Keypoint Localization for Efficient 6DoF Object Pose Estimation": {
          "authors": [
            "R Lian",
            "Y Lin",
            "LJ Latecki",
            "H Ling"
          ],
          "url": "https://arxiv.org/abs/2403.14559",
          "ref_texts": "[41] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without cad models,\u201d Advances in Neural Information Processing Systems , vol. 35, pp.",
          "ref_ids": [
            "41"
          ],
          "1": "OnePose++ [41] instead uses a keypoint-free feature matching pipeline for low-textured objects."
        },
        "OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes from Monocular RGB": {
          "authors": [
            "Y Lin",
            "Y Zhao",
            "FJ Chu",
            "X Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2410.06694",
          "ref_texts": "[10] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnePose++: Keypoint-free one-shot object pose estimation without CAD models,\u201d Advances in Neural Information Processing Systems (NeurIPS) , 2022.",
          "ref_ids": [
            "10"
          ],
          "1": "Prior efforts in this domain have predominantly been model-based, relying on CAD models [1]\u2013[3], predetermined categories [4]\u2013[6], or registration at inference time [7]\u2013[10].",
          "2": "Subsequent approaches at the category level employed CAD models for offline training and online template matching [6], [30], evolving to handle novel, unseen objects through techniques like 2D3D feature matching and 2D-2D image matching [8]\u2013[10], [31], [32].",
          "5": "We adopt the LoFTR outdoor model, pretrained on MegaDepth [35], following the previous object pose estimation method [10]."
        },
        "Dense-SfM: Structure from Motion with Dense Consistent Matching": {
          "authors": [
            "JM Lee",
            "S Yoo"
          ],
          "url": "https://arxiv.org/abs/2501.14277",
          "ref_texts": "[23] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without CAD models. In Advances in Neural Information Processing Systems, 2022. 1",
          "ref_ids": [
            "23"
          ],
          "1": "Recently, there has been an increasing demand for dense and accurate 3D reconstructions in various 3D vision tasks, including dense reconstruction [16, 67], object pose estimation [4, 23, 58] and neural rendering [27, 75]."
        },
        "Generalizable Single-view Object Pose Estimation by Two-side Generating and Matching": {
          "authors": [
            "Y Sun",
            "C Sun",
            "Y Liu",
            "Y Ma",
            "SM Yiu"
          ],
          "url": "https://arxiv.org/abs/2411.15860",
          "ref_texts": "[18] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without CAD models. In Advances in Neural Information Processing Systems , 2022. 2, 3",
          "ref_ids": [
            "18"
          ],
          "1": "In recent years, advancements in deep learning have led to increased focus on category-agnostic pose estimation [7, 18, 29, 36, 57, 77, 78] and resulted in continuous accuracy improvement.",
          "2": "Nevertheless, although these category-agnostic methods [7, 18, 29, 36, 57, 77, 78] demonstrate effectiveness in predicting unseen object poses in some cases, they still suffer from two major challenges.",
          "3": "Methods in this category aim to infer object pose with a set of dense reference images with known poses, leveraging category-specific priors [54, 61] or category-agnostic priors [2, 7, 18, 29, 36, 66, 77, 78]."
        },
        "HIPPo: Harnessing Image-to-3D Priors for Model-free Zero-shot 6D Pose Estimation": {
          "authors": [
            "Y Liu",
            "Z Jiang",
            "B Xu",
            "G Wu",
            "Y Ren",
            "T Cao",
            "B Liu"
          ],
          "url": "https://arxiv.org/abs/2502.10606",
          "ref_texts": "[10] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without cad models,\u201d Advances in Neural Information Processing Systems , vol. 35, pp.",
          "ref_ids": [
            "10"
          ],
          "2": "Thus, some research [1], [10], [11], [9] focuses on using reference images or a video of the object as input instead of a 3D model.",
          "5": "Similarly, OnePose [9] and OnePose++ [10] propose recording a video scan of the object and utilizing Structure-from-Motion [12] to reconstruct the object."
        },
        "EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models": {
          "authors": [
            "Z Hong",
            "K Zheng",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801359/",
          "ref_texts": "[30] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without cad models,\u201d NeurIPS, vol. 35, pp. 35 103\u201335 115, 2022.",
          "ref_ids": [
            "30"
          ],
          "1": "OnePose++ [30] proposed to substitute the COLMAP [25] with a learning-based feature matching approach [31] to improve the performance on textureless objects."
        },
        "SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting": {
          "authors": [
            "L Yang",
            "X Zhao",
            "Q Sun",
            "K Wang",
            "A Chen"
          ],
          "url": "https://arxiv.org/abs/2503.05174",
          "ref_texts": "[26] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without cad models,\u201d Advances in Neural Information Processing Systems , vol. 35, pp.",
          "ref_ids": [
            "26"
          ],
          "1": "While OnePose++ [26] employs point cloud-image matching and CamNet [27] directly regresses poses, both require extensive multi-scene training (\u2265500 images)."
        },
        "AxisPose: Model-Free Matching-Free Single-Shot 6D Object Pose Estimation via Axis Generation": {
          "authors": [
            "Y Zou",
            "Z Qi",
            "Y Liu",
            "Z Xu",
            "W Sun",
            "W Liu",
            "X Li"
          ],
          "url": "https://arxiv.org/abs/2503.06660",
          "ref_texts": "[5] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. In Advances in Neural Information Processing Systems (NeurIPS), pages 35103\u201335115, 2022. 1, 2, 3, 8",
          "ref_ids": [
            "5"
          ],
          "1": "We provide a visual comparison with two instance-level methods (CheckerPose [14], DProST [22]) and three unseen-object methods (NOPE [21], OnePose++[5] with 8 reference views, and Gen6D[19] with 50 reference views), all retrained in an instance-level manner for fair comparison.",
          "2": "Recently, methods for unseen object pose estimation [5, 19, 23, 32] have been proposed to generalize to unseen objects without retraining.",
          "3": "OnePose/OnePose++[5, 32] matches 2D key points in the query image with 3D points in the SfM model, shifting the focus to 2D-3D feature matching within the established pipeline of feature extraction, SfM, 2D-3D matching, and PnP.",
          "4": "To widen the scope of applications, RGB input-based methods [5, 12, 19, 21, 32] have been developed for pose estimation using only RGB images.",
          "5": "For example, Gen6D [19] performs model-free estimation using a series of reference images, while OnePose and OnePose++[5, 32] reconstruct point clouds from RGB images and estimate poses via 2D3D matching.",
          "6": "765 three model-free methods (OnePose++ [5], Gen6D [19], NOPE [21]) with varying numbers of reference views."
        },
        "Depth-PC: A Visual Servo Framework Integrated with Cross-Modality Fusion for Sim2Real Transfer": {
          "authors": [
            "H Zhang",
            "W Lin",
            "Y Jiang",
            "C Ye"
          ],
          "url": "https://arxiv.org/abs/2411.17195",
          "ref_texts": "[29] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without cad models,\u201d Advances in Neural Information Processing Systems , vol. 35, pp.",
          "ref_ids": [
            "29"
          ],
          "1": "OnePose series [28], [29] employ SfM [30] to generate a sparse point cloud, aligning the target view with point clouds to ascertain the pose."
        },
        "Impact-Aware Object Tracking: Exploiting Environment and Object Priors to Robustify Dynamic 6D Pose Estimation": {
          "authors": [
            "MJ Jongeneel",
            "S Dingemans",
            "A Bernardino"
          ],
          "url": "https://hal.science/hal-04674216/",
          "ref_texts": "[10] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypoint-free one-shot object pose estimation without CAD models,\u201d in Advances in Neural Information Processing Systems, 2022.",
          "ref_ids": [
            "10"
          ],
          "1": "These methods allow to obtain quite robust pose estimations, also in the presence of partial occlusions, and rely either on CAD models [3]\u2013[9], or SfM models [10]\u2013[12]."
        },
        "Vision-Based 6D Pose Estimation and Tracking: From Known to Novel Objects": {
          "authors": [
            "L Tian"
          ],
          "url": "https://qmro.qmul.ac.uk/xmlui/handle/123456789/99018",
          "ref_texts": "[52] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. OnePose++: Keypoint-free one-shot object pose estimation without CAD models. Advances in Neural Information Processing Systems, 35:35103\u201335115, 2022.",
          "ref_ids": [
            "52"
          ],
          "2": "Similarly, OnePose [121] and its extension work OnePose++ [52] propose a one-shot 6D object pose estimation method for novel objects."
        },
        "Robotic Collaborative workstation based on Object 6DOF Pose Estimation": {
          "authors": [
            "JR Vilanova S\u00e1nchez"
          ],
          "url": "https://repositorio.uloyola.es/handle/20.500.12412/6137",
          "ref_texts": "[37] X. He, J. Sun, Y. Wang, D. Huang, H. Bao, and X. Zhou, \u2018OnePose++: Keypoint-Free One -Shot Object Pose Estimation without CAD Models\u2019. ",
          "ref_ids": [
            "37"
          ],
          "1": "The unseen object segmentation has risen in the past years with numerous methods [33], [34], [35], [36], [37], [38], [39], [40] , [26] that have demonstrated significant advancements in the area.",
          "2": "One of the approaches from unseen object pose estimation is related to modelfree models, which rely on keypoints, like [37], using point clouds for the 2D-3D matching in real-time, without the need for given CAD models."
        },
        "Cuantificaci\u00f3n de la incertidumbre en la estimaci\u00f3n de la pose de objetos r\u00edgidos usando redes neuronales bayesianas profundas": {
          "authors": [
            "JI Sep\u00falveda Henao"
          ],
          "url": "https://repositorio.utp.edu.co/entities/publication/026ebc43-5890-4d16-acb1-ced3108b4447",
          "ref_texts": "[45] X. He, J. Sun, Y. Wang, D. Huang, H. Bao, and X. Zhou, \u201cOnepose++: Keypointfree one-shot object pose estimation without cad models,\u201d inAdvances in Neural Information Processing Systems(S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 35103\u201335115, Curran Associates, Inc., 2022. 1.2",
          "ref_ids": [
            "45"
          ],
          "1": "El primero de estos enfoques es a partir de la detecci\u00f3n de puntos de inter\u00e9s en los objetos [44, 45], este es com\u00fanmente utilizado en situaciones en las que se requiere una correspondencia precisa entre objetos y escenas, adem\u00e1s, este enfoque requiere del apoyo de otros algoritmos de procesamiento posterior, por lo cual es computacionalmente demandante."
        },
        "Zero-Shot Object Pose Estimation": {
          "authors": [
            "R Pellerito",
            "A Burzio",
            "D Machain",
            "L Piglia"
          ],
          "url": "https://senecobis.github.io/pdfs/Final_Report_Mixed_Reality_full_eval.pdf",
          "ref_texts": "[5] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypointfree one-shot object pose estimation without CAD models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 6",
          "ref_ids": [
            "5"
          ],
          "1": "In particular, in OnePose++ [5] the authors move from a sparse 3D object reconstruction to a dense one, which results in higher reconstruction detail."
        }
      }
    },
    {
      "title": "animatable implicit neural representations for creating realistic avatars from videos",
      "id": 28,
      "valid_pdf_number": "16/21",
      "matched_pdf_number": "14/16",
      "matched_rate": 0.875,
      "citations": {
        "Tela: Text to layer-wise 3d clothed human generation": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Z Huang",
            "X Xu",
            "J Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_2",
          "ref_texts": "33. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 4",
          "ref_ids": [
            "33"
          ],
          "1": "Benefiting from advancements in implicit functions [26,27,30], recent methods [7,10,32,33,44,45,47,49,52] have presented impressive clothed human reconstruction or generation from images and 3D scans."
        },
        "Relightable and animatable neural avatar from sparse-view video": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Chen Geng",
            "Linzhan Mou",
            "Zihan Yan",
            "Jiaming Sun",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_Relightable_and_Animatable_Neural_Avatar_from_Sparse-View_Video_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animatable implicit neural representations for creating realistic avatars from videos. arXiv preprint arXiv:2203.08133, 2022. 1, 2, 3, 7",
          "ref_ids": [
            "45"
          ],
          "1": "Recent neural scene representation-based methods [38, 45, 60] have demonstrated the ability to extract detailed geometry and photorealistic appearance of human performers from sparse-view videos without sophisticated studio setup.",
          "2": "For example, AniSDF [45] models the human geometry and appearance as neural signed distance and radiance fields, and deforms them using linear blend skinning (LBS) [35] and learned local deformation networks.",
          "4": "Inspired by previous methods [45, 60], we parameterize the human avatar as MLP networks, which predict material parameters and signed distance for any 3D point in canonical space.",
          "5": "Furthermore, we extend the SyntheticHuman dataset [45] with novel illuminations, enabling the evaluation of relightable neural avatars with ground-truth photometric properties and relighting results.",
          "6": "Another line of works [38, 45] introduces a neural displacement field to improve animation realism.",
          "7": "We formulate the relightable and animatable avatar using a set of canonical space neural fields and a warping between world and canonical space defined by the linear blend skinning algorithm [35] and a displacement field [38, 44, 45, 61]."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "29. Peng, S., Xu, Z., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Bao, H., Zhou, X.: Animatable implicit neural representations for creating realistic avatars from videos. TPAMI (2024) Inertia-aware 3D Human Modeling with Pose Sequence 17",
          "ref_ids": [
            "29"
          ],
          "1": "Previous efforts in dynamic human neural rendering have focused on digitalizing human avatars and modeling human motion [2,4,13,28,29,32,33,39,40,44].",
          "6": "Following the training setup in previous work [27,29,31], we use 4 cameras for training and the rest 19 cameras for testing.",
          "8": "3 Comparison with State of the Arts We compare with state-of-the-art counterparts, including Neural Body [28], AniNeRF [29], AniSDF [29], HumanNeRF [40] and 3DGS-Avatar [33]."
        },
        "PGAHum: prior-guided geometry and appearance learning for high-fidelity animatable human reconstruction": {
          "authors": [
            "H Wang",
            "Q Xu",
            "H Chen",
            "R Ma"
          ],
          "url": "https://arxiv.org/abs/2404.13862",
          "ref_texts": "[34] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2024. Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos. TPAMI (2024).",
          "ref_ids": [
            "34"
          ],
          "2": "The overall pipeline of PGAHum is similar to works [34, 40] which simultaneously learn the geometry and appearance of the 3D human, we make improvements on three key modules to achieve reconstruction with more fine-grained geometry and appearance details.",
          "5": "Different from previous works [34] which use the learned global SDF to obtain the mask, we directly use the prior base SDF for mask rendering to enhance the stability and facilitate the convergence of network learning.",
          "14": "It can be observed that the feet and elbow parts rendered by our method are clearer than those of other approaches, such as NeRF-NBW, NeRF-PDF, and SDF-PDF, which were reported in [34]."
        },
        "Efficient Integration of Neural Representations for Dynamic Humans": {
          "authors": [
            "W Li",
            "L Zeng",
            "C Gao",
            "N Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10666828/",
          "ref_texts": "[2] S. Peng, Z. Xu, J. Dong, Q. Wang, S. Zhang, Q. Shuai, H. Bao, and X. Zhou, \u201cAnimatable implicit neural representations for creating realistic avatars from videos,\u201d arXiv preprint arXiv:2203.08133 , 2022.",
          "ref_ids": [
            "2"
          ],
          "9": "Extending AN\u2019s work, AS [2] further improves deformation with a pose-dependent displacement field and use a signed distance field to better capture geometric detail.",
          "14": "AS [2] shows close to the best reconstruction results due to the use of signed distance field."
        },
        "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene": {
          "authors": [
            "S Biswas",
            "Q Wu",
            "B Banerjee",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2409.17459",
          "ref_texts": "[8] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animatable implicit neural representations for creating realistic avatars from videos. TPAMI, 2024.",
          "ref_ids": [
            "8"
          ],
          "1": "In NeRF-based dynamic scene reconstruction, the focus has predominantly been on human reconstruction [7, 8, 9, 10], utilizing template models such as SMPL [11], and CAPE [12].",
          "2": "Methods TemplateNo preReconstructsfree trainedfeatures Vid2Avatar [39] \u2717 \u2713 single entityAnimatableNeRF [7]\u2717 \u2713 single entitySDF-PDF [8] \u2717 \u2713 single entityHumanNeRF [40] \u2717 \u2713 single entityHOSNeRF [13] \u2717 \u2713 multiple entitiesNDR [15] \u2713 \u2713 single entityHyperNeRF [27] \u2713 \u2713 single entityD-NeRF [14] \u2713 \u2713 single entityBANMO [16] \u2713 \u2717 single entityRAC [17] \u2713 \u2717 single entityTA V A [28] \u2713 \u2713 single entity Ours \u2713 \u2713 multiple entitieswith semantic Table 1: Our approach vs."
        },
        "WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction": {
          "authors": [
            "Z Wang",
            "Z Dou",
            "Y Liu",
            "C Lin",
            "X Dong",
            "Y Guo"
          ],
          "url": "https://arxiv.org/abs/2502.01045",
          "ref_texts": "[2] S. Peng, Z. Xu, J. Dong, Q. Wang, S. Zhang, Q. Shuai, H. Bao, and X. Zhou, \u201cAnimatable implicit neural representations for creating realistic avatars from videos,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.",
          "ref_ids": [
            "2"
          ],
          "2": "We conduct extensive experiments to validate the effectiveness of our method across broad benchmarks including ZJU-Mocap dataset [1], Monocap dataset [2], MVHumanNet [22] and In-the-wild dataset [23].",
          "4": "Similar to ZJU-Mocap(revised), the Monocap dataset contains multi-view videos collected by AnimatableNeRF [2] from the DeepCap dataset [35] and the DynaCap dataset [95]."
        },
        "SAGA: Surface-Aligned Gaussian Avatar": {
          "authors": [
            "R Chen",
            "Y Cong",
            "J Liu"
          ],
          "url": "https://arxiv.org/abs/2412.00845",
          "ref_texts": "[9] S. Peng, Z. Xu, J. Dong, Q. Wang, S. Zhang, Q. Shuai, H. Bao, and X. Zhou, \u201cAnimatable implicit neural representations for creating realistic avatars from videos,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.",
          "ref_ids": [
            "9"
          ],
          "1": "The advent of neural rendering [5], particularly Neural Radiance Fields (NeRF) [6] has revolutionized novel view synthesis, enabling photorealistic human rendering [7], [8] and animation [7], [9]\u2013[11] from sparse-view images.",
          "3": "6 E XPERIMENT We evaluate SAGA against state-of-the-art methods for novel view and pose synthesis, as well as geometry reconstruction, using monocular videos from challenging datasets [7], [9], [69].",
          "5": "7: Comparison results of novel view synthesis on ZJU-MoCap dataset [7] and MonoCap dataset [9]."
        },
        "HR Human: Modeling Human Avatars with Triangular Mesh and High-Resolution Textures from Videos": {
          "authors": [
            "Q Chen",
            "R Xie",
            "K Huang",
            "Q Wang",
            "W Zheng"
          ],
          "url": "https://arxiv.org/abs/2405.11270",
          "ref_texts": "[36] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2022. Animatable implicit neural representations for creating realistic avatars from videos. arXiv preprint arXiv:2203.08133",
          "ref_ids": [
            "36"
          ],
          "1": "For the implicit animatable human reconstruction, recent works [10, 27, 36, 37, 53] have solved the challenging task of multi-view reconstruction without the supervision of 3D information and present the inherent challenges of rendering non-rigid bodies and skins under dynamic motion."
        }
      }
    },
    {
      "title": "fast and robust multi-person 3d pose estimation from multiple views",
      "id": 7,
      "valid_pdf_number": "132/188",
      "matched_pdf_number": "97/132",
      "matched_rate": 0.7348484848484849,
      "citations": {
        "Deep learning-based human pose estimation: A survey": {
          "authors": [
            "C Zheng",
            "W Wu",
            "C Chen",
            "T Yang",
            "S Zhu",
            "J Shen"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3603618",
          "ref_texts": "[48] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. 2019. Fast and Robust Multi-Person 3D Pose Estimation From Multiple Views. In CVPR.",
          "ref_ids": [
            "48"
          ],
          "1": "Deep Learning-Based Human Pose Estimation: A Survey 111:17 A group of methods [48, 49, 128, 198, 235] used body models to tackle the association problem by optimizing model parameters to match the model projection with the 2D pose."
        },
        "Capturing and inferring dense full-body human-scene contact": {
          "authors": [
            "Hao P. Huang",
            "Hongwei Yi",
            "Markus Hoschle",
            "Matvey Safroshkin",
            "Tsvetelina Alexiadis",
            "Senya Polikovsky",
            "Daniel Scharstein",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Huang_Capturing_and_Inferring_Dense_Full-Body_Human-Scene_Contact_CVPR_2022_paper.html",
          "ref_texts": "[13] Junting Dong, Qi Fang, W en Jiang, Y urou Y ang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3D pose estimation and tracking from multiple views.IEEE Transactions on P attern Analysis and Machine Intelligence , 2021.",
          "ref_ids": [
            "13"
          ],
          "1": "Powered by CNNs, recent methods leverage multiview consistency to improve keypoint detection [27, 31, 59, 73], to re-identify subjects across views [14] or across view and time [13, 96], but they estimate only joints, not body meshes.",
          "2": "Other methods that build such 4D associations [13, 96] could also be applied here."
        },
        "SPEC: Seeing people in the wild with an estimated camera": {
          "authors": [
            "Muhammed Kocabas",
            "Hao P. Huang",
            "Joachim Tesch",
            "Lea Muller",
            "Otmar Hilliges",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.html",
          "ref_texts": "[9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2",
          "ref_ids": [
            "9"
          ],
          "1": "They can be largely classified to \u201cbottom up\u201d approaches [4, 6, 9, 14, 57, 76] that assemble 3D body poses from multi-view image evidence (keypoints, silhouettes), and \u201ctop down\u201d approaches [3, 11, 12, 20, 27, 62] that deform a pre-defined 3D human template according to detected image features in each view."
        },
        "Single-stage multi-person pose machines": {
          "authors": [
            "Xuecheng Nie",
            "Jiashi Feng",
            "Jianfeng Zhang",
            "Shuicheng Yan"
          ],
          "url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.html",
          "ref_texts": "[8] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views.arXiv, 2019.",
          "ref_ids": [
            "8"
          ],
          "2": "Dong [8] performed top-down multi-person 2D pose estimation for images from multiple views and reconstructed 3D pose for each person from multi-view 2D poses.",
          "4": "Moreover, its single-stage design also significantly simplifies the pipeline for multi-person 3D pose estimation from a single monocular RGB image, alleviating the requirements of intermediate 2D pose estimations [25] or 3D pose reconstructions from multiple views [8]."
        },
        "Voxelpose: Towards multi-camera 3d human pose estimation in wild environment": {
          "authors": [
            "H Tu",
            "C Wang",
            "W Zeng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58452-8_12",
          "ref_texts": "4. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation from multiple views. In: CVPR. (2019) 7792\u20137801",
          "ref_ids": [
            "4"
          ],
          "4": "Our work differs from the previous methods [4,5] in that it elegantly avoids the two challenging association problems.",
          "6": "The 3D Pose Estimation MetricFollowing [4], we use the Percentage of Correct Parts (PCP3D) metric to evaluate the estimated 3D poses.",
          "8": "Our approach also achieves better results than [4] on the Shelf dataset."
        },
        "Direct multi-view multi-person 3d pose estimation": {
          "authors": [
            "J Zhang",
            "Y Cai",
            "S Yan",
            "J Feng"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/6da9003b743b65f4c0ccd295cc484e57-Abstract.html",
          "ref_texts": "[6] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, 2019.",
          "ref_ids": [
            "6"
          ],
          "1": "It is a fundamental task that benefits many real-world applications (such as surveillance, sportscast, gaming and mixed reality) and is mainly tackled by reconstruction-based [6, 14, 4] and volumetric [40] approaches in previous literature, as shown in Fig.",
          "2": "Current approaches mainly exploit a multi-stage pipeline for multi-person tasks, including reconstruction-based [6, 4, 14, 21, 26] and volumetric [40] paradigms."
        },
        "Hi4d: 4d instance segmentation of close human interaction": {
          "authors": [
            "Yifei Yin",
            "Chen Guo",
            "Manuel Kaufmann",
            "Juan Jose",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.html",
          "ref_texts": "[18] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019. 3, 7",
          "ref_ids": [
            "18"
          ],
          "4": "We evaluate the opensourced multi-view SMPL estimation method MVPose [18] on 4-view and 8-view settings."
        },
        "Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking": {
          "authors": [
            "N Dinesh",
            "Laurent Guigues",
            "Leonid Pishchulin",
            "Jayan Eledath",
            "Srinivasa G. Narasimhan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Reddy_TesseTrack_End-to-End_Learnable_Multi-Person_Articulated_3D_Pose_Tracking_CVPR_2021_paper.html",
          "ref_texts": "[13] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 7792\u20137801, 2019.",
          "ref_ids": [
            "13"
          ],
          "4": "Percentage of Correct Keypoints (3D-PCK) [13] provides a more global view on the accuracy of 3D pose estimation and is computed similarly to its 2D PCK counterpart [3]."
        },
        "Refit: Recurrent fitting network for 3d human recovery": {
          "authors": [
            "Yufu Wang",
            "Kostas Daniilidis"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wang_ReFit_Recurrent_Fitting_Network_for_3D_Human_Recovery_ICCV_2023_paper.html",
          "ref_texts": "[18] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7792\u20137801, 2019. 2, 5",
          "ref_ids": [
            "18"
          ],
          "2": "Motion capture with multiple calibrated cameras offers the highest accuracy [18, 27]."
        },
        "Deepmulticap: Performance capture of multiple characters using sparse multiview cameras": {
          "authors": [
            "Yang Zheng",
            "Ruizhi Shao",
            "Yuxiang Zhang",
            "Tao Yu",
            "Zerong Zheng",
            "Qionghai Dai",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zheng_DeepMultiCap_Performance_Capture_of_Multiple_Characters_Using_Sparse_Multiview_Cameras_ICCV_2021_paper.html",
          "ref_texts": "[12] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "12"
          ],
          "1": "Some of them even achieve real-time performance [6, 12, 66, 39]."
        },
        "Vision-based human pose estimation via deep learning: A survey": {
          "authors": [
            "G Lan",
            "Y Wu",
            "F Hu",
            "Q Hao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9955393/",
          "ref_texts": "[108] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3D pose estimation from multiple views,\u201d in CVPR 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "108"
          ],
          "1": "[108] proposed a multiway matching algorithm that combined both geometric and appearance cues to match the detected 2D poses across views."
        },
        "TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos": {
          "authors": [
            "Y Wang",
            "Z Wang",
            "L Liu",
            "K Daniilidis"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73247-8_27",
          "ref_texts": "15. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation from multiple views. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 7792\u20137801 (2019)",
          "ref_ids": [
            "15"
          ],
          "1": "In the optimization paradigm, the SMPL model can be fitted through energy minimization to images [1,7,15,71], videos [3,24,91], and other sensing modalities [83,90]."
        },
        "Selfpose3d: Self-supervised multi-person multi-view 3d pose estimation": {
          "authors": [
            "Vinkle Srivastav",
            "Keqi Chen",
            "Nicolas Padoy"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Srivastav_SelfPose3d_Self-Supervised_Multi-Person_Multi-View_3d_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[18] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "18"
          ],
          "1": "Introduction The task of estimating 3d poses for multiple persons using a few calibrated cameras is a challenging computer vision problem [10, 18, 32, 52, 57].",
          "2": "In contrast, the optimization-based methods formulate the 3d pose reconstruction as a mathematical optimization task, primarily focusing on aligning and matching the 2d poses across different camera views to infer 3d poses using triangulation within the framework of multi-view geometry [10, 18, 31\u201333, 49].",
          "3": "More recent approaches utilize multi-view 3d reconstruction in the optimization loop inferring 3D poses that are geometrically and spatially coherent [10, 18, 33]."
        },
        "Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild": {
          "authors": [
            "Y Zhang",
            "C Wang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9758679/",
          "ref_texts": "[25] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation from multiple views,\u201d in CVPR, 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "25"
          ],
          "4": "[25] propose a multi-way matching algorithm to find cycle-consistent correspondences of detected 2D poses across multiple views using both appearance and geometric cues, which is able to prune false detections and deal with partial overlaps between views."
        },
        "Tempo: Efficient multi-view pose estimation, tracking, and forecasting": {
          "authors": [
            "Rohan Choudhury",
            "Kris M. Kitani",
            "Laszlo A. Jeni"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Choudhury_TEMPO_Efficient_Multi-View_Pose_Estimation_Tracking_and_Forecasting_ICCV_2023_paper.html",
          "ref_texts": "[12] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "12"
          ],
          "1": "In the multi-person setting, early approaches like [3, 12, 13, 52, 6] associate 2D pose estimates from each view, then fuse the matched 2D poses into 3D.",
          "2": "Other methods aimed towards multi-person motion capture use Re-ID features [12, 52], 4D graph cuts [54], plane sweep stereo [32], crossview graph matching [50], or optimize SMPL parameters [14] to produce 3D poses from 2D pose estimates in each view."
        },
        "Multi-person 3d pose estimation and tracking in sports": {
          "authors": [
            "Lewis Bridgeman",
            "Marco Volino",
            "Yves Guillemaut",
            "Adrian Hilton"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPRW_2019/html/CVSports/Bridgeman_Multi-Person_3D_Pose_Estimation_and_Tracking_in_Sports_CVPRW_2019_paper.html",
          "ref_texts": "[12] J. Dong, W . Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation from multiple views. CVPR, 2019.",
          "ref_ids": [
            "12"
          ],
          "2": "Recent work [12] finds correspondences between 2D poses in multiple views in an optimization framework, combining epipolar geometry costs and CNN appearance descriptors.",
          "5": "However, all other methods use a 3DPS model to constrain the final joint positions; pictorial structure models have been shown to result in more accurate joint estimations than triangulation when the number of views is small [12].",
          "8": "The methods in [6] and [12], which both use pictorial structure models, run at approximately 1fps and 10fps respectively."
        },
        "Cross-view tracking for multi-human 3d pose estimation at over 100 fps": {
          "authors": [
            "Long Chen",
            "Haizhou Ai",
            "Rui Chen",
            "Zijie Zhuang",
            "Shuang Liu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Cross-View_Tracking_for_Multi-Human_3D_Pose_Estimation_at_Over_100_CVPR_2020_paper.html",
          "ref_texts": "[13] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 7792\u20137801, 2019. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "13"
          ],
          "2": "[13] propose solving the cross-view association problem at the body level in advance before applying 3DPS.",
          "4": "[13] propose to solve the cross-view association problem at the body level first.",
          "5": "Similar to [13], we associate the joints at the body level, but not just across views, also across times.",
          "10": "[13] propose to cluster joints at the body level to reduce the state space.",
          "14": "Note that, for the fair comparison, we use the same 2D pose detections for the experiments as that in [13], which are provided by an off-the-shelf 2D pose estimation method [11].",
          "18": "The following three baselines are taken from the official implementation of [13], which employs geometric information and human appearance features for matching 2D poses between camera views."
        },
        "Multi-view multi-person 3d pose estimation with plane sweep stereo": {
          "authors": [
            "Jiahao Lin",
            "Gim Hee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Lin_Multi-View_Multi-Person_3D_Pose_Estimation_With_Plane_Sweep_Stereo_CVPR_2021_paper.html",
          "ref_texts": "[7] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 7792\u20137801, 2019.",
          "ref_ids": [
            "7"
          ],
          "4": "[7] enhance the cross-view consistency with appearance features."
        },
        "Graph-based 3d multi-person pose estimation using multi-view images": {
          "authors": [
            "Size Wu",
            "Sheng Jin",
            "Wentao Liu",
            "Lei Bai",
            "Chen Qian",
            "Dong Liu",
            "Wanli Ouyang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Wu_Graph-Based_3D_Multi-Person_Pose_Estimation_Using_Multi-View_Images_ICCV_2021_paper.html",
          "ref_texts": "[11] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7792\u20137801, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "11"
          ],
          "1": "As shown in Figrue 1(a), 2D-to-3D lifting approaches [3, 4] first estimate 2D joints in each view through monocular pose estimator, then associate 2D poses across views, and finally lift the matched 2D single-view poses to 3D via triangulation [2] or Pictorial Structure Models (PSM) [11].",
          "2": "Previous methods perform association across views by multi-view geometric constraints [18] and appearance similarity [11].",
          "3": "Existing approaches can be mainly categorized into 2D-to-3D pose lifting approaches [1, 3, 4, 6, 11, 13, 15, 22, 44] and direct 3D pose estimation approaches [35].",
          "4": "2D-to-3D lifting approaches [1, 3, 4, 6, 11, 13] first estimate 2D joints of the same person in each view through monocular pose estimator, then lift the matched 2D singleview poses to 3D locations.",
          "5": "The 3D poses are recovered using triangulation [6] or singleperson 3D PSM [11].",
          "6": "We follow [3, 11, 35] to prepare the training and testing datasets.",
          "7": "We follow [3, 4, 5, 11, 13] to use the percentage of correctly estimated parts (PCP3D) to evaluate the estimated 3D poses.",
          "8": "[11] 98."
        },
        "4d association graph for realtime multi-person motion capture using multiple video cameras": {
          "authors": [
            "Yuxiang Zhang",
            "Liang An",
            "Tao Yu",
            "Xiu Li",
            "Kun Li",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_4D_Association_Graph_for_Realtime_Multi-Person_Motion_Capture_Using_Multiple_CVPR_2020_paper.html",
          "ref_texts": "[14] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InCVPR, 2019.",
          "ref_ids": [
            "14"
          ],
          "1": "For example, the state-of-theart methods [14, 10, 39] share a similar high-level framework by first performing per-view person parsing, followed by cross-view person matching, and temporal tracking sequentially.",
          "2": "Compared with [14, 24, 8, 7] which adopt sequential processing strategy on image space, viewpoint, and time dimensions, our 4D graph formulation enables unified optimization on all these dimensions, thereby allowing better mutual benefit among them.",
          "3": "Most recent work [14] matches per-view parsed human instances cross view with convex optimization method constrained by cycle-consistency.",
          "4": "Note that besides evaluating our method using the proposed dataset, we also provide evaluation results using Shelf and Panoptic Studio dataset following previous works [8, 7, 14].",
          "5": "Benefiting from our 4D association formulation, we achieve more accurate results than both temporal tracking methods based on 3DPS ([8, 6, 7, 16]) and appearancebased global optimization methods [14].",
          "6": "W e also compare with [14] on our testing dataset according to \u2018precision\u2019 (the ratio of correct joints in all estimated joints) and \u2018recall\u2019 (the ratio of correct joints in all ground truth joints).",
          "7": "2, our method outperforms [14] under both metrics.",
          "8": "[14] 97.",
          "9": "[14] 98.",
          "10": "Our Dataset Dong[14] Ours(final) Precision(%) 71.",
          "11": "Comparison with [14] using our testing dataset.",
          "12": "Qualitative Comparison T o further demonstrate the advantages of our bottomup system, we perform qualitative comparison with the state-of-the-art method [14], which utilizes top-down human pose detector [12] to perform single view parsing.",
          "13": "Qualitative comparison with Dong[14] on Shelf (left figure) and our captured data (right figure), both with 5 cameras.",
          "14": "Without using tracking edges, our method still exhibits competent result and out-performs state-of-the-art method [14] (93."
        },
        "Lightweight multi-person total motion capture using sparse multi-view cameras": {
          "authors": [
            "Yuxiang Zhang",
            "Zhe Li",
            "Liang An",
            "Mengcheng Li",
            "Tao Yu",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Lightweight_Multi-Person_Total_Motion_Capture_Using_Sparse_Multi-View_Cameras_ICCV_2021_paper.html",
          "ref_texts": "[14] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, 2019. 3",
          "ref_ids": [
            "14"
          ],
          "1": "[14] proposed a multi-way matching algorithm to guarantee cycle consistency across all the views."
        },
        "Shape-aware multi-person pose estimation from multi-view images": {
          "authors": [
            "Zijian Dong",
            "Jie Song",
            "Xu Chen",
            "Chen Guo",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Dong_Shape-Aware_Multi-Person_Pose_Estimation_From_Multi-View_Images_ICCV_2021_paper.html",
          "ref_texts": "[10] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "10"
          ],
          "1": "Due to the real-world importance of this problem, several recent approaches have attempted to predict the poses of multiple people, observed from multiple cameras [6, 10, 18, 45, 46, 54].",
          "2": "The first group formulates the problem as a cross-view matching and association problem [6, 10, 54].",
          "3": "Since we study the setting of multi-person pose estimation from multiple views [1, 2, 10, 12, 23, 24, 28, 54], the focus of this literature review is on multi-person pose estimation.",
          "4": "[10] first performs per-view person parsing, followed by a cross-view person matching via a convex optimization method constrained by cycle consistency.",
          "5": "We achieve slightly better results compared to methods [1, 2, 10, 12, 54] which do not rely on 3D supervision and achieve comparable performance compared to learning-based methods [18, 46] which train the model based on this dataset.",
          "6": "[10] No 98.",
          "7": "[10] 71.",
          "8": "We also compare our algorithm with matching-based methods [10, 54]."
        },
        "Fusing wearable imus with multi-view images for human pose estimation: A geometric approach": {
          "authors": [
            "Zhe Zhang",
            "Chunyu Wang",
            "Wenhu Qin",
            "Wenjun Zeng"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Fusing_Wearable_IMUs_With_Multi-View_Images_for_Human_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[4] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views.arXiv preprint arXiv:1901.04111 , 2019.",
          "ref_ids": [
            "4"
          ],
          "1": "The third class of methods such as [1, 3, 17, 2, 7, 11, 4, 19] adopt a two-step framework."
        },
        "Probabilistic triangulation for uncalibrated multi-view 3D human pose estimation": {
          "authors": [
            "Boyuan Jiang",
            "Lei Hu",
            "Shihong Xia"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Probabilistic_Triangulation_for_Uncalibrated_Multi-View_3D_Human_Pose_Estimation_ICCV_2023_paper.html",
          "ref_texts": "[12] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019.",
          "ref_ids": [
            "12"
          ],
          "1": "The first is the point-based method[26, 5, 12], which uses the spatial coordinates of the joint points as the human pose representation."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[15] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multiperson 3d pose estimation and tracking from multiple views. T-PAMI, 2021.",
          "ref_ids": [
            "15"
          ],
          "1": "Given four monocular part-specific videos (body, head, and two hands) of the performer, we first utilize the EasyMoCap [1, 16, 15] to estimate SMPL+H [42] parameters for the body and hands videos and utilize an adaptation of [49] to estimate the FLAME [28] parameters for the face video."
        },
        "DeepFuse: An IMU-aware network for real-time 3D human pose estimation from multi-view image": {
          "authors": [
            "Fuyang Huang",
            "Ailing Zeng",
            "Minhao Liu",
            "Qiuxia Lai",
            "Qiang Xu"
          ],
          "url": "http://openaccess.thecvf.com/content_WACV_2020/html/Huang_DeepFuse_An_IMU-Aware_Network_for_Real-Time_3D_Human_Pose_Estimation_WACV_2020_paper.html",
          "ref_texts": "[10] J. Dong, W . Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE Conference on Computer V ision and P attern Recognition , pages 7792\u20137801, 2019.",
          "ref_ids": [
            "10"
          ],
          "1": "Recently , many multi-view based methods[17, 43, 35, 10, 37, 19] try to get more effective and accurate information from different views."
        },
        "Multi-person 3d pose estimation in crowded scenes based on multi-view geometry": {
          "authors": [
            "H Chen",
            "P Guo",
            "P Li",
            "GH Lee",
            "G Chirikjian"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58580-8_32",
          "ref_texts": "13. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3D pose estimation from multiple views. In: Proc. CVPR. pp. 7792\u20137801 (2019)",
          "ref_ids": [
            "13"
          ],
          "1": "Several recent works have focused on multi-person scenarios in problem formulation either based on monocular setting [44] or multi-view setting [2,3,4,5,13,24].",
          "2": "The authors of [13] incorporated appearance cues by fusing reidentification with epipolar constraints.",
          "3": "2 Multi-view Correspondence with Graph Matching Previous methods [13,24] apply epipolar constraints to all joints in order to solve the correspondence problem."
        },
        "Dynamic multi-person mesh recovery from uncalibrated multi-view cameras": {
          "authors": [
            "B Huang",
            "Y Shu",
            "T Zhang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9665884/",
          "ref_texts": "[16] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, 2019. 1, 2, 6, 7",
          "ref_ids": [
            "16"
          ],
          "1": "Numerous previous works have been aimed at capturing multi-person motions from multi-view input via geometry constraints [2, 16, 9, 38, 62, 29] or optimization-based model fitting [61, 35, 40, 34, 59].",
          "2": "[16] considers geometric and appearance constraints simultaneously.",
          "5": "[6, 16, 9, 62, 11] are recent works based on calibrated cameras."
        },
        "Real-time multi-view 3D human pose estimation using semantic feedback to smart edge sensors": {
          "authors": [
            "S Bultmann",
            "S Behnke"
          ],
          "url": "https://arxiv.org/abs/2106.14729",
          "ref_texts": "[9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3D pose estimation from multiple views. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 7784\u20137793, 2019.",
          "ref_ids": [
            "9"
          ],
          "1": "Most state-of-the-art methods [24, 23, 7, 26, 9] follow a two-step approach: First, 2D pose detections are generated for each available view (cf.",
          "2": "With the increasing success of deep learning methods, more recent approaches [24, 23, 9] employ 2D convolutional neural networks (CNNs) for human joint detection [6, 30] and recover the 3D pose using variants of the Pictorial Structures Model (PSM) [2, 5].",
          "6": "[9] propose to reduce the PSM state-space and exploit appearance information for data association.",
          "7": "In terms of PCP score, our method largely outperforms the older method [3] and is on par with the recent approaches [9, 7]."
        },
        "End-to-end dynamic matching network for multi-view multi-person 3d pose estimation": {
          "authors": [
            "C Huang",
            "S Jiang",
            "Y Li",
            "Z Zhang",
            "J Traish"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-030-58604-1_29",
          "ref_texts": "10. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation from multiple views. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7792\u20137801 (2019)",
          "ref_ids": [
            "10"
          ],
          "1": "In contrast to the above two-step models, a recent direction is to use a matching algorithm that identifies matched 2d skeletons from multiple views before the estimation for 3d poses [10].",
          "6": "[10], which uses person re-id and geometry methods to match 2d poses."
        },
        "Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking": {
          "authors": [
            "Hau Chu",
            "Hong Lee",
            "Chih Lee",
            "Hsien Hsu",
            "Da Li",
            "Song Chen"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Chu_Part-Aware_Measurement_for_Robust_Multi-View_Multi-Human_3D_Pose_Estimation_and_CVPRW_2021_paper.html",
          "ref_texts": "[11] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition (CVPR), pages 7792\u20137801, 2019.",
          "ref_ids": [
            "11"
          ],
          "1": "Leveraging 2D poses, recent studies [4, 32, 11, 7] follow an initialization-and-tracking framework for 3D pose inference.",
          "2": "Previous approaches such as [11, 32, 36] construct the 3D skeletons from multiple current views at first, and then the 3D skeletons obtained are smoothed temporally for each individual.",
          "4": "Since CNN has a great performance in the human detector and 2D human pose estimation, most multi-person 3D pose estimation approaches [13, 11, 36] utilize sophisticated CNN-based 2D human pose estimation techniques at the beginning.",
          "5": "[11] utilize Faster R-CNN [29] with lightweight backbone network and Cascaded Pyramid Network [8] to detect humans\u2019 location and their 2D poses in multi-view images.",
          "9": "[11] develop a multi-way matching with circle consistency to match 2D poses in multiple views optimally after constructing 3D poses.",
          "10": "Furthermore, we also evaluate some approaches [11, 7] on the extended Campus testing set.",
          "13": "[11] test their experiment on GeForce 1080Ti GPU, which spent an average of 25ms to compute affinities and 20 ms for finding the crossview association, and 60 ms for reconstruction."
        },
        "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos": {
          "authors": [
            "F Lu",
            "Z Dong",
            "J Song",
            "O Hilliges"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73668-1_13.pdf",
          "ref_texts": "18. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Fast and robust multiperson 3d pose estimation and tracking from multiple views. In: T-PAMI (2021)",
          "ref_ids": [
            "18"
          ],
          "1": "To correct abnormal or missing pose estimations, some methods [18,21] leverage parametric body models like SMPL [37] as full-body priors and fit these 3D models to 2D joint estimations.",
          "2": "Some follow-ups [18,73] show that parametric models help in correcting implausible 3D pose estimates and filling in missing joints.",
          "4": "MVPose [18] adds temporal tracking and SMPL prior to MVPose* [19] and is regarded as a SMPLguided method."
        },
        "Graph neural networks for cross-camera data association": {
          "authors": [
            "E Luna",
            "JC SanMiguel",
            "JM Mart\u00ednez"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9893862/",
          "ref_texts": "[2] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation from multiple views,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "2"
          ],
          "1": "I NTRODUCTION D ATA association across camera views is an intermediate key step in many complex computer vision tasks, such as multi-camera 3D pedestrian detection [1], 3D pose estimation for multiple views [2]\u2013[4], multi-view multi-target tracking [5]\u2013[7], and even robotic perception [8], among others.",
          "3": "The task of multi-person 3D pose estimation from multiple views was tackled in [2] by carrying out data association across views limited to a pair of cameras at the same time.",
          "4": "In [2], they combine re-identification appearance and geometrical information performing a linear assignment based on the Euclidean distance for the similarity computation, for the task of multi-camera 3D pose estimation."
        },
        "Human-m3: A multi-view multi-modal dataset for 3d human pose estimation in outdoor scenes": {
          "authors": [
            "B Fan",
            "S Wang",
            "W Guo",
            "W Zheng",
            "J Feng"
          ],
          "url": "https://arxiv.org/abs/2308.00628",
          "ref_texts": "[4] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multiperson 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "4"
          ],
          "1": "[4] proposed a method based on singular value decomposition to solve the matching problem between multi-view human poses."
        },
        "Markerless multi-view 3D human pose estimation: A survey": {
          "authors": [
            "AFR Nogueira",
            "HP Oliveira",
            "LF Teixeira"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885625000253",
          "ref_texts": "[23] Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., Zhou, X.,2022. Fastandrobustmulti-person3dposeestimationandtracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 6981\u20136992. doi:10.1109/TPAMI.2021.",
          "ref_ids": [
            "23"
          ],
          "1": "Finding solutions for this task is essential for numerous applications ranging from human-robot interaction, animation, gaming, action recognition, rehabilitation assessments, surveillance systems, sports, live broadcasts, human-computer interaction, such as recognising sign language, among many others.",
          "2": "Later, in [23], the researchers extend their approach to also, track the poses by introducing temporal tracking and Riemannian Extended Kalman filtering.",
          "3": "Although, the presented method is faster than PS-based methods, in scenarios with a small number of cameras the use of PS for joint estimation has been proven to be more precise than triangulation.",
          "4": "Demonstrating the existing trade-off between speed and accuracy."
        },
        "Metafuse: A pre-trained fusion model for human pose estimation": {
          "authors": [
            "Rongchang Xie",
            "Chunyu Wang",
            "Yizhou Wang"
          ],
          "url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Xie_MetaFuse_A_Pre-trained_Fusion_Model_for_Human_Pose_Estimation_CVPR_2020_paper.html",
          "ref_texts": "[8] Junting Dong, W en Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InCVPR, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "8"
          ],
          "1": "With the development of 2D pose estimation techniques, some approaches such as [1, 7, 6, 24, 4, 8, 25] adopt a simple two-step framework."
        },
        "3D Human Pose Estimation from Multiple Dynamic Views via Single-view Pretraining with Procrustes Alignment": {
          "authors": [
            "R Gu",
            "J Zhu",
            "Y Si",
            "F Gao",
            "J Xu",
            "G Xu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3680990",
          "ref_texts": "[9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. 2019. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 7792\u20137801.",
          "ref_ids": [
            "9"
          ],
          "1": "Most methods [9, 14, 17, 33] obtain the 2D pose by running a CNN over 2D poses given in multiple views."
        },
        "ContactField: Implicit Field Representation for Multi-Person Interaction Geometry": {
          "authors": [
            "H Lee",
            "T You",
            "H Park",
            "W Shim"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/435422305988b73c6cc00bcb29ba2531-Abstract-Conference.html",
          "ref_texts": "[11] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. \u201cFast and robust multi-person 3d pose estimation from multiple views\u201d. In: CVPR. 2019.",
          "ref_ids": [
            "11"
          ],
          "1": "We use the public implementation of DMC 1 and apply LVD [10] on the SynMPI, as well as MVpose [11] on the Hi4D, to obtain SMPL parameters.",
          "2": "703 Table B: Ablation study on SPML initialization method for synthetic datasets in DMC Model SMPL method CD\u2193 P2S\u2193 NC\u2191 DMC MVPose [11] 0.",
          "3": "Specifically, for the HI4D dataset, we used MVPose [11], following the experimental setup described in the HI4D paper, where DMC [43] was run using MVPose for SMPL acquisition."
        },
        "A unified multi-view multi-person tracking framework": {
          "authors": [
            "F Yang",
            "S Odashima",
            "S Yamao",
            "H Fujimoto"
          ],
          "url": "https://link.springer.com/article/10.1007/s41095-023-0334-8",
          "ref_texts": "[7] Dong, J. T.; Fang, Q.; Jiang, W.; Yang, Y. R.; Huang, Q. X.; Bao, H. J.; Zhou, X. W. Fast and robust multiperson 3D pose estimation and tracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence Vol. 44, No. 10, 6981\u20136992, 2022.",
          "ref_ids": [
            "7"
          ],
          "1": "Most related works [4, 5, 7, 8] independently associate multi-view information in a single view, which can be suboptimal: different persons may have similar crossview consistencies in a single frame, and the correct assignment may not always be established."
        },
        "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters": {
          "authors": [
            "M Sun",
            "J Chen",
            "J Dong",
            "Y Chen",
            "X Jiang",
            "S Mao"
          ],
          "url": "https://arxiv.org/abs/2411.17423",
          "ref_texts": "[10] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 7792\u20137801, 2019. 3",
          "ref_ids": [
            "10"
          ],
          "1": "However, human-centered 3D reconstruction methods focus on high-fidelity digitization and reconstruction of clothed humans from minimal inputs [7, 10, 11, 14, 16, 17, 32, 40, 41, 53\u201355, 70]."
        },
        "Synergetic reconstruction from 2D pose and 3D motion for wide-space multi-person video motion capture in the wild": {
          "authors": [
            "T Ohashi",
            "Y Ikegami",
            "Y Nakamura"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885620301608",
          "ref_texts": "[18] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "18"
          ],
          "4": "In addition, the proposed method can achieve better or comparable performance than the previous studies [18, 13, 46].",
          "10": "The comparison of the proposed method to the existing method [18] demonstrates that the proposed method is superior in terms of accuracy, and only the proposed method can estimate a temporally continuous 3D motion."
        },
        "Motion-aware and data-independent model based multi-view 3D pose refinement for volleyball spike analysis": {
          "authors": [
            "Y Liu",
            "X Cheng",
            "T Ikenaga"
          ],
          "url": "https://link.springer.com/article/10.1007/s11042-023-16369-8",
          "ref_texts": "9. Dong J, Jiang W, Huang Q, Bao H, Zhou X (2019) Fast and robust multi-person 3D pose estimation from multiple views. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp 7792\u20137801",
          "ref_ids": [
            "9"
          ],
          "2": "[9] propose using the combination of geometric, appearance, and cycle-consistency constraints to design a matching algorithm to reconstruct 3D poses with clustering 2D poses."
        },
        "Center point to pose: Multiple views 3D human pose estimation for multi-person": {
          "authors": [
            "Huan Liu",
            "Jian Wu",
            "Rui He"
          ],
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0274450",
          "ref_texts": "9. Dong J, Jiang W, Huang Q, Bao H, Zhou X. Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views. Proceedings of the 2019 IEEE/CVF Conferen ce on Computer Vision and Pattern Recognition (CVPR); 2019 Jun 15\u201320; Long Beach, CA, USA: IEEE; 2019. p. 7792\u2013780 1.",
          "ref_ids": [
            "9"
          ],
          "1": "Then they increase the affinity matrix [9] to improve the performance.",
          "5": "[9] 98.",
          "6": "[9] 97."
        },
        "Sports analysis and VR viewing system based on player tracking and pose estimation with multimodal and multiview sensors": {
          "authors": [
            "W Guo",
            "Z Pan",
            "Z Xi",
            "A Tuerxun",
            "J Feng"
          ],
          "url": "https://arxiv.org/abs/2405.01112",
          "ref_texts": "[11] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3D pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7792\u20137801, 2019. 3, 5",
          "ref_ids": [
            "11"
          ],
          "3": "1 PointVoxel Similar to [11,53,57], we adopt the top-down manner to estimate the 3D pose."
        },
        "Simultaneously recovering multi-person meshes and multi-view cameras with human semantics": {
          "authors": [
            "B Huang",
            "J Ju",
            "Y Shu",
            "Y Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10299685/",
          "ref_texts": "[1] J. Dong, Q. Fang, W. Jiang, Y . Yang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation and tracking from multiple views,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6981\u20136992, 2021.",
          "ref_ids": [
            "1"
          ],
          "2": "MvPose [1] considers both geometric and appearance constraints simultaneously.",
          "5": "[1] 97."
        },
        "Multi-person 3d pose estimation from multi-view uncalibrated depth cameras": {
          "authors": [
            "YJ Li",
            "Y Xu",
            "R Khirodkar",
            "J Park",
            "K Kitani"
          ],
          "url": "https://arxiv.org/abs/2401.15616",
          "ref_texts": "[14] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019. 1, 2, 4, 6, 7, 16",
          "ref_ids": [
            "14"
          ],
          "9": "[14] \u2713 94.",
          "10": "[14] \u2713 92.",
          "11": "[14] \u2713 93.",
          "12": "[14] \u2713 90."
        },
        "Recursive bayesian filtering for multiple human pose tracking from multiple cameras": {
          "authors": [
            "Hun Kwon",
            "Julian Tanke",
            "Juergen Gall"
          ],
          "url": "http://openaccess.thecvf.com/content/ACCV2020/html/Kwon_Recursive_Bayesian_Filtering_for_Multiple_Human_Pose_Tracking_from_Multiple_ACCV_2020_paper.html",
          "ref_texts": "5. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and rob ust multi-person 3d pose estimation from multiple views. In: Conference on Comput er Vision and Pattern Recognition. (2019)",
          "ref_ids": [
            "5"
          ],
          "3": "[5] 97.",
          "4": "[5] 97.",
          "6": "We argue that the top-down pose estimation model and the appearance model of [5] are beneficial when the full bodies are visible and the scenes are relatively uncluttered, as it is the case with the Campus dataset (Figure 5 top row)."
        },
        "3DSA: Multi-view 3D Human Pose Estimation With 3D Space Attention Mechanisms": {
          "authors": [
            "BH Chen",
            "C Tsai"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73383-3_19",
          "ref_texts": "9. Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence44(10), 6981\u2013",
          "ref_ids": [
            "9"
          ],
          "1": "In 2D-3D lifting approaches [9,10,42], a monocular pose estimator identifies 2D bounding boxes and 2D poses for individuals in each view.",
          "2": "Existing methods can be categorized into three types: (1)2D to 3D lifting methods [1\u20133,5,9,10,18,25,42] (2)Voxel-based methods [6,7,19,20,27,30,32,35,38,41] (3)Direct regression method [40].",
          "3": "[9,10]proposeMvPose."
        },
        "Ellipose: Stereoscopic 3d human pose estimation by fitting ellipsoids": {
          "authors": [
            "Christian Grund",
            "Julian Tanke",
            "Jurgen Gall"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Grund_ElliPose_Stereoscopic_3D_Human_Pose_Estimation_by_Fitting_Ellipsoids_WACV_2023_paper.html",
          "ref_texts": "[13] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019.",
          "ref_ids": [
            "13"
          ],
          "1": "Other approaches learn 3D poses directly from 2D pose [46] or image data [13, 54]."
        },
        "Multiple View Geometry Transformers for 3D Human Pose Estimation": {
          "authors": [
            "Ziwei Liao",
            "Jialiang Zhu",
            "Chunyu Wang",
            "Han Hu",
            "Steven L. Waslander"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liao_Multiple_View_Geometry_Transformers_for_3D_Human_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[8] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 7792\u20137801, 2019. 1, 2, 5, 6, 7",
          "ref_ids": [
            "8"
          ],
          "1": "Some methods [4, 8, 9, 27, 37] address the task from a primarly geometric direction.",
          "2": "One group of works [8, 9, 27] follows a two-stage design, first solving associations with a hand-crafted affinity matrix, and then estimating 3D poses with triangulation.",
          "8": "[8] gets reasonable results on the Shelf and Campus dataset."
        },
        "Multi-agent deep reinforcement learning for online 3D human poses estimation": {
          "authors": [
            "Zhen Fan",
            "Xiu Li",
            "Yipeng Li"
          ],
          "url": "https://www.mdpi.com/2072-4292/13/19/3995",
          "ref_texts": "10. Dong, J.; Jiang, W.; Huang, Q.; Bao, H.; Zhou, X. Fast and Robust Multi-Person 3D Pose Estimation From Multiple Views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019; pp. 7792\u20137801.",
          "ref_ids": [
            "10"
          ],
          "1": "After 2D pose detection from images, various algorithms [9,10] are exploited to find cross-view joints correspondence and triangulate all 2D joints to 3D in an epipolar geometry framework."
        },
        "Reconstructing People, Places, and Cameras": {
          "authors": [
            "L M\u00fcller",
            "H Choi",
            "A Zhang",
            "B Yi",
            "J Malik"
          ],
          "url": "https://arxiv.org/abs/2412.17806",
          "ref_texts": "[12] Junting Dong, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7792\u20137801, 2019. 2, 4",
          "ref_ids": [
            "12"
          ],
          "1": "Furthermore, unlike prior multi-view human pose estimation methods that depend on precise camera calibration [12, 20, 64], our approach operates with minimal constraints on the capture setup and does not require prior knowledge of the environment.",
          "2": "Ex3 isting multi-person methods focus on re-identification [6, 12, 22] or, for video, on re-identification and tracking [20]."
        },
        "Multi-View Person Matching and 3D Pose Estimation with Arbitrary Uncalibrated Camera Networks": {
          "authors": [
            "Y Xu",
            "K Kitani"
          ],
          "url": "https://arxiv.org/abs/2312.01561",
          "ref_texts": "[9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019. 1, 2, 3, 6, 7",
          "ref_ids": [
            "9"
          ],
          "6": "We want to specifically mention one of the most impactful multi-stage methods in recent years, MVPose ([9]), since it can also function without using camera poses.",
          "7": "As a comparison, our method is more stable and outperforms MVPose ([9]) even when it uses camera poses.",
          "8": "[9] 97.",
          "9": "[9] w/o cam pose 97."
        },
        "SOAR improved artificial neural network for multistep decision-making tasks": {
          "authors": [
            "Guoyu Zuo"
          ],
          "url": "https://link.springer.com/article/10.1007/s12559-020-09716-6",
          "ref_texts": "23. Dong J, Jiang W, Huang Q, Bao H, Zhou X. Fast and robust multi-person 3d pose estimation from multiple views. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2019. p. 7792\u20137801.",
          "ref_ids": [
            "23"
          ],
          "1": "In addition, with the development of deep reinforcement learning, DNNs have also been successfully applied to the robot-related tasks, such as motion planning [20, 21], pose estimation [22, 23], 3D environment sensation [16, 24], robot-human interaction [21, 25], and related games, such as Atari 2600 games [26] and DeepMind Go games [27]."
        },
        "3D semantic scene perception using distributed smart edge sensors": {
          "authors": [
            "S Bultmann",
            "S Behnke"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-22216-0_22",
          "ref_texts": "9. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3D pose estimation from multiple views. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) pp. 7784\u20137793 (2019)",
          "ref_ids": [
            "9"
          ],
          "1": "3D poses are recovered from 2D keypoint detections from multiple, calibrated camera views via variants of the Pictorial Structures Model (PSM) [15,9] or based on direct triangulation [6,18]."
        },
        "Continuous-Time Human Motion Field from Events": {
          "authors": [
            "Z Wang",
            "R Zhang",
            "ZY Liu",
            "Y Wang"
          ],
          "url": "https://arxiv.org/abs/2412.01747",
          "ref_texts": "[9] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 7792\u20137801, 2019. 2",
          "ref_ids": [
            "9"
          ],
          "1": "To recover SMPL parameters, methods employ an optimization-based approach by fitting to the image evidence [2, 5, 9], or learn from the data to directly regress the pose and shape parameters [13, 19, 21, 23, 38, 49]."
        },
        "An evaluation of different methods for 3d-driver-body-pose estimation": {
          "authors": [
            "M Martin",
            "M Voit",
            "R Stiefelhagen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9564676/",
          "ref_texts": "[26] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and Robust Multi-Person 3D Pose Estimation From Multiple Views,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "26"
          ],
          "1": "Using a 3D pictorial structure model (3DPS) for the whole human body instead of triangulating each joint separately can improve results further [26].",
          "2": "This can be done for example with a reidentification network [26] or geometric constraints [27] and can also be integrated into 3D pictorial structure models [18]."
        },
        "3D hypothesis clustering for cross-view matching in multi-person motion capture": {
          "authors": [
            "M Li",
            "Z Zhou",
            "X Liu"
          ],
          "url": "https://link.springer.com/article/10.1007/s41095-020-0171-y",
          "ref_texts": "[14] Dong, J.; Jiang, W.; Huang, Q.; Bao, H.; Zhou X. Fast and robust multi-person 3D pose estimation from multiple views. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7792\u2013",
          "ref_ids": [
            "14"
          ],
          "1": "[14] address these challenges with a convex optimization based multi-way matching algorithm, which determines correspondences for all views at once.",
          "4": "[14] is a very recent method, which combines a human detector [15] and a single-person pose estimator [16] to obtain multi-person 2D poses in all views, and then uses a convex optimization based multi-way matching algorithm to match detected poses across views, from which the 3D pose of each person is inferred."
        },
        "Matching and recovering 3D people from multiple views": {
          "authors": [
            "Alejandro Perez",
            "Antonio Agudo"
          ],
          "url": "http://openaccess.thecvf.com/content/WACV2022/html/Perez-Yus_Matching_and_Recovering_3D_People_From_Multiple_Views_WACV_2022_paper.html",
          "ref_texts": "[20] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3D pose estimation from multiple views. In CVPR, 2019.",
          "ref_ids": [
            "20"
          ],
          "1": "More recently, other works have suggested to match the detected 2D poses among multiple views at the body level [20], and then inferring the 3D pose in a reduced state space, decreasing drastically the computational cost without sacrificing the accuracy.",
          "5": "To make the analysis more complete, we include the partial affinities based on Geometry, appearance cues Campus Shelf Affinity Optimization Precision Recall F1-score Precision Recall F1-score Geometry [20] 95.",
          "6": "40 ReID [20] 98.",
          "7": "+ ReID [20] [20] 99.",
          "8": "+ ReID [20] Ours 99.",
          "9": "39 Ours [20] 99.",
          "10": "Multiple combinations are provided by considering the partial affinities geometry and re-identification (ReID), as well as their combination as it was done by [20]; and our proposal.",
          "11": "Moreover, it is also included the optimization algorithms provided by [20] and ours.",
          "12": "(ReID) and the combination of both, as proposed by [20], as well as their optimization algorithm.",
          "20": "3D reconstruction error in terms of an MPJPE, as well as the accuracy by using PCP2 (actors 1-3 and 4, respectively) and PCK (with 50, 100 and 150mm threshold) in datasets Campus and Shelf, compared to [20].",
          "21": "Additionally, we use the source code provided by [20] to report a full analysis, including the remaining metrics defined previously."
        },
        "Adaptive affinity for associations in multi-target multi-camera tracking": {
          "authors": [
            "Y Hou",
            "Z Wang",
            "S Wang",
            "L Zheng"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9646485/",
          "ref_texts": "[86] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation from multiple views,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7792\u20137801. Yunzhong Hou received his bachelor degree in electronic engineering from Tsinghua University in 2018. He is now working towards a PhD degree at Australian National University under the supervision of Dr. Liang Zheng and Prof. Stephen Gould. His research interests lies in computer vision and deep learning. Zhongdao Wang received his B.S degree in the Department of Physics at Tsinghua University in 2017. He is now working towards the Ph.D. degree in the Department of Electronic Engineering at Tsinghua University. His research interests include computer vision, pattern recognition and particularly person/face recognition and retrieval. Shengjin Wang received the B.E. degree from Tsinghua University, China, in 1985 and the Ph.D. degree from the Tokyo Institute of Technology, Tokyo, Japan, in 1997. From 1997 to 2003, he was a member of Research Staff in the Internet System Research Laboratories, NEC Corporation, Japan. Since 2003, he has been a Professor with the Department of Electronic Engineering, Tsinghua University. He has published over 80 papers on image processing, computer vision, and pattern recognition. His current research interests include image processing, computer vision, video surveillance, and pattern recognition. He is a member of the IEEE and the IEICE. Liang Zheng is a Lecturer and a Computer Science Futures Fellow in the Research School of Computer Science, Australian National University. He received the PhD degree in Electronic Engineering from Tsinghua University, China, in 2015, and the B.E. degree in Life Science from Tsinghua University, China, in 2010. He was a postdoc researcher in the Center for Artificial Intelligence, University of Technology Sydney, Australia. His research interests include image retrieval, classification, and person reidentification.",
          "ref_ids": [
            "86"
          ],
          "1": ", MOTA and IDF1 both around 98% from [86])."
        },
        "Light3DPose: real-time multi-person 3D pose estimation from multiple views": {
          "authors": [
            "A Elmi",
            "D Mazzini",
            "P Tortella"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9412652/",
          "ref_texts": "[36] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation from multiple views,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "36"
          ],
          "4": "[36] 98.",
          "6": "For reference we can compare our method with the one presented in [36], see Table III."
        },
        "Motion-based feature analysis for the design of full-body interactions in the context of computer vision and large volume spaces": {
          "authors": [
            "A Escamilla Pinilla"
          ],
          "url": "https://openaccess.uoc.edu/handle/10609/150344",
          "ref_texts": "[82] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and Robust MultiPerson 3D Pose Estimation From Multiple Views\u201d, in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Long Beach, CA, USA: IEEE, Jun. 2019, pp. 7784\u20137793, ISBN : 978-1-72813-293-8. DOI : 10.1109/ CVPR.2019.00798.",
          "ref_ids": [
            "82"
          ],
          "2": "[82] proposed a multi-way matching algorithm to guarantee cycle consistency across all views and a re-ID model to get appearance features for each person to enhance cross-view consistency.",
          "3": "[82] in which appearance similarity and geometric compatibility cues are combined to calculate the affinity score between bounding boxes."
        },
        "3D Holistic OR Anonymization": {
          "authors": [
            "TD Wang"
          ],
          "url": "https://arxiv.org/abs/2405.05261",
          "ref_texts": "[11] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. \u201cFast and Robust Multi-Person 3D Pose Estimation from Multiple Views.\u201d In:CoRRabs/1901.04111 (2019). arXiv: 1901.04111.url: http://arxiv.org/abs/1901.04111.",
          "ref_ids": [
            "11"
          ],
          "1": "Current approaches mainly rely on triangulating 2D poses and reconstructing them in 3D via a volumetric [42, 51] or an analytical approach [7, 11, 19, 26, 28].",
          "2": "Most approaches explicitly create cross-view correspondences first in order to group 2D joints from multiple cameras and then reconstruct the 3D poses from the clustered 2D poses for each person [7, 11, 20, 26]."
        },
        "LiCamPose: Combining Multi-View LiDAR and RGB Cameras for Robust Single-frame 3D Human Pose Estimation": {
          "authors": [
            "Z Pan",
            "Z Zhong",
            "W Guo",
            "Y Chen",
            "J Feng"
          ],
          "url": "https://arxiv.org/abs/2312.06409",
          "ref_texts": "[8] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3D pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019. 2, 3",
          "ref_ids": [
            "8"
          ],
          "1": "The basic 3D pose estimation method typically follows a two-stage process: first estimating the 2D pose and then lifting it into 3D space [8, 22, 32, 42, 47, 50].",
          "2": "For multi-person settings, some methods [8, 32] match pedestrians from different views and locate them through 2D pose similarity.",
          "3": "3D Human Pose Estimation Similar to [8, 42, 47], we adopt a top-down approach to estimate 3D poses."
        },
        "E3Pose: Energy-Efficient Edge-assisted Multi-camera System for Multi-human 3D Pose Estimation": {
          "authors": [
            "L Zhang",
            "J Xu"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3576842.3582370",
          "ref_texts": "[8] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "8"
          ],
          "1": "We measure the 3D pose estimation performance by both the Mean Per Joint Position Error (MPJPE) and the Percentage of Correct Parts (PCP) between the estimated 3D pose and the ground truth, following the same evaluation protocol in the literature [8, 9, 11, 19, 20].",
          "2": "We follow the previous works [8, 9, 11, 19, 20] to evaluate the accuracy of 3D pose estimation.",
          "3": "3D Pose Estimation Accuracy : For the Shelf dataset, we use the Percentage of Correctly estimated Parts (PCP) as a metric to evaluate the accuracy of the estimated 3D poses to enable a direct comparison with existing works [8, 9, 11, 19, 20]."
        },
        "3D Human Pose Estimation from multi-view thermal vision sensors": {
          "authors": [
            "M Lupi\u00f3n Lorente",
            "A Polo-Rodriguez",
            "J Medina-Quero"
          ],
          "url": "https://repositorio.ual.es/handle/10835/17902",
          "ref_texts": "[55] J. Dong, W. Jiang, Q. Huang, H. Bao, X. Zhou, Fast and robust multi-person 3d pose estimation from multiple views, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "55"
          ],
          "1": "[55] presents a multi-way matching algorithm to cluster detected 2D poses across multiple calibrated camera views, enabling effective inference of 3D poses."
        },
        "Domain Adapted Visual Representation Learning for Machine Perception": {
          "authors": [
            "YJ Li"
          ],
          "url": "https://search.proquest.com/openview/9f948f9c0399d78de90e7f592c756b48/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[63] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019. 95, 97, 99, 104, 105, 106",
          "ref_ids": [
            "63"
          ],
          "1": "MULTI-OBJECT 3D POSE ESTIMATION FROM MULTI-VIEW UNCALIBRATED DIVERSE SENSORS 277, 341] or a 3D regression-free [14, 15, 63, 66] manners.",
          "2": "3D regression-free approaches are usually multi-stage [14, 15, 63, 66].",
          "3": "They first obtain the 2D poses [28, 44, 148, 264] before matching the 2D poses using appearance features [319] and geometry cues [63].",
          "4": "Following [63, 66, 318], we detect 2D body key points across all camera views using YOLOv3 [220] for bounding box detection and HRNet [264] for top-down pose estimation.",
          "6": "For 3D human pose estimation, we follow the same evaluation protocols as previous works [63,318] and report the percentage of correctly estimated parts (PCP) to measure the accuracy of the predicted 3D poses.",
          "7": "We compare MVD-HPE against baselines using both calibrated ([14, 63, 66]) and uncalibrated ([318]) cameras in Table 7.",
          "9": "[63] \u2713 94.",
          "10": "[63] \u2713 92.",
          "11": "[63] \u2713 93."
        },
        "Occlusion-aware heatmap generation for enhancing 3D human pose estimation in multi-person environments": {
          "authors": [
            "S Lee",
            "JT Lee"
          ],
          "url": "https://www.researchsquare.com/article/rs-3820469/latest",
          "ref_texts": "8. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation from multiple views. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7792\u20137801",
          "ref_ids": [
            "8"
          ],
          "2": "Following [8,38], we employ the Percentage of Correct Parts (PCP3D) metric for evaluating the Shelf and Campus datasets, calculating it as the ratio of accurately matched ground truth (GT) poses to their nearest estimations."
        },
        "Multi-view matching (mvm): Facilitating multi-person 3d pose estimation learning with action-frozen people video": {
          "authors": [
            "Y Shen",
            "CCJ Kuo"
          ],
          "url": "https://arxiv.org/abs/2004.05275",
          "ref_texts": "[3] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "3"
          ],
          "9": "As shown in Table 2, the proposed MVM method achieves comparable performance with the state-of-the-art optimization method [3] on short clips at a much lower computational cost."
        },
        "3D Human Pose and Shape Estimation for Autonomous Telerehabilitation Systems": {
          "authors": [
            "M\u00c2SC Varandas"
          ],
          "url": "https://estudogeral.uc.pt/retrieve/269393/Disserta%C3%A7%C3%A3o_Miguel_Varandas_Final.pdf",
          "ref_texts": "[10] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou,Fast and robust multi-person 3d pose estimation from multiple views, 2019. arXiv:1901.04111 [cs.CV].",
          "ref_ids": [
            "10",
            "cs\\.CV"
          ],
          "1": "2 Illustrative example of a model whose aim is to remove the three-dimensional pose of several people from different viewpoints, with problems such as occlusion and overlapping fields of vision [10].",
          "2": "2: Illustrativeexampleofamodelwhoseaimistoremovethethree-dimensionalpose of several people from different viewpoints, with problems such as occlusion and overlapping fields of vision [10]."
        },
        "Collaborative Wireless Deep Learning System for Edge Intelligence": {
          "authors": [
            "L Zhang"
          ],
          "url": "https://scholarship.miami.edu/view/pdfCoverPage?instCode=01UOML_INST&filePid=13417569070002976&download=true",
          "ref_texts": "[30] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "30"
          ],
          "1": "Recent efforts have been shifted to studying multi-view approaches [30, 31, 32, 33] where the 3D poses are constructed from multiple camera views.",
          "2": "2 3D Pose Estimation Accuracy Multi-human 3D pose estimation solutions [91, 92, 30, 31, 32, 33, 93, 94] in the computer vision literature focus on how to improve the estimation accuracy with a given set of camera views.",
          "3": "3D pose and the ground truth, following the same evaluation protocol in the literature [33, 93, 94, 31, 30].",
          "4": "We follow the previous works [33, 93, 94, 31, 30] to evaluate the accuracy of 3D pose estimation.",
          "5": "\u0088 3D Pose Estimation Accuracy: For the Shelf dataset, we use the Percentage of Correctly estimated Parts (PCP) as a metric to evaluate the accuracy of the estimated 3D poses to enable a direct comparison with existing works [33, 93, 94, 31, 30].",
          "6": "Among these methods, [91, 92, 30, 31, 32, 109] study 3D pose estimation from a pure computer vision view without considering system design.",
          "7": "0 CVPR2019 [30] 98.",
          "8": "2 3D Pose Estimation Depending on the number of input cameras, 3D human pose estimation methods are categorized into single-view-based methods [27, 28, 29, 118, 119, 120] and multiview-based methods [93, 91, 92, 30, 31, 32, 34, 33, 94].",
          "9": "Most state-of-the-art multi-human 3D pose estimation methods [91, 92, 30, 31, 32, 33] match the 2D poses estimation results from cross-view cameras, and fuse the matched 2D poses into 3D human poses."
        },
        "LG-Hand: Advancing 3D Hand Pose Estimation with Locally and Globally Kinematic Knowledge": {
          "authors": [
            "T Le-Xuan",
            "T Tran-Quang",
            "TNH Doan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9953799/",
          "ref_texts": "[2] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019.",
          "ref_ids": [
            "2"
          ],
          "1": "When it comes from 2D to 3D prediction, both hand and human pose estimation suffer from depth ambiguity [1] and multi-view variation [2]."
        },
        "Skeleton-based Approaches based on Machine Vision: A Survey": {
          "authors": [
            "J Li",
            "B Li",
            "M Gao"
          ],
          "url": "https://arxiv.org/abs/2012.12447",
          "ref_texts": "[7] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3D pose estimation from multiple views,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "7"
          ],
          "1": "In regard to multiple persons, a multi-way matching algorithm [7] clusters detected 2D skeletons by sticking the keypoints of a person in various views."
        },
        "Human Object Ownership Tracking in Autonomous Retail": {
          "authors": [
            "JD Lisboa De Menezes Falc\u00e3o"
          ],
          "url": "https://kilthub.cmu.edu/articles/thesis/Human_Object_Ownership_Tracking_in_Autonomous_Retail/21514734",
          "ref_texts": "[23] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019. Cited on page 101.",
          "ref_ids": [
            "23"
          ],
          "1": "There are works [13, 15, 23, 59] that focus on leveraging cameras to track people continuously across multiple camera views [15], focused on dense environments [59] or on the ability to reconstruct the 3D motion of people [104] in order to understand their behavior."
        },
        "LiCamPose: Combining Multi-View LiDAR and RGB Cameras for Robust Single-timestamp 3D Human Pose Estimation": {
          "authors": [
            "ZPZZW Guo",
            "YCJ Feng",
            "J Zhou"
          ],
          "url": "https://ivg.au.tsinghua.edu.cn/~jfeng/pubs/Pan_WACV2025_LiCamPose.pdf",
          "ref_texts": "[10] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3D pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7792\u20137801, 2019. 2, 4",
          "ref_ids": [
            "10"
          ],
          "1": "The basic 3D pose estimation method typically follows a two-stage process: first estimating the 2D pose and then lifting it into 3D space [10, 25, 35, 47, 52, 55].",
          "2": "For multi-person settings, some methods [10, 35] match pedestrians from different views and locate them through 2D pose similarity.",
          "3": "3D Human Pose Estimation Similar to [10, 47, 52], we adopt a top-down approach to estimate 3D poses."
        },
        "TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting Supplementary Material": {
          "authors": [
            "RCKMK L\u00e1szl\u00f3",
            "A Jeni"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Choudhury_TEMPO_Efficient_Multi-View_ICCV_2023_supplemental.pdf",
          "ref_texts": "[2] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
          "ref_ids": [
            "2"
          ],
          "1": "Furthermore, TEMPO significantly exceeds the performance of MVPose [2], a method that is based on graph optimization and is dataset-agnostic by design, underscoring the strength of volumetric pose estimation methods."
        },
        "Vision-based 3D human and hand pose analysis": {
          "authors": [
            "Y Cai"
          ],
          "url": "https://dr.ntu.edu.sg/handle/10356/153319",
          "ref_texts": "[48] Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7792\u20137801, 2019. 10",
          "ref_ids": [
            "48"
          ],
          "1": "Though multiple viewpoints [48, 49] or other sensors such as depth sensors [50, 51] and IMUs (Wearable Inertial Measurement Units ) [52, 53] can achieve accurate results by incorporating rich 3D information, most recent research work [54, 55] focus on the challenging 3D human pose estimation from monocular images or videos, as it is more favored in real world applications."
        },
        "Markerless Motion Analysis from Synchronized 2D Camera Views: A Convolutional Neural Network Approach": {
          "authors": [
            "F Piemontese"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/24614",
          "ref_texts": "[57] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cF ast and robust multiperson 3d pose estimation from multiple views,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 7792\u20137801.",
          "ref_ids": [
            "57"
          ],
          "1": "The approach most similar to ours is perhaps the one presented in [57].",
          "2": "Compared to other tracking techniques making use of neural networks [56, 57], a greater emphasis was placed on the retrieval of anatomically accurate movement information, via the implementation of a subject-specific prediction refinement routine."
        },
        "Supplementary Material: Dynamic Multi-Person Mesh Recovery From Uncalibrated Multi-View Cameras": {
          "authors": [
            "B Huang",
            "Y Shu",
            "T Zhang",
            "Y Wang"
          ],
          "url": "http://www.buzhenhuang.com/publications/papers/3DV2021-SupplementaryMaterial.pdf",
          "ref_texts": "[3] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation from multiple views. In CVPR, 2019. 1",
          "ref_ids": [
            "3"
          ],
          "1": "We follow the same evaluation protocol as in previous works [3] and compute the PCP (percentage of correctly estimated parts) scores to measure the accuracy of 3D pose estimation."
        },
        "3D EgoPose Estimation: Leveraging Multi-View perspectives in the Egobody Dataset": {
          "authors": [
            "SR RAZA"
          ],
          "url": "https://thesis.unipd.it/handle/20.500.12608/64836",
          "ref_texts": "[3] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation from multiple views,\u201d 2019.",
          "ref_ids": [
            "3"
          ],
          "1": "4 Overview of the proposed approach of [3].",
          "2": "Some approaches leverage prior knowledge about the scene layout or interaction context to improve pose estimation accuracy [3].",
          "3": "For instance, [3] utilizes knowledge about object affordances and hand-object interactions to enhance pose estimation.",
          "5": "[3] FastMvPose worked on identifying and matching correspondences using for cross-view matching."
        },
        "\u59ff\u52e2\u63a8\u5b9a\u306b\u3088\u308b\u4eba\u9593\u62e1\u5f35\u306e\u305f\u3081\u306e\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u4eba\u5f71\u751f\u6210": {
          "authors": [
            "\u5409\u7530\u5320\u543e\uff0c \u8b1d\u6d69\u7136\uff0c \u5bae\u7530\u4e00\u4e58"
          ],
          "url": "https://dspace.jaist.ac.jp/dspace/handle/10119/17614",
          "ref_texts": "[12] Dong, J., Jiang, W., Huang, Q., Bao, H., & Zhou, X. (2019). Fast and robust multi-person 3d pose estimation from multiple views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7792 -7801). ",
          "ref_ids": [
            "12"
          ],
          "1": "\u3053\u306e\u8ab2\u984c\u306f,\u8907\u6570\u306e\u30ab\u30e1\u30e9\u3092\u7528\u3044\u305f\u30e2\u30fc\u30b7\u30e7\u30f3\u30ad\u30e3\u30d7\u30c1\u30e3\u306e\u9ad8\u901f\u5316\u624b\u6cd5[12]\u3092\u5fdc\u7528\u3059\u308b\u3053\u3068\u3067\u89e3\u6c7a\u3067\u304d\u308b\u3068\u8003\u3048\u3089\u308c\u308b."
        }
      }
    },
    {
      "title": "fast and robust multi-person 3d pose estimation and tracking from multiple views",
      "id": 13,
      "valid_pdf_number": "47/64",
      "matched_pdf_number": "31/47",
      "matched_rate": 0.6595744680851063,
      "citations": {
        "Capturing and inferring dense full-body human-scene contact": {
          "authors": [
            "Hao P. Huang",
            "Hongwei Yi",
            "Markus Hoschle",
            "Matvey Safroshkin",
            "Tsvetelina Alexiadis",
            "Senya Polikovsky",
            "Daniel Scharstein",
            "Michael J. Black"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Huang_Capturing_and_Inferring_Dense_Full-Body_Human-Scene_Contact_CVPR_2022_paper.html",
          "ref_texts": "[13] Junting Dong, Qi Fang, W en Jiang, Y urou Y ang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3D pose estimation and tracking from multiple views.IEEE Transactions on P attern Analysis and Machine Intelligence , 2021.",
          "ref_ids": [
            "13"
          ],
          "1": "Powered by CNNs, recent methods leverage multiview consistency to improve keypoint detection [27, 31, 59, 73], to re-identify subjects across views [14] or across view and time [13, 96], but they estimate only joints, not body meshes.",
          "2": "Other methods that build such 4D associations [13, 96] could also be applied here."
        },
        "Gm-nerf: Learning generalizable model-based neural radiance fields from multi-view images": {
          "authors": [
            "Jianchuan Chen",
            "Wentao Yi",
            "Liqian Ma",
            "Xu Jia",
            "Huchuan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.html",
          "ref_texts": "[11] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. PAMI, 2021. 2",
          "ref_ids": [
            "11"
          ],
          "1": "Given m calibrated multi-view images {Ik}m k=1 of a person, we use Easymocap [11] to obtain the SMPL [23] parameters M(\u03b8, \u03b2) of the person."
        },
        "Learning analytical posterior probability for human mesh recovery": {
          "authors": [
            "Qi Fang",
            "Kang Chen",
            "Yinghui Fan",
            "Qing Shuai",
            "Jiefeng Li",
            "Weidong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.html",
          "ref_texts": "[7] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE TPAMI, 44(10):6981\u20136992, 2021. 2",
          "ref_ids": [
            "7"
          ],
          "1": "Multi-sensor fusion: Recently an increasing number of approaches attempt to integrate extra observations from other sensors, such as IMUs [10,65,66] and muli-view cameras [7, 71, 73], to obtain more reliable estimations."
        },
        "HOI-M^ 3: Capture Multiple Humans and Objects Interaction within Contextual Environment": {
          "authors": [
            "Juze Zhang",
            "Jingyan Zhang",
            "Zining Song",
            "Zhanhe Shi",
            "Chengfeng Zhao",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_HOI-M3_Capture_Multiple_Humans_and_Objects_Interaction_within_Contextual_Environment_CVPR_2024_paper.html",
          "ref_texts": "[13] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE TPAMI, 44(10):6981\u20136992, 2021. 4",
          "ref_ids": [
            "13"
          ],
          "1": "Specifically, we formulate a cross-view affinity matrix and address the multi-view matching problem using an established algorithm [13]."
        },
        "Markerless multi-view 3D human pose estimation: A survey": {
          "authors": [
            "AFR Nogueira",
            "HP Oliveira",
            "LF Teixeira"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0262885625000253",
          "ref_texts": "[23] Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., Zhou, X.,2022. Fastandrobustmulti-person3dposeestimationandtracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 6981\u20136992. doi:10.1109/TPAMI.2021.",
          "ref_ids": [
            "23"
          ],
          "1": "Finding solutions for this task is essential for numerous applications ranging from human-robot interaction [92], animation [37], gaming, action recognition [53], rehabilitation assessments, surveillance systems [23], sports [60, 9], live broadcasts [28], human-computer interaction, such as recognising sign language [25], among many others.",
          "2": "Later, in [23], the researchers extend their approach to also, track the poses by introducing temporal tracking and Riemannian Extended Kalman filtering.",
          "3": "Although, the presented method is faster than PS-based methods, in scenarios with a small number of cameras the use of PS for joint estimation has been proven to be more precise than triangulation [23, 9].",
          "4": "Demonstrating the existing trade-off between speed and accuracy [23]."
        },
        "InterCap: Joint markerless 3D tracking of humans and objects in interaction": {
          "authors": [
            "Y Huang",
            "O Taheri",
            "MJ Black",
            "D Tzionas"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-16788-1_18",
          "ref_texts": "[9] Dong, J., Fang, Q., Jiang, W., Yang, Y ., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3D pose estimation and tracking from multiple views. Transactions on Pattern Analysis and Machine Intelligence (TPAMI)14(8), 1\u201312 (2021)",
          "ref_ids": [
            "9"
          ],
          "1": "Many methods estimate 3D bodies from multi-view images but focus on skeletons and not 3D bodies [9, 10, 19, 24, 46, 55, 66]."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[15] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multiperson 3d pose estimation and tracking from multiple views. T-PAMI, 2021.",
          "ref_ids": [
            "15"
          ],
          "1": "Given four monocular part-specific videos (body, head, and two hands) of the performer, we first utilize the EasyMoCap [1, 16, 15] to estimate SMPL+H [42] parameters for the body and hands videos and utilize an adaptation of [49] to estimate the FLAME [28] parameters for the face video."
        },
        "Gaussian shadow casting for neural characters": {
          "authors": [
            "Luis Bolanos",
            "Yang Su",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Bolanos_Gaussian_Shadow_Casting_for_Neural_Characters_CVPR_2024_paper.html",
          "ref_texts": "[10] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. In TPAMI, 2021.",
          "ref_ids": [
            "10"
          ],
          "1": "We capture the data using 3 cameras (Canon EOS R8, Canon EOS 70D, iPhone12) and obtain SMPL estimates using EasyMocap [1,10,11]."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
          "authors": [
            "Chiu Ma",
            "Anqi Joyce",
            "Shenlong Wang",
            "Raquel Urtasun",
            "Antonio Torralba"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.html",
          "ref_texts": "[21] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. InTPAMI, 2021.",
          "ref_ids": [
            "21"
          ],
          "1": "With the flourishing of deep learning, these methods have made tremendous progress, either from a single image [41, 44, 47] or multi-view images [21,22,25,61]."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "8. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Fast and robust multiperson 3d pose estimation and tracking from multiple views. In: T-PAMI (2021)",
          "ref_ids": [
            "8"
          ],
          "1": "Since then, plenty of work has contributed to estimating the SMPL parameters of subjects from 2D inputs [1,8,35]."
        },
        "Pov-surgery: A dataset for egocentric hand and tool pose estimation during surgical activities": {
          "authors": [
            "R Wang",
            "S Ktistakis",
            "S Zhang",
            "M Meboldt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-43996-4_42",
          "ref_texts": "5. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Easymocap make human motion capture easier. Github (2021),https://github.com/zju3dv/ EasyMocap 6. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Fast and robust multiperson 3d pose estimation and tracking from multiple views. In: T-PAMI (2021)",
          "ref_ids": [
            "5"
          ],
          "1": "We adopt the popular [5][6] module for SMPLX body reconstruction.",
          "2": "[5] is used to reconstruct 3D hand poses from different camera observations."
        },
        "AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction from Sparse Multi-view Videos": {
          "authors": [
            "F Lu",
            "Z Dong",
            "J Song",
            "O Hilliges"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73668-1_13.pdf",
          "ref_texts": "18. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Fast and robust multiperson 3d pose estimation and tracking from multiple views. In: T-PAMI (2021)",
          "ref_ids": [
            "18"
          ],
          "1": "To correct abnormal or missing pose estimations, some methods [18,21] leverage parametric body models like SMPL [37] as full-body priors and fit these 3D models to 2D joint estimations.",
          "2": "Some follow-ups [18,73] show that parametric models help in correcting implausible 3D pose estimates and filling in missing joints.",
          "4": "MVPose [18] adds temporal tracking and SMPL prior to MVPose* [19] and is regarded as a SMPLguided method."
        },
        "A dual-masked auto-encoder for robust motion capture with spatial-temporal skeletal token completion": {
          "authors": [
            "J Jiang",
            "J Chen",
            "Y Guo"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3547796",
          "ref_texts": "[10] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. 2021. Fast and Robust Multi-Person 3D Pose Estimation and Tracking from Multiple Views.IEEE Transactions on Pattern Analysis and Machine Intelligence 14, 8 (2021), 1\u20131. https://doi.org/10.1109/TPAMI.2021.3098052",
          "ref_ids": [
            "10"
          ],
          "6": "[10] proposed an affinity matrix consisting of 2D appearance features and epipolar geometry to establish the correspondences of the 2D candidates across the view and filter the false detections."
        },
        "Improved Trajectory Reconstruction for Markerless Pose Estimation": {
          "authors": [
            "RJ Cotton",
            "A Cimorelli",
            "K Shah"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10340745/",
          "ref_texts": "[20] J. Dong, Q. Fang, W. Jiang, Y . Yang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation and tracking from multiple views,\u201d in T-PAMI, 2021.",
          "ref_ids": [
            "20"
          ],
          "1": "We developed an annotation tool using EasyMocap to identify the participant [19], [20]."
        },
        "Pressim: An end-to-end framework for dynamic ground pressure profile generation from monocular videos using physics-based 3d simulation": {
          "authors": [
            "LSS Ray",
            "B Zhou",
            "S Suh"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10150221/",
          "ref_texts": "[17] J. Dong, Q. Fang, W. Jiang, Y . Yang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation and tracking from multiple views,\u201d in T-PAMI, 2021.",
          "ref_ids": [
            "17"
          ],
          "1": "[7] For volumetric pose estimation, we used easymocap [17] that estimates SMPL+H body shape and pose from monocular videos."
        },
        "Simultaneously recovering multi-person meshes and multi-view cameras with human semantics": {
          "authors": [
            "B Huang",
            "J Ju",
            "Y Shu",
            "Y Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10299685/",
          "ref_texts": "[1] J. Dong, Q. Fang, W. Jiang, Y . Yang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation and tracking from multiple views,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6981\u20136992, 2021.",
          "ref_ids": [
            "1"
          ],
          "2": "MvPose [1] considers both geometric and appearance constraints simultaneously.",
          "4": "Other recent works [1], [3], [32], [75], [76] are based on calibrated cameras."
        },
        "3DSA: Multi-view 3D Human Pose Estimation With 3D Space Attention Mechanisms": {
          "authors": [
            "BH Chen",
            "C Tsai"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73383-3_19",
          "ref_texts": "9. Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence44(10), 6981\u2013",
          "ref_ids": [
            "9"
          ],
          "1": "In 2D-3D lifting approaches [9,10,42], a monocular pose estimator identifies 2D bounding boxes and 2D poses for individuals in each view.",
          "2": "Existing methods can be categorized into three types: (1)2D to 3D lifting methods [1\u20133,5,9,10,18,25,42] (2)Voxel-based methods [6,7,19,20,27,30,32,35,38,41] (3)Direct regression method [40].",
          "3": "[9,10]proposeMvPose."
        },
        "Multiple View Geometry Transformers for 3D Human Pose Estimation": {
          "authors": [
            "Ziwei Liao",
            "Jialiang Zhu",
            "Chunyu Wang",
            "Han Hu",
            "Steven L. Waslander"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liao_Multiple_View_Geometry_Transformers_for_3D_Human_Pose_Estimation_CVPR_2024_paper.html",
          "ref_texts": "[9] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6981\u20136992, 2021. 1, 2, 4",
          "ref_ids": [
            "9"
          ],
          "1": "Some methods [4, 8, 9, 27, 37] address the task from a primarly geometric direction.",
          "2": "One group of works [8, 9, 27] follows a two-stage design, first solving associations with a hand-crafted affinity matrix, and then estimating 3D poses with triangulation.",
          "3": "Video input has also been exploited for 3d pose estimation [9, 29, 37, 38] relying on temporal information to improve pose tracking over time."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[7] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. In T-PAMI, 2021. 2",
          "ref_ids": [
            "7"
          ],
          "1": "The widespread adoption of SMPL(-X) has further been boosted for human body animation, driven by methods [7, 46] that estimate parameters of SMPL(-X) from 2D image inputs."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[6] J. Dong, Q. Fang, W. Jiang, Y . Yang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. In T-PAMI, 2021. 2",
          "ref_ids": [
            "6"
          ],
          "1": "Furthermore, it has been widely adopted for human body animation, thanks to the methods [6, 39] of estimating SMPL parameters from 2D inputs."
        },
        "Artistic control over the glitch in AI-generated motion capture": {
          "authors": [
            "J Knight",
            "A Johnston",
            "A Berry"
          ],
          "url": "https://arxiv.org/abs/2308.08576",
          "ref_texts": "[5] Dong, J., Fang, Q., Jiang, W., Yang, Y., Huang, Q., Bao, H., & Zhou, X. (2021). Fast and Robust Multi -Person 3D Pose Estimation and Tracking from Multiple Views. IEEE Transactions on Pattern Analysis and Machine Intelligence, ",
          "ref_ids": [
            "5"
          ],
          "1": "This research explores the application of machine learning in this area, using the single-camera VIBE [8] and multi-camera EasyMocap [5] models to drive performer-driven abstract animation."
        },
        "Real-time pose estimation and motion tracking for motion performance using deep learning models": {
          "authors": [
            "L Liu",
            "Y Dai",
            "Z Liu"
          ],
          "url": "https://www.degruyter.com/document/doi/10.1515/jisys-2023-0288/html",
          "ref_texts": "[5] Dong J, Fang Q, Jiang W, Yang Y, Huang Q, Bao H, et al. Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE Trans Pattern Anal Mach Intell. 2021;44(10):6981\u201392. doi: 10.1109/TPAMI.2021.3098052.",
          "ref_ids": [
            "5"
          ],
          "1": "used multi-directional matching algorithms based on convex optimization to recognize human postures, improving robustness [5]."
        },
        "Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud": {
          "authors": [
            "J Jiang",
            "J Chen",
            "HY Au",
            "M Chen",
            "W Xue"
          ],
          "url": "https://arxiv.org/abs/2502.02936",
          "ref_texts": "[15] J. Dong, Q. Fang, W. Jiang, Y. Yang, Q. Huang, H. Bao, and X. Zhou, \u201cFast and robust multi-person 3d pose estimation and tracking from multiple views,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 14, no. 8, pp. 1\u20131, 2021.",
          "ref_ids": [
            "15"
          ],
          "1": "Current studies [12], [14], [15], [16], [17], [18] show that multi-view multi-person 3D HPE can be formulated as a multi-stage framework, i.",
          "2": "To solve those issues, existing approaches [14], [15], [16], [21], [22] select the most accurate 2D candidates (discarding others) and then associate these with the target ID and joint type.",
          "3": "[15] propose two affinity matrices for associating target bodies.",
          "4": "Inspired by [14], [15], Zhou et al.",
          "5": "Existing works [14], [15], [30] rely on hard-code interpolation to convert the reconstructed skeleton format to the ground-truth skeleton format for training or evaluation.",
          "6": "2 Evaluation Metrics Following the evaluation protocols in previous works [14], [15], [21], [22], [23], [30], [51], we first used the Percentage of Correctly estimated Parts (PCP) for evaluation.",
          "7": "6 CVPR\u201920 MVPose [15] 98.",
          "8": "Following [15], [21], [30], we adopted HRNet [9] as the 2D detector for fairness of comparison.",
          "9": "We compared our method with recent state-of-the-art 3D HPE methods which can be generally divided into two categories: (1) optimizationbased approaches [14], [15], [18], [51] and (2) learning-based approaches [21], [22], [23], [25], [28], [29], [30].",
          "10": "1 CVPR\u201920 MVPose [15] 97.",
          "11": "We compared the performances among three optimizationbased approaches [14], [15], [51] and five learning-based approaches [21], [25], [28], [29], [30].",
          "13": "9 MVPose [15] 96.",
          "14": "Table 5 presents the inference speed of three optimization-based approaches [14], [15], [18] and five learning-based approaches [21], [22], [23], [28], [30] (Our framework falls into the second category) as well as the performance of our model when the number of input views TABLE 7: Performance for JCSAT variants on the Shelf dataset.",
          "15": "[15] J.",
          "16": "[15] (TPAMI\u201921) and Jiang et al.",
          "25": "[15] (TPAMI\u201921), Jiang et al.",
          "26": "[15] (TPAMI\u201921), Jiang et al.",
          "27": "[15] (TPAMI\u201921), Jiang et al.",
          "28": "[15] (TPAMI\u201921), Jiang et al."
        },
        "Aerial View 3D Human Pose Estimation Using Double Vector Quantized-Variational AutoEncoders": {
          "authors": [
            "Juheon Hwang",
            "Jiwoo Kang"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Hwang_Aerial_View_3D_Human_Pose_Estimation_Using_Double_Vector_Quantized-Variational_WACVW_2024_paper.html",
          "ref_texts": "[5] Junting Dong, Qi Fang, Wen Jiang, Yurou Yang, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6981\u20136992, 2021. 2",
          "ref_ids": [
            "5"
          ],
          "1": "Methods that employ multi-view cameras [5,6,39,42] use calibrated information from cameras to transform the 2D human pose, predicted from images captured by each camera, into a 3D pose using bundle adjustment [35]."
        },
        "A Synthetic Benchmarking Pipeline to Compare Camera Calibration Algorithms": {
          "authors": [
            "LSS Ray",
            "B Zhou",
            "L Krupp",
            "S Suh"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-78125-4_16",
          "ref_texts": "7. Dong, J., Fang, Q., Jiang, W., Yang, Y., Bao, H., Zhou, X.: Fast and robust multiperson 3d pose estimation and tracking from multiple views. In: T-PAMI (2021)",
          "ref_ids": [
            "7"
          ],
          "1": "Using SynthCal pipeline with DMCB [15] and EasyMocap [7] to estimate 3D pose from multiple view points Table 5."
        },
        "Deep Learning in Graph Domains for Sensorised Environments": {
          "authors": [
            "D Rodriguez-Criado"
          ],
          "url": "https://publications.aston.ac.uk/id/eprint/46074/",
          "ref_texts": "[46] J. Dong, Q. Fang, W. Jiang, Y. Yang, Q. Huang, H. Bao, and X. Zhou. Fast and robust multi-person 3d pose estimation and tracking from multiple views.IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021.",
          "ref_ids": [
            "46"
          ],
          "1": "Traditional approaches address the issue by employing geometric and appearance cues, as well as epipolar geometry [46, 18].",
          "2": "Examples include employing epipolar geometry to assign a cost to each detected pose [22] or embedding appearance features using a pre-trained model to provide affinity scores between bounding boxes [46].",
          "3": "Previous research has tackled this issue using algorithms based on appearance and geometric information [46, 18].",
          "4": "[46] construct affinity matrices based on the appearance between two views and utilise them as input to their model to infer the correspondence matrix."
        }
      }
    },
    {
      "title": "4k4d: real-time 4d view synthesis at 4k resolution",
      "id": 0,
      "valid_pdf_number": "27/32",
      "matched_pdf_number": "20/27",
      "matched_rate": 0.7407407407407407,
      "citations": {
        "Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes": {
          "authors": [
            "Hua Huang",
            "Tian Sun",
            "Ziyi Yang",
            "Xiaoyang Lyu",
            "Pei Cao",
            "Xiaojuan Qi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Huang_SC-GS_Sparse-Controlled_Gaussian_Splatting_for_Editable_Dynamic_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[55] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Y ujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution.arXiv preprint arXiv:2310.11448, 2023. 2",
          "ref_ids": [
            "55"
          ],
          "1": "DeVRF [25] introduces a grid representation, and IBR-based methods [20, 22, 23, 55] use multi-camera information for quality and efficiency."
        },
        "Spacetime gaussian feature splatting for real-time dynamic view synthesis": {
          "authors": [
            "Zhan Li",
            "Zhang Chen",
            "Zhong Li",
            "Yi Xu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Li_Spacetime_Gaussian_Feature_Splatting_for_Real-Time_Dynamic_View_Synthesis_CVPR_2024_paper.html",
          "ref_texts": "[97] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution.arXiv preprint arXiv:2310.11448, 2023. 3",
          "ref_ids": [
            "97"
          ],
          "2": "4K4D [97] combines 4D point clouds with K-Planes [28] and discrete image-based rendering, and uses differentiable depth peeling to train the model."
        },
        "Control4d: Efficient 4d portrait editing with text": {
          "authors": [
            "Ruizhi Shao",
            "Jingxiang Sun",
            "Cheng Peng",
            "Zerong Zheng",
            "Boyao Zhou",
            "Hongwen Zhang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Shao_Control4D_Efficient_4D_Portrait_Editing_with_Text_CVPR_2024_paper.html",
          "ref_texts": "[79] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. InCVPR, 2024.",
          "ref_ids": [
            "79"
          ],
          "1": "Meanwhile, more compact and efficient representations, such as [4, 12, 25, 79] are proposed, significantly boosting the rendering quality and efficiency."
        },
        "Tetrirf: Temporal tri-plane radiance fields for efficient free-viewpoint video": {
          "authors": [
            "Minye Wu",
            "Zehao Wang",
            "Georgios Kouros",
            "Tinne Tuytelaars"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_TeTriRF_Temporal_Tri-Plane_Radiance_Fields_for_Efficient_Free-Viewpoint_Video_CVPR_2024_paper.html",
          "ref_texts": "[44] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Y ujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution.arXiv preprint arXiv:2310.11448, 2023.",
          "ref_ids": [
            "44"
          ],
          "1": "T o accelerate the speeds, methods have been developed using grid representations [12, 22], 4D plane-based representation [3, 33, 44], and tensor factorization [2, 9, 15]."
        },
        "Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting": {
          "authors": [
            "R Zhu",
            "Y Liang",
            "H Chang",
            "J Deng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/b88318174aad2cc174a4e05ab6bfad80-Abstract-Conference.html",
          "ref_texts": "[57] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. arXiv preprint arXiv:2310.11448, 2023.",
          "ref_ids": [
            "57"
          ],
          "1": "Other works propose to use time-varying NeRFs [6, 3, 52, 53, 54] or explicit representations [55, 56, 7, 8, 57, 58, 59] to represent and render dynamic scenes."
        },
        "Gear-NeRF: free-viewpoint rendering and tracking with motion-aware spatio-temporal sampling": {
          "authors": [
            "Xinhang Liu",
            "Wing Tai",
            "Keung Tang",
            "Pedro Miraldo",
            "Suhas Lohit",
            "Moitreya Chatterjee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_Gear-NeRF_Free-Viewpoint_Rendering_and_Tracking_with_Motion-aware_Spatio-Temporal_Sampling_CVPR_2024_paper.html",
          "ref_texts": "[89] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution.arXiv preprint arXiv:2310.11448, 2023. 2",
          "ref_ids": [
            "89"
          ],
          "1": "Neural Representations for Dynamic Scenes: NeRF-like representations have recently been extended to model dynamic scenes in high fidelity [17, 28, 33, 39, 46\u201348, 53, 63, 66, 84, 89, 90, 97]."
        },
        "Brightdreamer: Generic 3d gaussian generative framework for fast text-to-3d synthesis": {
          "authors": [
            "L Jiang",
            "X Zheng",
            "Y Lyu",
            "J Zhou",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2403.11273",
          "ref_texts": "[81] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. arXiv preprint arXiv:2310.11448, 2023. 3",
          "ref_ids": [
            "81"
          ],
          "1": ", anti-aliasing novel view synthesis [83, 90], SLAM [25, 45, 82, 91], human reconstruction [1, 28, 32, 34, 40, 49], dynamic scene reconstruction [44, 74, 81, 86, 87], and 3D content generation [10, 35, 37, 65, 77, 88]."
        },
        "GaussianCity: Generative Gaussian splatting for unbounded 3D city generation": {
          "authors": [
            "H Xie",
            "Z Chen",
            "F Hong",
            "Z Liu"
          ],
          "url": "https://arxiv.org/abs/2406.06526",
          "ref_texts": "[57] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4K4D: Real-time 4D view synthesis at 4K resolution. In CVPR, 2024. 2",
          "ref_ids": [
            "57"
          ],
          "1": "Benefiting from 3D-GS, efficiently applying it to 4D object [15, 44] and human [25, 27, 57] generation with deformation becomes feasible."
        },
        "Motion-oriented compositional neural radiance fields for monocular dynamic human modeling": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72933-1_27",
          "ref_texts": "80. Xu, Z., Peng, S., Lin, H., He, G., Sun, J., Shen, Y., Bao, H., Zhou, X.: 4K4D: Real-time 4d view synthesis at 4k resolution. In: CVPR (2024)",
          "ref_ids": [
            "80"
          ],
          "1": "[7,80] focus on improving rendering efficiency to achieve realtime free-view rendering of dynamic humans."
        },
        "Compact 3d gaussian splatting for static and dynamic radiance fields": {
          "authors": [
            "JC Lee",
            "D Rho",
            "X Sun",
            "JH Ko",
            "E Park"
          ],
          "url": "https://arxiv.org/abs/2408.03822",
          "ref_texts": "[73] Z. Xu, S. Peng, H. Lin, G. He, J. Sun, Y . Shen, H. Bao, and X. Zhou, \u201c4k4d: Real-time 4d view synthesis at 4k resolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 20 029\u201320 040.",
          "ref_ids": [
            "73"
          ],
          "1": "To improve efficiency in modeling temporal movements, a line of works used additional architectures such as MLP [72] or grids [17], [73], following the NeRF paradigm."
        },
        "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos": {
          "authors": [
            "S Girish",
            "T Li",
            "A Mazumdar"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html",
          "ref_texts": "[87] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. arXiv preprint arXiv:2310.11448, 2023.",
          "ref_ids": [
            "87"
          ],
          "1": "[1, 46, 77, 87, 93] incorporate efficient NeRF representations [9, 19, 91] for higher fidelity."
        },
        "GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views": {
          "authors": [
            "B Zhou",
            "S Zheng",
            "H Tu",
            "R Shao",
            "B Liu",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2411.11363",
          "ref_texts": "[69] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2024 15 view synthesis at 4k resolution. InCVPR, pages 20029\u201320040, 2024.",
          "ref_ids": [
            "69"
          ],
          "1": "Point-based representation has shown great efficiency and simplicity for various 3D human-centered tasks [63], [64], [65], [66], [67], [68], [69].",
          "2": "Then differentiable point-based [18] and sphere-based [19], [69] rendering have been developed, which demonstrates promising rendering qualities, especially attaching them to a conventional network pipeline [60], [61].",
          "3": "Targeting different applications, there are two feasible schemes to produce free-viewpoint videos, one uses a compact 4D representation [10], [15], [78], [79], [80], [81], and the other formulate an individual 3D representation for each discrete timestamp, which can be further subdivided into on-the-fly optimization methods [23], [69], [77], [82] and feed-forward inference methods [26], [30], [83], [84].",
          "4": "Mobile-Stage dataset [69]) could break through the limitations of the fixed in-door capture system."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[51] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. InCVPR, 2024.",
          "ref_ids": [
            "51"
          ],
          "1": "Other approaches [33, 51] utilize temporal embeddings instead of SMPL poses to model each frame independently, achieving high-quality rendering but making it challenging to animate avatars."
        },
        "Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction": {
          "authors": [
            "D Chen",
            "B Oberson",
            "I Feldmann",
            "O Schreer"
          ],
          "url": "https://arxiv.org/abs/2411.06602",
          "ref_texts": "[72] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20029\u201320040, 2024. 1, 2, 6, 9",
          "ref_ids": [
            "72"
          ],
          "1": "Our approach not only achieves photorealistic novel view rendering with significantly reduced training time compared to the recent method [72], but also produces finer surface meshes, surpassing the state-of-the-art results [67].",
          "5": "5, our method achieves rendering quality comparable to 4K4D [72] and NeuS2 [67], while significantly outperforming other methods."
        }
      }
    },
    {
      "title": "niid-net: adapting surface normal knowledge for intrinsic image decomposition in indoor scenes",
      "id": 23,
      "valid_pdf_number": "23/27",
      "matched_pdf_number": "20/23",
      "matched_rate": 0.8695652173913043,
      "citations": {
        "Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing": {
          "authors": [
            "B Yang",
            "C Bao",
            "J Zeng",
            "H Bao",
            "Y Zhang",
            "Z Cui"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_34",
          "ref_texts": "26. Luo, J., Huang, Z., Li, Y., Zhou, X., Zhang, G., Bao, H.: NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes. IEEE Transactions on Visualization and Computer Graphics 26(12), 3434\u20133445",
          "ref_ids": [
            "26"
          ],
          "1": "Early methods mainly focus on editing a single static view by inserting [16], compositing [37], moving [18,46] objects or changing lighting [26] for an existing photograph."
        },
        "Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis": {
          "authors": [
            "Weicai Ye",
            "Shuo Chen",
            "Chong Bao",
            "Hujun Bao",
            "Marc Pollefeys",
            "Zhaopeng Cui",
            "Guofeng Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.html",
          "ref_texts": "[40] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes. IEEE Transactions on Visualization and Computer Graphics, 26(12):3434\u20133445, 2020.",
          "ref_ids": [
            "40"
          ],
          "1": "Recently, deep learning methods [3, 17, 36, 40, 73, 81] have emerged to perform intrinsic decomposition, and with large datasets [34, 35, 53], they have shown further improvement."
        },
        "Intrinsic image decomposition via ordinal shading": {
          "authors": [
            "C Careaga",
            "Y Aksoy"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3630750",
          "ref_texts": "2021. OpenRooms: An Open Framework for Photorealistic Indoor Scene Datasets. Proc. CVPR. G. Lin, A. Milan, C. Shen, and I. Reid. 2017. RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation. In Proc. CVPR. Yunfei Liu, Yu Li, Shaodi You, and Feng Lu. 2020. Unsupervised Learning for Intrinsic Image Decomposition from a Single Image. In Proc. CVPR. Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. 2020. NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes. IEEE Trans. Vis. Comp. Graph.(2020). Wei-Chiu Ma, Hang Chu, Bolei Zhou, Raquel Urtasun, and Antonio Torralba. 2018. Single Image Intrinsic Decomposition Without a Single Intrinsic Image. In Proc. ECCV. Abhimitra Meka, Maxim Maximov, Michael Zollhoefer, Avishek Chatterjee, HansPeter Seidel, Christian Richardt, and Christian Theobalt. 2018. LIME: Live Intrinsic Material Estimation. In Proc. CVPR. S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, and Ya\u011f\u0131z Aksoy. 2021. Boosting Monocular Depth Estimation Models to High-Resolution via ContentAdaptive Multi-Resolution Merging. In Proc. CVPR. Lukas Murmann, Michael Gharbi, Miika Aittala, and Fredo Durand. 2019. A MultiIllumination Dataset of Indoor Object Appearance. In Proc. ICCV. Takuya Narihira, Michael Maire, and Stella X Yu. 2015. Learning Lightness from Human Judgement on Relative Reflectance. In Proc. CVPR. Thomas Nestmeyer and Peter V Gehler. 2017. Reflectance Adaptive Filtering Improves Intrinsic Image Estimation. In Proc. CVPR. Patrick P\u00e9rez, Michel Gangnet, and Andrew Blake. 2003. Poisson Image Editing. In ACM SIGGRAPH. 313\u2013318. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.",
          "ref_ids": [
            "2021"
          ],
          "1": "[2021] that introduces a method for boosting the resolution of pre-trained monocular depth networks.",
          "2": "[2021] for ordinal depth estimation networks.",
          "3": "[2021], in their analysis of monocular depth estimation at different resolutions, proposes to use the image edge density to determine the resolution at which the network can still produce consistent results."
        },
        "GR-PSN: Learning to estimate surface normal and reconstruct photometric stereo images": {
          "authors": [
            "Y Ju",
            "B Shi",
            "Y Chen",
            "H Zhou",
            "J Dong"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10306333/",
          "ref_texts": "[1] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao, \u201cNiid-net: adapting surface normal knowledge for intrinsic image decomposition in indoor scenes,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3434\u20133445, 2020.",
          "ref_ids": [
            "1"
          ],
          "1": "\u2726 1 I NTRODUCTION R ECOVERING the 3D shape of an object is a pivotal problem in many computer graphics and vision applications because it can further improve the understanding of images and scenes [1], [2], [3], [4]."
        },
        "Blinkvision: A benchmark for optical flow, scene flow and point tracking estimation using rgb frames and events": {
          "authors": [
            "Y Li",
            "Y Shen",
            "Z Huang",
            "S Chen",
            "W Bian",
            "X Shi"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72855-6_2",
          "ref_texts": "41. Luo, J., Huang, Z., Li, Y., Zhou, X., Zhang, G., Bao, H.: Niid-net: adapting surface normal knowledge for intrinsic image decomposition in indoor scenes. IEEE Transactions on Visualization and Computer Graphics26(12), 3434\u20133445 (2020)",
          "ref_ids": [
            "41"
          ],
          "1": "BlinkVision allows the evaluation of point tracking with different trajectories\u2019 lengths, from small to big, corresponding to different downstream applications like video editing [25,41] or augmented reality [31,39]."
        },
        "Measured albedo in the wild: Filling the gap in intrinsics evaluation": {
          "authors": [
            "J Wu",
            "S Chowdhury",
            "H Shanmugaraja"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10233761/",
          "ref_texts": "[25] J. Luo, Z. Huang, Y. Li, X. Zhou, G. Zhang, and H. Bao, \u201cNiid-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes,\u201d IEEE Transactions on Visualization and Computer Graphics, 2020.",
          "ref_ids": [
            "25"
          ],
          "2": "We finetune various SOTA algorithms (Sengupta 2019 [12], Li 2020 [13], and NIID-Net [25]) on our MAW dataset using all four metrics (WHDR, intensity, chromaticity, and texture).",
          "4": "On the other hand, such texture patterns are much more well preserved in NIIDNet [25].",
          "6": "0005 for NIID-Net [25].",
          "7": "We run 9 algorithms, which are Bell 2014 [18], CGI [7], Nestmeyer 2017 [21], Revisit [6], BigTime [22], Sengupta 2019 [12], Li 2020 [13], USI3D [4], and NIID-Net [25], on our dataset and on the IIW portion of the SAW dataset with officially released code.",
          "10": "On the third row, despite that Revisit [6] has better WHDR score, all the texture details are smoothed out, and thus has worse texture score than NIID-Net [25].",
          "12": "As shown in Figure 10, other than NIID-Net [25], and maybe USI3D [4], all methods performed poorly on this example, as barely any wood texture pattern can be seen in the predicted albedos.",
          "13": "Across all examples, we again found all algorithms other than NIID-Net [25] perform poorly on texture metric.",
          "14": "2 Finetuning on MAW We pick three SOTA algorithms, Sengupta 2019 [12], Li 2020 [13], and NIID-Net [25] for finetuning."
        },
        "IDTransformer: transformer for intrinsic image decomposition": {
          "authors": [
            "Partha Das",
            "Maxime Gevers",
            "Sezer Karaoglu",
            "Theo Gevers"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Das_IDTransformer_Transformer_for_Intrinsic_Image_Decomposition_ICCVW_2023_paper.html",
          "ref_texts": "[32] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. Niid-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes.IEEE Transactions on Visualization and Computer Graphics, 26(12):3434\u20133445, 2020. 3",
          "ref_ids": [
            "32"
          ],
          "1": "Edge maps [18], depth [17, 25] and surface normals [32] are studied as additional inputs to the network to constrain the search space and guide the decomposition problem."
        },
        "Intrinsic appearance decomposition using point cloud representation": {
          "authors": [
            "Xiaoyan Xing",
            "Konrad Groh",
            "Sezer Karaoglu",
            "Theo Gevers"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Xing_Intrinsic_Appearance_Decomposition_Using_Point_Cloud_Representation_ICCVW_2023_paper.html",
          "ref_texts": "[13] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. Niid-net: adapting surface normal knowledge for intrinsic image decomposition in indoor scenes. IEEE TVCG, 26(12):3434\u20133445, 2020.",
          "ref_ids": [
            "13"
          ],
          "1": "Previous methods mainly investigate the priors from images, such as human perception [8, 14], context-based priors [4, 5, 20], and geometric priors [1\u20133, 9, 13, 23]."
        },
        "Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training": {
          "authors": [
            "S Sato",
            "T Kaneko",
            "K Murasaki",
            "T Yoshida"
          ],
          "url": "https://arxiv.org/abs/2403.14089",
          "ref_texts": "41. Luo, J., Huang, Z., Li, Y., Zhou, X., Zhang, G., Bao, H.: NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes. IEEE TVCG26(12), 3434\u20133445 (2020) 2 LIET for IID 17",
          "ref_ids": [
            "41"
          ],
          "1": "Recently, a notable development has been the emergence of supervised learning models [15,29,41,47,48,68,69,72], trained on the ground-truth albedo and shade corresponding to an input image with sparsely-annotated datasets [4] or synthetic datasets [9,10,37]."
        },
        "Star-tm: structure aware reconstruction of textured mesh from single image": {
          "authors": [
            "T Wu",
            "L Gao",
            "LX Zhang",
            "YK Lai"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10226420/",
          "ref_texts": "[66] J. Luo, Z. Huang, Y. Li, X. Zhou, G. Zhang, and H. Bao, \u201cNIIDNet: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes,\u201d IEEE Trans. Vis. Comput. Graph., vol. 26, no. 12, pp. 3434\u20133445, 2020.",
          "ref_ids": [
            "66"
          ],
          "1": "2 Texture Reconstruction Before reconstructing texture, we remove the shadow from the input image and extract the albedo component using the recently proposed intrinsic image decomposition method [66].",
          "2": "To reduce lighting effects from the environment, we pass the input image through an intrinsic image decomposition network [66]."
        },
        "Life: Lighting invariant flow estimation": {
          "authors": [
            "Z Huang",
            "X Pan",
            "R Xu",
            "Y Xu",
            "G Zhang",
            "H Li"
          ],
          "url": "https://arxiv.org/abs/2104.03097",
          "ref_texts": "[23] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. Niid-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes. IEEE Transactions on Visualization and Computer Graphics, 26(12):3434\u20133445, 2020.",
          "ref_ids": [
            "23"
          ],
          "1": "Pixel-wise dense correspondences further enable image editing [19, 23], 3D reconstruction [1], etc."
        },
        "Relighting from a Single Image: Datasets and Deep Intrinsic-based Architecture": {
          "authors": [
            "Y Yang",
            "HA Sial",
            "R Baldrich"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10855561/",
          "ref_texts": "[43] J. Luo, Z. Huang, Y. Li, X. Zhou, G. Zhang, and H. Bao, \u201cNiid-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3434\u20133445, 2020.",
          "ref_ids": [
            "43"
          ],
          "1": "Most of the previous supervised approaches are based on deep architectures that extend the U-Net paradigm to a oneto-two encoder-decoder version [40]\u2013[43]."
        },
        "Event-based shape from polarization with spiking neural networks": {
          "authors": [
            "P Kang",
            "S Banerjee",
            "H Chopp",
            "A Katsaggelos"
          ],
          "url": "https://arxiv.org/abs/2312.16071",
          "ref_texts": "[2] Jundan Luo, Zhaoyang Huang, Yijin Li, Xiaowei Zhou, Guofeng Zhang, and Hujun Bao. Niid-net: adapting surface normal knowledge for intrinsic image decomposition in indoor scenes. IEEE Transactions on Visualization and Computer Graphics , 26(12):3434\u20133445, 2020.",
          "ref_ids": [
            "2"
          ],
          "1": "Introduction Precise surface normal estimation can provide valuable information about a scene\u2019s geometry and is useful for many computer vision tasks, including 3D Reconstruction [1], Augmented Reality (AR) and Virtual Reality (VR) [2, 3], Material Classification [4], and Robotics Navigation [5]."
        },
        "A Self-Occlusion Aware Lighting Model for Real-Time Dynamic Reconstruction": {
          "authors": [
            "C Zheng",
            "W Lin",
            "F Xu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9783067/",
          "ref_texts": "[56] J. Luo, Z. Huang, Y . Li, X. Zhou, G. Zhang, and H. Bao, \u201cNiid-net: Adapting surface normal knowledge for intrinsic image decomposition in indoor scenes,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3434\u20133445, 2020.",
          "ref_ids": [
            "56"
          ],
          "1": "[56] proposed a novel learning-based framework that adapted surface normal knowledge to decompose a natural image into a reflectance image and a shading image.",
          "3": "Based on a state-of-theart intrinsic decomposition method NIID-Net [56], we fuse the output albedo map of each frame into a model."
        }
      }
    },
    {
      "title": "world-grounded human motion recovery via gravity-view coordinates",
      "id": 50,
      "valid_pdf_number": "2/2",
      "matched_pdf_number": "1/2",
      "matched_rate": 0.5,
      "citations": {
        "Joint Optimization for 4D Human-Scene Reconstruction in the Wild": {
          "authors": [
            "Z Liu",
            "J Lin",
            "W Wu",
            "B Zhou"
          ],
          "url": "https://arxiv.org/abs/2501.02158",
          "ref_texts": "[18] Z. Shen, H. Pi, Y . Xia, Z. Cen, S. Peng, Z. Hu, H. Bao, R. Hu, and X. Zhou, \u201cWorld-grounded human motion recovery via gravity-view coordinates,\u201d in SIGGRAPH Asia Conference Proceedings, 2024. 2, 1",
          "ref_ids": [
            "18"
          ],
          "1": "While estimating human motion in the local camera coordinate is straightforward with end-to-end models [15] and large-scale training data [16], how to transform the camera-frame human motion to the global frame still remains an open-ended problem [17, 18].",
          "3": "GVHMR [18] 109."
        }
      }
    },
    {
      "title": "efficient neural radiance fields for interactive free-viewpoint video",
      "id": 9,
      "valid_pdf_number": "90/105",
      "matched_pdf_number": "66/90",
      "matched_rate": 0.7333333333333333,
      "citations": {
        "Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction": {
          "authors": [
            "Ziyi Yang",
            "Xinyu Gao",
            "Wen Zhou",
            "Shaohui Jiao",
            "Yuqing Zhang",
            "Xiaogang Jin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Yang_Deformable_3D_Gaussians_for_High-Fidelity_Monocular_Dynamic_Scene_Reconstruction_CVPR_2024_paper.html",
          "ref_texts": "[27] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 2",
          "ref_ids": [
            "27"
          ],
          "1": "Other methods seek to enhance the quality of dynamic neural rendering from various aspects, including segmenting static and dynamic objects in the scene [45, 48], incorporating depth information [1] to introduce geometric prior, introducing 2D CNN to encode scene priors [27, 39], and leveraging the redundant information in 20332"
        },
        "Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes": {
          "authors": [
            "Hua Huang",
            "Tian Sun",
            "Ziyi Yang",
            "Xiaoyang Lyu",
            "Pei Cao",
            "Xiaojuan Qi"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Huang_SC-GS_Sparse-Controlled_Gaussian_Splatting_for_Editable_Dynamic_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[22] Haotong Lin, Sida Peng, Zhen Xu, Y unzhi Y an, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In ACM SIGGRAPH ASIA, pages 1\u20139, 2022.2",
          "ref_ids": [
            "22"
          ],
          "1": "DeVRF [25] introduces a grid representation, and IBR-based methods [20, 22, 23, 55] use multi-camera information for quality and efficiency."
        },
        "Vastgaussian: Vast 3d gaussians for large scene reconstruction": {
          "authors": [
            "Jiaqi Lin",
            "Zhihao Li",
            "Xiao Tang",
            "Jianzhuang Liu",
            "Shiyong Liu",
            "Jiayue Liu",
            "Yangdi Lu",
            "Xiaofei Wu",
            "Songcen Xu",
            "Youliang Yan",
            "Wenming Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Lin_VastGaussian_Vast_3D_Gaussians_for_Large_Scene_Reconstruction_CVPR_2024_paper.html",
          "ref_texts": "[25] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. InSIGGRAPH Asia, 2022. 3",
          "ref_ids": [
            "25"
          ],
          "1": "As NeRF [31] becomes a popular 3D representation for photo-realistic novel-view synthesis in recent years [35], many variants are proposed to improve quality [2\u20134, 24, 45, 47\u201349, 57], increase speed [8, 9, 11, 14, 20, 32, 36, 37, 40, 43, 46, 58, 60], extend to dynamic scenes [7, 15, 18, 25, 27, 50], and so on."
        },
        "Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis": {
          "authors": [
            "Shunyuan Zheng",
            "Boyao Zhou",
            "Ruizhi Shao",
            "Boning Liu",
            "Shengping Zhang",
            "Liqiang Nie",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_GPS-Gaussian_Generalizable_Pixel-wise_3D_Gaussian_Splatting_for_Real-time_Human_Novel_CVPR_2024_paper.html",
          "ref_texts": "[19] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, pages 1\u20139, 2022. 1, 2, 3, 6, 7",
          "ref_ids": [
            "19"
          ],
          "1": "The performance outperforms the state-of-the-art feedforward NVS methods ENeRF [19], FloRen [47] and 3D-GS [12], which are representative approaches in implicit neural human rendering, image-based human rendering and per-subject optimization, respectively.",
          "3": ", PixelNeRF [68], IBRNet [57], MVSNeRF [3] and ENeRF [19] resort to image-based features as potent prior cues for feed-forward scene modeling.",
          "5": "Similar to ENeRF [19], we evaluate PSNR, SSIM [59] and LPIPS [71] as metrics for the rendering results in foreground regions determined by the bounding box of humans.",
          "6": "Considering that our goal is instant novel view synthesis, we compare our GPS-Gaussian against three generalizable methods including implicit method ENeRF [19], image-based rendering method FloRen [47] and hybrid method IBRNet [57]."
        },
        "Gauhuman: Articulated gaussian splatting from monocular human videos": {
          "authors": [
            "Shoukang Hu",
            "Tao Hu",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_GauHuman_Articulated_Gaussian_Splatting_from_Monocular_Human_Videos_CVPR_2024_paper.html",
          "ref_texts": "[66] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 3",
          "ref_ids": [
            "66"
          ],
          "1": "Another line of works reconstruct 3D human performers with implicit neural representations from sparse-view videos [12, 48, 50, 66, 68, 80, 82, 83, 83, 103, 114, 116, 120] or even a single image [8, 43, 65, 117]."
        },
        "4k4d: Real-time 4d view synthesis at 4k resolution": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Haotong Lin",
            "Guangzhao He",
            "Jiaming Sun",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html",
          "ref_texts": "[49] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 1, 2, 4, 5, 6, 7, 8",
          "ref_ids": [
            "49"
          ],
          "4": "1, 4K4D significantly outperforms previous dynamic view synthesis approaches [19, 49] in terms of the rendering speed, while being competitive in the rendering quality.",
          "5": "We empirically find that the image blending model [49] achieves higher rendering quality than the SH model used by 3DGS [33].",
          "13": "Our method achieves much higher rendering quality and can be rendered 14\u00d7 faster than ENeRF[49].",
          "14": "1, our method renders 30x faster than the SOTA realtime dynamic view synthesis method ENeRF [49] with superior quality.",
          "15": "Other image-based methods [48, 49, 90] produce high-quality appearance."
        },
        "MuRF: multi-baseline radiance fields": {
          "authors": [
            "Haofei Xu",
            "Anpei Chen",
            "Yuedong Chen",
            "Christos Sakaridis",
            "Yulun Zhang",
            "Marc Pollefeys",
            "Andreas Geiger",
            "Fisher Yu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_MuRF_Multi-Baseline_Radiance_Fields_CVPR_2024_paper.html",
          "ref_texts": "[16] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 1, 2, 4, 5, 6",
          "ref_ids": [
            "16"
          ],
          "5": "As shown in Table 2, we achieve more than 1dB PSNR improvement compared to previous best method ENeRF [16]."
        },
        "Cogs: Controllable gaussian splatting": {
          "authors": [
            "Heng Yu",
            "Joel Julin",
            "Zoltan A. Milacski",
            "Koichiro Niinuma",
            "Laszlo A. Jeni"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Yu_CoGS_Controllable_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[20] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2",
          "ref_ids": [
            "20"
          ],
          "1": "Research on dynamic scenes often also addresses the use of multiple synchronized cameras [17, 18, 37, 38] and focuses on accelerating both training [4, 5, 8, 27, 34, 44] and inference processes [3, 20, 26, 45]."
        },
        "AIM 2024 sparse neural rendering challenge: Methods and results": {
          "authors": [
            "M Nazarczuk",
            "S Catley-Chandar",
            "T Tanay"
          ],
          "url": "https://arxiv.org/abs/2409.15045",
          "ref_texts": "16. Lin, H., Peng, S., Xu, Z., Yan, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient Neural Radiance Fields for Interactive Free-viewpoint Video. In: SIGGRAPH Asia Conference Proceedings (2022)",
          "ref_ids": [
            "16"
          ],
          "1": "rendering speed and training time [6,16,21,35], reconstruction accuracy [3,5,37], editing [2,11,27], rasterisation paradigms [13]."
        },
        "Learning neural duplex radiance fields for real-time view synthesis": {
          "authors": [
            "Ziyu Wan",
            "Christian Richardt",
            "Aljaz Bozic",
            "Chao Li",
            "Vijay Rengarajan",
            "Seonghyeon Nam",
            "Xiaoyu Xiang",
            "Tuotuo Li",
            "Bo Zhu",
            "Rakesh Ranjan",
            "Jing Liao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wan_Learning_Neural_Duplex_Radiance_Fields_for_Real-Time_View_Synthesis_CVPR_2023_paper.html",
          "ref_texts": "[23] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. InSIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022.",
          "ref_ids": [
            "23"
          ],
          "1": "ENeRF [23] tries to tackle interactive free-viewpoint video by skipping the sampling of empty space."
        },
        "Finerecon: Depth-aware feed-forward network for detailed 3d reconstruction": {
          "authors": [
            "Noah Stier",
            "Anurag Ranjan",
            "Alex Colburn",
            "Yajie Yan",
            "Liang Yang",
            "Fangchang Ma",
            "Baptiste Angles"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Stier_FineRecon_Depth-aware_Feed-forward_Network_for_Detailed_3D_Reconstruction_ICCV_2023_paper.html",
          "ref_texts": "[13] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2",
          "ref_ids": [
            "13"
          ],
          "1": "For instance, multiple works [13, 19, 27] use depth estimates to improve ray sampling efficiency."
        },
        "G3R: Gradient Guided Generalizable Reconstruction": {
          "authors": [
            "Y Chen",
            "J Wang",
            "Z Yang",
            "S Manivasagam"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72658-3_18",
          "ref_texts": "27. Lin, H., Peng, S., Xu, Z., Yan, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient neural radiance fields for interactive free-viewpoint video. In: SIGGRAPH Asia 2022 Conference Papers (2022)",
          "ref_ids": [
            "27"
          ],
          "1": "These methods utilize an encoder to predict the intermediate scene representation by aggregating image features extracted from multiple source views according to camera and geometry priors, and then decode the representation for NVS via volume rendering or a transformer [7,27,71,72,90].",
          "5": "For generalizable NVS, we compare against MVSNeRF [7], ENeRF [27], GNT [71] and concurrent work PixelSplat [6].",
          "6": "Models Novel View Synthesis Inference Time PSNR\u2191 SSIM\u2191 LPIPS\u2193 Recon Time Render FPS Generalizable ENeRF [27] 15.",
          "7": "2 ENeRF ENeRF [27] constructs a sequential cost volume to predict the approximate geometry and conducts efficient depth-guided sampling."
        },
        "Efficient large-scale scene representation with a hybrid of high-resolution grid and plane features": {
          "authors": [
            "Y Zhang",
            "G Chen",
            "S Cui"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0031320324007520",
          "ref_texts": "[24] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Aisa Conference, 2022. 3",
          "ref_ids": [
            "24"
          ],
          "1": "To improve 2 the rendering speed of NeRF, some follow-up works have been proposed [18, 24, 27, 28, 32, 53, 56, 57, 64]."
        },
        "PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling": {
          "authors": [
            "Xiaoyun Zheng",
            "Liwei Liao",
            "Xufeng Li",
            "Jianbo Jiao",
            "Rongjie Wang",
            "Feng Gao",
            "Shiqi Wang",
            "Ronggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_PKU-DyMVHumans_A_Multi-View_Video_Benchmark_for_High-Fidelity_Dynamic_Human_Modeling_CVPR_2024_paper.html",
          "ref_texts": "[18] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2, 3",
          "ref_ids": [
            "18"
          ],
          "1": "This approach has also shown promising results in human modeling [18, 30, 52], although it still relies on relatively dense multiview videos as input."
        },
        "GSNeRF: Generalizable semantic neural radiance fields with enhanced 3D scene understanding": {
          "authors": [
            "Ting Chou",
            "Yu Huang",
            "Jieh Liu",
            "Chiang Frank"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Chou_GSNeRF_Generalizable_Semantic_Neural_Radiance_Fields_with_Enhanced_3D_Scene_CVPR_2024_paper.html",
          "ref_texts": "[16] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2",
          "ref_ids": [
            "16"
          ],
          "1": "In response to this limitation, some methods [3, 14, 16, 18, 27, 29, 32, 36] propose on-the-fly construction of NeRF, allowing trained NeRFs applied for synthesizing novel views in unseen scenes."
        },
        "Learning visibility field for detailed 3d human reconstruction and relighting": {
          "authors": [
            "Ruichen Zheng",
            "Peng Li",
            "Haoqian Wang",
            "Tao Yu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.html",
          "ref_texts": "[27] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 6",
          "ref_ids": [
            "27"
          ],
          "1": "We also extract patches using depth-guided raymarching [27] and supervise its albedo using VGG perceptual loss."
        },
        "Nerf in robotics: A survey": {
          "authors": [
            "G Wang",
            "L Pan",
            "S Peng",
            "S Liu",
            "C Xu",
            "Y Miao"
          ],
          "url": "https://arxiv.org/abs/2405.01333",
          "ref_texts": "[178] H. Lin, S. Peng, Z. Xu, Y . Yan, Q. Shuai, H. Bao, and X. Zhou, \u201cEfficient neural radiance fields for interactive free-viewpoint video,\u201d in SIGGRAPH Asia, 2022, pp. 1\u20139.",
          "ref_ids": [
            "178"
          ],
          "1": "These works [174], [176]\u2013[178] use depth as a geometric prior guiding ray sampling on the surface.",
          "2": "ENeRF [178] leverages explicit geometry provided by Multi-View Stereo (MVS)."
        },
        "3D multi-frame fusion for video stabilization": {
          "authors": [
            "Zhan Peng",
            "Xinyi Ye",
            "Weiyue Zhao",
            "Tianqi Liu",
            "Huiqiang Sun",
            "Baopu Li",
            "Zhiguo Cao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Peng_3D_Multi-frame_Fusion_for_Video_Stabilization_CVPR_2024_paper.html",
          "ref_texts": "[15] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In ACM SIGGRAPH Asia, pages 39:1\u201339:9, 2022. 2, 3, 4, 8",
          "ref_ids": [
            "15"
          ],
          "1": "As a significant work in view synthesis, NeRF[15] attains photorealistic synthesized images through implicit volumetric representation and volume rendering.",
          "3": "Inspired by generalized rendering technologies from IBRNet[34] and ENeRF[15], which utilize multi-view images and associated features to predict radiance fields, we further propose the Stabilized Rendering.",
          "4": "A direct method to define the ray range entails treating the sequence of frames as a static scene: estimating the coarse geometry of each ray and rendering through spatial points sampled from re-defined fine ranges, such as [15, 34].",
          "5": "We compare various range strategies to affirm the importance of ARR: (1) IBRNet [34] and ENeRF [15] employ coarse-to-fine range strategy, and (2) we adopt even sampling of 128 points following setting of IBRNet as a substitution for ARR."
        },
        "Cvt-xrf: Contrastive in-voxel transformer for 3d consistent radiance fields from sparse inputs": {
          "authors": [
            "Yingji Zhong",
            "Lanqing Hong",
            "Zhenguo Li",
            "Dan Xu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zhong_CVT-xRF_Contrastive_In-Voxel_Transformer_for_3D_Consistent_Radiance_Fields_from_CVPR_2024_paper.html",
          "ref_texts": "[17] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH, 2022. 8",
          "ref_ids": [
            "17"
          ],
          "2": "For 3-view, our approach achieves the best results compared with other methods, and also achieves higher performance compared to the works that require pretraining [3, 12, 17]."
        },
        "Consistentnerf: Enhancing neural radiance fields with 3d consistency for sparse view synthesis": {
          "authors": [
            "S Hu",
            "K Zhou",
            "K Li",
            "L Yu",
            "L Hong",
            "T Hu",
            "Z Li"
          ],
          "url": "https://arxiv.org/abs/2305.11031",
          "ref_texts": "[16] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2, 3, 6, 7, 8",
          "ref_ids": [
            "16"
          ],
          "1": "Our proposed method achieves state-of-the-art results compared to existing approaches, including NeRF [21], DSNeRF [5], MipNeRF [1], InfoNeRF [14], DietNeRF [9], RegNeRF [22], MVSNeRF [2], GeoNeRF [12], and ENeRF [16], across various datasets such as DTU dataset [11], Forward-Facing LLFF dataset [20] and Realistic Synthetic NeRF dataset [21]."
        },
        "Leia: Latent view-invariant embeddings for implicit 3d articulation": {
          "authors": [
            "A Swaminathan",
            "A Gupta",
            "K Gupta",
            "SR Maiya"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72640-8_12",
          "ref_texts": "16. Lin, H., Peng, S., Xu, Z., Yan, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient neural radiance fields for interactive free-viewpoint video. In: SIGGRAPH Asia Conference Proceedings (2022)",
          "ref_ids": [
            "16"
          ],
          "1": "[16,26,45] followed this work, by learning a 5D spatiotemporal neural field."
        },
        "Ghnerf: Learning generalizable human features with efficient neural radiance fields": {
          "authors": [
            "Arnab Dey",
            "Di Yang",
            "Rohith Agaram",
            "Antitza Dantcheva",
            "Andrew I. Comport",
            "Srinath Sridhar",
            "Jean Martinet"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NRI/html/Dey_GHNeRF_Learning_Generalizable_Human_Features_with_Efficient_Neural_Radiance_Fields_CVPRW_2024_paper.html",
          "ref_texts": "[21] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 2, 3, 4, 5, 6, 7",
          "ref_ids": [
            "21"
          ],
          "1": "Recent advancements in Neural Radiance Fields (NeRF) have demonstrated remarkable potential in generating photorealistic virtual human avatars from mere 2D images [14, 21, 37].",
          "2": "[21] that allows for near real-time inference.",
          "3": "The works [7, 21, 28, 44] address the long training and inference time of the NeRF by using faster sampling techniques, voxel representation, and hash encoding.",
          "4": "Similarly, ENeRF [21] extracts multiscale image features from a CNNbased encoder, and then the encodings are also used as input and to create a cost volume.",
          "5": "We take inspiration from previous generalizable methods [21, 43, 45], and we use two different encoders: one to generate a human feature and the other for multiscale image features similar to [21].",
          "6": "Each query point is projected on the input images, and then the pixel-aligned image features from each image are combined using a pooling operator [21] that is denoted as fimg = \u03c8(f1, .",
          "7": "Multiscale features are also used to generate voxel-aligned features similar to [21] denoted by fvoxel.",
          "8": "We also add perceptual loss lperc to image patches similar to [21].",
          "9": "We have extended ENeRF[21] to output heatmaps by adding an additional output branch and reported its performance as a baseline for the joint estimation task.",
          "10": "The experiments show that our method maintains the same level of performance in novel-view synthesis compared to state-of-the-art ENeRF [21] but performs significantly better in joint estimation compared to the baseline ENeRF."
        },
        "Neural scene chronology": {
          "authors": [
            "Haotong Lin",
            "Qianqian Wang",
            "Ruojin Cai",
            "Sida Peng",
            "Hadar Averbuch",
            "Xiaowei Zhou",
            "Noah Snavely"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Neural_Scene_Chronology_CVPR_2023_paper.html",
          "ref_texts": "[21] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, 2022. 3",
          "ref_ids": [
            "21"
          ],
          "1": "Many works [8,17,18,21,34 \u201336,51,53,54] extend NeRF to model dynamic scenes with moving objects given a monocular or multi-view video as input."
        },
        "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos": {
          "authors": [
            "S Girish",
            "T Li",
            "A Mazumdar"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html",
          "ref_texts": "[45] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022.",
          "ref_ids": [
            "45"
          ],
          "1": "[45] focuses on generalizable NeRF reconstruction and shows good promise to adapt to a new frame but has a high memory footprint due to an MVSNet-style neural network [8, 90]."
        },
        "Efficient ray sampling for radiance fields reconstruction": {
          "authors": [
            "S Sun",
            "M Liu",
            "Z Fan",
            "Q Jiao",
            "Y Liu",
            "L Dong",
            "L Kong"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S0097849323002868",
          "ref_texts": "[15] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. SIGGRAPH Asia 2022 Conference Papers, 2021. 1, 2, 3, 5, 6, 7, 8, 11, 12, 13, 14",
          "ref_ids": [
            "15"
          ],
          "2": "We demonstrate this by applying our approach to ENeRF [15], achieving state-of-the-art performance on both view synthesis and training time, as shown in Fig.",
          "6": "ENeRF [15] uses geometric feature bodies to guide the radiance fields sampling near surfaces, significantly improving the training and rendering efficiency.",
          "20": "Analysis of convergence efficiency Owing to the various ray sampling strategies proposed herein, the training efficiency of our model is markedly superior to existing generalizable neural radiance fields [4, 15, 35, 42].",
          "21": "The ENeRF framework [15] integrating our ray sampling method demonstrates superior convergence efficiency and rendering quality compared to the vanilla ENeRF [15], while also generalizing better across diverse testing datasets with abbreviated training durations, surpassing concurrent works [4, 15, 25, 35].",
          "22": "With merely 2-3 input views, our model requires dramatically fewer iterations for fine-tuning on novel scenes to achieve competitive performance versus these baselines [4, 15, 25, 35].",
          "24": "155 eRF [15], with all models fine-tuned for 2k iterations, our model demonstrates superior details compared to ENeRF."
        },
        "Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures": {
          "authors": [
            "G Sun",
            "R Dabral",
            "H Zhu",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://arxiv.org/abs/2412.13183",
          "ref_texts": "[40] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 1, 6, 7, 15, 16",
          "ref_ids": [
            "40"
          ],
          "1": "Our method consistently beats baseline approaches [34, 40, 54, 61] in terms of rendering quality and inference speed.",
          "6": "ENeRF [40] is a real-time generalizable neural rendering method for general scenes.",
          "8": "Though ENeRF [40] is not specifically designed for human rendering, their method has some generalization ability."
        },
        "Real-time high-resolution view synthesis of complex scenes with explicit 3D visibility reasoning": {
          "authors": [
            "T Zhou",
            "Y Liu",
            "X Chu",
            "C Cao",
            "C Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10753629/",
          "ref_texts": "[7] H. Lin, S. Peng, Z. Xu, Y . Yan, Q. Shuai, H. Bao, and X. Zhou, \u201cEfficient neural radiance fields for interactive free-viewpoint video,\u201d in SIGGRAPH Asia 2022 Conference Papers, 2022, pp. 1\u20139. I, I, II-A, II-A, II-B, II-C, II-C, III-C3, IV-B, IV-C2, IV",
          "ref_ids": [
            "7"
          ],
          "3": "A more similar work to our method is ENeRF [7], which uses a depth-guided strategy to reduce the sampled points to accelerate generalizable NeRF methods, achieving real-time when rendering images at a resolution of 512 \u00d7512.",
          "4": "ENeRF [7] seeks to accelerate generalizable NeRF methods to achieve real-time rendering of dynamic scenes.",
          "5": "As a generalizable NeRF rendering method, ENeRF [7] uses a depth-guided sampling strategy to reduce the number of sampled points to accelerate the generalizable NeRF pipeline, achieving real-time when rendering images at a resolution of 512 \u00d7 512.",
          "8": "As a generalizable rendering method, we mainly compared with recent state-of-the-art NeRF-based generalizable rendering methods, including MVSNeRF [9], ENeRF [7] and NeuRay [8]."
        },
        "Research on 3D Visualization of Drone Scenes Based on Neural Radiance Fields": {
          "authors": [
            "Pengfei Jin",
            "Zhuoyuan Yu"
          ],
          "url": "https://www.mdpi.com/2079-9292/13/9/1682",
          "ref_texts": "23. Lin, H.; Peng, S.; Xu, Z.; Yan, Y.; Shuai, Q.; Bao, H.; Zhou, X. Efficient neural radiance fields for interactive free-viewpoint video. In Proceedings of the SIGGRAPH Asia 2022 Conference Papers, Daegu, Republic of Korea, 6\u20139 December 2022; pp. 1\u20139. [CrossRef]",
          "ref_ids": [
            "23"
          ],
          "1": "Enerf [23] boosts rendering speed with a depth-guided sampling that relies on predicted coarse geometry."
        },
        "Gaussian time machine: A real-time rendering methodology for time-variant appearances": {
          "authors": [
            "L Shen",
            "HN Chow",
            "L Wang",
            "T Zhang",
            "M Wang"
          ],
          "url": "https://arxiv.org/abs/2405.13694",
          "ref_texts": "15. Lin, H., Peng, S., Xu, Z., Yan, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient neural radiance fields for interactive free-viewpoint video. In: SIGGRAPH Asia 2022 Conference Papers. pp. 1\u20139 (2022) 3",
          "ref_ids": [
            "15"
          ],
          "1": "Efficient-NeRF [15] takes use of geometric consistency on videos recorded in different angles and guides ray sampling with predicted depth, enabling interactive free-viewpoint videos."
        },
        "Eva-Gaussian: 3D Gaussian-based real-time human novel view synthesis under diverse camera settings": {
          "authors": [
            "Y Hu",
            "Z Liu",
            "J Shao",
            "Z Lin",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2410.01425",
          "ref_texts": "[11] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 1, 3, 6, 7",
          "ref_ids": [
            "11"
          ],
          "1": "We compare our proposed EV A-Gaussian against the state-of-the-art approaches GPSGaussian [36] and ENeRF [11].",
          "4": "We first compare our approach against state-of-the-art (SOTA) feed-forward reconstruction methods, including ENeRF [11], pixelSplat [2], MVSplat [4], MVSGaussian [13], and GPS-Gaussian [36]."
        },
        "NeVRF: Neural Video-Based Radiance Fields for Long-Duration Sequences": {
          "authors": [
            "M Wu",
            "T Tuytelaars"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550605/",
          "ref_texts": "[19] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 3",
          "ref_ids": [
            "19"
          ],
          "1": "The recent method, ENeRF [19], exploits neural networks to inference per-frame geometry via multi-view consistency and achieves efficient rendering."
        },
        "GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views": {
          "authors": [
            "B Zhou",
            "S Zheng",
            "H Tu",
            "R Shao",
            "B Liu",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2411.11363",
          "ref_texts": "[12] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, pages 1\u20139, 2022. 1, 3, 6, 7, 8, 12",
          "ref_ids": [
            "12"
          ],
          "1": "Our methods outperform the state-of-the-art feed-forward implicit rendering method ENeRF [12], explicit rendering method MVSplat [13] and optimization-based methods 3D-GS [14] and 4D-GS [15].",
          "2": "XX, XX 2024 3 IBRNet [50], MVSNeRF [51] and ENeRF [12] resort to image-based features as potent priors for feed-forward scene modeling.",
          "5": "Following ENeRF [12], we evaluate our method and other baselines with PSNR, SSIM [94] and LPIPS [96] as metrics for the rendering results in valid regions of novel views.",
          "12": "ENeRF [12] constructs two cascade cost volumes on the target viewpoint, then predicts the target view depth followed by a depth-guided sampling for volume rendering."
        },
        "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features": {
          "authors": [
            "A Dey",
            "CY Lu",
            "AI Comport",
            "S Sridhar",
            "CT Lin"
          ],
          "url": "https://arxiv.org/abs/2411.03086",
          "ref_texts": "[33] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 2, 6, 7",
          "ref_ids": [
            "33"
          ],
          "2": "For all experiments, we utilize the official versions of GHNeRF [13] and ENeRF [33], training them for 100,000 iterations.",
          "3": "To demonstrate the generalization capability of the proposed method, we evaluate our pre-trained model directly 6 Image Dense Pose 3D Pose 2D Pose Inference Time Method PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 MPJPE \u2193 PCK \u2191 FPS \u2191 ENeRF [33] 32.",
          "4": "Image Dense Pose 3D Pose 2D Pose Subject Method PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 MPJPE \u2193 PCK \u2191 S00 ENeRF [33] 31."
        },
        "Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos": {
          "authors": [
            "X Luo",
            "J Peng",
            "Z Cai",
            "L Yang",
            "F Yang",
            "Z Cao"
          ],
          "url": "https://arxiv.org/abs/2501.13335",
          "ref_texts": "[38] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, 2022. 1, 2",
          "ref_ids": [
            "38"
          ],
          "1": "To tackle the challenge, implicit neural radiance field (NeRF) [38, 51, 69, 83] is introduced to combine with body articulation [10, 12, 20, 35, 39, 57, 77] or condition on body-related encodings [69, 83].",
          "2": "With the advent in neural radiance fields [46] (NeRF), there has been a significant surge in neural rendering for human avatars [2, 19, 38, 51, 80, 85] from sparse view videos or a single image [15, 37]."
        },
        "EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View": {
          "authors": [
            "Z Wang",
            "Y Kanamori",
            "Y Endo"
          ],
          "url": "https://arxiv.org/abs/2410.12242",
          "ref_texts": "[26] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 39:1\u2013",
          "ref_ids": [
            "26"
          ],
          "3": "GP-NeRF [8] and ENeRF [26] achieve efficient rendering by reducing the number of sample positions in volume rendering.",
          "4": "ENeRF [26] constructs a cost volume to estimate depth that guides the sampling.",
          "6": "We also compared with the speed-prioritized NeRF-based methods, including GP-NeRF [8], ENeRF [26]."
        },
        "Ray-Patch: An Efficient Querying for Light Field Transformers": {
          "authors": [
            "Tomas Berriel",
            "Javier Civera"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Martins_Ray-Patch_An_Efficient_Querying_for_Light_Field_Transformers_ICCVW_2023_paper.html",
          "ref_texts": "[15] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022.",
          "ref_ids": [
            "15"
          ],
          "1": "3D querying cost using depth [38, 15, 26, 8], geometry [34, 6, 40, 37], or changing the discretization [16, 39, 20]; and avoided per-scene optimization using latent vectors [34, 16, 6, 40, 37, 11, 14]."
        },
        "No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics": {
          "authors": [
            "D Shi",
            "S Cao",
            "L Fan",
            "B Wu",
            "J Guo",
            "R Chen"
          ],
          "url": "https://arxiv.org/abs/2502.19800",
          "ref_texts": "[15] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 2",
          "ref_ids": [
            "15"
          ],
          "1": "Although having some works in improving efficiency [15, 18], the time-consuming training and rendering still limit its practicality."
        },
        "Fast Underwater Scene Reconstruction using Multi-View Stereo and Physical Imaging": {
          "authors": [
            "S Hu",
            "Q Liu"
          ],
          "url": "https://arxiv.org/abs/2501.11884",
          "ref_texts": "[45] H. Lin, S. Peng, Z. Xu, Y . Yan, Q. Shuai, H. Bao, X. Zhou, Efficient neural radiance fields for interactive free-viewpoint video, in: SIGGRAPH Asia 2022 Conference Papers, 2022, pp. 1\u20139.",
          "ref_ids": [
            "45"
          ],
          "1": "Thus, in the field of novel view synthesis, [43, 44, 45, 46, 47] attempted to combine MVS methods with NeRF methods or 3D-GS methods.",
          "2": "Traditional MVS methods typically use cost volume only for geometric reconstruction, but recent works [44, 45, 46] show that it can also be leveraged to infer the complete appearance of the scene.",
          "3": "Inspired by ENeRF [45], we propose to apply an underwater imaging model to MVS and reconstruct the entire underwater scene.",
          "4": "To demonstrate improvements specific to underwater scenes, we first compare our method with other MVS-based approaches, such as ENeRF [45] and MVSGaussian [46], as well as the widely used 3D reconstruction method, 3D-GS.",
          "5": "For evaluation, we adopt the criteria established in prior works, including ENeRF [45], MVSNeRF [44], and MVSGaussian [46]."
        },
        "ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events": {
          "authors": [
            "K Chen",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2409.14103",
          "ref_texts": "[36] H. Lin, S. Peng, Z. Xu, Y . Yan, Q. Shuai, H. Bao, and X. Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022.",
          "ref_ids": [
            "36"
          ],
          "1": "Recent works have facilitated NeRF to human reconstruction for various applications and scenarios, such as monocular video [78, 23], rendering efficiency [36, 8, 22], animation [87], occluded modelling [82, 81]."
        },
        "Improved Neural Radiance Fields Using Pseudo-depth and Fusion": {
          "authors": [
            "J Li",
            "Q Zhou",
            "C Yu",
            "Z Lu",
            "J Xiao",
            "Z Wang"
          ],
          "url": "https://arxiv.org/abs/2308.03772",
          "ref_texts": "[10] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2022. Efficient Neural Radiance Fields for Interactive Free-viewpoint Video. In SIGGRAPH Asia 2022 Conference Papers . 1\u20139.",
          "ref_ids": [
            "10"
          ],
          "1": "Recently, some multi-view rendering literature [5, 10, 25, 29, 33] have been proposed to address these shortcomings, which can generalize well across scenes by taking advantage of the nearby input views.",
          "2": "MVSNeRF [5] and ENeRF [10] utilize a plane swept 3D cost volume for geometric-aware scene understanding, using only a few images as input."
        },
        "Bringing Telepresence to Every Desk": {
          "authors": [
            "S Wang",
            "Z Wang",
            "R Schmelzle",
            "L Zheng"
          ],
          "url": "https://arxiv.org/abs/2304.01197",
          "ref_texts": "[30] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022. 2, 3, 4, 8",
          "ref_ids": [
            "30"
          ],
          "1": "ENeRF [30] demonstrated notable improvements in generalization over prior works [64, 4, 59, 7] while achieving real-time rendering without optimization.",
          "2": "Many RGBonly approaches [4, 62, 18, 6, 64, 30, 13, 68, 20, 8, 15] estimate scene geometry by depth-sweeping from the novel view camera.",
          "3": "We then estimate density values for each candidate similar to prior works [4, 62, 30, 18, 6].",
          "5": "(3) ENeRF [30]: a realtime generalizable radiance field optimized with a planesweep."
        },
        "AG-NeRF: Attention-Guided Neural Radiance Fields for Multi-height Large-Scale Outdoor Scene Rendering": {
          "authors": [
            "J Guo",
            "X Zhang",
            "B Zhao",
            "Q Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-8508-7_8",
          "ref_texts": "12. Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cEfficient Neural Radiance Fields for Interactive Free-viewpoint Video,\u201d in SIGGRAPH Asia 2022 Conference Papers, 2022, pp. 1\u20139.",
          "ref_ids": [
            "12"
          ],
          "1": "Recent works have been proposed to extend NeRF to unbounded scenes [8,9,10], dynamic scenes [11,12,13,14], few-shot setting [15,16,17,18,19], and large-scale outdoor scenes [2,3,4,5,6]."
        },
        "RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent": {
          "authors": [
            "Y Deng",
            "L Han",
            "T Lin",
            "L Li",
            "J Zhang"
          ],
          "url": "https://yijie21.github.io/publication/conference-paper/conference-paper.pdf",
          "ref_texts": "[7] H. Lin, S. Peng, Z. Xu, Y. Yan, Q. Shuai, H. Bao, and X. Zhou, \u201cEfficient neural radiance fields for interactive free-viewpoint video,\u201d SIGGRAPH Asia 2022 Conference Papers, 2021, doi: 10.1145/3550469.3555376.",
          "ref_ids": [
            "7"
          ],
          "1": "Rendering quality and efficiency comparison with state-of-the-art novel view synthesis methods [5], [6], [7] and light field reconstruction methods [8] on Real Forward-Facing [9] of image size512 \u00d7 384.",
          "2": "Also, in comparison with methods [7] relying on surrogate geometry, our method achieves superior visual performance while maintaining high temporal efficiency.",
          "7": "Our models are a little bit faster than ENeRF [7] in generation time; however, our method exhibits superior visual metrics on three datasets.",
          "8": "\u2022 Compared with the online novel view synthesis method [7], our method achieves better visual performance, while maintaining comparable time efficiency.",
          "9": "\u2022 Novel views can be rendered at significantly higher FPS than other novel view synthesis methods [5], [6], [7] from our generated MPIs.",
          "12": "ENeRF [7], MVSNeRF [51], and IBRNet [5] produce varying degrees of blurriness, while our method and SIMPLI-8L [8] restore the complicated texture details very well."
        },
        "MuRF: Multi-Baseline Radiance Fields Supplementary Material": {
          "authors": [
            "H Xu",
            "A Chen",
            "Y Chen",
            "C Sakaridis",
            "Y Zhang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Xu_MuRF_Multi-Baseline_Radiance_CVPR_2024_supplemental.pdf",
          "ref_texts": "[5] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 1, 4",
          "ref_ids": [
            "5"
          ],
          "2": "Our MuRF consistently outpeforms previous state-of-the-art small baseline method ENeRF [5], and the performance gap becomes larger for larger baselines."
        },
        "Free-Viewpoint Video in the Wild Using a Flying Camera": {
          "authors": [
            "Z Hong",
            "W Shen"
          ],
          "url": "https://openreview.net/forum?id=AsPHD00Wer",
          "ref_texts": "23. Lin, H., Peng, S., Xu, Z., Y an, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient neural radiance fields for interactive free-viewpoint video. In: SIGGRAPH Asia Conference Proceedings (2022)",
          "ref_ids": [
            "23"
          ],
          "1": "1 Existing Systems Multi-view systems like [15, 23, 35, 43] have showcased outstanding results on free-viewpoint rendering of dynamic scenes."
        },
        "Free-Viewpoint Video of Outdoor Sports Using a Drone": {
          "authors": [
            "Z Hong"
          ],
          "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02178.pdf",
          "ref_texts": "23. Lin, H., Peng, S., Xu, Z., Y an, Y., Shuai, Q., Bao, H., Zhou, X.: Efficient neural radiance fields for interactive free-viewpoint video. In: SIGGRAPH Asia Conference Proceedings (2022)",
          "ref_ids": [
            "23"
          ],
          "1": "Existing Systems Multi-view systems like [15, 23, 35, 42] have showcased outstanding results on free-viewpoint rendering of dynamic scenes."
        }
      }
    },
    {
      "title": "shape prior guided instance disparity estimation for 3d object detection",
      "id": 30,
      "valid_pdf_number": "5/14",
      "matched_pdf_number": "4/5",
      "matched_rate": 0.8,
      "citations": {
        "3D object detection for autonomous driving: A comprehensive survey": {
          "authors": [
            "J Mao",
            "S Shi",
            "X Wang",
            "H Li"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-023-01790-1",
          "ref_texts": "32. Chen L., Sun J., Xie Y ., Zhang S., Shuai Q., Jiang Q., Zhang G., Bao H., Zhou X. (2021) Shape prior guided instance disparity estimation for 3d object detection. IEEE T-PAMI",
          "ref_ids": [
            "32"
          ],
          "1": "[231] \u2713 \u2713 CDN [80] \u2713 \u2713 RT3D-Stereo [116] \u2713 \u2713 \u2713 CG-Stereo [128] \u2713 \u2713 \u2713 LIGA-Stereo [89] \u2713 \u2713 DSGN [46] \u2713 \u2713 PLUMENet [304] \u2713 \u2713 learning sub-network at the second stage; [331, 223, 267, 32] learn instance-level disparity by object-centric stereo matching and instance segmentation; [216] proposes adaptive instance disparity estimation; [160, 217] introduce single-stage stereo detection frameworks; [38, 40] propose an energy-based framework for stereo-based 3D object detection."
        },
        "Robustness-aware 3d object detection in autonomous driving: A review and outlook": {
          "authors": [
            "Z Song",
            "L Liu",
            "F Jia",
            "Y Luo",
            "C Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10637966/",
          "ref_texts": "[194] L. Chen, J. Sun, Y . Xie, S. Zhang, Q. Shuai, Q. Jiang, G. Zhang, H. Bao, and X. Zhou, \u201cShape prior guided instance disparity estimation for 3d object detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5529\u20135540, 2021.",
          "ref_ids": [
            "194"
          ],
          "2": "This paradigm has been widely adopted by subsequent works [14], [194], [213]\u2013[217]."
        },
        "3d object detection from images for autonomous driving: a survey": {
          "authors": [
            "X Ma",
            "W Ouyang",
            "A Simonelli"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10373157/",
          "ref_texts": "[167] L. Chen, J. Sun, Y. Xie, S. Zhang, Q. Shuai, Q. Jiang, G. Zhang, H. Bao, and X. Zhou, \u201cShape prior guided instance disparity estimation for 3d object detection,\u201d in T-P AMI, 2021.",
          "ref_ids": [
            "167"
          ],
          "1": "64 Disp R-CNN[167] T-P AMI\u201921 \u2713 67."
        }
      }
    },
    {
      "title": "high-fidelity and real-time novel view synthesis for dynamic scenes",
      "id": 26,
      "valid_pdf_number": "18/20",
      "matched_pdf_number": "15/18",
      "matched_rate": 0.8333333333333334,
      "citations": {
        "4d gaussian splatting for real-time dynamic scene rendering": {
          "authors": [
            "Guanjun Wu",
            "Taoran Yi",
            "Jiemin Fang",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Qi Tian",
            "Xinggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wu_4D_Gaussian_Splatting_for_Real-Time_Dynamic_Scene_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[27] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2, 5, 6, 7, 8",
          "ref_ids": [
            "27"
          ],
          "1": "[14, 27, 38, 50, 51, 53] are efficient methods to handle multi-view setups.",
          "3": "To assess the quality of novel view synthesis, we conducted benchmarking against several state-of-the-art methods in the field, including [5, 8, 11, 12, 17, 19, 27, 49].",
          "4": "Though [27] addresses the high quality in comparison to ours, the need for multi-cam setups makes it hard to model monocular scenes and other methods also limit freeview rendering speed and storage."
        },
        "4k4d: Real-time 4d view synthesis at 4k resolution": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Haotong Lin",
            "Guangzhao He",
            "Jiaming Sun",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_4K4D_Real-Time_4D_View_Synthesis_at_4K_Resolution_CVPR_2024_paper.html",
          "ref_texts": "[48] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2, 6",
          "ref_ids": [
            "48"
          ],
          "4": "Other image-based methods [48, 49, 90] produce high-quality appearance."
        },
        "Hifi4g: High-fidelity human performance rendering via compact gaussian splatting": {
          "authors": [
            "Yuheng Jiang",
            "Zhehao Shen",
            "Penghao Wang",
            "Zhuo Su",
            "Yu Hong",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jiang_HiFi4G_High-Fidelity_Human_Performance_Rendering_via_Compact_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[36] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, New York, NY , USA, 2023. Association for Computing Machinery. 3",
          "ref_ids": [
            "36"
          ],
          "1": "Method Given human performance videos captured by multi-view RGB cameras, HiFi4G integrates recent advancements in differentiable rasterization with traditional non-rigid tracking, significantly outperforming existing rendering approaches [22, 36, 69, 81] in terms of optimization speed, rendering quality, and storage overhead."
        },
        "Vivid-zoo: Multi-view video generation with diffusion model": {
          "authors": [
            "B Li",
            "C Zheng",
            "W Zhu",
            "J Mai",
            "B Zhang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/71c9eb0913e6c7fda3afd69c914b1a0c-Abstract-Conference.html",
          "ref_texts": "[48] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023.",
          "ref_ids": [
            "48"
          ],
          "1": "More importantly, the availability of such data holds substantial promise for facilitating progress in research areas such as 4D reconstruction [44, 48], 4D generation [3, 49], and long video generation [9, 101] with 3D consistency."
        },
        "Neural parametric gaussians for monocular non-rigid object reconstruction": {
          "authors": [
            "D Das",
            "C Wewer",
            "R Yunus",
            "E Ilg"
          ],
          "url": "https://arxiv.org/abs/2312.01196",
          "ref_texts": "[26] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In ACM SIGGRAPH, 2023. 2",
          "ref_ids": [
            "26"
          ],
          "1": "One approach, dubbed Space-Time Neural Fields, directly adds an extra time dimension to the scene representation to reconstruct the dynamic scene from multi-view [1, 21, 23, 26, 42, 50, 59\u201361, 63, 70] or monocular [4, 8, 10, 11, 24, 25, 48, 51, 67] video.",
          "2": "Hybrid methods further achieve acceleration by parameterizing the 4D scene representation using voxel grids [21, 42, 51, 59\u201361, 63] or planar factorization [1, 4, 10, 26, 48, 70]."
        },
        "Fast View Synthesis of Casual Videos with Soup-of-Planes": {
          "authors": [
            "YC Lee",
            "Z Zhang",
            "K Blackburn-Matzen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72920-1_16",
          "ref_texts": "38. Lin, H., Peng, S., Xu, Z., Xie, T., He, X., Bao, H., Zhou, X.: High-fidelity and realtime novel view synthesis for dynamic scenes. In: SIGGRAPH Asia Conference Proceedings (2023)",
          "ref_ids": [
            "38"
          ],
          "1": "To make this problem more tractable, many existing methods [2,3,5,8,12,31,38,39,45,58,68,92] reconstruct 4D scenes from multiple cameras capturing the dynamic scene simultaneously."
        },
        "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos": {
          "authors": [
            "S Girish",
            "T Li",
            "A Mazumdar"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html",
          "ref_texts": "[46] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. Highfidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u20139, 2023.",
          "ref_ids": [
            "46"
          ],
          "1": "[1, 46, 77, 87, 93] incorporate efficient NeRF representations [9, 19, 91] for higher fidelity."
        },
        "Fast high dynamic range radiance fields for dynamic scenes": {
          "authors": [
            "G Wu",
            "T Yi",
            "J Fang",
            "W Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550564/",
          "ref_texts": "[29] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time 9 novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 2",
          "ref_ids": [
            "29"
          ],
          "1": "CV] 11 Jan 2024 to dynamic scenes [4, 12, 17, 29, 39, 44, 55], learning from unposed images [3, 7, 8, 28, 33, 52, 56] etc."
        },
        "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time rendering of temporally complex dynamic scenes": {
          "authors": [
            "J Yan",
            "R Peng",
            "L Tang",
            "R Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681463",
          "ref_texts": "[28] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. 2023. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers . 1\u20139.",
          "ref_ids": [
            "28"
          ],
          "1": "Many extensions of NeRF to dynamic scenes either utilize deformation fields and canonical fields to model the motion of objects relative to canonical frames over time[18, 29, 36, 42, 51, 53], or decompose the 4D volume into spatial-only and spatial-temporal spaces[7, 13, 28, 49], representing space through combinations of dimensionally reduced features.",
          "2": "Other methods[7, 13, 28, 49, 56] reduce the dimensionality of the 4D space by decomposing it into a set of planar grids or hash grids."
        },
        "Denser: 3d gaussians splatting for scene reconstruction of dynamic urban environments": {
          "authors": [
            "MA Mohamad",
            "G Elghazaly",
            "A Hubert"
          ],
          "url": "https://arxiv.org/abs/2409.10041",
          "ref_texts": "[21] H. Lin, S. Peng, Z. Xu, T . Xie, X. He, H. Bao, and X. Zhou, \u201cHighfidelity and real-time novel view synthesis for dynamic scenes,\u201d in SIGGRAPH Asia 2023 Conference Proceedings, pp. 1\u20139, 2023.",
          "ref_ids": [
            "21"
          ],
          "1": "R ELATED WORK Dynamic scene representation has seen remarkable progress, especially in the domain of 4D neural scene representations focusing on scenes of single dynamic object, where time is considered as an additional dimension besides spatial ones [18], [19], [20], [21], [22], [23], [24]."
        },
        "GAST: Sequential Gaussian Avatars with Hierarchical Spatio-temporal Context": {
          "authors": [
            "W Xu",
            "Y Zhan",
            "Z Zhong",
            "X Sun"
          ],
          "url": "https://arxiv.org/abs/2411.16768",
          "ref_texts": "[33] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023. 1, 3",
          "ref_ids": [
            "33"
          ],
          "1": "Other approaches [33, 51] utilize temporal embeddings instead of SMPL poses to model each frame independently, achieving high-quality rendering but making it challenging to animate avatars.",
          "3": "Motivated by these, a stream of human researches [33, 51] focus on pure rendering quality, employing temporal embeddings instead of human pose to encode each frame of human videos."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[25] H. Lin, S. Peng, Z. Xu, T. Xie, X. He, H. Bao, and X. Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia Conference Proceedings, 2023.",
          "ref_ids": [
            "25"
          ],
          "1": "Although these methods don\u2019t guarantee an animatable human avatar, they can achieve high-quality rendering, among which, we compare the rendering quality of Im4D [25] with our method.",
          "2": "Im4d* [25] achieves high-quality rendering but cannot be animated."
        },
        "Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction": {
          "authors": [
            "D Chen",
            "B Oberson",
            "I Feldmann",
            "O Schreer"
          ],
          "url": "https://arxiv.org/abs/2411.06602",
          "ref_texts": "[42] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u20139, 2023. 2",
          "ref_ids": [
            "42"
          ],
          "1": "Building on this foundation, numerous subsequent works [3,17,37,42,51,53,55] have further explored the synthesis of free-viewpoint videos for dynamic scenes.",
          "2": "To enhance training and rendering efficiency, recent works factorize the 4D space into lowerdimensional components, such as planes, thereby reducing computational complexity [3,17,26,42,55]."
        }
      }
    },
    {
      "title": "animatable neural radiance fields for modeling dynamic human bodies. 2021 ieee",
      "id": 47,
      "valid_pdf_number": "314/416",
      "matched_pdf_number": "263/314",
      "matched_rate": 0.8375796178343949,
      "citations": {
        "Nerf: Neural radiance field in 3d vision, a comprehensive review": {
          "authors": [
            "K Gao",
            "Y Gao",
            "H He",
            "D Lu",
            "L Xu",
            "J Li"
          ],
          "url": "https://arxiv.org/abs/2210.00379",
          "ref_texts": "[178] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , October 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "178"
          ],
          "1": "[175],DoubleField [176], LISA [177], Animatable NeRF [178], TA V A [179], NeuMan [61] Fig.",
          "2": "State of the art models from top tier conferences in 2021/2022 such as A-NeRF [183] (Feb 2021), Animatable NeRF [178] (May 2021), DoubleField [176] (Jun 2021), HumanNeRF [174] (Jan 2022), Zheng et al.",
          "3": "[178] S."
        },
        "Ref-nerf: Structured view-dependent appearance for neural radiance fields": {
          "authors": [
            "D Verbin",
            "P Hedman",
            "B Mildenhall"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9879796/",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "NeRF has inspired many subsequent works, which extend its neural volumetric scene representation to application domains including dynamic and deformable scenes [26], avatar animation [11, 27], and even phototourism [21]."
        },
        "A survey on 3d gaussian splatting": {
          "authors": [
            "G Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2401.03890",
          "ref_texts": "[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "307"
          ],
          "1": "47 AnimNeRF [307][ICCV21] 29.",
          "2": "[307] S."
        },
        "Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling": {
          "authors": [
            "Zhe Li",
            "Zerong Zheng",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_Animatable_Gaussians_Learning_Pose-dependent_Gaussian_Maps_for_High-fidelity_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[58] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 4",
          "ref_ids": [
            "58"
          ],
          "1": "In the past few years, with the rise of implicit representations, particularly neural radiance fields (NeRF) [54], many researchers tend to represent the 3D human as a pose-conditioned NeRF [40, 44, 58, 103] to automatically learn a neural avatar from RGB videos.",
          "3": "Animatable NeRF [58] introduces SMPL deformation into NeRF for animatable human modeling.",
          "5": "We also modulate the output color attributes on Gaussian maps with a view direction mapV to model viewdependent variance like NeRF-based approaches [58]."
        },
        "3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting": {
          "authors": [
            "Zhiyin Qian",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Andreas Geiger",
            "Siyu Tang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Qian_3DGS-Avatar_Animatable_Avatars_via_Deformable_3D_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of ICCV, 2021. 1, 2, 3, 6",
          "ref_ids": [
            "35"
          ],
          "1": "Recent advances in implicit neural fields [27, 30, 32, 36, 47, 50, 51, 53, 55, 65, 66] have enabled high-quality reconstruction of geometry [8, 38, 57, 61] and appearance [13, 20, 22, 31, 35, 37, 42, 58, 70] of clothed human bodies from sparse multi-view or monocular videos.",
          "2": "Animation of such reconstructed clothed human bodies is also possible by learning the geometry and appearance representations in a predefined canonical pose [13, 20, 35, 57, 58, 70].",
          "3": "To achieve state-of-the-art rendering quality, existing methods rely on training a neural radiance field (NeRF) [27] combined with either explicit body articulation [8, 12, 13, 20, 35, 38, 57, 58, 70] or conditioning the NeRF on human body related encodings [31, 37, 48, 61].",
          "4": "The majority of the works focus on either learning a NeRF conditioned on human body related encodings [31, 48, 61], or learning a canonical NeRF representation and warp camera rays from the observation space to the canonical space to query radiance and density values from the canonical NeRF [8, 12, 13, 20, 35, 38, 57, 58, 70].",
          "5": "To model human articulations, a widely adopted paradigm is to represent geometry and appearance in a shared canonical space [8, 12, 13, 20, 35, 38, 57, 58] and use Linear Blend Skinning (LBS) [2, 9, 24, 33, 34, 60] to deform the parametric human body under arbitrary poses.",
          "6": "For fair comparison, we use the provided poses optimized by Anim-NeRF [35] and do not further optimize it during our training."
        },
        "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields": {
          "authors": [
            "L Song",
            "A Chen",
            "Z Li",
            "Z Chen",
            "L Chen"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10049689/",
          "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, October 2021. 2",
          "ref_ids": [
            "61"
          ],
          "1": "The scene representation in NeRF inspired a number of works focusing on 3D modeling, such as human face and body capture [24, 42, 55, 61, 62, 72], relighting [4, 5, 71] and 3D content generation [9, 10, 22, 25, 31, 69, 78]."
        },
        "Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting": {
          "authors": [
            "Zhijing Shao",
            "Zhaolong Wang",
            "Zhuang Li",
            "Duotun Wang",
            "Xiangru Lin",
            "Yu Zhang",
            "Mingming Fan",
            "Zeyu Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Shao_SplattingAvatar_Realistic_Real-Time_Human_Avatars_with_Mesh-Embedded_Gaussian_Splatting_CVPR_2024_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "34"
          ],
          "1": "Recent advances in the field have seen a shift towards using Neural Radiance Fields (NeRF) [32], especially for capturing high-frequency details in 3D avatar modeling [3, 18, 21, 26, 27, 34, 35, 53].",
          "2": "This is done by tracing ray samples backward from their posed positions to their canonical origins [21, 34, 35, 53].",
          "3": "To achieve convincing rendering beyond the limitation of triangle mesh, especially on the hair, glasses, and clothes, some recent works [3, 16, 21, 26, 34, 35, 44, 47, 53] focus on constructing NeRF in the canonical space (usually T -pose of SMPL [28] or neutral expression of FLAME [25]) and conduct volume rendering at the posed space.",
          "4": "Existing works propose to adopt pose conditioned inverse LBS field [17, 34] or to optimize a root-finding loop with multiple initialization [8, 9, 21]."
        },
        "Gart: Gaussian articulated template models": {
          "authors": [
            "Jiahui Lei",
            "Yufu Wang",
            "Georgios Pavlakos",
            "Lingjie Liu",
            "Kostas Daniilidis"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.html",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Con19885",
          "ref_ids": [
            "51"
          ],
          "1": "To address this issue, recent studies [7\u20139, 11\u201313, 16, 20\u201325, 32, 33, 38, 40, 48, 51, 54, 57\u201359, 64, 67, 72, 73, 82, 86, 88] propose to use neural representations, such as NeRF, to capture high-fidelity humans from multiple views or videos."
        },
        "Structured local radiance fields for human avatar modeling": {
          "authors": [
            "Zerong Zheng",
            "Han Huang",
            "Tao Yu",
            "Hongwen Zhang",
            "Yandong Guo",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.html",
          "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "54"
          ],
          "1": "Recently, neural radiance representations, which implicitly encode shape and appearance using neural networks, are also applied in pursuit of higher-fidelity results [35,49, 54].",
          "3": "Recently, neural scene representations and rendering techniques are adopted for higher-fidelity results [35,54, 55].",
          "5": "to transform the points in the posed space to a global canonical space, and has been the basis of previous methods [35, 54, 63].",
          "9": "Compared to [54], our method can produce more appearance details, and generate the non-rigid mo15898 Figure 6.",
          "11": "1 also prove that our method can achieve higher-quality results than [54]."
        },
        "Neuman: Neural human radiance field from a single video": {
          "authors": [
            "W Jiang",
            "KM Yi",
            "G Samei",
            "O Tuzel",
            "A Ranjan"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_24",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021) 1, 3, 4, 6",
          "ref_ids": [
            "32"
          ],
          "1": "Recent efforts also focus on animation of these radiance field models [19,33,32,12,40] of human, with the aid of large controlled datasets, further extending the application domain of radiance-field-based modeling to enable augmented reality experiences.",
          "2": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [22] or 3D skeleton [19,33,32,12,40].",
          "3": "NeRF [32] learns a blending weight field in both observation space and canonical space, and optimize for a new blending weight field for novel poses."
        },
        "Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition": {
          "authors": [
            "Chen Guo",
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "41"
          ],
          "1": "Recent works [20, 22, 29, 39, 41, 54, 59, 60] attempt to reconstruct humans from more sparse settings by deploying neural rendering."
        },
        "Humannerf: Efficiently generated human radiance field from sparse inputs": {
          "authors": [
            "Fuqiang Zhao",
            "Wei Yang",
            "Jiakai Zhang",
            "Pei Lin",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "More recent implicit manner based work [23, 29, 32,40,42,49,56] achieves impressive results for novel view synthesis for a specific scene."
        },
        "Monohuman: Animatable human neural field from monocular video": {
          "authors": [
            "Zhengming Yu",
            "Wei Cheng",
            "Xian Liu",
            "Wayne Wu",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 1, 2, 3, 4",
          "ref_ids": [
            "32"
          ],
          "4": "NeuralBody [32] uses structured pose features generated from SMPL [25] to condition the radiance field, enabling it to recover human performers and produce free-viewpoint images from sparse multi-view videos.",
          "5": "[32] create an animatable model by conditioning the NeRF with pose-dependent latent code."
        },
        "Instantavatar: Learning avatars from monocular video in 60 seconds": {
          "authors": [
            "Tianjian Jiang",
            "Xu Chen",
            "Jie Song",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.html",
          "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "48"
          ],
          "1": "Recently, neural representations [37, 41, 45, 46] have emerged as a powerful tool to model 3D humans [3,6,8,10, 11, 13, 14, 22\u201326, 30, 31, 34, 38, 39, 39, 43, 44, 48, 49, 52, 53, 57, 59\u201363, 63, 64, 67, 69, 70].",
          "2": "Using neural representations, many works [6, 18, 26, 27, 30, 34, 43, 48, 49, 61, 62, 64, 69] can directly reconstruct high fidelity neural human avatars from a sparse set of views or a monocular video without prescanning personalized template."
        },
        "Efficientnerf efficient neural radiance fields": {
          "authors": [
            "Tao Hu",
            "Shu Liu",
            "Yilun Chen",
            "Tiancheng Shen",
            "Jiaya Jia"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.html",
          "ref_texts": "[22] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2",
          "ref_ids": [
            "22"
          ],
          "1": "With Neural Radiance Fields (NeRF) [17] proposed, NVS tasks [20, 24], like large-scale or dynamic synthesis [21,22,25], were successfully dealt with in high quality.",
          "2": "Neural Actor [13] and Animatable-NeRF [22] also adopt similar functions to synthesize human body with novel poses."
        },
        "Ash: Animatable gaussian splats for efficient and photoreal human rendering": {
          "authors": [
            "Haokai Pang",
            "Heming Zhu",
            "Adam Kortylewski",
            "Christian Theobalt",
            "Marc Habermann"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Pang_ASH_Animatable_Gaussian_Splats_for_Efficient_and_Photoreal_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Int. Conf. Comput. Vis., pages 14314\u201314323, 2021. 1, 3",
          "ref_ids": [
            "45"
          ],
          "1": "Hybrid approaches usually attach a neural radiance field (NeRF) [38] onto a (deformable) human model [15, 32, 45].",
          "2": "To better model the pose-dependent appearance of humans, recent studies [10, 15, 27, 33, 45, 76, 80, 81] further introduce motion-aware residual deformations in the canonicalized space."
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
          "authors": [
            "Chen Geng",
            "Sida Peng",
            "Zhen Xu",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.html",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
          "ref_ids": [
            "56"
          ],
          "1": "We extend the technique of displacement map [14, 15] to represent human motions by restricting the originally 3D deformation field [40, 56, 59] on the 2D surface of a parametric human model, such as SMPL [43].",
          "2": "Another line of works [28, 30, 34, 36, 40, 56, 58, 65, 92, 93, 95, 101\u2013103, 105] exploits dynamic implicit neural representations and differentiable renderers to reconstruct 3D human models from 8760.",
          "3": "In contrast to [40, 56] which use a single neural radiance field (NeRF) to represent the canonical human model, we decompose the human body into multiple parts with different complexity and adopt a structured set of MHE-augmented NeRF with varying resolutions as the body representation.",
          "4": "(8) In contrast to [40, 56] that represent the body with a single NeRF network, our part-based voxelized human representation can assign different densities of model parameters to different human parts with different complexity, thereby enabling us to efficiently distribute the representational power of the network.",
          "5": "Our method is implemented purely with the PyTorch framework [53] to demonstrate the effectiveness of our representation It also enables us to fairly compare with baseline methods [34, 56, 58] implemented in PyTorch.",
          "6": "Animatable NeRF (AN) [56] deforms the canonical NeRF with the skeleton-driven framework and models non-rigid deformations by learning blend weight fields.",
          "7": "[57] extend [56] with a signed distance field and pose-dependent deformation field to better model the residual deformation and geometric 8763.",
          "8": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
          "9": "[57, 93] exhibit better results than [56, 58].",
          "10": "Although [56,58] have shown impressive rendering results given 4-view videos, they do not perform well on monocular inputs.",
          "11": "18 AN [56] \u02dc10 h 29.",
          "12": "[56] uses a learnable blend weight field to model human motion, which has a higher dimension and could be hard to converge well given single-view supervision."
        },
        "CelebV-HQ: A large-scale video facial attributes dataset": {
          "authors": [
            "H Zhu",
            "W Wu",
            "W Zhu",
            "L Jiang",
            "S Tang",
            "L Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_38",
          "ref_texts": "62. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 16",
          "ref_ids": [
            "62"
          ],
          "1": "These features on video modality could not only be further exploited to improve the quality of current models, but also stimulate the emerging of several budding topics, such as Dynamic NeRF [63] and Animatable NeRF [62]."
        },
        "Rignerf: Fully controllable neural 3d portraits": {
          "authors": [
            "Rukh Athar",
            "Zexiang Xu",
            "Kalyan Sunkavalli",
            "Eli Shechtman",
            "Zhixin Shu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "37"
          ],
          "1": "The photorealism of volumetric and implicit representations have encouraged works that combine them with classical representations in order improve reconstruction [5] or lend control over foreground [12, 30, 37].",
          "2": "Similarly, [37] learns a 3D skinning field to accurately deform points according to the target pose."
        },
        "Banmo: Building animatable 3d neural models from many casual videos": {
          "authors": [
            "Gengshan Yang",
            "Minh Vo",
            "Natalia Neverova",
            "Deva Ramanan",
            "Andrea Vedaldi",
            "Hanbyul Joo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "Similar to our goal, some recent works [24, 32, 36, 37, 44] produce posecontrollable NeRFs, but they rely on a human body model, or synchronized multi-view video inputs."
        },
        "Fourier plenoctrees for dynamic radiance field rendering in real-time": {
          "authors": [
            "Liao Wang",
            "Jiakai Zhang",
            "Xinhang Liu",
            "Fuqiang Zhao",
            "Yanshun Zhang",
            "Yingliang Zhang",
            "Minye Wu",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "40"
          ],
          "1": "skeleton [40] or parametric models [27, 41]) to explicitly calculate stable motion flows from model animations."
        },
        "Selfrecon: Self reconstruction your digital avatar from monocular video": {
          "authors": [
            "Boyi Jiang",
            "Yang Hong",
            "Hujun Bao",
            "Juyong Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer V ision , pages 14314\u201314323, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "T o this end, technologies such as poserelated skinning weights prediction [24, 38, 47] and specific inverse articulated deformation design [13] are proposed at the cost of high complexity and poor generalization."
        },
        "Gomavatar: Efficient animatable human modeling from monocular video using gaussians-on-mesh": {
          "authors": [
            "Jing Wen",
            "Xiaoming Zhao",
            "Zhongzheng Ren",
            "Alexander G. Schwing",
            "Shenlong Wang"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Wen_GoMAvatar_Efficient_Animatable_Human_Modeling_from_Monocular_Video_Using_Gaussians-on-Mesh_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. InICCV, 2021.3",
          "ref_ids": [
            "53"
          ],
          "1": "To address this limitation, human modeling from videos has received a lot of attention from the community: many prior efforts utilize implicit representations and differentiable renderers for either non-animatable [54] or animatable [16, 22, 25, 37, 39, 53, 55, 64, 69, 70, 75, 81, 83, 89] scene-specific human modeling while other efforts focus on scene-agnostic modeling [9, 14, 21, 31, 34, 35, 56, 85, 87]."
        },
        "Tava: Template-free animatable volumetric actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_25",
          "ref_texts": "34. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021) TAVA: Template-free Animatable Volumetric Actors 23",
          "ref_ids": [
            "34"
          ],
          "1": "TAVA: Template-free Animatable Volumetric Actors 3 Methods Template-free No Per-frame Latent Code 3D Canonical Space Deformation NARF [29] \u2714 \u2714 \u2718 Inverse A-NeRF [42] \u2714 \u2718 \u2718 Inverse Animatable-NeRF [34] \u2718 \u2718 \u2714 Inverse HumanNeRF [45] \u2718 \u2714 \u2714 Inverse NeuralBody [35] \u2718 \u2718 \u2714\u2020 Forward Ours (TAVA) \u2714 \u2714 \u2714 Forward Table 1.",
          "2": "The follow-up work Animatable-NeRF [34] establishes a transformation between view and canonical space through optimizing the inverse deformation field.",
          "7": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 PSNR\u2191 SSIM\u2191 SMPL-based Methods Animatable-NeRF [34] 30.",
          "8": "We compare our work with two types of previous methods: 1) Templatefree methods, including NARF [29] and A-NeRF [42], as well as 2) SMPL-based methods, including Animatable-NeRF [34] and NeuralBody [35].",
          "12": "TAVA: Template-free Animatable Volumetric Actors 19 Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 Subject 313 Animatable-NeRF [34] 29.",
          "13": "957 Subject 315 Animatable-NeRF [34] 27.",
          "14": "960 Subject 377 Animatable-NeRF [34] 32.",
          "15": "980 Subject 386 Animatable-NeRF [34] 34."
        },
        "Dreamwaltz: Make a scene with complex 3d animatable avatars": {
          "authors": [
            "Y Huang",
            "J Wang",
            "A Zeng",
            "H Cao"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0e769ec2c2cd99b6ad69c9d75113e386-Abstract-Conference.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u2013",
          "ref_ids": [
            "30"
          ],
          "1": "The advancement of deep learning methods has enabled promising methods which can reconstruct 3D human models from monocular images [36, 45] or videos [44, 14, 46, 41, 12, 30]."
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
          "authors": [
            "Y Kwon",
            "D Kim",
            "D Ceylan"
          ],
          "url": "https://proceedings.neurips.cc/paper/2021/hash/cf866614b6b18cda13fe699a3a65661b-Abstract.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "32"
          ],
          "1": "Despite the promising results, these general deformable NeRF [17, 53] and human-specific NeRF [11, 9, 33, 32] methods must be optimized for each new video separately, and generalize poorly on unseen scenarios."
        },
        "Fenerf: Face editing in neural radiance fields": {
          "authors": [
            "Jingxiang Sun",
            "Xuan Wang",
            "Yong Zhang",
            "Xiaoyu Li",
            "Qi Zhang",
            "Yebin Liu",
            "Jue Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "41"
          ],
          "1": "Various follow-ups extend NeRF to faster training and testing [8,13,17,43,49], pose-free [26,31], dynamic scenes [5,50] and animating avatars [15,27,41]."
        },
        "Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering": {
          "authors": [
            "Wei Cheng",
            "Ruixiang Chen",
            "Siming Fan",
            "Wanqi Yin",
            "Keyu Chen",
            "Zhongang Cai",
            "Jingbo Wang",
            "Yang Gao",
            "Zhengming Yu",
            "Zhengyu Lin",
            "Daxuan Ren",
            "Lei Yang",
            "Ziwei Liu",
            "Chen Change",
            "Chen Qian",
            "Wayne Wu",
            "Dahua Lin",
            "Bo Dai",
            "Yee Lin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "To reconstruct the human body from sequences, AnimatableNeRF [43] learns a static canonical radiance field together with a ray blending network from the current frame to canonical space.",
          "2": "Recent dynamic human rendering works like NeuralBody [44], A-NeRF [53], AnimatableNeRF [43], and NeuralV olumes [32] obtained impressive results by training on a single case with multi-view Interaction Simple Hard Medium No Deformation Simple Hard Medium Instant-NGP NeuS Neural Volumes A-NeRF Neural Body AnimatableNeRF HumanNeRF Scaled 1/PSNR Scaled 1/SSIM Scaled LPIPS Motion Simple Hard Medium Texture Simple Hard Medium Instant-NGP NeuS Neural Volumes A-NeRF Neural Body AnimatableNeRF HumanNeRF Figure 4: Quantitative results visualization of novel view synthesis test across benchmarks splits and difficulties.",
          "3": "For a fair comparison, we unify the training setting of NeuralV olumes [32], A-NeRF [53], NeuralBody [44], AnimatableNeRF [43], and HumanNeRF [61] with 42 dense training views.",
          "4": "Similar to the novel view synthesis task, we conduct novel pose animation benchmark on the four dynamic methods [44, 43, 53, 61].",
          "5": "Besides, for the SMPLguided pose animation methods [44, 43, 61], we provide the SMPL parameters of test images for the models to infer rendering.",
          "6": "We abbreviate NeuralV olumes [32] as \u2018NV\u2019, A-NeRF [53] as \u2018AN\u2019, NeuralBody [44] as \u2018NB\u2019, AnimatableNeRF [43] as \u2018AnN\u2019 and HumanNeRF [61] as \u2018HN\u2019.",
          "7": "From top to bottom, we illustrate the reposing results generated by(a-e): NeuralV olumes [32], A-NeRF [53], NeuralBody [44] AnimatableNeRF [43], and HumanNeRF [61]."
        },
        "Devrf: Fast deformable voxel radiance fields for dynamic scenes": {
          "authors": [
            "JW Liu",
            "YP Cao",
            "W Mao",
            "W Zhang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eeb57fdf745eb31a3c7ef22c59a4661d-Abstract-Conference.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "26"
          ],
          "1": "Lastly, several NeRF-based approaches have been proposed for modeling dynamic humans [8, 41, 17, 26, 32] but can not directly generalize to other scenes."
        },
        "Nerf-det: Learning geometry-aware volumetric representation for multi-view 3d object detection": {
          "authors": [
            "Chenfeng Xu",
            "Bichen Wu",
            "Ji Hou",
            "Sam Tsai",
            "Ruilong Li",
            "Jialiang Wang",
            "Wei Zhan",
            "Zijian He",
            "Peter Vajda",
            "Kurt Keutzer",
            "Masayoshi Tomizuka"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "24"
          ],
          "1": "Early works [21, 1, 18, 24, 48] directly optimize for per-scene density fields using differentiable volumetric rendering [20]."
        },
        "Representing volumetric videos as dynamic mlp maps": {
          "authors": [
            "Sida Peng",
            "Yunzhi Yan",
            "Qing Shuai",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "To model high-resolution scenes, [14, 16, 31, 42, 44, 46, 47, 71] extend NeRF to represent dynamic scenes."
        },
        "Dp-nerf: Deblurred neural radiance field with physical scene priors": {
          "authors": [
            "Dogyoon Lee",
            "Minhyeok Lee",
            "Chajin Shin",
            "Sangyoun Lee"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "Due to the success of the NeRF in neural rendering, several studies have applied NeRF to other areas such as dynamic scenes [17\u201319, 29, 30, 33, 49, 55], generative models [28, 38], relighting [3, 23, 32, 42], human avatars [31, 45, 58], and 3D reconstruction [47, 50]."
        },
        "Deforming radiance fields with cages": {
          "authors": [
            "T Xu",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_10",
          "ref_texts": "37. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "37"
          ],
          "1": "Harada For some specific object categories, such as the human body or articulated objects, recent studies [24,32,33,37,38,41,46] enable the generation of the unseen scene by controlling the body shape or bone pose.",
          "2": "For the specific task of human body modeling, various works proposed to combine NeRF with a parametric human model to enable human body reposing [37, 38], shape control [24] or even clothing changes [46]."
        },
        "Sifu: Side-view conditioned implicit function for real-world usable clothed human reconstruction": {
          "authors": [
            "Zechuan Zhang",
            "Zongxin Yang",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zhang_SIFU_Side-view_Conditioned_Implicit_Function_for_Real-world_Usable_Clothed_Human_CVPR_2024_paper.html",
          "ref_texts": "[62] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "62"
          ],
          "1": "The rise of Neural Radiance Fields (NeRF) has seen methods [18, 20, 33, 34, 56, 59, 62, 63, 78, 88] using videos or multi-view images to optimize NeRF for human form capture."
        },
        "Nerfacc: A general nerf acceleration toolbox": {
          "authors": [
            "R Li",
            "M Tancik",
            "A Kanazawa"
          ],
          "url": "https://arxiv.org/abs/2210.04847",
          "ref_texts": "[6] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang , Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human b odies. In Proceedings of the IEEE/CVF International Conference on Computer V ision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "6"
          ],
          "1": "In the past two years, they have been proven to be quite powerful in many downstream applications in 3D such as static/dynamic scene reconstruction [3, 4, 5, 6], relighting [7, 8, 9, 10, 11] and content generation [12, 13, 14]."
        },
        "Vmrf: View matching neural radiance fields": {
          "authors": [
            "J Zhang",
            "F Zhan",
            "R Wu",
            "Y Yu",
            "W Zhang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3548078",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "25"
          ],
          "1": "Due to the multi-view consistency of the generated images, NeRF as well as its variants [1, 3, 9, 17, 20, 28, 42] has attracted increasing attention in different tasks such as dynamic scene representation [7, 8, 10, 24, 31], color and shape editing [18, 35], compositional scene modeling [23], scene relighting [2, 30] and skeleton-driven synthesis [25]."
        },
        "Learning neural light fields with ray-space embedding": {
          "authors": [
            "Benjamin Attal",
            "Bin Huang",
            "Michael Zollhofer",
            "Johannes Kopf",
            "Changil Kim"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Attal_Learning_Neural_Light_Fields_With_Ray-Space_Embedding_CVPR_2022_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "36"
          ],
          "1": "Others leverage coordinate embedding for modeling articulated objects such as the human body [24, 36]."
        },
        "Robust e-nerf: Nerf from sparse & noisy events under non-uniform motion": {
          "authors": [
            "Weng Fei",
            "Gim Hee"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Low_Robust_e-NeRF_NeRF_from_Sparse__Noisy_Events_under_Non-Uniform_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "This trend is further driven by the exceptional capabilities and photorealism of Neural Radiance Field (NeRF) [24]-based works [49, 23, 45, 30, 31, 32]."
        },
        "Physavatar: Learning the physics of dressed 3d avatars from visual observations": {
          "authors": [
            "Y Zheng",
            "Q Zhao",
            "G Yang",
            "W Yifan",
            "D Xiang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72913-3_15",
          "ref_texts": "86. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323",
          "ref_ids": [
            "86"
          ],
          "1": "Several different types of representations have been explored for clothed avatars, including meshes [68] with dynamic textures [4,33,131], neural surface [16,90,107] and radiance fields [14,18,24,27,44\u201346, 86,87,99,133], point sets [66,67,69], and 3D Gaussians [34,58,82,134]."
        },
        "Cagenerf: Cage-based neural radiance field for generalized 3d deformation and animation": {
          "authors": [
            "Y Peng",
            "Y Yan",
            "S Liu",
            "Y Cheng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb78e6b5246b03e0b82b4acc8b11cc21-Abstract-Conference.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303, 2021.",
          "ref_ids": [
            "32"
          ],
          "1": "To address this issue, when modeling dynamic objects, most works optimize inside a deformation-invariant canonical space with data-dependent priors, such as 3DMM [4] for face deformation [1, 14], and SMPL [26] for human animation [34, 32].",
          "2": "Following the majority of previous literature [29, 32, 34, 45], we use PSNR/SSIM [48] and LPIPS [54] as evaluation metrics.",
          "3": "AniNeRFOurs Ground Truth Figure 8: Qualitative comparison of CageNeRF and AniNeRF [32] for human synthesis on Human 3.",
          "4": "6M), and compare with the state-of-the-art method AniNeRF [32] as well as other human synthesis methods [41, 49].",
          "5": "PSNR SSIM NT [41] NHR [49] AniNeRF [32] Ours NT NHR AniNeRF Ours S5 19."
        },
        "Synbody: Synthetic dataset with layered human models for 3d human perception and modeling": {
          "authors": [
            "Zhitao Yang",
            "Zhongang Cai",
            "Haiyi Mei",
            "Shuai Liu",
            "Zhaoxi Chen",
            "Weiye Xiao",
            "Yukun Wei",
            "Zhongfei Qing",
            "Chen Wei",
            "Bo Dai",
            "Wayne Wu",
            "Chen Qian",
            "Dahua Lin",
            "Ziwei Liu",
            "Lei Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3, 9",
          "ref_ids": [
            "40"
          ],
          "1": "Introduction The fields of 3D human perception [21, 23\u201325, 37, 53] and human reconstruction [13, 14, 26, 40, 41] have become increasingly important, but the lack of available data has limited their development.",
          "2": "NeuralBody [41] incorporates prior from a statistical body template to learn dynamic sequence, while Animatable NeRF [40] proposes to reconstruct an animatable human model that generalizes to new poses.",
          "3": "108 AnimNeRF [40] 27.",
          "4": "We benchmark five methods in total, including the vanilla NeRF [34], NeuralBody [41] and HumanNeRF [50] for novel view synthesis, AnimNeRF [40] for novel pose synthesis, NHP [26] for generalizable human NeRF (novel identity synthesis)."
        },
        "Imface: A nonlinear 3d morphable face model with implicit neural representations": {
          "authors": [
            "Mingwu Zheng",
            "Hongyu Yang",
            "Di Huang",
            "Liming Chen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_ImFace_A_Nonlinear_3D_Morphable_Face_Model_With_Implicit_Neural_CVPR_2022_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021.",
          "ref_ids": [
            "41"
          ],
          "1": "Such design is inspired by the recent INRs study [41] on human body , which introduces the linear blend skinning algorithm [30] to make the network learn from separate transformations of body parts."
        },
        "Unsupervised learning of efficient geometry-aware neural articulated representations": {
          "authors": [
            "A Noguchi",
            "X Sun",
            "S Lin",
            "T Harada"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_36",
          "ref_texts": "56. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "56"
          ],
          "1": "We demonstrate this approach by modeling the pose prior as a skeletal distribution [51,63], while noting that other models like meshes [57,56] may bring potential performance benefits.",
          "2": "Photorealistic rendering of articulated objects, especially for humans, is also achieved with 3D implicit representations [56,51,63,2,69,39].",
          "3": "They have achieved the state-of-the art in learning 3D shape [12,43,53], static [62,44,4] and dynamic scenes [58,37,54], articulated objects [57,14,7,65,11,56,51,63,2,69,39], and image synthesis [60,10].",
          "4": "Recently, articulated representations based on NeRF have been proposed [57,51,63,56,69,70,39].",
          "5": "In order to train a single tri-plane for all parts, the second change is to further transform the local coordinates xl k into a canonical space defined by a canonical pose oc, similar to Animatable NeRF [56].",
          "6": "1, we compare the proposed Efficient NARF (ENARF) with the state-of-the-art methods [51,56] in terms of both efficiency and effectiveness, and we conduct ablation studies on the deformation modeling and the design choices for the selector.",
          "7": "1 Training on a Dynamic Scene Following the training setting in Animatable NeRF [56], we train our ENARF model on synchronized multi-view videos of a single moving articulated object.",
          "8": "Cost Novel view Novel pose #Memory #FLOPS Time(s)PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191SSIM\u2191 LPIPS\u2193 Animatable NeRF [56] 0.",
          "9": "First, we compare our method with the state-of-the-art supervised methods NARF [51] and Animatable NeRF [56]."
        },
        "Skinned motion retargeting with residual perception of motion semantics & geometry": {
          "authors": [
            "Jiaxu Zhang",
            "Junwu Weng",
            "Di Kang",
            "Fang Zhao",
            "Shaoli Huang",
            "Xuefei Zhe",
            "Linchao Bao",
            "Ying Shan",
            "Jue Wang",
            "Zhigang Tu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.html",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "19"
          ],
          "1": "[19] introduced the neural blend weight fields to reconstruct an animatable human model from a multi-view video."
        },
        "Dinar: Diffusion inpainting of neural textures for one-shot human avatars": {
          "authors": [
            "David Svitov",
            "Dmitrii Gudkov",
            "Renat Bashirov",
            "Victor Lempitsky"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "38"
          ],
          "1": "The highest photorealism and temporal consistency in the video avatar task are currently achieved by NeRF-based methods [38, 54, 24]."
        },
        "Expressive whole-body 3D gaussian avatar": {
          "authors": [
            "G Moon",
            "T Shiratori",
            "S Saito"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72940-9_2",
          "ref_texts": "38. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "38"
          ],
          "3": "[38,39] created a 3D human avatar from a capture studio, which provides accurate 3D pose and 4 Moon et al."
        },
        "Headgas: Real-time animatable head avatars via 3d gaussian splatting": {
          "authors": [
            "H Dhamo",
            "Y Nie",
            "A Moreau",
            "J Song",
            "R Shaw"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72627-9_26",
          "ref_texts": "38. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. ICCV (2021) 4",
          "ref_ids": [
            "38"
          ],
          "1": "NeRFs have also been used to represent dynamic scenes including human bodies [9,38,50], human heads [14,62], and generic time-varying scenes [12,25,36,37,39,46]."
        },
        "Fast-snarf: A fast deformer for articulated neural fields": {
          "authors": [
            "X Chen",
            "T Jiang",
            "J Song",
            "M Rietmann"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10112633/",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "These methods serve as a foundation for many tasks such as generative modeling of articulated objects or humans [3, 8, 12, 20, 40, 63], and reconstructing animatable avatars from scans [9, 13, 27, 34, 35, 51, 56], depth [14, 42, 57], videos [7, 24, 25, 26, 29, 39, 45, 47, 58, 59, 64] or a single image [18, 21, 60]."
        },
        "Anifacegan: Animatable 3d-aware face image generation for video avatars": {
          "authors": [
            "Y Wu",
            "Y Deng",
            "J Yang",
            "F Wei"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eae78bf2712f222f101bd7d12f875a57-Abstract-Conference.html",
          "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "48"
          ],
          "1": "The original NeRF and most of its successors [43, 33, 45, 49, 48, 40, 32, 73] focus on learning scene-specific representation using a set of posed images or a video sequence of a static or dynamic scene."
        },
        "gdna: Towards generative detailed neural avatars": {
          "authors": [
            "Xu Chen",
            "Tianjian Jiang",
            "Jie Song",
            "Jinlong Yang",
            "Michael J. Black",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_gDNA_Towards_Generative_Detailed_Neural_Avatars_CVPR_2022_paper.html",
          "ref_texts": "[49] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "49"
          ],
          "1": "To overcome the topology and resolution limitations of meshes, other representations, including point clouds [35, 37,65], implicit surfaces [12,47,52,55,58,60], and radiance fields [32, 45, 49, 56, 63], have been explored."
        },
        "Gm-nerf: Learning generalizable model-based neural radiance fields from multi-view images": {
          "authors": [
            "Jianchuan Chen",
            "Wentao Yi",
            "Liqian Ma",
            "Xu Jia",
            "Huchuan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303. IEEE, 2021. 2, 6, 8",
          "ref_ids": [
            "30"
          ],
          "1": "To alleviate this limitation, some works [6, 30, 31, 41, 48, 51] combine neural radiance fields [27] with SMPL [23] to represent the human body, which can be rendered to 2D images by differentiable rendering.",
          "3": "We also compare with per-scene optimization methods NB [31], Ani-NeRF [30], A-NeRF [41], ARAH [48]."
        },
        "Hosnerf: Dynamic human-object-scene neural radiance fields from a single video": {
          "authors": [
            "Wei Liu",
            "Pei Cao",
            "Tianyuan Yang",
            "Zhongcong Xu",
            "Jussi Keppo",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "Subsequent works have further improved on the generalizability [17, 4, 9] and animatability [31, 18] of human bodies."
        },
        "Surmo: surface-based 4D motion modeling for dynamic human rendering": {
          "authors": [
            "Tao Hu",
            "Fangzhou Hong",
            "Ziwei Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Hu_SurMo_Surface-based_4D_Motion_Modeling_for_Dynamic_Human_Rendering_CVPR_2024_paper.html",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "42"
          ],
          "1": "Firstly, in contrast to existing pose-guided methods [25, 42, 43, 61] that focus on static poses as a conditional variable, we extract an expressive 4D motion input from the 3D body mesh sequences obtained from training video as our input, which includes both a static pose represented by a spatial 3D mesh and its temporal dynamics.",
          "2": "For stable view synthesis, recent papers [6, 25, 38, 42, 43, 53] propose to unify geometry reconstruction with view synthesis by volume rendering, which, however, is computationally heavy."
        },
        "Pina: Learning a personalized implicit neural avatar from a single rgb-d video sequence": {
          "authors": [
            "Zijian Dong",
            "Chen Guo",
            "Jie Song",
            "Xu Chen",
            "Andreas Geiger",
            "Otmar Hilliges"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.html",
          "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, October 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "Furthermore, several methods that learn a neural avatar for a specific outfit from watertight meshes [13,16,24,32,47,53,56] have been proposed.",
          "2": "These methods either require complete full-body scans with accurate surface normals and registered poses [13, 16, 53, 56] or rely on complex and intrusive multi-view setups [24,32,47]."
        },
        "HandNeRF: Neural radiance fields for animatable interacting hands": {
          "authors": [
            "Zhiyang Guo",
            "Wengang Zhou",
            "Min Wang",
            "Li Li",
            "Houqiang Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.html",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 4, 6, 8",
          "ref_ids": [
            "25"
          ],
          "1": "To address the above issues and push the boundary of realistic human hand modeling, motivated by the recent success of NeRF [17] in modeling human body [11, 25, 26], we propose HandNeRF, a novel framework that unifiedly models the geometry and texture of animatable interacting This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "2": "Similar LBS-based pipelines are adopted by a lot of works [11,12,25,32,37,40].",
          "3": "Most methods [11, 37] regress an extra point-wise offset for samples, while some works like Animatable-NeRF [25] try to jointly optimize NeRF with the LBS weights for deformation.",
          "4": "Therefore, we follow previous works on NeRF for dynamic human body [11, 25, 37, 40] to leverage the parameterized human priors.",
          "5": "(5) Note that different from previous works [25, 32] relying on per-pose latent code to guide the deformation, we use pose representation instead, ensuring robustness to unseen poses.",
          "6": "1) Pose-NeRF: we modify Mip-NeRF [1] to learn a NeRF conditioned on pose; 2) Ani-NeRF: we adapt [25] to the setup of human hands; 3) NeuMan: we re-implement the \u201cHuman NeRF\u201d module of [11] on the settings of hands while preserving its various training losses.",
          "7": "Since AniNeRF [25] cannot directly generalize to unseen poses, we report its pose adaptation performance after re-training with blend weight consistency."
        },
        "Animatable and Relightable Gaussians for High-fidelity Human Avatar Modeling": {
          "authors": [
            "Z Li",
            "Y Sun",
            "Z Zheng",
            "L Wang",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2311.16096",
          "ref_texts": "[5] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021, pp. 14 314\u201314 323. 1, 2, 5",
          "ref_ids": [
            "5"
          ],
          "1": "In the past few years, with the rise of implicit representations, particularly neural radiance fields (NeRF) [4], many researchers tend to represent the 3D human as a poseconditioned NeRF [5]\u2013[8] to automatically learn a neural avatar from RGB videos.",
          "2": ", signed distance function (SDF) [35], [36], occupancy [37], and radiance (NeRF) [5] fields.",
          "3": "Animatable NeRF [5] introduces SMPL deformation JOURNAL OF LATEX CLASS FILES, VOL.",
          "4": "2) Template-guided Parameterization: Previous human avatar representations in NeRF-based approaches [5]\u2013[7] necessitate the coordinate-based MLPs for the formulation of the implicit NeRF function.",
          "5": "We also modulate the output color attributes on Gaussian maps with a view direction map V to model view-dependent variance like NeRF-based approaches [5]."
        },
        "Tela: Text to layer-wise 3d clothed human generation": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Z Huang",
            "X Xu",
            "J Wang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72698-9_2",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021) 4",
          "ref_ids": [
            "32"
          ],
          "1": "Benefiting from advancements in implicit functions [26,27,30], recent methods [7,10,32,33,44,45,47,49,52] have presented impressive clothed huTELA: Text to Layer-wise 3D Clothed Human Generation 5 man reconstruction or generation from images and 3D scans."
        },
        "Gaussianbody: Clothed human reconstruction via 3d gaussian splatting": {
          "authors": [
            "M Li",
            "S Yao",
            "Z Xie",
            "K Chen"
          ],
          "url": "https://arxiv.org/abs/2401.09720",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Extensions of NeRF [14] into dynamic scenes [35\u201337] and methods for animatable 3D human models in multi-view scenarios [18\u201321, 38, 39] or monocular videos [15\u201317, 40] have shown promising results."
        },
        "Reacto: Reconstructing articulated objects from a single video": {
          "authors": [
            "Chaoyue Song",
            "Jiacheng Wei",
            "Chuan Sheng",
            "Guosheng Lin",
            "Fayao Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Song_REACTO_Reconstructing_Articulated_Objects_from_a_Single_Video_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.3",
          "ref_ids": [
            "44"
          ],
          "1": "To address these challenges, several works [16, 37, 44, 61] employ the parametric 3D human models, such as SMPL [29], while other methods [28, 44, 45] utilize synchronized multi-view video inputs."
        },
        "Editablenerf: Editing topologically varying neural radiance fields by key points": {
          "authors": [
            "Chengwei Zheng",
            "Wenbin Lin",
            "Feng Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zheng_EditableNeRF_Editing_Topologically_Varying_Neural_Radiance_Fields_by_Key_Points_CVPR_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.3",
          "ref_ids": [
            "29"
          ],
          "1": "By using human body parametric models and skinning techniques such as SMPL [22], neural radiance fields have been extended to model the human body and can be animated by controlling skeleton poses [3, 19, 25, 29, 36]."
        },
        "One-shot implicit animatable avatars with model-based priors": {
          "authors": [
            "Yangyi Huang",
            "Hongwei Yi",
            "Weiyang Liu",
            "Haofan Wang",
            "Boxi Wu",
            "Wenxiao Wang",
            "Binbin Lin",
            "Debing Zhang",
            "Deng Cai"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 2, 6, 7",
          "ref_ids": [
            "37"
          ],
          "1": ", multi-view videos captured by well-calibrated multi-camera systems [39, 62, 58, 37, 68], or long monocular videos [57] where almost all parts of the human body are visible.",
          "2": "Method Subject data Extra training data Invisible area completion Animatable NeuralBody [39] Ani-NeRF [37] HumanNeRF [57] multi-view images, monocular videos data-free \u2717 \u2713 PiFU [47] PaMIR [72] ARCH [16] ARCH++ [13] PHORHUM [1] monocular images 3D scans \u2713 \u2713 MPS-NeRF [11] NHP [22] sparse videos, multi-view images multi-view videos \u2717 \u2713 MonoNHR [7] monocular images multi-view images \u2713 \u2717 EV A3D [14] monocular images monocular images \u2713 \u2713 ELICIT (ours) monocular images data-free \u2713 \u2713 Table 1: Recent human rendering methods that are most relevant to our work.",
          "3": "Among which [39] learns structured latent codes on SMPL [29] mesh vertices, other methods construct the representation in a canonical space by modeling pose-driven deformation [57, 62, 52, 71, 38, 37].",
          "4": "We selected three state-of-the-art methods as baselines: Neural Body [39] (NB) and Animatable NeRF [37] (AniNeRF) from per-subject optimization methods, and Neural Human Performer [22] (NHP) from generalizable methods.",
          "5": "Compared with state-of-the-art NeRF based methods[39, 22, 37] on novel view synthesis and novel pose synthesis, ELICIT generates human 3D renderings with more consistent appearance and realistic details from a single image.",
          "6": "For per-subject optimization methods NB [39] and Ani-NeRF [37], we optimized one model for each frame.",
          "7": "264 Ani-NeRF[37] 20."
        },
        "Mps-nerf: Generalizable 3d human rendering from multiview images": {
          "authors": [
            "X Gao",
            "J Yang",
            "J Kim",
            "S Peng",
            "Z Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/9888037/",
          "ref_texts": "[6] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323. 1, 2, 3, 5, 6, 7, 10",
          "ref_ids": [
            "6"
          ],
          "1": "While traditional methods [1], [2], [3] use dense multiview camera rigs or depth sensors to accomplish this task, recent neural rendering approaches [4], [5], [6], [7] have shown that free-view rendering and animation can be achieved using sparse color cameras, which could significantly reduce the device setup and capture cost.",
          "2": "In particular, promising results have been shown by methods [5], [6], [7] that are based on the neural radiance field (NeRF) [8] representation.",
          "3": "However, due to the high complexity of human motion and appearance, existing methods [5], [6], [7] are typically trained in a person-specific setup, i.",
          "4": "Some methods [6], [40], [41] also leverage SMPL to deform the 3D space to a canonical pose, where posedependent residual deformation can be considered.",
          "5": "To render the target image, we follow recent works [5], [6], [7] and base our rendering scheme on NeRF [8], which is a compact yet powerful representation for neural rendering.",
          "6": "The notion of a canonical space has been used in previous deformable NeRF schemes such as [6], [22], [23].",
          "7": "Following [6], [38], [49], for each point in the volume, we assign the skinning weights of its closest body vertex.",
          "8": ", a residual skinning weight field is learned in the personspecific model of [6]).",
          "9": "In [6], a per-frame latent code is jointly learned to alleviate this issue, and a test-time optimization is further needed to optimize the latent code for a novel pose.",
          "10": "Following [6], we conduct experiments on 7 subject: S1, S5, S6, S7, S8, S9, and S11.",
          "11": "As in AniNeRF [6], we use three views (#0, #1, #2) out of the four as the input to our method and all four views for supervision during training.",
          "12": "We train and test our method using fitted SMPL parameters and image masks provided by [6] which are obtained using [30] and [59], respectively.",
          "13": "Instead of directly calculating PSNR and SSIM for the whole image, we follow AniNeRF [6] and NeuralBody [5] to project the 3D bounding box of a body onto image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "14": "TABLE 2: Comparison of our method with NB [5], AniNeRF [6] on the Human3.",
          "15": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera parameters to render a novel view of these trained subjects.",
          "16": "Hence, for reference purpose, we compare our method with recent person-specific models NeuralBody (NB) [5] and Animatable NeRF (AniNeRF) [6].",
          "17": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera and pose parameters to render these trained subjects.",
          "18": "TABLE 3: Comparison of NB [5], AniNeRF [6], and our method on the THuman dataset.",
          "19": "Note that we did not apply any post-processing on the extracted 3D shapes such as the Gaussian smoothing used in AniNeRF [6].",
          "20": "1, 2, 3, 5, 6, 7, 10 [6] S."
        },
        "Omg: Towards open-vocabulary motion generation via mixture of controllers": {
          "authors": [
            "Han Liang",
            "Jiacheng Bao",
            "Ruichi Zhang",
            "Sihan Ren",
            "Yuecheng Xu",
            "Sibei Yang",
            "Xin Chen",
            "Jingyi Yu",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liang_OMG_Towards_Open-vocabulary_Motion_Generation_via_Mixture_of_Controllers_CVPR_2024_paper.html",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human 491",
          "ref_ids": [
            "53"
          ],
          "1": "With CLIP\u2019s strong ability, many works are able to generate high-quality zero-shot text-driven images [17, 51, 83] or 3D objects [31, 32, 49, 53, 65, 80]."
        },
        "Humangen: Generating human radiance fields with explicit priors": {
          "authors": [
            "Suyi Jiang",
            "Haoran Jiang",
            "Ziyu Wang",
            "Haimin Luo",
            "Wenzheng Chen",
            "Lan Xu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.html",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "56"
          ],
          "1": "Embracing the developing of NeRF techniques [8,9, 40\u201342, 44, 45, 48, 69, 71, 73, 82, 86], the human shape prior augmented NeRFs achieve modeling realistic human bodies [32, 37, 50, 57, 89], learning animatable avatars [34, 56, 74] and generalizing across different persons [32, 70, 89] from temporal data."
        },
        "Actorsnerf: Animatable few-shot human rendering with generalizable nerfs": {
          "authors": [
            "Jiteng Mu",
            "Shen Sang",
            "Nuno Vasconcelos",
            "Xiaolong Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303, 2021. 1, 2",
          "ref_ids": [
            "32"
          ],
          "1": "However, to achieve high-quality rendering, existing approaches [33, 23, 32, 29, 42] require a combination of synchronized multi-view videos and an instance-level NeRF network, trained on a specific human video sequence.",
          "2": "Recently, NeRF-based human representations have shown promise for high-quality view synthesis [33, 32, 50, 29, 19, 23, 42, 48, 55, 15, 10, 20, 45, 40, 41].",
          "3": "To better animate the human actor, subsequent works [23, 32] introduce a canonical space to align different body poses."
        },
        "Transhuman: A transformer-based human representation for generalizable neural human rendering": {
          "authors": [
            "Xiao Pan",
            "Zongxin Yang",
            "Jianxin Ma",
            "Chang Zhou",
            "Yi Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.1, 2, 6, 15",
          "ref_ids": [
            "31"
          ],
          "1": "Recent works [33, 31, 44, 38] integrate the Neural Radiance Fields (NeRF) [29] technology with parametric human prior models (e.",
          "2": "With the recent success of Neural Radiance Fields (NeRF) [29, 2], many works [33, 31, 44, 38] have attempted to learn the 3D human representation from image inputs via differentiable rendering."
        },
        "Npc: Neural point characters from video": {
          "authors": [
            "Yang Su",
            "Timur Bagautdinov",
            "Helge Rhodin"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "34"
          ],
          "2": "Recent work applies NeRF to reconstruct dynamic human appearance [16, 27, 35, 38, 52] and learning animatable human models [19, 22, 30, 34, 43, 44, 50, 62] from video sequences.",
          "8": "Experiments We quantify the improvements our NPC brings over the most recent surface-free approach TA V A [19], DANBO [43] and A-NeRF [44], as well as most recent and established approaches that leverage template or scan-based prior, including ARAH [50], Anim-NeRF [34] and NeuralBody [35]."
        },
        "Structured 3d features for reconstructing controllable avatars": {
          "authors": [
            "Enric Corona",
            "Mihai Zanfir",
            "Thiemo Alldieck",
            "Eduard Gabriel",
            "Andrei Zanfir",
            "Cristian Sminchisescu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR, 2021. 3",
          "ref_ids": [
            "45"
          ],
          "1": "NeRFs have been recently explored for novel human view synthesis [11, 14, 25, 45, 46, 55, 60, 61, 63]."
        },
        "Neca: Neural customizable human avatar": {
          "authors": [
            "Junjin Xiao",
            "Qing Zhang",
            "Zhan Xu",
            "Shi Zheng"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xiao_NECA_Neural_Customizable_Human_Avatar_CVPR_2024_paper.html",
          "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 5, 6, 7",
          "ref_ids": [
            "41"
          ],
          "1": "Method Pose Lighting Shadow Shape Texture Neural Body [42] % % % % % Neural Actor [31] ! % % ! ! Ani-NeRF [41] ! % % % % DS-NeRF [75] ! % % ! ! SA-NeRF [63] ! % % ! ! HumanNeRF [59] ! % % % % ARAH [56] ! % % % % TA V A [29] ! % % % ! Relighting4D [9] % ! % % % MonoHuman [68] ! % % % % CustomHumans [20] ! % % ! ! UV V olumes [8] ! % % ! ! PoseV ocab [30] ! % % % % Sun et al.",
          "2": "Various later methods [19, 22, 31, 41, 75] are designed to address this limitation by deforming the human body from observation space to canonical space [59, 68].",
          "3": "Particularly, Animatable NeRF [41] proposes to learn per-pose related latent codes for skinning weights in the observation space for deformation, but it requires extra fine-tuning in the inference stage.",
          "6": "on novel pose synthesis: Neural Body (NB) [42], Animatable NeRF (AN) [41], Dual-Space NeRF (DS) [75], ARAH [56], and PoseV ocab (PV) [30]."
        },
        "Surface-aligned neural radiance fields for controllable 3d human synthesis": {
          "authors": [
            "Tianhan Xu",
            "Yasuhiro Fujita",
            "Eiichi Matsumoto"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.html",
          "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "35"
          ],
          "1": "Because manually designing high-quality 3D human models is usually labor-intensive, increasing studies [1\u20133,24,27,30,35,36] have proposed the reconstruction of 3D human models using only 2D observations.",
          "2": "Several approaches [24, 30, 35, 36] have been proposed to incorporate knowledge from a statistical 3D human model and its pose estimation with NeRF.",
          "3": "Deformationbased approaches [24, 35] use a deformation field to transform the query point from the observation space to a poseindependent canonical space and then build NeRF in the canonical space.",
          "4": "For modeling a dynamic human body, recent studies [24, 30, 35, 36] have proposed the use of prior knowledge of human pose and skinning weights of SMPL [26] to ease the learning of a deformation field.",
          "5": "Animatable NeRF [35] uses the skinning weight of SMPL [26] to predict the neural blend shape fields.",
          "6": "Animatable NeRF [35] and Neural Actor [24] rely on this way of projection for learning a deformation field and/or utilizing a texture map.",
          "7": "Following the previous studies [28,35,36], we minimize the per-pixel mean squared error (MSE) between the rendered image and the ground truth image.",
          "8": "For detailed settings, we refer to [35].",
          "9": "6M dataset, we compare with Animatable NeRF [35], which learns the neural blend weight field and builds a NeRF within a canonical space.",
          "10": "6M dataset, our approach outperforms both [30] and [35] by a large margin.",
          "11": "For both datasets, the performance of our approach almost consistently outperforms [36], [35], and [30].",
          "12": "Also, while we do not explicitly model the time-varying deformation components (such as using per-frame embedding in [35,36]), neural networks could implicitly model such components by inferring time from the skeleton pose information."
        },
        "Relightable and animatable neural avatar from sparse-view video": {
          "authors": [
            "Zhen Xu",
            "Sida Peng",
            "Chen Geng",
            "Linzhan Mou",
            "Zihan Yan",
            "Jiaming Sun",
            "Hujun Bao",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Xu_Relightable_and_Animatable_Neural_Avatar_from_Sparse-View_Video_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 7",
          "ref_ids": [
            "44"
          ],
          "1": "Albeit showing the capability of novel pose synthesis, the reconstructed avatars in these works [38, 44, 60] are not relightable as they This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
          "2": "Note that it is not trivial to combine DFSS with previous methods [44, 60, 63], as they cannot produce world-space distance values from 3D points to the scene surface along an arbitrary direction.",
          "3": "Several methods [14, 26, 44, 49] opt to optimize personalized skinning weights for the target human subject, where they represent the skinning weights as an MLP network and learn it from input data, such as human shapes [14, 49] or multiview videos [26, 37, 44].",
          "4": "We formulate the relightable and animatable avatar using a set of canonical space neural fields and a warping between world and canonical space defined by the linear blend skinning algorithm [35] and a displacement field [38, 44, 45, 61].",
          "5": "We follow the previous literature[38, 44] and use the linear blend skinning algorithm [35] to perform the inverse warping.",
          "6": "Note that we evaluate PSNR using the same protocal as [44], only computing metrics on the human region."
        },
        "Generalizable human gaussians for sparse view synthesis": {
          "authors": [
            "Y Kwon",
            "B Fang",
            "Y Lu",
            "H Dong",
            "C Zhang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73229-4_26",
          "ref_texts": "35. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "Whileutilizing3Dhumanpriorhasprovenits effectiveness in the human rendering task [11,25,35,36,43], representing the geometry gap between human template and the real geometry (e."
        },
        "Ohta: One-shot hand avatar via data-driven implicit priors": {
          "authors": [
            "Xiaozheng Zheng",
            "Chao Wen",
            "Zhuo Su",
            "Zeran Xu",
            "Zhaohu Li",
            "Yang Zhao",
            "Zhou Xue"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_OHTA_One-shot_Hand_Avatar_via_Data-driven_Implicit_Priors_CVPR_2024_paper.html",
          "ref_texts": "[55] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "55"
          ],
          "1": "NeRF-based methods [24, 55, 66, 71, 73, 78] typically require dense inputs but have evolved to accommodate few-shot or one-shot scenarios by incorporating various priors."
        },
        "Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos": {
          "authors": [
            "R Jena",
            "GS Iyer",
            "S Choudhary",
            "B Smith"
          ],
          "url": "https://arxiv.org/abs/2311.10812",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 6, 7",
          "ref_ids": [
            "28"
          ],
          "1": "Recently, human-specific neural rendering methods have demonstrated state-of-the-art results in controllable human synthesis [3, 4, 6, 7, 28, 31, 32, 45, 50].",
          "5": "For ZJU MoCap, we consider SMPLPix which uses deferred rendering, NeuralBody, Animatable NeRF (AniNeRF) [28], Surface-Aligned NeRF (SA-NeRF) [50], and HumanNeRF [45] which are state-of-the-art neural rendering methods for animatable humans."
        },
        "Human101: Training 100+ fps human gaussians in 100s from 1 view": {
          "authors": [
            "M Li",
            "J Tao",
            "Z Yang",
            "Y Yang"
          ],
          "url": "https://arxiv.org/abs/2312.15258",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 4, 6, 7, 12, 13, 16, 17",
          "ref_ids": [
            "43"
          ],
          "8": "AnimatableNeRF(AnimNeRF) [43] and AnimatableSDF(AnimSDF) [45] use SMPL deformation and posedependent neural blend weight field to model dynamic humans.",
          "9": "1 presents a comprehensive quantitative comparison between our method and other prominent techniques like InstantNvr [15], InstantAvatar [22], 3D GS [24], HumanNeRF [62], AnimSDF [45], NeuralBody [44], and AnimNeRF [43]."
        },
        "Uv volumes for real-time rendering of editable free-view human performance": {
          "authors": [
            "Yue Chen",
            "Xuan Wang",
            "Xingyu Chen",
            "Qi Zhang",
            "Xiaoyu Li",
            "Yu Guo",
            "Jue Wang",
            "Fei Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 3, 6",
          "ref_ids": [
            "38"
          ],
          "2": "To validate our method, we compare it against several state-of-the-art free-view video synthesis techniques: 1) DN: DyNeRF [25], which takes time-varying latent codes as the conditions for dynamic scenes; and 2) NB: NeuralBody [39], which takes as input the posed human model with structured time-invariant latent codes and generates a pose-conditioned neural radiance field; 3) AN: Animatable-NeRF [38], which uses neural blend weight fields to generate correspondences between observation and canonical space."
        },
        "Total-recon: Deformable scene reconstruction for embodied view synthesis": {
          "authors": [
            "Chonghyuk Song",
            "Gengshan Yang",
            "Kangle Deng",
            "Yan Zhu",
            "Deva Ramanan"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE International Conference on Computer Vision (ICCV), 2021. 3",
          "ref_ids": [
            "38"
          ],
          "1": "One group of work leverages human-specific priors [38, 53, 32, 39, 24, 16, 19, 37] such as human body models (e."
        },
        "Rendering humans from object-occluded monocular videos": {
          "authors": [
            "Tiange Xiang",
            "Adam Sun",
            "Jiajun Wu",
            "Ehsan Adeli",
            "Li Fei"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Rendering_Humans_from_Object-Occluded_Monocular_Videos_ICCV_2023_paper.html",
          "ref_texts": "[50] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "50"
          ],
          "1": "Since the emergence of Neural Radiance Fields (NeRF) [43], different extensions have been recently developed to enable high-quality rendering of static scenes [22, 57, 2, 3, 63, 61, 58, 44], moving objects [18, 36, 47, 48, 52, 46], and dynamic humans [51, 4, 7, 11, 9, 13, 14, 17, 20, 21, 26, 27, 35, 37, 45, 50, 62, 64, 68, 49, 28, 30]."
        },
        "Neuraldome: A neural modeling pipeline on multi-view human-object interactions": {
          "authors": [
            "Juze Zhang",
            "Haimin Luo",
            "Hongdi Yang",
            "Xinru Xu",
            "Qianyang Wu",
            "Ye Shi",
            "Jingyi Yu",
            "Lan Xu",
            "Jingya Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "Existing works equip NeRF with pose-embeddings [23, 27, 40, 45, 80], learnable skinning weights [25, 44, 70] and even generalization across individuals [23, 66, 80]."
        },
        "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints": {
          "authors": [
            "M Mihajlovic",
            "A Bansal",
            "M Zollhoefer",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19784-0_11",
          "ref_texts": "44. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "44"
          ],
          "1": "Recent approaches have incorporated priors specific to human faces [8, 9, 15, 17, 50, 61, 72] and human bodies [44,45,64,66,67,71,73,74] to reduce the dependence on multi-view captures."
        },
        "Learning implicit templates for point-based clothed human modeling": {
          "authors": [
            "S Lin",
            "H Zhang",
            "Z Zheng",
            "R Shao",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_13",
          "ref_texts": "49. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "49"
          ],
          "1": "Recently, based on neural radiance fields (NeRF) [43], attempts have been made to bypass the underlying geometry and synthesize rendered images of clothed humans directly [49,50,62,67]."
        },
        "Caphy: Capturing physical properties for animatable human avatars": {
          "authors": [
            "Zhaoqi Su",
            "Liangxiao Hu",
            "Siyou Lin",
            "Hongwen Zhang",
            "Shengping Zhang",
            "Justus Thies",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Su_CaPhy_Capturing_Physical_Properties_for_Animatable_Human_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "31"
          ],
          "1": "Especially, recent methods [38, 24, 4, 51, 20, 27, 31] that rely on deep neural networks to represent appearance and geometry information show promising results.",
          "2": "Some methods leverage Nerf representation [22, 31, 51, 42, 53, 58, 18] for generating articulated human Nerf models, which propose pose-deformable neural radiance fields for representing human dynamics."
        },
        "PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling": {
          "authors": [
            "Xiaoyun Zheng",
            "Liwei Liao",
            "Xufeng Li",
            "Jianbo Jiao",
            "Rongjie Wang",
            "Feng Gao",
            "Shiqi Wang",
            "Ronggang Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Zheng_PKU-DyMVHumans_A_Multi-View_Video_Benchmark_for_High-Fidelity_Dynamic_Human_Modeling_CVPR_2024_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "29"
          ],
          "1": "To address this limitation, some recent studies [15, 29] propose using human body priors to assist in learning human representations."
        },
        "Nsf: Neural surface fields for human modeling from monocular depth": {
          "authors": [
            "Yuxuan Xue",
            "Bharat Lal",
            "Riccardo Marin",
            "Nikolaos Sarafianos",
            "Yuanlu Xu",
            "Gerard Pons",
            "Tony Tung"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xue_NSF_Neural_Surface_Fields_for_Human_Modeling_from_Monocular_Depth_ICCV_2023_paper.html",
          "ref_texts": "[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "46"
          ],
          "1": "There\u2019s a plethora of NeRFbased approaches for humans modeling that provide animatable avatars starting from monocular RGB videos [16, 19, 46, 53, 61, 72]."
        },
        "Nvfi: Neural velocity fields for 3d physics learning from dynamic videos": {
          "authors": [
            "J Li",
            "Z Song",
            "B Yang"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html",
          "ref_texts": "[44] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. ICCV, 2021.",
          "ref_ids": [
            "44"
          ],
          "1": "In parallel, there are also a number of domain-specific NeRFs to model particular dynamic objects such as human bodies [44, 50, 45, 70, 68] and faces [4, 20, 66, 67]."
        },
        "Meshavatar: Learning high-quality triangular human avatars from multi-view videos": {
          "authors": [
            "Y Chen",
            "Z Zheng",
            "Z Li",
            "C Xu",
            "Y Liu"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-73113-6_15.pdf",
          "ref_texts": "71. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "71"
          ],
          "1": "Previous works project the spatial points onto the fitted SMPL surface to obtain an initial guess, and learn some residuals for refinements [71].",
          "2": "1 Evaluations on Synthetic Data For the task of intrinsic decomposition, we further evaluate our method on a synthetic dataset SyntheticHuman++ [71] and compare with state-of-the-art methods [13,97,103]."
        },
        "Danbo: Disentangled articulated neural body representations via graph neural networks": {
          "authors": [
            "SY Su",
            "T Bagautdinov",
            "H Rhodin"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_7",
          "ref_texts": "40. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "40"
          ],
          "3": "While the skinning weights in SMPL provide an initialization, [40] showed that fine-tuning the deformation fields via self-supervision helps rendering unseen poses.",
          "4": "4 Experiments In the following, we evaluate the improvements upon the most recent surface-free neural body model A-NeRF [46], and compare against recent model-based solutions NeuralBody [41] and Anim-NeRF [40].",
          "8": "Table 1 verifies these improvements on the test set of Anim-NeRF [40]."
        },
        "Avatarcap: Animatable avatar conditioned monocular human volumetric capture": {
          "authors": [
            "Z Li",
            "Z Zheng",
            "H Zhang",
            "C Ji",
            "Y Liu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_19",
          "ref_texts": "53. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "53"
          ],
          "1": "They create animatable avatars from various inputs, including scans [9, 58, 44, 46, 8], multi-view RGB videos [53, 39] and monocular depth measurements [7, 71].",
          "2": "Recent works proposed to directly learn an animatable avatar from the database, including scans [45, 58, 44, 46, 8], multi-view RGB videos [39, 53] and depth frames [7, 71, 10]."
        },
        "Deformable 3d gaussian splatting for animatable human avatars": {
          "authors": [
            "HJ Jung",
            "N Brasch",
            "J Song",
            "E Perez-Pellitero"
          ],
          "url": "https://arxiv.org/abs/2312.15059",
          "ref_texts": "[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 3",
          "ref_ids": [
            "46"
          ],
          "1": "Recent works utilize implicit representations combined with volume rendering [8, 31, 46] which enables photo-realistic rendering with a significantly higher amount of details at the cost of denser camera views and human poses during training.",
          "2": "Animatable NeRF [46] and HumanNerf [69] integrates motion priors in the form of SMPL [35] parameters to regularize field prediction."
        },
        "Rana: Relightable articulated neural avatars": {
          "authors": [
            "Umar Iqbal",
            "Akin Caliskan",
            "Koki Nagano",
            "Sameh Khamis",
            "Pavlo Molchanov",
            "Jan Kautz"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3, 9",
          "ref_ids": [
            "45"
          ],
          "1": "Existing methods commonly aim to learn such neural avatars using monocular videos [47, 46, 45, 54, 66, 58, 31].",
          "2": "Hence, the generated images may not be the true representation of the 23143 novel viewnovel posegeneralizablerelightableMethod \u2713 NeuralBody [46], SelfRecon [31] \u2713 \u2713 AnimatableNeRf [45, 14], NeuMan [32] \u2713 \u2713 \u2713 ANR [47], TNA [52], StylePeople [22] \u2713 \u2713 Relighting4D [16] \u2713 \u2713 \u2713 \u2713 RANA (Ours) Table 1.",
          "3": "The 3D neural rendering methods represent the person using neural radiance fields [42] and render the target images using volume rendering [46, 62, 32, 45, 14, 58, 24, 38].",
          "4": "Thanks to the design of RANA, we can pretrain on as many subjects as available, which is not possible with most of the state-of-the-art methods for human synthesis [45, 46, 58, 31]."
        },
        "AniPortraitGAN: animatable 3D portrait generation from 2D image collections": {
          "authors": [
            "Y Wu",
            "S Xu",
            "J Xiang",
            "F Wei",
            "Q Chen",
            "J Yang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3610548.3618164",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 3, 4",
          "ref_ids": [
            "42"
          ],
          "1": "The related techniques have undergone a significant growth recently, with a variety of promising methods being proposed [1, 7, 8, 16, 21, 38, 40, 42, 51, 52, 54, 60, 62].",
          "2": "Human Image and Video ManipulationOur method is also related to human image and video manipulation approaches [13, 16, 17, 23, 27, 31, 42, 43, 48, 55, 58, 59, 63, 64] that also produce human animation videos.",
          "3": "In fact, this strategy is widely used in state-of-the-art animatable human body modeling and generation methods [2, 12, 17, 22, 42, 65]."
        },
        "Get3dhuman: Lifting stylegan-human into a 3d generative model using pixel-aligned reconstruction priors": {
          "authors": [
            "Zhangyang Xiong",
            "Di Kang",
            "Derong Jin",
            "Weikai Chen",
            "Linchao Bao",
            "Shuguang Cui",
            "Xiaoguang Han"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xiong_Get3DHuman_Lifting_StyleGAN-Human_into_a_3D_Generative_Model_Using_Pixel-Aligned_ICCV_2023_paper.html",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "45"
          ],
          "1": "Some other representations are also proposed for dealing with more complex clothed bodies, like point clouds [33, 35, 58], radiance fields [29, 41, 45, 54] and implicit fields [7, 37, 43, 57]."
        },
        "Smplitex: A generative model and dataset for 3d human texture estimation from single image": {
          "authors": [
            "D Casas",
            "M Comino-Trinidad"
          ],
          "url": "https://arxiv.org/abs/2309.01855",
          "ref_texts": "[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. InProc. of IEEE International Conference on Computer Vision (ICCV), 2021.",
          "ref_ids": [
            "42"
          ],
          "1": "Similarly, other works leverage neural rendering pipelines [23, 41, 42, 44, 58] to generate view-dependent posed avatars but do not output 3D texture maps either."
        },
        "Watch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects": {
          "authors": [
            "Atsuhiro Noguchi",
            "Umar Iqbal",
            "Jonathan Tremblay",
            "Tatsuya Harada",
            "Orazio Gallo"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "40"
          ],
          "1": "They allow novel view and pose synthesis, but require ground truth poses [8,36,49], or dense 3D meshes [24, 30, 40, 41, 53] annotations for the training image."
        },
        "Novel-view synthesis and pose estimation for hand-object interaction from sparse views": {
          "authors": [
            "Wentian Qu",
            "Zhaopeng Cui",
            "Yinda Zhang",
            "Chenyu Meng",
            "Cuixia Ma",
            "Xiaoming Deng",
            "Hongan Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "38"
          ],
          "1": "Although existing neural rendering approaches perform well on static scenes [31, 2], rigid objects [54, 12] and human models [39, 38, 45], they barely considered scene context in interaction (such as contact [63] and model penetration [4, 24]).",
          "2": "Prior arts [9, 30, 6, 23, 39, 38, 35] use implicitly shape representation to reconstruct articulated human body shape.",
          "3": "In order to learn generative novel view synthesis, several methods [39, 38, 35, 45, 51, 64, 50, 8] integrate bone transformation [45] and linear blend skinning [38, 64, 50, 8] with neural radiance fields.",
          "4": "Compared to implicit articulated shape representation such as NASA [9], Animatable NeRF [38] and DD-NeRF [56] that need parametric shape models or skinning weight supervision, our method can learn the geometry and appearance of hand and object with sparse-view only."
        },
        "Reloo: Reconstructing humans dressed in loose garments from monocular video in the wild": {
          "authors": [
            "C Guo",
            "T Jiang",
            "M Kaufmann",
            "C Zheng"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72673-6_2",
          "ref_texts": "37. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "37"
          ],
          "1": "Recent works employ neural rendering to fit neural fields to videos to obtain an articulated human model [9,14,19,21,22,29,37,38,40,48,50]."
        },
        "Generalizing neural human fitting to unseen poses with articulated se (3) equivariance": {
          "authors": [
            "Haiwen Feng",
            "Peter Kulits",
            "Shichen Liu",
            "Michael J. Black",
            "Victoria Fernandez"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Feng_Generalizing_Neural_Human_Fitting_to_Unseen_Poses_With_Articulated_SE3_ICCV_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 1",
          "ref_ids": [
            "36"
          ],
          "1": "Introduction The three-dimensional (3D) capture of humans in varied poses is increasingly common and has many applications including synthetic data generation [35], human health analysis [57], apparel design and sizing [51], and avatar creation [10, 36, 50, 55]."
        },
        "Instant continual learning of neural radiance fields": {
          "authors": [
            "Ryan Po",
            "Zhengyang Dong",
            "Alexander W. Bergman",
            "Gordon Wetzstein"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Po_Instant_Continual_Learning_of_Neural_Radiance_Fields_ICCVW_2023_paper.html",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
          "ref_ids": [
            "43"
          ],
          "1": "The success of NeRFs has spawned a line of works on improving the quality and efficiency of the method [5, 4, 11, 20, 24, 6, 32, 33, 38, 40, 45, 55, 56, 60, 61, 64, 65, 67], while extending the method to a range of applications [62, 12, 34, 43, 19, 22, 42, 53, 68]."
        },
        "Totalselfscan: Learning full-body avatars from self-portrait videos of faces, hands, and bodies": {
          "authors": [
            "J Dong",
            "Q Fang",
            "Y Guo",
            "S Peng"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/589c5bd0aa4322e37813e8e41ddf8034-Abstract-Conference.html",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "39"
          ],
          "1": "Recently, optimizing a network to represent a person-specific model shows impressive results [14, 46, 10, 41, 39].",
          "2": "Furthermore, in order to achieve better animatable effects, learning blend weights automatically from data [39] and incorporating articulated structures [36] are explored.",
          "3": "98 AniNeRF [39] 1.",
          "4": "2 Comparison with the baselines Since most previous methods only focus on body modeling, we extend the state-of-the-art methods AniNeRF [39] and AniSDF [40] with hands to compare with our method.",
          "5": "239 AniNeRF [39] 20."
        },
        "Geometry transfer for stylizing radiance fields": {
          "authors": [
            "Hyunyoung Jung",
            "Seonghyeon Nam",
            "Nikolaos Sarafianos",
            "Sungjoo Yoo",
            "Alexander Sorkine",
            "Rakesh Ranjan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Jung_Geometry_Transfer_for_Stylizing_Radiance_Fields_CVPR_2024_paper.html",
          "ref_texts": "[60] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "60"
          ],
          "1": "Building on this concept, subsequent studies [16, 59, 60, 70] have addressed the view synthesis challenge in dynamic scenes."
        },
        "Interactive nerf geometry editing with shape priors": {
          "authors": [
            "YJ Yuan",
            "YT Sun",
            "YK Lai",
            "Y Ma",
            "R Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10252034/",
          "ref_texts": "[19] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "19"
          ],
          "2": "However, NeRF still has shortcomings and plenty of work has extended the original NeRF, including better synthesis effects [2], [43], [44], applicable to dynamic scenes [8], [14], [18], [19], [45], [46], [47], [48], [49], faster training and rendering speed [3], [4], [50], [51], [52], generalization to different scenes [53], [54], relighting [6], [7], [55], [56], and various kinds of editing [20], [57], [58], [59], [60], [61]."
        },
        "Livehand: Real-time and photorealistic neural hand rendering": {
          "authors": [
            "Akshay Mundra",
            "Mallikarjun B",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "28"
          ],
          "1": "Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9].",
          "2": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_1",
          "ref_texts": "58. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
          "ref_ids": [
            "58"
          ],
          "3": "Clothed Humans as Implicit Functions: Neural implicit functions [13, 44, 45, 55, 61] have been used to model clothed humans from various sensor inputs including monocular images [22,23,25,33,64\u201366,72,80,93], multi-view videos [30, 38, 52, 58, 60, 81], sparse point clouds [6, 14, 16, 77, 78, 94], or 3D meshes [11, 12, 15,47,48,67,74].",
          "4": "[38, 52, 58, 60, 81] take multi-view videos as inputs and do not need ground-truth geometry during training.",
          "5": "Neural Rendering of Animatable Clothed Humans: Differentiable neural rendering has been extended to model animatable human bodies by a number of recent works [52, 58, 60, 63, 72, 81].",
          "6": "Several recent works [52, 58, 72] propose to model the radiance field in canonical space and use a pre-defined or learned backward mapping to map query points from observation space to this canonical space.",
          "8": "4 Experiments We validate the generalization ability and reconstruction quality of our proposed method against several recent baselines [58, 60, 72].",
          "10": "Baselines: We compare against three major baselines: Neural Body [60](NB), Ani-NeRF [58](AniN), and A-NeRF [72](AN).",
          "13": "Neural Body [60], Ani-NeRF [58], and A-NeRF [72]."
        },
        "Mimo: Controllable character video synthesis with spatial decomposed modeling": {
          "authors": [
            "Y Men",
            "Y Yao",
            "M Cui",
            "L Bo"
          ],
          "url": "https://arxiv.org/abs/2409.16160",
          "ref_texts": "[23] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "23"
          ],
          "1": ", NeRF [22] and 3D Gaussian splatting [12]), a series of works [8, 15, 17, 23, 27] tend to represent the dynamic human as a pose-conditioned NeRF or Gaussian to learn animatable avatars in highfidelity rendering quality."
        },
        "Uv gaussians: Joint learning of mesh deformation and gaussian textures for human avatar modeling": {
          "authors": [
            "Y Jiang",
            "Q Liao",
            "X Li",
            "L Ma",
            "Q Zhang",
            "C Zhang"
          ],
          "url": "https://arxiv.org/abs/2403.11589",
          "ref_texts": "39. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "39"
          ],
          "1": "Subsequently, a significant amount of work [9,14,26,39,40,46,47,50] has been explored to utilize NeRF representation for human modeling.",
          "2": "As a result, these representations have been also adopted for modelingclothedhumans[9,10,14,18,26,28,34,39,40,44,46\u201348,50].",
          "3": "To map posed space into a canonical pose, Animatable NeRF [39] learns neural blend weight fields and Neural Actor [28] utilizes a coarse body model as a proxy.",
          "6": "2 Comparison We compare our algorithm with several baselines: Neural Body [40], AnimNeRF [39], UV Volumes [9], and a recent state-of-the-art human avatar methods using GS with source code available, 3DGS-Avatar [41]."
        },
        "Manus: Markerless grasp capture using articulated 3d gaussians": {
          "authors": [
            "Chandradeep Pokhariya",
            "Ishaan Nikhil",
            "Angela Xing",
            "Zekun Li",
            "Kefan Chen",
            "Avinash Sharma",
            "Srinath Sridhar"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Pokhariya_MANUS_Markerless_Grasp_Capture_using_Articulated_3D_Gaussians_CVPR_2024_paper.html",
          "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "47"
          ],
          "1": "Several methods specifically address articulated shapes [32] like human bodies [32, 35, 47, 48, 66], or hands [14, 28, 34, 45, 50]."
        },
        "PointNeRF++: a multi-scale, point-based neural radiance field": {
          "authors": [
            "W Sun",
            "E Trulls",
            "YC Tseng",
            "S Sambandam"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72920-1_13",
          "ref_texts": "36. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021) 3",
          "ref_ids": [
            "36"
          ],
          "1": "Among many applications [14], NeRFs have been used to reconstruct individual objects [29] and unbounded scenes [2], in uncontrolled [8,28,52] or dynamic environments [22,33,34,36,37], in few-shot settings [7,21,31,51,54] and large urban landscapes [42,46,50]."
        },
        "Differentiable visual computing for inverse problems and machine learning": {
          "authors": [
            "A Spielberg",
            "F Zhong",
            "K Rematas"
          ],
          "url": "https://www.nature.com/articles/s42256-023-00743-0",
          "ref_texts": "[7] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "7"
          ],
          "1": "Recently, [7] demonstrated the extraction of geometry, animation and rendering parameters from multiview video, effectively combining all elements of the differentiable pipeline."
        },
        "Neural transformation fields for arbitrary-styled font generation": {
          "authors": [
            "Bin Fu",
            "Junjun He",
            "Jianjun Wang",
            "Yu Qiao"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "29"
          ],
          "1": "For example, Animatable NeRF [29] employs NeRF to embed 3D static human bodies and utilizes a deformation field to model body movement by transforming observation-space points to the canonical space."
        },
        "MetaCap: Meta-learning Priors from Multi-view Imagery for Sparse-View Human Performance Capture and Rendering": {
          "authors": [
            "G Sun",
            "R Dabral",
            "P Fua",
            "C Theobalt"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72952-2_20",
          "ref_texts": "59. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "59"
          ],
          "1": "Several volumetric rendering-based methods focus on learning the human geometry and appearance in the canonical space [29,54,59,61,63,69,80,81], which learns shared features across different poses."
        },
        "Hvtr: Hybrid volumetric-textural rendering for human avatars": {
          "authors": [
            "T Hu",
            "T Yu",
            "Z Zheng",
            "H Zhang",
            "Y Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044408/",
          "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 6, 11, 13",
          "ref_ids": [
            "56"
          ],
          "1": "Existing methods parameterize poses by global pose parameter conditioning [28, 39, 54, 84], 3D sparse points [57], or skinning weights [7, 29, 56].",
          "2": "2D : EDN [5], vid2vid [76] GAN \u0017 \u0013\n2D Plus : SMPLpix [58], DNR [73], ANR[61] GAN \u0017 \u0013\n3D : NB[57], AniNeRF[56] V olR \u0013 \u0017\n3D : Ours Hybrid \u0013 \u0013 Table 1: A set of recent human synthesis approaches classified by feature representations (2D/3D) and renderers.",
          "3": "For stable view synthesis, recent papers [7, 29, 48, 56, 57, 71, 83] propose to unify geometry reconstruction with view 2 Figure 1: We illustrate the differences between (left) GAN-based methods (DNR), (middle) our hybrid approach, and (right) NeRF methods (Neural Body [57]).",
          "7": "The geometry-guided ray marching algorithm and UV conditioned architecture enable us to train a PD-NeRF\n12 Figure 14: Comparisons of backward skinning method (AniNeRF [56]) and forward skinning method (ours).",
          "8": "LPIPS \u2193 FID \u2193 SSIM\u2191 PSNR\u2191 AniNeRF [56] ."
        },
        "Dynamic plenoctree for adaptive sampling refinement in explicit nerf": {
          "authors": [
            "Haotian Bai",
            "Yiqi Lin",
            "Yize Chen",
            "Lin Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Bai_Dynamic_PlenOctree_for_Adaptive_Sampling_Refinement_in_Explicit_NeRF_ICCV_2023_paper.html",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian W ang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human 8794",
          "ref_ids": [
            "29"
          ],
          "1": "Introduction Rendering photo-realistic scenes and objects is crucial for providing users with an immersive and interactive experience in virtual reality [11, 45] and metaverse [19, 29, 50]."
        },
        "Entity-nerf: Detecting and removing moving entities in urban scenes": {
          "authors": [
            "Takashi Otonari",
            "Satoshi Ikehata",
            "Kiyoharu Aizawa"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Otonari_Entity-NeRF_Detecting_and_Removing_Moving_Entities_in_Urban_Scenes_CVPR_2024_paper.html",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021.2",
          "ref_ids": [
            "25"
          ],
          "1": "To address this issue, subsequent research has proposed methods that either explicitly learn scene dynamics by category-specific methods [6, 8, 14, 15, 25, 26, 30, 34, 37, 46, 50, 57], detection [10, 22], deformation [18, 23, 24, 27, 41, 44, 48, 54, 56], flow [4, 5, 7, 12, 55], multiple synchronized videos [11, 47, 58], depth-based approaches [52], or treat moving objects as outliers in a robust approach [33]."
        },
        "Implicit neural head synthesis via controllable local deformation fields": {
          "authors": [
            "Chuhan Chen",
            "Matthew O",
            "Gaurav Bharaj",
            "Pablo Garrido"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Implicit_Neural_Head_Synthesis_via_Controllable_Local_Deformation_Fields_CVPR_2023_paper.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303. IEEE, 2021. 4",
          "ref_ids": [
            "37"
          ],
          "1": "Local Deformation Fields Inspired by advances in part-based implicit rigging [33, 37, 67, 70], we overcome limitations of previous work by decomposing the global deformation field into multiple local fields, each centered around a pre-defined facial landmark location, to model non-linear local expression deformations with higher level of details, as shown in Fig."
        },
        "Within the dynamic context: Inertia-aware 3d human modeling with pose sequence": {
          "authors": [
            "Y Chen",
            "Y Zhan",
            "Z Zhong",
            "W Wang",
            "X Sun"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72967-6_27",
          "ref_texts": "27. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "27"
          ],
          "1": "Following the training setup in previous work [27,29,31], we use 4 cameras for training and the rest 19 cameras for testing."
        },
        "RoGUENeRF: a robust geometry-consistent universal enhancer for NeRF": {
          "authors": [
            "S Catley-Chandar",
            "R Shaw",
            "G Slabaugh"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73254-6_4",
          "ref_texts": "30. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "30"
          ],
          "1": "The NeRF paradigm has been very popular in recent years [53], with active research in the field proposing new functionalities [27,30,31,48], applications [10,21,24] and also tackling some of the open challenges present in [23]."
        },
        "Clothed human performance capture with a double-layer neural radiance fields": {
          "authors": [
            "Kangkan Wang",
            "Guofeng Zhang",
            "Suxu Cong",
            "Jian Yang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021. 2, 3, 4, 5",
          "ref_ids": [
            "24"
          ],
          "1": "Clothed humans can be reconstructed through implicit representation-based methods such as voxel representation [33, 42], implicit function [27, 28], or neural radiance fields (NeRFs) [20, 24, 25].",
          "2": "Neuralbody [25] and AniNeRF [24] extend NeRFs for a dynamic human with defor21099",
          "3": "Double-layer NeRFs for Dynamic Humans Unlike a single NeRFs for clothed humans [17,24,25,34, 41], we represent the clothing with an independent NeRFs on the body, which forms a double-layer NeRFs.",
          "4": "The color network Fc in canonical frame is formulated as, ci(x)= Fc(\u03b3x(x),\u03d5i), (3) where \u03d5i is an appearance latent code [24, 25] for framei.",
          "5": ", peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) as [24, 34]."
        },
        "Animal avatars: Reconstructing animatable 3D animals from casual videos": {
          "authors": [
            "R Sabathier",
            "NJ Mitra",
            "D Novotny"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72986-7_16",
          "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Int. Conf. Comput. Vis. pp. 14294\u201314303 (2021)",
          "ref_ids": [
            "32"
          ],
          "1": "Additionally, there are several works [5,11,13,32] targeting human reconstruction with texture from monocular and multi-view sources achieving high rendering quality by leveraging off-the-shelf human pose estimation models."
        },
        "TexVocab: texture vocabulary-conditioned human avatars": {
          "authors": [
            "Yuxiao Liu",
            "Zhe Li",
            "Yebin Liu",
            "Haoqian Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Liu_TexVocab_Texture_Vocabulary-conditioned_Human_Avatars_CVPR_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 4, 6",
          "ref_ids": [
            "44"
          ],
          "2": "On the other end of the spectrum, lots of works focus on creating animatable textured avatars from RGB videos [21, 22, 24, 26, 29, 44, 64, 71, 72].",
          "3": "AniNeRF [44], ARAH [64], NeuralBody [45] TotalSelfScan [8] and PoseV ocab [26] auto-decode [40] latent embeddings to encode the dynamic appearances with perframe latent code or joint-structured feature lines.",
          "6": "Pioneer works like AniNeRF [44], ARAH [64] and PoseV ocab [26] assign global latent codes or jointstructured feature lines as the pose conditions.",
          "9": "Then we compare our method against other approaches, such as TA V A [24], ARAH [64], AniNeRF [44], PoseV ocab [26] and NeuralActor [29]."
        },
        "RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control": {
          "authors": [
            "Xiang Deng",
            "Zerong Zheng",
            "Yuxiang Zhang",
            "Jingxiang Sun",
            "Chao Xu",
            "Xiaodong Yang",
            "Lizhen Wang",
            "Yebin Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Deng_RAM-Avatar_Real-time_Photo-Realistic_Avatar_from_Monocular_Videos_with_Full-body_Control_CVPR_2024_paper.html",
          "ref_texts": "[49] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "49"
          ],
          "1": "1996",
          "2": "[49] introduce a neural blend weight field, which recovers animatable human models by combining NeRF and 3D human skeletons."
        },
        "Neural radiance fields: Past, present, and future": {
          "authors": [
            "A Mittal"
          ],
          "url": "https://arxiv.org/abs/2304.10050",
          "ref_texts": "171. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323",
          "ref_ids": [
            "171"
          ],
          "1": "\u2022 Animatable Neural Radiance Fields [171] is an extension of NeRF that focuses on modeling dynamic human bodies by incorporating a parametric human body model into the neural radiance field framework."
        },
        "Gravitationally lensed black hole emission tomography": {
          "authors": [
            "Aviad Levis",
            "Pratul P. Srinivasan",
            "Andrew A. Chael",
            "Ren Ng",
            "Katherine L. Bouman"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Levis_Gravitationally_Lensed_Black_Hole_Emission_Tomography_CVPR_2022_paper.html",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 2",
          "ref_ids": [
            "39"
          ],
          "1": "Our work is related to extensions of NeRF to dynamic scenes [17, 32, 38, 39, 45].",
          "2": "Previous methods use priors on deformation sparsity and rigidity [38, 45], pre-trained monocular depth estimation [32], or explicit models of human faces and bodies [17, 39] to relate 3D points across time."
        },
        "Slimmerf: Slimmable radiance fields": {
          "authors": [
            "S Yuan",
            "H Zhao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550817/",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "45"
          ],
          "1": "Scene Representation with Radiance Fields Recently, models based on Neural Radiance Fields (NeRF) [38] have become popular in areas such as generative modelling [2, 3, 13, 29, 33\u201335, 41, 47, 48, 53, 61], pose estimation [8, 21, 30, 52, 57, 60, 64, 73], human body modelling [9,22,28,45,46,54,69,72], mobile 3D rendering [6,12, 25, 42, 49, 62, 63], and so on."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
          "authors": [
            "X Zhou",
            "S Peng",
            "Z Xu",
            "J Dong",
            "Q Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10401886/",
          "ref_texts": "[21] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021.",
          "ref_ids": [
            "21"
          ],
          "1": "A preliminary version of this work appeared in ICCV 2021 [21].",
          "2": "Similar to [21], [75], [76], [78], [80] leverage the LBS model to establish observation-to-canonical correspondences, which enables them to aggregate temporal observations in the input video.",
          "3": "We decompose the human motion into articulated and non-rigid deformations, which is represented by the LBS model [5], [21] and a neural displacement field, respectively.",
          "4": "Table 3 compares our method with [17], [21], [89] in terms of the P2S and CD metrics.",
          "5": "[17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRFPDF\u201d representations model the human geometry with the volume density field, while our \u201cSDF-PDF\u201d representation adopt the signed distance field.",
          "6": "We empirically set the threshold of volume density to extract the geometry of [17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRF-PDF\u201d.",
          "7": "Our representation \u201cSDF-PDF\u201d significantly outperforms baseline methods [17], [21], [89] by a margin of at least 0.",
          "8": "Our method adopts the network of [26], while the preliminary version of this work [21] uses the network of NeRF [6].",
          "9": "[21] S.",
          "10": "Following [21], we use three camera views for training and test on the remaining view.",
          "11": "[21] select video clips from the action \u201cPosing\u201d of S1, S5, S6, S7, S8, S9, and S11."
        },
        "SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image": {
          "authors": [
            "Yunhao Li",
            "Xiaodong Wang",
            "Ping Wang",
            "Xin Yuan",
            "Peidong Liu"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Li_SCINeRF_Neural_Radiance_Fields_from_a_Snapshot_Compressive_Image_CVPR_2024_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "28"
          ],
          "1": "Others focus on developing NeRF which can deal with non-rigid object reconstruction [1, 9, 28, 29]."
        },
        "Flexnerf: Photorealistic free-viewpoint rendering of moving humans from sparse views": {
          "authors": [
            "Vinoj Jayasundara",
            "Amit Agrawal",
            "Nicolas Heron",
            "Abhinav Shrivastava",
            "Larry S. Davis"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.html",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14294\u201314303, 2021. 1, 2",
          "ref_ids": [
            "27"
          ],
          "1": "Human-specific NeRFs have recently become popular for learning models using input videos [27, 40].",
          "2": "Hence, most methods [27, 28, 41] begin with assuming SMPL template as a prior [18]."
        },
        "Motion-oriented compositional neural radiance fields for monocular dynamic human modeling": {
          "authors": [
            "J Kim",
            "D Wee",
            "D Xu"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72933-1_27",
          "ref_texts": "50. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "50"
          ],
          "1": "We present our extensive results on ZJU-MoCap [52] and MonoCap [17,18,50] to demonstrate the effectiveness of the proposed MoCo-NeRF in learning photo-realistic representation and modeling complex non-rigid motions from monocular videos.",
          "2": "In recent days, there have been many works learning an implicit human representation for novel pose synthesis [22,23, 25,27,34,50,56,64,71,87\u201389] or for a novel-view synthesis [27,28,31,37,51,52, 64,73,74,78,85,86].",
          "4": "Experiments We conduct extensive experiments on ZJU-MoCap [52] and MonoCap [17,18,50] to verify the effectiveness of our proposed approach on both singleand multisubject settings."
        },
        "Dual-space nerf: Learning animatable avatars and scene lighting in separate spaces": {
          "authors": [
            "Y Zhi",
            "S Qian",
            "X Yan",
            "S Gao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044388/",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 5, 6, 7, 11, 12",
          "ref_ids": [
            "19"
          ],
          "2": "Animatable NeRF [19] resolves novel-pose synthesis by mapping observed points into a canonical space with inverse linear blend skinning (LBS).",
          "3": "Since the LBS weight of a spatial point varies for different poses, Animatable NeRF [19] learns a neural blending weight network conditioned on the pose, which requires additional training for novel poses.",
          "4": "AniNeRF [19] learns a neural blending weight field to learn the LBS weights for each particular pose.",
          "10": "5, our method produces fewer artifacts than AniNeRF [19], indicating a better correspondences across frames.",
          "15": "Our method exceeds AniNeRF [19], which explicitly builds correspondences across frames like our method, by a large margin in all metrics.",
          "17": "Our method outperforms AniNeRF [19] and is comparable to Neural Body [20] on the perceptual metric.",
          "20": "The results of Neural Body [20] and our method exhibit fewer artifacts compared with AniNeRF [19]."
        },
        "Generalizable neural human renderer": {
          "authors": [
            "M Masuda",
            "J Park",
            "S Iwase",
            "R Khirodkar"
          ],
          "url": "https://arxiv.org/abs/2404.14199",
          "ref_texts": "74. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "74"
          ],
          "1": "Following its developments, many NeRF-based animatable human rendering methods have been proposed for both body part rendering[7,15,18,26,43,95]andfullbodyrendering[17,37,38,47,55,63,68,74,75,86,89\u201391,98,100,104,107,112,115]."
        },
        "Ghunerf: Generalizable human nerf from a monocular video": {
          "authors": [
            "C Li",
            "J Lin",
            "GH Lee"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550750/",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani11 matable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "To model the motion of human body, existing human NeRFs [34, 41, 30, 23, 14, 29, 39] rely on human-prior information, such as a skeleton or a parametric model.",
          "2": "The other line of works [41, 23, 14, 29, 39] map all observations to a shared canonical space to model the large deformation of human bodies.",
          "3": "We follow previous works [29, 13, 2] to model the blending weights by leveraging prior knowledge from the SMPL model."
        },
        "iVS-Net: learning human view synthesis from internet videos": {
          "authors": [
            "Junting Dong",
            "Qi Fang",
            "Tianshuo Yang",
            "Qing Shuai",
            "Chengyu Qiao",
            "Sida Peng"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.html",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3",
          "ref_ids": [
            "36"
          ],
          "1": "four views), recent works [37, 36] have achieved photo-realistic novel view synthesis based on the neural radiance field (NeRF) [32]i na per-scene optimization setting.",
          "2": "Following the NeRF [31] that represents the scene as a neural radiance field, some works propose to overfit a multi-view human video via per-scene optimization [37, 36, 28, 34].",
          "3": "To improve the generalization ability to novel human poses, [36, 28, 34, 7] propose to define the human model in the canonical space and build the correspondence between canonical space and observation space.",
          "4": "Human model Based on the constructed feature of the sample points, we aim to reconstruct the 3D human model, which is represented as the neural radiance field similar to [37, 36]."
        },
        "NDF: Neural deformable fields for dynamic human modelling": {
          "authors": [
            "R Zhang",
            "J Chen"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19824-3_3",
          "ref_texts": "24. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "24"
          ],
          "2": "To further simplify the learning of the deformation fields, Animatable NeRF [24] resorts to a parametric human body model as a strong geometry prior to the deformation fields.",
          "7": "Animatable NeRF [24] predicts the blend weights for each sample point and aggregates observations across frames to a shared canonical representation and further improves on novel pose synthesis by fine-tuning on novel pose images."
        },
        "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting": {
          "authors": [
            "R Shaw",
            "M Nazarczuk",
            "J Song",
            "A Moreau"
          ],
          "url": "https://arxiv.org/abs/2312.13308",
          "ref_texts": "45. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021)",
          "ref_ids": [
            "45"
          ],
          "1": "Such an approach has been popular [12,13,28,32,33,44], and has also been used for dynamic human reconstruction [45,74]."
        },
        "Avatarone: Monocular 3d human animation": {
          "authors": [
            "Akash Karthikeyan",
            "Robert Ren",
            "Yash Kant",
            "Igor Gilitschenski"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.html",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021. 1, 2",
          "ref_ids": [
            "44"
          ],
          "2": "Several studies have proposed controllable animatable NeRFs [29, 38, 44, 45, 56, 64], introducing an array of techniques such as pose-dependent radiance fields, latent codes anchored on deformable meshes, and transformation optimization between view and canonical space.",
          "3": "With the advent of NeRFs, new methods have proposed modeling human geometry as radiance fields [10, 21, 23, 41, 42, 44, 45, 64, 78] or distance functions [59, 67], offering more flexibility and improved rendering quality."
        },
        "Moda: Modeling deformable 3d objects from casual videos": {
          "authors": [
            "C Song",
            "J Wei",
            "T Chen",
            "Y Chen",
            "CS Foo",
            "F Liu"
          ],
          "url": "https://link.springer.com/article/10.1007/s11263-024-02310-5",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "38"
          ],
          "2": "To extend NeRF to dynamic scenes, recent methods [40, 35, 36, 66, 38] introduce a canonical neural radiance field that models the shape and appearance, and a deformation model that achieves 3D point transformation between the observation space and the canonical space.",
          "4": "With the popularity of neural radiance fields [29], there are many works [25, 39, 38, 32, 51, 57, 11, 36, 35, 71, 3, 50, 19, 21] learning to reconstruct the shape and appearance from images or videos with a NeRF-based template."
        },
        "Neural novel actor: Learning a generalized animatable neural representation for human actors": {
          "authors": [
            "Q Gao",
            "Y Wang",
            "L Liu",
            "L Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10221769/",
          "ref_texts": "[44] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14314\u201314323, 2021.",
          "ref_ids": [
            "44"
          ],
          "2": "Leveraging the Skinned Multi-Person Linear (SMPL) model [36], several works [3], [6], [31], [44], [56] manage to obtain an animatable neural representation for humans.",
          "6": "Our method achieves the best performance in two metrics, compared to three person-specific animatable human models, NeuralBody (NB) [45], AnimatbleNerf (AN) [44], and Neural Actor (NA) [31] and MPS-NeRF (MPS) [17]."
        },
        "Recovering a Molecule's 3D Dynamics from Liquid-phase Electron Microscopy Movies": {
          "authors": [
            "Enze Ye",
            "Yuhang Wang",
            "Hong Zhang",
            "Yiqin Gao",
            "Huan Wang",
            "He Sun"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Ye_Recovering_a_Molecules_3D_Dynamics_from_Liquid-phase_Electron_Microscopy_Movies_ICCV_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "26"
          ],
          "1": "It has achieved remarkable success in graphical rendering of static and dynamic scenes from limited views[5, 13, 25, 26, 27, 36], as well as in scientific imaging applications, including clinical X-ray CT[4], surgical endoscopy[39], optical microscopy[1, 14], and black hole emission tomography[10]."
        },
        "UNIF: United neural implicit functions for clothed human reconstruction and animation": {
          "authors": [
            "S Qian",
            "J Xu",
            "Z Liu",
            "L Ma",
            "S Gao"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-20062-5_8",
          "ref_texts": "[28] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "28"
          ],
          "2": "2 Human Body Reconstruction and Animation As the most popular mesh-based human body model, SMPL [17] and its variations [12, 30, 27] dominate the area of human body reconstruction for its expressiveness and flexibility, supporting innumerable downstream task [15, 26, 2, 9, 29, 28, 14]."
        },
        "Multimodal neural radiance field": {
          "authors": [
            "H Zhu",
            "Y Sun",
            "C Liu",
            "L Xia",
            "J Luo",
            "N Qiao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10160388/",
          "ref_texts": "[8] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021.",
          "ref_ids": [
            "8"
          ],
          "1": "Based on NeRF, researchers have developed different methods for image and video-related tasks, such as video synthesis and animation [6], [7], [8], [9], model reconstruction [10], [11], etc."
        },
        "Neural puppeteer: Keypoint-based neural rendering of dynamic shapes": {
          "authors": [
            "Simon Giebenhain",
            "Urs Waldmann",
            "Ole Johannsen",
            "Bastian Goldluecke"
          ],
          "url": "https://openaccess.thecvf.com/content/ACCV2022/html/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.html",
          "ref_texts": "42. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV. pp. 14314\u2013",
          "ref_ids": [
            "42"
          ],
          "5": "Compared to A-NeRF [52] and AniNeRF [42] the fundamental differences in the rendering pipeline result in a significant speed increase."
        },
        "Camm: Building category-agnostic and animatable 3d models from monocular videos": {
          "authors": [
            "Tianshu Kuai",
            "Akash Karthikeyan",
            "Yash Kant",
            "Ashkan Mirzaei",
            "Igor Gilitschenski"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.html",
          "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2",
          "ref_ids": [
            "39"
          ],
          "1": "They serve as base templates for methods that focus on reconstructing and animating 3D humans and animals [7,21,26,36,39,40,55,58,60].",
          "2": "Many works [7,12,13,21, 26,36,39,40,55,58,60\u201362,65] utilize these template shapes or pose priors from shape models to recover 3D shapes and perform animations."
        },
        "Humannerf-se: A simple yet effective approach to animate humannerf with diverse poses": {
          "authors": [
            "Caoyuan Ma",
            "Lun Liu",
            "Zhixiang Wang",
            "Wu Liu",
            "Xinchen Liu",
            "Zheng Wang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Ma_HumanNeRF-SE_A_Simple_yet_Effective_Approach_to_Animate_HumanNeRF_with_CVPR_2024_paper.html",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, 2021.1, 2, 3, 5, 6, 7",
          "ref_ids": [
            "40"
          ],
          "6": "We also take Ani-NeRF [40] as one of the baselines because this work presents SMPL-based neural blend weight that can better generalize novel poses."
        },
        "Cat-nerf: Constancy-aware tx2former for dynamic body modeling": {
          "authors": [
            "Haidong Zhu",
            "Zhaoheng Zheng",
            "Wanrong Zheng",
            "Ram Nevatia"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/html/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.html",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 2, 3, 4, 5, 6, 7, 11, 12",
          "ref_ids": [
            "32"
          ],
          "1": "To align the points between these two spaces, researchers use SE(3) [29] or a translation vector field [32,35] for building correspondences between the canonical and observation spaces.",
          "2": "To solve this problem, we separate appearance constancy and uniqueness between the frames based on the neural blend weight fields [32] with Constancy Awareness Tx2Former, abbreviated as CAT-NeRF.",
          "3": "Recently some papers [20, 27, 29, 32, 33, 35] introduce decomposing the neural radiance from the observation space to canonical space for modeling the movement of an object.",
          "4": "By predicting the correspondences in the canonical space [29,32,35], deformable NeRF finds the connection between every point in the observation space and the canonical space and uses the moving object in the video for the construction of a unique object.",
          "5": "To animate the body shapes and render them in the scene, animatable NeRF, different from the deformable methods, projects the body shape into a canonical space [18, 32, 33, 49] or a common shape shapes [34, 36] for projecting the human body shape from different frames into a shared shape or space.",
          "6": "SNARF [4] and Animatable NeRF [32] utilize the neural blend weight field with frame-level correction for building such correspondence with statistical methods, while TA V A [18] uses the linear blend skinning (LBS) and apply a constant change for modeling muscles and clothing dynamics.",
          "7": "We first briefly review Animatable NeRF [32] in 3.",
          "8": "Animatable NeRF for Human Modeling To represent a dynamic scene or object in the video, Animatable NeRF [32] constructs two spaces: one observation space representing the shape we observe for each individual frame and one canonical space shared by all the frames describing the same object with a default pose.",
          "9": "Objective To train the model, we follow [32] to build the objective function for training \u03c8c, \u03c8u i , Fconst \u2206w , Funique \u2206w , G(\u00b7), F\u03c3 and Fc jointly.",
          "10": "3, we follow [32] for establishing Lnsf to minimize the difference.",
          "11": "We follow [32] to select the frames and generate the splits for training and inference in our experiment.",
          "12": "In our experiment, we follow [32] to select the videos from subjects S1, S5, S6, S7, S8, S9 and S11.",
          "13": "We follow [32] to use the videos in four categories, \u201cTwirl\u201d, \u201cTaichi\u201d, \u201cWarmup\u201d, and \u201cPunch1\u201d in our experiment.",
          "14": "To train the network for novel camera viewpoints, we follow [32] to implement our network.",
          "15": "For both training steps, we use the Adam optimizer [17] and set the initial learning rate as 5e \u2212 4 and decay it to 1 10 after 1,000 epochs with an exponential training scheduler following [32].",
          "16": "In our experiment, we compare our method with Neural Texture [44], NHR [50] and Animated NeRF [32].",
          "17": "We follow [32] to use the body shape reconstruction from SMPL [22] as the input for Neural Texture [44] as the coarse mesh to render the image and use the points sampled from the SMPL body shape reconstruction as the input for NHR.",
          "18": "For the results in the tables, NT is the result for Neural Texture [44] and AN is the result from Animatable NeRF [32].",
          "19": "For the results on the novel view setting shown in Table 1, we outperform the state-of-the-art baseline method, Animatable NeRF [32], on all splits in the dataset for both metrics we assessed.",
          "20": "In addition to the Neural Texture [44], NHR [50] and Animatable NeRF [32], we also compared with D-NeRF [35] for reconstruction."
        },
        "Free-viewpoint rgb-d human performance capture and rendering": {
          "authors": [
            "P Nguyen-Ha",
            "N Sarafianos",
            "C Lassner"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_27",
          "ref_texts": "50. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021) 1, 4",
          "ref_ids": [
            "50"
          ],
          "2": "Given multi-view input frames or videos, recent works on rendering animate humans from novel views show impressive results [46,50,51,66]."
        },
        "Human 3d avatar modeling with implicit neural representation: A brief survey": {
          "authors": [
            "M Sun",
            "D Yang",
            "D Kou",
            "Y Jiang",
            "W Shan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10218567/",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3, 4, 7",
          "ref_ids": [
            "38"
          ],
          "3": "[38] develop NeRF for the re-animation of an avatar."
        },
        "Nephi: Neural deformation fields for approximately diffeomorphic medical image registration": {
          "authors": [
            "L Tian",
            "H Greer",
            "RS Jos\u00e9 Est\u00e9par",
            "R Sengupta"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73223-2_13",
          "ref_texts": "35. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "35"
          ],
          "1": "2 Neural Deformation Models Functional representations of deformation fields (parameterized via MLPs) for natural images in dynamic scenes [14,24,25,25,33,34,36,51], for dynamic objects [13,23,31, 47, 56], and for animatable humans [8, 15, 26, 35, 40, 59, 61, 62] have attracted significant recent interest."
        },
        "A comprehensive benchmark for neural human radiance fields": {
          "authors": [
            "K Liu",
            "D Jin",
            "A Zeng",
            "X Han"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/6e566c91d381bd7a45647d9a90838817-Abstract-Datasets_and_Benchmarks.html",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "37"
          ],
          "1": "The extended NeRFs for humans [38, 49, 12, 54, 10, 37, 32, 8, 24] follow similar pathways of development.",
          "2": "While these methods progress greatly, some emerging problems should be taken seriously.",
          "3": "Among these variants, NeRFs for human body rendering [38, 49, 12, 54, 10, 37, 32, 8, 24] have attracted a lot of attention due to their broad applications.",
          "4": "1 Scene-specific NeRFs for Human Scene-specific methods [38, 37, 47, 49, 54, 10, 20] for human body rendering require only sparse view videos for training as different video frames can be treated equivalent to dense view images by exploiting the human body prior.",
          "5": "To strengthen the performance of novel pose rendering, some works [37, 54] introduce additional constraints to learn more reasonable skinning weights."
        },
        "Vaxnerf: Revisiting the classic for voxel-accelerated neural radiance field": {
          "authors": [
            "N Kondo",
            "Y Ikeda",
            "A Tagliasacchi",
            "Y Matsuo"
          ],
          "url": "https://arxiv.org/abs/2111.13112",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1",
          "ref_ids": [
            "51"
          ],
          "1": "Many extensions and applications quickly ensued, including dynamic scene rendering [16, 31, 47, 50, 51, 67], controllable relighting [3, 61, 77, 87], latent appearance and shape priors [6, 57, 68], scene composition [21, 45, 49, 79], and pose estimation [24, 32, 38, 62, 82]."
        },
        "Localised-NeRF: Specular Highlights and Colour Gradient Localising in NeRF": {
          "authors": [
            "Dharmendra Selvaratnam",
            "Dena Bazazian"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/NRI/html/Selvaratnam_Localised-NeRF_Specular_Highlights_and_Colour_Gradient_Localising_in_NeRF_CVPRW_2024_paper.html",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "34"
          ],
          "1": "Subsequent models have extended NeRF\u2019s application to dynamic scenes [33], avatar animation [34], and phototourism [26], by focusing on improving view-dependent appearance and geometry\u2019s smoothness."
        },
        "High-fidelity eye animatable neural radiance fields for human face": {
          "authors": [
            "H Wang",
            "Z Zhang",
            "Y Cheng",
            "HJ Chang"
          ],
          "url": "https://arxiv.org/abs/2308.00773",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "28"
          ],
          "1": "Further explorations [11, 17, 26, 28, 29] adapt NeRF to represent dynamic scenes."
        },
        "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters": {
          "authors": [
            "M Sun",
            "J Chen",
            "J Dong",
            "Y Chen",
            "X Jiang",
            "S Mao"
          ],
          "url": "https://arxiv.org/abs/2411.17423",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "32"
          ],
          "1": "However, human-centered 3D reconstruction methods focus on high-fidelity digitization and reconstruction of clothed humans from minimal inputs [7, 10, 11, 14, 16, 17, 32, 40, 41, 53\u201355, 70]."
        },
        "Placing human animations into 3d scenes by learning interaction-and geometry-driven keyframes": {
          "authors": [
            "James F. Mullen",
            "Divya Kothandaraman",
            "Aniket Bera",
            "Dinesh Manocha"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2023/html/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.html",
          "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14294\u201314303, Montreal, QC, Canada, Oct. 2021. IEEE.",
          "ref_ids": [
            "38"
          ],
          "1": "[38, 34, 58, 47] worked towards extending NeRF to articulated objects like humans."
        },
        "Morf: Mobile realistic fullbody avatars from a monocular video": {
          "authors": [
            "Renat Bashirov",
            "Alexey Larionov",
            "Evgeniya Ustinova",
            "Mikhail Sidorenko",
            "David Svitov",
            "Ilya Zakharkin",
            "Victor Lempitsky"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Bashirov_MoRF_Mobile_Realistic_Fullbody_Avatars_From_a_Monocular_Video_WACV_2024_paper.html",
          "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiancefields for modeling dynamic human bodies. InICCV, 2021. 1, 2",
          "ref_ids": [
            "48"
          ],
          "2": "We compare against InstantAvatar [26], HF-Avatar [71], Anim-NeRF [48], HumanNeRF [66], StylePeople [17], ANR [53] and NeuMan [27].",
          "3": "Some of the monocular fullbody avatar methods model the human geometry implicitly [36, 48, 66], others output the classical mesh+texture format [1, 2, 4, 17, 53, 59, 71]."
        },
        "Wild2avatar: Rendering humans behind occlusions": {
          "authors": [
            "T Xiang",
            "A Sun",
            "S Delp",
            "K Kozuka",
            "L Fei-Fei"
          ],
          "url": "https://arxiv.org/abs/2401.00431",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "40"
          ],
          "1": "While past works were able to achieve good quality renderings of humans from dense [5, 10, 54] and sparse [14, 25, 27, 40, 41, 47, 55] camera views, a recent research focus involves rendering a moving human from a single camera angle [1, 2, 9, 11, 15, 17, 18, 20, 44, 51].",
          "2": "With the above transformations, we are able to first optimize a static neural field for the dynamic human in the canonical space and then deform the ray samples to the observation space for volume rendering [39, 40, 42]."
        },
        "Meil-nerf: Memory-efficient incremental learning of neural radiance fields": {
          "authors": [
            "J Chung",
            "K Lee",
            "S Baik",
            "KM Lee"
          ],
          "url": "https://arxiv.org/abs/2212.08328",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "43"
          ],
          "1": "Among the attempts, neural radiance fields (NeRF) [36] has emerged as one of promising methods for its high reconstruction quality, spurring a large number of research works on improvements [3, 4, 9, 18, 22, 27, 28, 33, 37, 44, 52\u201355, 57\u201359] and a wide range of applications [56]: controllable human avatar [10, 29, 43], realistic game [17], robotics [20, 41, 50, 62], 3D-aware image generation [6, 7], and data compression [13]."
        },
        "NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via Generative Prior": {
          "authors": [
            "G Kim",
            "K Seo",
            "S Cha",
            "J Noh"
          ],
          "url": "https://arxiv.org/abs/2405.05749",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "43"
          ],
          "1": "Furthermore, by embedding facial features such as landmarks and parameters of a facial model [6, 14, 15, 18, 22, 43, 62, 72] or audio features [20, 30, 32, 49, 56, 67, 69], these methods facilitate the representation of dynamics in the desired direction through the learned features."
        },
        "Dynamic NeRFs for soccer scenes": {
          "authors": [
            "S Lewin",
            "M Vandegar",
            "T Hoyoux",
            "O Barnich"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3606038.3616158",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14314\u201314323.",
          "ref_ids": [
            "28"
          ],
          "1": "They often work by learning the motion of a skinned multi-person linear model (SMPL [22]) along with its appearance [28, 29, 42]."
        },
        "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction": {
          "authors": [
            "L Qiu",
            "S Zhu",
            "Q Zuo",
            "X Gu",
            "Y Dong",
            "J Zhang"
          ],
          "url": "https://arxiv.org/abs/2412.02684",
          "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "54"
          ],
          "1": "Other works focus on animatable avatar reconstruction from monocular videos [31, 43, 58, 72] or multi-view videos [13, 41, 54]."
        },
        "Generalizable neural voxels for fast human radiance fields": {
          "authors": [
            "T Yi",
            "J Fang",
            "X Wang",
            "W Liu"
          ],
          "url": "https://arxiv.org/abs/2303.15387",
          "ref_texts": "[58] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 4",
          "ref_ids": [
            "58"
          ],
          "1": "Some works [60, 86, 72, 12, 59, 58, 15, 33, 89] successfully apply NeRF methods to human body rendering frameworks.",
          "2": "To solve this problem, Animatable NeRF [58, 59] maps human poses from the observation space to the predefined canonical space.",
          "3": "Representing Canonical Bodies with Neural Voxels Most previous NeRF-based methods for human bodies [60, 86, 72, 12, 59, 58, 15, 33, 89] adopt purely implicit representations."
        },
        "Dynamic nerf: A review": {
          "authors": [
            "J Lin"
          ],
          "url": "https://arxiv.org/abs/2405.08609",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "53"
          ],
          "1": "[53] also proposed a reconstruction method of dynamic human body model.",
          "3": "[53] proposed a novel method developed based on traditional human armature animation deformation, which is abbreviated as Skeleton-NeRF in this paper."
        },
        "Human image generation: A comprehensive survey": {
          "authors": [
            "Z Jia",
            "Z Zhang",
            "L Wang",
            "T Tan"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3665869",
          "ref_texts": "[127] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. InICCV. 14314\u201314323.",
          "ref_ids": [
            "127"
          ],
          "1": "Subsequent works [23, 36, 127] leverage NeRF to predict the color and density of 3D human bodies."
        },
        "Deformable NeRF using Recursively Subdivided Tetrahedra": {
          "authors": [
            "Z Qiu",
            "C Ren",
            "K Song",
            "X Zeng",
            "L Yang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681019",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV.",
          "ref_ids": [
            "29"
          ],
          "1": "NeRF has found extensive applications in versatile computer graphics and computer vision tasks, such as human avatar creation [8, 29], pose estimation [2, 6, 21], reconstruction [3, 33], robotics [1, 20] and simulation [18, 19]."
        },
        "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale Environments": {
          "authors": [
            "Leif Van",
            "Patrick Stotko",
            "Stefan Krumpen",
            "Reinhard Klein",
            "Michael Weinmann"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Van_Holland_Efficient_3D_Reconstruction_Streaming_and_Visualization_of_Static_and_Dynamic_ICCVW_2023_paper.html",
          "ref_texts": "[118] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV. 2021. 3",
          "ref_ids": [
            "118"
          ],
          "1": "In particular, this includes implicit scene representations based on Neural Radiance Fields (NeRFs) [97] and respective extensions towards speeding up model training [125, 39, 16, 26, 10, 164, 147, 105, 37, 9, 11, 188, 179, 102] with training times of seconds, the adaptation to unconstrained image collections [94, 13, 63], deformable scenes [115, 123, 43, 158, 124, 111, 160, 118, 116, 12, 87, 58, 83, 37] and video inputs [82, 174, 30, 119, 44, 81, 151, 79], the refinement or complete estimation of camera pose parameters for the input images [181, 165, 146, 20, 192, 191, 130, 187, 95, 84, 57, 173, 90, 6, 17, 15, 14, 52, 148, 86], combining NeRFs with semantics regarding objects in the scene [163, 189, 40], incorporating depth cues [166, 26, 128, 126, 3] to guide the training and allow handling textureless regions, handling large-scale scenarios [150, 161, 96], and streamable representations [18, 149]."
        },
        "Inv: Towards streaming incremental neural videos": {
          "authors": [
            "S Wang",
            "A Supikov",
            "J Ratcliff",
            "H Fuchs"
          ],
          "url": "https://arxiv.org/abs/2302.01532",
          "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "32"
          ],
          "1": "Some recent works [15, 21, 32, 33, 40] focus on animating clothed humans only."
        },
        "PGAHum: prior-guided geometry and appearance learning for high-fidelity animatable human reconstruction": {
          "authors": [
            "H Wang",
            "Q Xu",
            "H Chen",
            "R Ma"
          ],
          "url": "https://arxiv.org/abs/2404.13862",
          "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "33"
          ],
          "1": "The skeleton pose has also been used to define a pose-driven deformation field [33, 34] for the observation-to-canonical space transformation so that the geometry and color can be optimized in the canonical space.",
          "2": "NB[35] GT Ours Ani-NeRF[33] ARAH[40] InstantNVR[6] Figure 3: Qualitative results on ZJU-MoCap dataset for novel view synthesis on training poses.",
          "5": "086 Example Input Frame Ours ARAH[40] NB[35] Ani-NeRF[33] A-NeRF[38] GT Figure 4: Qualitative results on ZJU-MoCap dataset for geometry reconstruction."
        },
        "Explicifying neural implicit fields for efficient dynamic human avatar modeling via a neural explicit surface": {
          "authors": [
            "R Zhang",
            "J Chen",
            "Q Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3611707",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "28"
          ],
          "1": "On the other hand, other methods [28, 39, 40, 44] aggregate information across video frames and demonstrate better ability to handle novel poses.",
          "4": "AniSDF is an extended version of AniNeRF [28] that replaces the neural radiance fields in AniNeRF with signed distance fields, leading to better performance."
        },
        "Canonical fields: Self-supervised learning of pose-canonicalized neural fields": {
          "authors": [
            "Rohith Agaram",
            "Shaurya Dewan",
            "Rahul Sajnani",
            "Adrien Poulenard",
            "Madhava Krishna",
            "Srinath Sridhar"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023/html/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.html",
          "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1",
          "ref_ids": [
            "33"
          ],
          "1": "In particular, neural radiance fields (NeRFs) [24], have been successfully used in problems such as novel view synthesis [3, 4, 66], scene geometry extraction [55, 61], capturing dynamic scenes [19, 29, 30, 33, 52], 3D semantic segmentation [53, 68], and robotics [1, 13, 21]."
        },
        "Crim-gs: Continuous rigid motion-aware gaussian splatting from motion blur images": {
          "authors": [
            "J Lee",
            "D Kim",
            "D Lee",
            "S Cho",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2407.03923",
          "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "37"
          ],
          "1": "NeRF has led to a wide range of studies, including 3D mesh reconstruction [22, 46, 49, 50], dynamic scene [20, 21, 31, 32, 38, 47], and human avatars [14, 37, 53]."
        },
        "Surfel-based gaussian inverse rendering for fast and relightable dynamic human reconstruction from monocular video": {
          "authors": [
            "Y Zhao",
            "C Wu",
            "B Huang",
            "Y Zhi",
            "C Zhao",
            "J Wang"
          ],
          "url": "https://arxiv.org/abs/2407.15212",
          "ref_texts": "[77] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "77"
          ],
          "1": "In addition to the textured mesh, NeRFs [16] also became a useful representation for photo-realistic clothed avatar reconstruction [34], [35], [38], [77], [78], [79], [80], [81], [82], [83], [84] from monocular or multi-view videos.",
          "2": "Animatable NeRF [77] further proposed a pose-dependent deformation network to fit the clothed human conditioned on human."
        },
        "Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos": {
          "authors": [
            "S Jeon",
            "I Cho",
            "M Kim",
            "WO Cho",
            "SJ Kim"
          ],
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-72684-2_23.pdf",
          "ref_texts": "33. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "33"
          ],
          "1": "Recent methods [19,33,51,52] have suggested an alternative yet effective approach for general users: building animatable models from casually captured videos.",
          "5": "Recent advances in NeRF have also spurred active research in these approaches [19,23,33,34,36,43,52]."
        },
        "Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering": {
          "authors": [
            "Chuanyue Shen",
            "Letian Zhang",
            "Zhangsihao Yang",
            "Masood Mortazavi",
            "Xiyun Song",
            "Liang Peng",
            "Heather Yu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.html",
          "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV, 2021. 1, 2",
          "ref_ids": [
            "24"
          ],
          "1": "It catalyzes a wave of human neural rendering methods that deliver high fidelity results [9,15,24,26,32,33].",
          "2": "Due to its high-quality performance while being simple and extendable, the use of NeRF as a core algorithm has been widely explored in a variety of scene representation and rendering tasks, such as pose estimation [24,26,29,32], lighting [1,2,37], scene labeling and understanding [30,39], and scene composition [23, 34].",
          "3": "Animatable NeRF [24] introduces a per-frame neural blend weight field to be combined with NeRF, while using human priors from SMPL to regularize the learned blend weight."
        },
        "Neural kaleidoscopic space sculpting": {
          "authors": [
            "Byeongjoo Ahn",
            "Michael De",
            "Ioannis Gkioulekas",
            "Aswin C. Sankaranarayanan"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.html",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2",
          "ref_ids": [
            "26"
          ],
          "1": "The most successful among these work use class-specific priors for faces and the human body [17,26,27,32,35]."
        },
        "Hdhuman: High-quality human novel-view rendering from sparse views": {
          "authors": [
            "T Zhou",
            "J Huang",
            "T Yu",
            "R Shao"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10168294/",
          "ref_texts": "[36] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proc. IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323. 2.2",
          "ref_ids": [
            "36"
          ],
          "1": "For human rendering from sparse views, some works [33], [34], [35], [36], [37] use SMPL template as a prior, which helps to constrain the motion space and improve the rendering quality."
        },
        "Efficient view synthesis with neural radiance distribution field": {
          "authors": [
            "Yushuang Wu",
            "Xiao Li",
            "Jinglu Wang",
            "Xiaoguang Han",
            "Shuguang Cui",
            "Yan Lu"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Wu_Efficient_View_Synthesis_with_Neural_Radiance_Distribution_Field_ICCV_2023_paper.html",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "31"
          ],
          "1": "These include the handling of dynamic scenes [10, 46, 27, 33, 28], human digitization [39, 14, 37, 31, 20], shape and appearance modeling [9, 17, 51, 5], 3D-aware synthesis [6, 8], and many others."
        },
        "A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields": {
          "authors": [
            "K Ye",
            "H Wu",
            "X Tong",
            "K Zhou"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10584300/",
          "ref_texts": "[31] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u2013",
          "ref_ids": [
            "31"
          ],
          "1": "dynamic scenes [32], human bodies [31] or illumination variations [27])."
        },
        "PAV: Personalized Head Avatar from Unstructured Video Collection": {
          "authors": [
            "A Caliskan",
            "B Kicanaoglu",
            "H Kim"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72940-9_7",
          "ref_texts": "27. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "27"
          ],
          "1": "For human body, a series of approaches have been proposed to target this challenge [20,27]."
        },
        "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time rendering of temporally complex dynamic scenes": {
          "authors": [
            "J Yan",
            "R Peng",
            "L Tang",
            "R Wang"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3664647.3681463",
          "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
          "ref_ids": [
            "40"
          ],
          "1": "The photo-realistic view synthesis capability of NeRF has inspired a series of works across various domains, including enhancing rendering quality[4\u20136, 8, 54, 55, 72, 74], sparse inputs[3, 34, 63], surface representation and segmentation[38, 59, 79], accelerating training and rendering[6, 14, 19, 20, 33, 43, 44, 49, 52, 69, 73], as well as human modeling[40, 41, 60, 71, 80], among others."
        },
        "SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations": {
          "authors": [
            "Y Jiang",
            "Q Liao",
            "Z Wang",
            "X Lin",
            "Z Lu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10687388/",
          "ref_texts": "[16] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021.",
          "ref_ids": [
            "16"
          ],
          "1": "Recent works suggest that we can learn the deformation of a general character template from scanned data [11], [14] or RGB video data [15], [16] to get a drivable avatar directly.",
          "2": "Comparison with Baselines Additionally, We conduct comparisons with two baselines, Neural Body (NB) [29] and Ani-NeRF (AN) [16]."
        },
        "FastHuman: Reconstructing High-Quality Clothed Human in Minutes": {
          "authors": [
            "L Lin",
            "S Peng",
            "Q Gan",
            "J Zhu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550690/",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), pages 14294\u201314303, 2021. 1, 2, 7",
          "ref_ids": [
            "43"
          ],
          "1": "In order to model human avatars, some approaches [6, 43, 45, 60, 61] incorporate the estimated human skeleton and neural rendering to model animatable human avatars in an implicit Recovered Mesh Textured MeshReposed Mesh Input Video Figure 1.",
          "2": "[43, 45, 61] dynamically synthesize the human image."
        },
        "High-degrees-of-freedom dynamic neural fields for robot self-modeling and motion planning": {
          "authors": [
            "L Schulze",
            "H Lipson"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10611047/",
          "ref_texts": "[11] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "11"
          ],
          "1": "Different from methods using deformation from a canonical representation [11], we propose a DOFencoder-based dynamic neural density field, which is suitable for modeling complex changing scenes beyond robotics."
        },
        "ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation": {
          "authors": [
            "H Li",
            "HX Yu",
            "J Li",
            "J Wu"
          ],
          "url": "https://arxiv.org/abs/2412.18600",
          "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "61"
          ],
          "1": "Neural rendering techniques have significantly advanced the synthesis of realistic human appearances [12, 36, 42, 44, 46, 54, 61, 62, 83].",
          "2": "AnimatableNeRF [61] introduces a neural blend weight field and achieves superior novel view and novel pose synthesis results."
        },
        "Efficient Integration of Neural Representations for Dynamic Humans": {
          "authors": [
            "W Li",
            "L Zeng",
            "C Gao",
            "N Liu"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10666828/",
          "ref_texts": "[35] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "35"
          ],
          "5": "AN [35] uses neural blend weight fields with human skeletons for deformation fields, enabling the conversion between canonical and observation representations.",
          "6": "Additionally, compared to [1], [35], [4], our reconstruction results exhibit fewer noisy points."
        },
        "Neural texture puppeteer: A framework for neural geometry and texture rendering of articulated shapes, enabling re-identification at interactive speed": {
          "authors": [
            "Urs Waldmann",
            "Ole Johannsen",
            "Bastian Goldluecke"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Waldmann_Neural_Texture_Puppeteer_A_Framework_for_Neural_Geometry_and_Texture_WACVW_2024_paper.html",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, October 2021. 1, 2, 3, 4",
          "ref_ids": [
            "30"
          ],
          "2": "Good results can be achieved with NeRF-based [26] approaches [30, 38], approaches based on implicit neural representations [31, 36] or approximate differentiable rendering [47]."
        },
        "Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View": {
          "authors": [
            "D Lee",
            "D Kim",
            "J Lee",
            "M Lee",
            "S Lee",
            "S Lee"
          ],
          "url": "https://arxiv.org/abs/2407.06613",
          "ref_texts": "[36] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "36"
          ],
          "1": "In addition, its implicit representation capability leads to explosive development of other graphical tasks such as modeling dynamic scenes [28]\u2013[31], relighting [32], [33], 3D reconstructions [34], [35], and human avatar [36]."
        },
        "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text": {
          "authors": [
            "G Shim",
            "S Lee",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2502.11642",
          "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2, 3",
          "ref_ids": [
            "26"
          ],
          "1": "Building on diverse 3D representations [16, 22, 38, 40], numerous studies have explored the reconstruction of 3D human avatars from various data sources, including 3D scans [35, 36, 48], video sequences [7, 26, 42], single images [9, 11], and even text [1, 17, 19, 21].",
          "2": "These studies draw inspiration from human deformation concepts derived from deformable neural representations [2, 26, 27, 42], which address how 3D coordinates on a human model are deformed across different poses."
        },
        "Report on Methods and Applications for Crafting 3D Humans": {
          "authors": [
            "L Liu",
            "K Zhao"
          ],
          "url": "https://arxiv.org/abs/2406.01223",
          "ref_texts": "[60] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "60"
          ],
          "1": "Further advancements include deformable and animatable NeRFs, such as those proposed by previous works [60]\u2013[62], which synthesize humans from novel poses and views."
        },
        "Dreamo: articulated 3d reconstruction from a single casual video": {
          "authors": [
            "T Tu",
            "MF Li",
            "CH Lin",
            "YC Cheng",
            "M Sun"
          ],
          "url": "https://arxiv.org/abs/2312.02617",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "36"
          ],
          "1": "While some of these methods can generate plausible 3D models, they require specific inputs such as multi-view videos [36, 63], predefined 3D skeletons [36, 46, 49, 50], or 3D rest-pose point clouds [47]."
        },
        "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
          "authors": [
            "H Wang",
            "W Zhang",
            "S Liu",
            "X Zhou",
            "J Li",
            "Z Tang"
          ],
          "url": "https://arxiv.org/abs/2405.12477",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 6, 7",
          "ref_ids": [
            "34"
          ],
          "1": "Quantitative Results To verify the effectiveness of our method in solving the geometric distortion problem in reconstructing the human body, we compare our method with NeuralBody [35], HumanNeRF [43] AnimateNeRF [34], InstantNVR [5] InstantAvatar [15] GauHuman [11] on the ZJU dataset and the Monocap dataset, as shown in Table 1."
        },
        "Diversity-Aware Sign Language Production through a Pose Encoding Variational Autoencoder": {
          "authors": [
            "MI Lakhal",
            "R Bowden"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10581951/",
          "ref_texts": "[30] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the International Conference on Computer Vision (ICCV) , 2021.",
          "ref_ids": [
            "30"
          ],
          "1": "Human-NeRF [48], [30], [31] uses a human 3D mesh template (usually SMPL [24]) which is deformed with a given body pose and rendered to the target viewpoint using Volume Rendering [23]."
        },
        "Flnerf: 3d facial landmarks estimation in neural radiance fields": {
          "authors": [
            "H Zhang",
            "T Dai",
            "YW Tai",
            "CK Tang"
          ],
          "url": "https://arxiv.org/abs/2211.11202",
          "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2",
          "ref_ids": [
            "53"
          ],
          "1": "Further studies [32, 67, 68, 30, 77, 59, 83, 39, 92, 56, 38, 72, 44, 23, 58, 88, 28, 12] have been done to improve the performance, efficiency and generalization of NeRF, with its variants quickly and widely adopted in dynamic scene reconstruction [52, 82, 37, 55, 18], novel scene composition [51, 89, 48, 25, 41, 85, 46, 35], articulated 3D shape reconstruction [86, 61, 78, 93, 31, 94, 84, 11, 49, 53], and various computer vision tasks, including face NeRFs [21, 5, 50, 69, 16, 29], the focus of this paper."
        },
        "Neural Radiance Fields with Torch Units": {
          "authors": [
            "B Ni",
            "H Wang",
            "D Bai",
            "M Weng",
            "D Qi",
            "W Qiu"
          ],
          "url": "https://arxiv.org/abs/2404.02617",
          "ref_texts": "[37] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H., 2021a. Animatable neural radiance fields for modeling dynamic human bodies, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14314\u201314323.",
          "ref_ids": [
            "37"
          ],
          "1": "Besides, neural radiance fields are also applied to digital human body [10, 14, 26, 37, 55, 59].",
          "2": "Besides, Animatable [37] introduces neural radiance fields to generate a deformation field and enables human body modeling."
        },
        "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training": {
          "authors": [
            "R Yin",
            "V Yugay",
            "Y Li",
            "S Karaoglu",
            "T Gevers"
          ],
          "url": "https://arxiv.org/abs/2411.02229",
          "ref_texts": "[25] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Bao, H., Zhou, X.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
          "ref_ids": [
            "25"
          ],
          "1": "Due to its capabilities, NeRF became widely adopted for 3D scene reconstruction [1, 18, 8, 34, 14], human body modeling [26, 25, 36, 16], robotics [41, 28], and medical imaging [7]."
        },
        "LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes": {
          "authors": [
            "Z Qu",
            "K Xu",
            "GP Hancke",
            "RWH Lau"
          ],
          "url": "https://arxiv.org/abs/2411.06757",
          "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "36"
          ],
          "1": ", accelerating the training and rendering of NeRF [14, 18, 40, 41], handling dynamic scenes [21, 34, 39, 44, 61] and digital humans body [1, 34, 36, 37, 10] or human head [53, 68, 16] modeling, and the manipulation [4, 38, 28, 23] or generation [8, 15, 25, 33] of scene contents.",
          "2": "For example, [14, 18, 40, 41] are proposed to accelerate the NeRF training procedure, [21, 34, 39, 44, 61] are applied to render dynamic scenarios, [4, 38, 28, 23] are focused on the NeRF relighting methods, [1, 34, 36, 37, 53] are expanded to the non-rigid object rendering, [8, 15, 25, 33] are used for the generation models."
        },
        "TEDRA: Text-based Editing of Dynamic and Photoreal Actors": {
          "authors": [
            "B Sunagad",
            "H Zhu",
            "M Mendiratta",
            "A Kortylewski"
          ],
          "url": "https://arxiv.org/abs/2408.15995",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14314\u201314323, 2021. 1, 4",
          "ref_ids": [
            "45"
          ],
          "1": "Introduction Digital avatars of real humans play a vital role in various applications, including augmented and virtual reality, gaming, movie production, and synthetic data generation [10, 11, 13, 29, 30, 35, 45, 66, 72].",
          "2": "To better model the pose-dependent appearance of humans, recent studies [10, 13, 29, 35, 45, 66, 72] incorporate motion-aware residual deformations in the canonicalized space."
        },
        "Representing Animatable Avatar via Factorized Neural Fields": {
          "authors": [
            "C Song",
            "Z Wu",
            "B Wandt",
            "L Sigal",
            "H Rhodin"
          ],
          "url": "https://arxiv.org/abs/2406.00637",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "51"
          ],
          "1": "Other methods [50, 44, 51, 52] aim to improve results with an image-to-image translation network and a per-frame latent code."
        },
        "Few-Shot Multi-Human Neural Rendering Using Geometry Constraints": {
          "authors": [
            "VF Abrevaya",
            "F Multon",
            "A Boukhayma"
          ],
          "url": "https://arxiv.org/abs/2502.07140",
          "ref_texts": "[73] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
          "ref_ids": [
            "73"
          ],
          "1": "For humans, this has been leveraged to obtain geometry and appearance from monocular video [12, 37], RGB-D video [19], and sparse multi-view video [46, 52, 73, 76, 91, 93, 99, 116].",
          "2": "between 2 and 15), where the lack of views and presence of wide baselines is compensated by tracking a pre-scanned template [10, 17, 23, 88, 97], using a parametric body model [6, 32], or more recently, by the use of deep learning [33, 46, 50, 52, 73, 76, 91, 93, 99].",
          "3": "For generating free-viewpoint video, image-based rendering has been considered as an alternative or complement to 3D reconstruction [10, 46, 52, 52, 73, 93, 98, 99]."
        },
        "Innovative AI techniques for photorealistic 3D clothed human reconstruction from monocular images or videos: a survey": {
          "authors": [
            "Shuo Yang"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03641-7",
          "ref_texts": "149. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV , pp. 14294\u201314303 (2021).https://doi. org/10.1109/ICCV48922.2021.01405",
          "ref_ids": [
            "149"
          ],
          "3": "AnimatableNeRF [149] avails a consistency loss between the blend weights of the posed-to-canonical and canonical-to-posed transformations to optimize the canonical neural blend weight field and novel pose latent code.",
          "4": "Typically, a neural network is enlisted to predict the offset stemming from nonrigid pose-dependent deformation [10, 149, 152, 153, 163].",
          "6": "6M [193] ZJU-MoCap [143] Inference time \u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Animatable_NeRF [149] 22."
        },
        "AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation": {
          "authors": [
            "M Li",
            "S Yao",
            "C Kai",
            "Z Xie",
            "K Chen",
            "YG Jiang"
          ],
          "url": "https://arxiv.org/abs/2502.19441",
          "ref_texts": "[45] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "45"
          ],
          "1": "Extensions of NeRF [38] into dynamic scenes [39]\u2013[41] and methods for animatable 3D human models in multi-view scenarios [20], [43]\u2013[45], [58], [67] or monocular videos [8], [11], [14], [36] have shown promising results."
        },
        "Anipixel: Towards animatable pixel-aligned human avatar": {
          "authors": [
            "J Fan",
            "J Zhang",
            "Z Hou",
            "D Tao"
          ],
          "url": "https://dl.acm.org/doi/abs/10.1145/3581783.3612058",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV. 14314\u201314323.",
          "ref_ids": [
            "29"
          ],
          "2": "With the recent success of neural radiance field (NeRF) representation [23], a line of works has tried to reconstruct volumetric avatars in radiance field [29, 30, 44].",
          "4": "CV] 17 Oct 2023 MM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada Jinlong Fan, Jing Zhang, Zhi Hou, & Dacheng Tao human reconstruction, skeleton motion is often taken as prior to constraining the deformation field [29, 30, 50].",
          "5": "To account for dynamic humans, deformation fields are devised to deform the posed body in target space to canonical space, where the density and color are predicted [29, 30].",
          "6": "The other way is to predict the per-point weights using a neural network [17, 29, 44].",
          "8": "For each part, there are (a) the input three views and results of (b) AniNeRF [29], (c) NeuralBody [30], (d) MPS-NeRF [6], (e) our method, and (f) the ground truth.",
          "9": "893 Table 2: Comparison of our method with NeuralBody [30], AniNeRF [29], MPS-NeRF [6] on the Human3.",
          "10": "895 Table 3: Comparison of NeuralBody [30], AniNeRF [29], KeypointNeRF [22], and our method on ZJUMoCAP.",
          "11": "We predict two versions of masks, one is rendered by volume density accumulation, and the other is generated by minimum SDF rendering as in [29].",
          "12": "Following [6, 29], we conduct experiments on 7 subjects: S1, S5, S6, S7, S8, S9 and S11.",
          "13": "for the whole image, we follow previous methods [6, 29, 30] to project the 3D bounding box of the fitted SMPL mesh onto the image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
          "14": "We compare our method with recent two animatable methods, NeuralBody [30] and AniNeRF [29], and two generalizable methods, KeypointNeRF [22] and MPS-NeRF [6]."
        },
        "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic Scene": {
          "authors": [
            "S Biswas",
            "Q Wu",
            "B Banerjee",
            "H Rezatofighi"
          ],
          "url": "https://arxiv.org/abs/2409.17459",
          "ref_texts": "[7] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "7"
          ],
          "6": "SDF modeling contributes smoother surface reconstructions compared to models like [28, 7].",
          "8": "Ours TA V A [28] AnimatableNeRF[7]I/P Figure 4: Qualitative comparison on ZJUMocap dataset [9]."
        },
        "DiHuR: Diffusion-Guided Generalizable Human Reconstruction": {
          "authors": [
            "J Chen",
            "C Li",
            "GH Lee"
          ],
          "url": "https://arxiv.org/abs/2411.11903",
          "ref_texts": "[17] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 7",
          "ref_ids": [
            "17"
          ],
          "1": "Ani-NeRF [17] learns a canonical NeRF model and a backward LBS network which predicts residuals to the deterministic SMPLbased backward LBS (Linear Blending Skinning) to animate the learned human NeRF model.",
          "2": "For THuman dataset, we follow [5] and compare with existing human NeRF methods [4, 5, 17, 18]."
        },
        "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments": {
          "authors": [
            "Y Zhan",
            "Q Zhu",
            "M Niu",
            "M Ma",
            "J Zhao",
            "Z Zhong"
          ],
          "url": "https://arxiv.org/abs/2410.08082",
          "ref_texts": "[35] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "35"
          ],
          "1": "Animatable NeRF [35] defines a per-frame latent code to capture appearance variations across each frame."
        },
        "RustNeRF: Robust Neural Radiance Field with Low-Quality Images": {
          "authors": [
            "M Li",
            "M Lu",
            "X Li",
            "S Zhang"
          ],
          "url": "https://arxiv.org/abs/2401.03257",
          "ref_texts": "[18] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "18"
          ],
          "1": "There are also plenty of works extending NeRF to various application scenarios such as scalable large scene [23, 25], 3D human face [6, 7], 3D human body [18, 42], and few-shot reconstruction [33, 36]."
        },
        "Editing Implicit and Explicit Representations of Radiance Fields: A Survey": {
          "authors": [
            "A Hubert",
            "G Elghazaly",
            "R Frank"
          ],
          "url": "https://arxiv.org/abs/2412.17628",
          "ref_texts": "[117] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
          "ref_ids": [
            "117"
          ],
          "1": "Geometry editing can be used to modify facial expressions [113], body pose [117] or human movement in a video [104], both of which can be extended with human body parametric models [118]."
        },
        "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors": {
          "authors": [
            "J Yin",
            "W Yin",
            "H Chen",
            "X Ren",
            "Z Ma",
            "J Guo"
          ],
          "url": "https://arxiv.org/abs/2311.15171",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Int. Conf. Comput. Vis., pages 14314\u201314323, 2021.",
          "ref_ids": [
            "30"
          ],
          "2": "Animatable NeRF [30] uses a parametric human body model as a strong geometry prior to canonical NeRF model, and achieves impressive visual fidelity on novel view and pose synthesis results.",
          "19": "5, our method produces higher visual quality with fewer artifacts than existing methods [30,32,33,38,40], which also indicates a better correspondence across frames.",
          "20": "87% improvement for PSNR on subject \u201c313\u201d with seen poses than Ani-NeRF [30].",
          "23": "Compared with Ani-NeRF [30], our approach achieves a better performance of novel view synthesis on seen and unseen poses.",
          "25": "Specifically, compared with Ani-NeRF [30] and Ani-SDF [31], our approach not only reconstructs smoother geometry results with high-quality but also captures more geometry details (e."
        },
        "NeRF-FF: a plug-in method to mitigate defocus blur for runtime optimized neural radiance fields": {
          "authors": [
            "Tristan Wirth"
          ],
          "url": "https://link.springer.com/article/10.1007/s00371-024-03507-y",
          "ref_texts": "38. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human 123",
          "ref_ids": [
            "38"
          ],
          "1": "Some publications attempt to overcome the static nature of NeRFs regarding baked lighting and rigid scene geometry by applying them to generative models [34, 44], enabling dynamic relighting of the captured scenes [32, 46, 54, 64, 67], using them for animatable human avatar generation [13, 38, 47, 68] and through approaches that allow scene editing [5, 12, 19, 24, 34, 60, 63]."
        },
        "UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing": {
          "authors": [
            "J Fan",
            "J Zhang",
            "D Tao"
          ],
          "url": "https://arxiv.org/abs/2304.06969",
          "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "31"
          ],
          "1": "NeRF [26] \u0014 \u0018 \u0018 \u0018 Neumesh [51] \u0014 \u0018 \u0014 \u0014 Ani-NeRF [31] \u0014 \u0014 \u0018 \u0018 UV-V olumes [6] \u0014 \u0014 \u0014 \u0018 Ours (UV A) \u0014 \u0014 \u0014 \u0014 Table 1: Comparison of the rendering and editing abilities of different methods.",
          "4": "Novel view and Novel pose synthesis For novel view synthesis and novel pose rendering, we compare our method against five existing approaches: 1) Neural Body (NB) [32] diffuses per-SMPL-vertex latent codes in observation space to condition the NeRF model and achieves high-quality novel view synthesis results on training poses; 2) Ani-NeRF [31] learns a backward LBS weight field and a canonical NeRF to reconstruct the human avatar;"
        },
        "Interactive Rendering of Relightable and Animatable Gaussian Avatars": {
          "authors": [
            "Y Zhan",
            "T Shao",
            "H Wang",
            "Y Yang",
            "K Zhou"
          ],
          "url": "https://arxiv.org/abs/2407.10707",
          "ref_texts": "[40] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "40"
          ],
          "1": "In recent years, many works [34], [35], [36], [37], [38], [38], [39], [40], [41], [42], [43] have used NeRF [7] to represent the human body by learning from multi-view videos, achieving pleasant rendering results.",
          "2": "Previous methods have successfully constructed the body geometry from the multiview or monocular videos [36], [40], [43]."
        },
        "TIFu: Tri-directional Implicit Function for High-Fidelity 3D Character Reconstruction": {
          "authors": [
            "B Lim",
            "SW Lee"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-981-97-8705-0_10",
          "ref_texts": "26. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao: Animatable neural radiance fields for modeling dynamic human bodies, in ICCV, 2021",
          "ref_ids": [
            "26"
          ],
          "1": "NeRF-based techniques [26] are gaining popularity for creating photorealistic renderings of novel viewpoints from a single or few images."
        },
        "Neural rendering of humans in novel view and pose from monocular video": {
          "authors": [
            "T Wang",
            "N Sarafianos",
            "MH Yang",
            "T Tung"
          ],
          "url": "https://arxiv.org/abs/2204.01218",
          "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2, 3, 6, 7",
          "ref_ids": [
            "34"
          ],
          "1": "Given a monocular video, we predict novel views with body poses unseen from training with fine-level details (wrinkles) that works such as NeuralBody [34] or HumanNeRF [46] struggle to obtain.",
          "5": "The qualitative comparisons are provided in Fig 4 where our approach captures fine-level details on the body (1st,3rd rows) and head (2nd row) better than prior works [34, 35, 46]."
        },
        "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images": {
          "authors": [
            "J Lee",
            "S Cho",
            "T Kim",
            "HD Jang",
            "M Lee",
            "G Cha"
          ],
          "url": "https://arxiv.org/abs/2412.16028",
          "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
          "ref_ids": [
            "27"
          ],
          "1": "NeRF and 3DGS has enabled a wide range of related research, including dynamic scene representation [14, 15, 23, 24, 28, 33], human avatars [8, 27, 39], 3D mesh reconstruction [16, 32, 35, 36], 3D scene representation from sparse-view images [22, 34, 42, 43], and 3D scene representation from blurry images [4, 10\u201313, 18, 25, 26, 37, 46]."
        },
        "RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians": {
          "authors": [
            "S Peng",
            "W Xie",
            "Z Wang",
            "X Guo",
            "Z Chen",
            "B Yang"
          ],
          "url": "https://arxiv.org/abs/2501.07104",
          "ref_texts": "[40] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H., 2021a. Animatable neural radiance fields for modeling dynamic human bodies, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14314\u201314323.",
          "ref_ids": [
            "40"
          ],
          "4": "We use Anim-NeRF [40] to get the refined poses of ZJU-MoCap in our training and inference."
        },
        "SAGA: Surface-Aligned Gaussian Avatar": {
          "authors": [
            "R Chen",
            "Y Cong",
            "J Liu"
          ],
          "url": "https://arxiv.org/abs/2412.00845",
          "ref_texts": "[37] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in IEEE/CVF International Conference on Computer Vision , 2021, pp.",
          "ref_ids": [
            "37"
          ],
          "1": "Given the unprecedented success of Neural Radiance Fields [6], many methods have applied NeRF to reconstruct and render humans from videos [7], [8], [10], [11], [37]\u2013[40].",
          "7": "For NeRF-based methods: NeuralBody [7] anchors latent codes on the SMPL mesh and diffuse in 3D space with 3D convolution networks for neural volume rendering; AnimatableNeRF [37] represents the scene with a large MLP, and learns a forward and backward blending weight MLP for animation; HumanNeRF [8] further incorporates a non-rigid deformation network and achieves SOTA performance; InstantAvatar [19] applies the efficient Instant-NGP [12] as the canonical representation; InstantNVR [18] designs multi-part hash encoder based on [12]."
        },
        "Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting with State-Space Modeling": {
          "authors": [
            "J Deng",
            "Y Luo"
          ],
          "url": "https://arxiv.org/abs/2412.00333",
          "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2",
          "ref_ids": [
            "44"
          ],
          "1": "While classical approaches using the plenoptic function [1], image-based rendering [10, 29], or explicit geometry [49, 50] face memory limitations, implicit representations [18, 31, 42, 59, 62] have shown promise through deformation fields [42, 43, 45] and specialized priors [2, 4, 6, 24, 44, 59]."
        },
        "2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting": {
          "authors": [
            "Q Yan",
            "M Sun",
            "L Zhang"
          ],
          "url": "https://arxiv.org/abs/2503.02452",
          "ref_texts": "[16] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 314\u201314 323.",
          "ref_ids": [
            "16"
          ],
          "1": "Learning Gaussian Parameters Directly Methods [9], [11] that directly learn Gaussian parameters typically follow a pipeline that is similar to NeRF-based approaches [2], [16], [17], where the avatar is represented in a canonical space and subsequently transformed into the posed space using LBS, after which the Gaussian primitives are rendered into images through the 3DGS rasterizer."
        },
        "PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars": {
          "authors": [
            "S Sasaki",
            "J Wu",
            "K Nishino"
          ],
          "url": "https://arxiv.org/abs/2412.04433",
          "ref_texts": "[22] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
          "ref_ids": [
            "22"
          ],
          "1": "Following the release of NeRF, the earliest works in this area [7, 13, 22, 23, 27, 35] rely on the SMPL model [14] to skin an optimized NeRF of a human from a canonical space (unskinned) space to the observation space (skinned) and vice versa, which is key to both animation and dynamic reconstruction."
        },
        "NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction": {
          "authors": [
            "Z Zhang",
            "J Song",
            "E P\u00e9rez-Pellitero"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10550790/",
          "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 2, 3",
          "ref_ids": [
            "30"
          ],
          "1": "NeRF has later been adapted to represent dynamic human body [20, 30, 31, 37, 38, 44], achieving compelling results on modeling a clothed human."
        },
        "DRaCoN--Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars": {
          "authors": [
            "A Raj",
            "U Iqbal",
            "K Nagano",
            "S Khamis"
          ],
          "url": "https://arxiv.org/abs/2203.15798",
          "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR, 2021. 2, 3, 5, 6, 7, 8",
          "ref_ids": [
            "25"
          ],
          "3": "A-NeRF [39] and AnimatableNeRF [25] use body pose information to canonicalize the sampled rays and learn neural radiance fields in the canonical space, which helps the learned avatar to generalize across different poses."
        },
        "ExFMan: Rendering 3D Dynamic Humans with Hybrid Monocular Blurry Frames and Events": {
          "authors": [
            "K Chen",
            "Z Wang",
            "L Wang"
          ],
          "url": "https://arxiv.org/abs/2409.14103",
          "ref_texts": "[57] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of ICCV, 2021.",
          "ref_ids": [
            "57"
          ],
          "2": "Following the emergence of Neural Radiance Fields (NeRF) [44], a variety of advancements have been made to facilitate the highfidelity rendering of static scenes [1, 73, 70, 68, 46], moving subjects [13, 34, 53, 54, 59, 50], and dynamic humans [12, 20, 33, 39, 57, 76, 85, 22, 23]."
        },
        "TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering": {
          "authors": [
            "Sadia Mubashshira",
            "Kevin Desai"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2025W/ImageQuality/html/Mubashshira_TE-NeRF_Triplane-Enhanced_Neural_Radiance_Field_for_Artifact-Free_Human_Rendering_WACVW_2025_paper.html",
          "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 4",
          "ref_ids": [
            "28"
          ],
          "1": "Similar to the approach in Animatable Neural Radiance Fields [28], this allows modeling complex deformations such as cloth wrinkles and muscle bulges."
        },
        "Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video": {
          "authors": [
            "Y Rao",
            "EP Pellitero",
            "B Busam",
            "Y Zhou"
          ],
          "url": "https://arxiv.org/abs/2312.04784",
          "ref_texts": "52. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
          "ref_ids": [
            "52"
          ],
          "1": "monocular video, approaches like SelfRecon [21] and NeuMan [22] as well as Animatable NeRF [52] and HumanNeRF [70] integrate motion priors for regularization to allow this even with a single input video."
        },
        "PixelHuman: Animatable Neural Radiance Fields from Few Images": {
          "authors": [
            "G Shim",
            "J Lee",
            "J Hyung",
            "J Choo"
          ],
          "url": "https://arxiv.org/abs/2307.09070",
          "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 2, 3, 6",
          "ref_ids": [
            "19"
          ],
          "1": "Creating human avatars has been developed in various ways, ranging from creating textured human scans using various 3D representations [23, 24, 36, 26, 6], to rendering human images using implicit representations [19, 31, 14].",
          "2": "As neural radiance fields [18] has emerged with promising rendering performance on 3D objects, various human rendering methods [19, 31, 14] have been proposed to learn 3D human bodies only from images.",
          "3": "Similarly, Animatable NeRF [19] is suggested to generate novel human poses by deforming human body into a canonical human model represented by a neural radiance field.",
          "4": "2 Skeletal Deformation Following previous studies [19, 31, 14], we define the skeletal deformation by utilizing the linear blend skinning (LBS) function [30] to transform coordinates between different pose spaces.",
          "5": "We select HumanNeRF [31], Ani-NeRF [19], and KeypointNeRF [17] as our baselines which are state-of-theart human rendering methods."
        },
        "Bringing Telepresence to Every Desk": {
          "authors": [
            "S Wang",
            "Z Wang",
            "R Schmelzle",
            "L Zheng"
          ],
          "url": "https://arxiv.org/abs/2304.01197",
          "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 3",
          "ref_ids": [
            "45"
          ],
          "1": "Some recent works [46, 31, 54, 45, 26] exclusively focus on animating clothed humans."
        },
        "Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera": {
          "authors": [
            "H Lee",
            "J Cha",
            "Y Ku",
            "JS Yoon",
            "S Baek"
          ],
          "url": "https://arxiv.org/abs/2312.16842",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021.",
          "ref_ids": [
            "29"
          ],
          "1": "Animatable NeRF [29] designed a novel transformation field between view and canonical space to better memorize the seen poses."
        },
        "Talking Head (?) Anime from a Single Image 4: Improved Model and Its Distillation": {
          "authors": [
            "P Khungurn"
          ],
          "url": "https://arxiv.org/abs/2311.17409",
          "ref_texts": "[59] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., and Bao, H. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV (2021).",
          "ref_ids": [
            "59"
          ],
          "1": "While INRs can be used to directly represent articulated characters [14, 89, 59, 97], we take the view that our signal is a parameterized collection of images like Bemana et al.",
          "2": "[59] Peng, S."
        },
        "Human View Synthesis using a Single Sparse RGB-D Input": {
          "authors": [
            "P Nguyen",
            "N Sarafianos",
            "C Lassner",
            "J Heikkila"
          ],
          "url": "https://3dvar.com/Nguyen2021Human.pdf",
          "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1, 2",
          "ref_ids": [
            "51"
          ],
          "1": "However, synthesizing novel views of humans in motion requires methods to handle dynamic scenes with various deformations which is a challenging task [66, 72]; especially in those regions with fine details such as the face or the clothes [49, 51, 71].",
          "2": "Given multi-view input frames or videos, recent works on rendering animatable humans from novel views show impressive results [49,51, 52, 71]."
        },
        "Building 4D Models of Objects and Scenes from Monocular Videos": {
          "authors": [
            "G Yang"
          ],
          "url": "https://kilthub.cmu.edu/ndownloader/files/42228117",
          "ref_texts": "[183] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 4.2",
          "ref_ids": [
            "183"
          ],
          "1": "Similar to our goal, some recent works [138, 175, 183, 184, 238] produce pose-controllable NeRFs, but they rely on a human body model, or synchronized multi-view video inputs."
        },
        "Towards Robust 3D Pose Transfer with Adversarial Learning\u2013Supplementary Materials\u2013": {
          "authors": [
            "H Chen",
            "H Tang",
            "E Adeli",
            "G Zhao"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chen_Towards_Robust_3D_CVPR_2024_supplemental.pdf",
          "ref_texts": "[7] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1",
          "ref_ids": [
            "7"
          ],
          "1": "Following previous works regarding the 3D pose learning [4, 7], we modified the LayerNorm operation from a classical Transformer architecture into an instance normalization (InsNorm) block inspired by [10] presented in Table 3."
        },
        "Learning Trajectories from Human Demonstration via Time Invariant Dynamical Systems": {
          "authors": [
            "Paul Gesel"
          ],
          "url": "https://search.proquest.com/openview/f05f133e34860031703766a346358317/1?pq-origsite=gscholar&cbl=18750&diss=y",
          "ref_texts": "[71] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
          "ref_ids": [
            "71"
          ],
          "1": "Some recent approaches have parameterized NeRF models such that they can be animated/controlled [70, 71]."
        },
        "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos\u2014Supplementary Material": {
          "authors": [
            "S Hu",
            "T Hu",
            "Z Liu"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_GauHuman_Articulated_Gaussian_CVPR_2024_supplemental.pdf",
          "ref_texts": "[11] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1",
          "ref_ids": [
            "11"
          ],
          "1": "Animatable NeRF (AN) [11] learns a canonical human NeRF through skeleton-driven deformation and learned blend weight fields.",
          "2": "AS[13] further extends [11] by learning a signed distance field and a posedependent deformation field for residual information and geometric details of dynamic 3D humans."
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors* Supplemental Material": {
          "authors": [
            "Y Huang",
            "H Yi",
            "W Liu",
            "H Wang",
            "B Wu",
            "W Wang",
            "B Lin"
          ],
          "url": "https://openaccess.thecvf.com/content/ICCV2023/supplemental/Huang_One-shot_Implicit_Animatable_ICCV_2023_supplemental.pdf",
          "ref_texts": "[11] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021. 1, 2, 3",
          "ref_ids": [
            "11"
          ],
          "1": "1 Data splitting For per-subject optimization methods Animatable NeRF[11] (Ani-NeRF) and NeuralBody[13] (NB), we use all subjects of ZJU-MoCap data-set (313, 315, 377, 386, 387, 390, 392, 393, 394) and the \u201dPosing\u201d sequences of Human 3.",
          "4": "5, we replaced the HumanNeRF model used in ELICIT with an SDF-based model from Animatable NeRF[12, 11]."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
          "authors": [
            "JA Rahim"
          ],
          "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
          "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProc. of International Conference on Computer Vision (ICCV), pages 14314\u201314323, 2021.",
          "ref_ids": [
            "29"
          ],
          "2": "Animatable NeRF-based human rendering methods propose conditioning NeRF on an extra human pose parameter to control the rendering [29, 21, 42, 44, 30].",
          "3": "Below we discuss three important works in this category: Animatable NeRF [29], Neural Body [30], and Structured Local Radiance Fields for Human Avatar Modeling [44].",
          "4": "Animatable NeRF [29] renders a novel view of a human model in a target pose, by leveraging SMPL as human body prior and using deformations to canonical pose (which in this case is a T-pose).",
          "8": "We compare against three methods; Structured Local Radiance Fields for Human Avatar Modeling (SLRF) [44], Animatable Nerf (AN) [29], Neural Body (NB) [30]."
        },
        "Posed Neural Radiance Fields for Human Animation/submitted by Paul Knoll": {
          "authors": [
            "P Knoll"
          ],
          "url": "https://epub.jku.at/urn/urn:nbn:at:at-ubl:1-74368",
          "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021.",
          "ref_ids": [
            "43"
          ],
          "1": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies This approach [43] proposes a novel method for creating animatable human models from multi-view video by decomposing a non-rigidly deforming scene into a canonical NeRF and a set of deformation fields that map observation-space points to the canonical space."
        },
        "Learning Neural Volumetric Representations of Dynamic Humans in Minutes Supplemental Material": {
          "authors": [
            "C Geng",
            "S Peng",
            "Z Xu",
            "H Bao",
            "X Zhou"
          ],
          "url": "https://chen-geng.com/instant_nvr/files/supp.pdf",
          "ref_texts": "[9] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 1, 3",
          "ref_ids": [
            "9"
          ],
          "1": "Specifically, MHE-encoded (x, y, z) coordinates in canonical space are first fed into the first MLP to output a 16D geometric feature z, which is concatenated with positional encoded [6] canonical view direction, z and an 8D time-varying latent code [9].",
          "2": "Details of baselines Neural Body [11], Ani-SDF [10], HumanNeRF [14] and Ani-NeRF [9] We use the released code and conduct experiments on a single NVIDIA RTX 3090 GPU."
        },
        "Supplementary for ARAH: Animatable Volume Rendering of Articulated Human SDFs": {
          "authors": [
            "S Wang",
            "K Schwarz",
            "A Geiger",
            "S Tang"
          ],
          "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920001-supp.pdf",
          "ref_texts": "12. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
          "ref_ids": [
            "12"
          ],
          "1": "For inference, we follow [12, 13] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
          "2": "For unseen test poses we follow the practice of [12, 13] and use the latent code Z of the last training frame as the input.",
          "5": "We also report quantitative results on the H36M dataset [5], following the testing protocols proposed by [12] in Table 2."
        },
        "Supplemental Materials for TAVA: Template-free Animatable Volumetric Actors": {
          "authors": [
            "R Li",
            "J Tanke",
            "M Vo",
            "M Zollh\u00f6fer",
            "J Gall",
            "A Kanazawa"
          ],
          "url": "https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920417-supp.pdf",
          "ref_texts": "5. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021)",
          "ref_ids": [
            "5"
          ],
          "3": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [5,6,8].",
          "4": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 PSNR \u2191 SSIM \u2191 Subject 313 Animatable-NeRF [5] 29.",
          "5": "957 Subject 315 Animatable-NeRF [5] 27.",
          "6": "960 Subject 377 Animatable-NeRF [5] 32.",
          "7": "980 Subject 386 Animatable-NeRF [5] 34."
        }
      }
    },
    {
      "title": "vs-net: voting with segmentation for visual localization",
      "id": 19,
      "valid_pdf_number": "37/40",
      "matched_pdf_number": "28/37",
      "matched_rate": 0.7567567567567568,
      "citations": {
        "Flowformer: A transformer architecture for optical flow": {
          "authors": [
            "Z Huang",
            "X Shi",
            "C Zhang",
            "Q Wang",
            "KC Cheung"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_40",
          "ref_texts": "19. Huang, Z., Zhou, H., Li, Y., Yang, B., Xu, Y., Zhou, X., Bao, H., Zhang, G., Li, H.: Vs-net: Voting with segmentation for visual localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.",
          "ref_ids": [
            "19"
          ],
          "1": "Visual correspondence tasks [44,19,7,27,51] is a main stream in computer vision."
        },
        "Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation": {
          "authors": [
            "Xiaoyu Shi",
            "Zhaoyang Huang",
            "Dasong Li",
            "Manyuan Zhang",
            "Ka Chun",
            "Simon See",
            "Hongwei Qin",
            "Jifeng Dai",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.html",
          "ref_texts": "[25] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "25"
          ],
          "1": "Image correspondence is a critical intermediate representation for downstream applications [6, 7, 23, 25, 35\u201337, 46, 53, 76]."
        },
        "Deeplsd: Line segment detection and refinement with deep image gradients": {
          "authors": [
            "Remi Pautrat",
            "Daniel Barath",
            "Viktor Larsson",
            "Martin R. Oswald",
            "Marc Pollefeys"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Pautrat_DeepLSD_Line_Segment_Detection_and_Refinement_With_Deep_Image_Gradients_CVPR_2023_paper.html",
          "ref_texts": "[21] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In Computer Vision and Pattern Recognition (CVPR), 2021. 3",
          "ref_ids": [
            "21"
          ],
          "1": "Attraction fields have also been leveraged for keypoint detection [21], where 2D vectors are voting for the closest keypoint in the image."
        },
        "Sfd2: Semantic-guided feature detection and description": {
          "authors": [
            "Fei Xue",
            "Ignas Budvytis",
            "Roberto Cipolla"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xue_SFD2_Semantic-Guided_Feature_Detection_and_Description_CVPR_2023_paper.html",
          "ref_texts": "[26] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In CVPR, 2021. 2, 3",
          "ref_ids": [
            "26"
          ],
          "2": "Compared to local features, high-level semantics are more robust to appearance changes and have been widely used in visual localization [7, 25, 26, 30, 31, 44, 62\u201364, 66, 73, 78]."
        },
        "Pats: Patch area transportation with subdivision for local feature matching": {
          "authors": [
            "Junjie Ni",
            "Yijin Li",
            "Zhaoyang Huang",
            "Hongsheng Li",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Guofeng Zhang"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.html",
          "ref_texts": "[19] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "19"
          ],
          "1": "In the past decades, local feature matching [3, 40] has been widely used in a large number of applications such as structure from motion (SfM) [44, 64], simultaneous localization and mapping (SLAM) [30,36,62], visual localization [19,41], object pose estimation [22, 61], etc."
        },
        "Rnnpose: Recurrent 6-dof object pose refinement with robust correspondence field estimation and pose optimization": {
          "authors": [
            "Yan Xu",
            "Yee Lin",
            "Guofeng Zhang",
            "Xiaogang Wang",
            "Hongsheng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.html",
          "ref_texts": "[17] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "17"
          ],
          "1": "Non-linear least squares optimization algorithms, such as Gauss-Newton [30] and Levenberg-Marquardt [31], are widely used in computer vision [17, 25, 32, 53], given their efficient and effective nature."
        },
        "A survey on monocular re-localization: From the perspective of scene map representation": {
          "authors": [
            "J Miao",
            "K Jiang",
            "T Wen",
            "Y Wang",
            "P Jia"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10475537/",
          "ref_texts": "[262] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVs-net: V oting with segmentation for visual localization,\u201d 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6097\u20136107, 2021.",
          "ref_ids": [
            "262"
          ],
          "1": "More recent works [43], [255]\u2013[262] have instead applied DNNbased models fed by RGB images, which greatly improves the usability of the SCR solution.",
          "2": "Such a joint classification and regression strategy is then well studied [258], [262], [264], [265]."
        },
        "Visual localization via few-shot scene region classification": {
          "authors": [
            "S Dong",
            "S Wang",
            "Y Zhuang",
            "J Kannala"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10044413/",
          "ref_texts": "[22] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6101\u20136111, 2021.",
          "ref_ids": [
            "22"
          ],
          "2": "During the last decade, scene coordinate regression based two-step approaches [9,22,28,43] achieve state-of-the-art localization accuracy on public benchmarks [25,39,46], which is the main focus of this paper.",
          "3": "Another popular direction is scene coordinate regression based methods [7 \u20139, 22, 28, 29, 51] that estimate the 2D3D correspondences implicitly by memorizing the mapping from image pixels to scene coordinates."
        },
        "SGLoc: Scene geometry encoding for outdoor LiDAR localization": {
          "authors": [
            "Wen Li",
            "Shangshu Yu",
            "Cheng Wang",
            "Guosheng Hu",
            "Siqi Shen",
            "Chenglu Wen"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2023_paper.html",
          "ref_texts": "[22] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In CVPR, pages 6101\u20136111, 2021. 2",
          "ref_ids": [
            "22"
          ],
          "1": "Regression-based Localization Recently, in camera localization, the scene coordinate regression method has been designed, which regresses 2D-3D correspondences and estimates the pose with PnPRANSAC [4,6, 7, 22, 54]."
        },
        "OFVL-MS: Once for visual localization across multiple indoor scenes": {
          "authors": [
            "Tao Xie",
            "Kun Dai",
            "Siyi Lu",
            "Ke Wang",
            "Zhiqiang Jiang",
            "Jinghan Gao",
            "Dedong Liu",
            "Jie Xu",
            "Lijun Zhao",
            "Ruifeng Li"
          ],
          "url": "http://openaccess.thecvf.com/content/ICCV2023/html/Xie_OFVL-MS_Once_for_Visual_Localization_across_Multiple_Indoor_Scenes_ICCV_2023_paper.html",
          "ref_texts": "[20] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "20"
          ],
          "1": "The structured-based methods demonstrate state-of-the-art performance in large-scale scenes thanks to expeditious image retrieval techniques and feature matching algorithms, while they are limited in small-scale static scenes such as indoor scenes [26, 20]."
        },
        "Sacreg: Scene-agnostic coordinate regression for visual localization": {
          "authors": [
            "Jerome Revaud",
            "Yohann Cabon",
            "Romain Bregier",
            "Jongmin Lee",
            "Philippe Weinzaepfel"
          ],
          "url": "https://openaccess.thecvf.com/content/CVPR2024W/3DMV/html/Revaud_SACReg_Scene-Agnostic_Coordinate_Regression_for_Visual_Localization_CVPRW_2024_paper.html",
          "ref_texts": "[28] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In CVPR, 2021. 2",
          "ref_ids": [
            "28"
          ],
          "1": "More recent works [6\u20139, 19, 28, 36, 66, 80, 82, 87] have replaced regression forests with CNN-based models that only require an RGB image.",
          "2": "[28] propose to add a segmentation branch to obtain segmentation on scene-specific landmarks, which can then be associated with 3D landmarks in the scene to estimate camera pose."
        },
        "Segment anything model is a good teacher for local feature learning": {
          "authors": [
            "J Wu",
            "R Xu",
            "Z Wood-Doughty",
            "C Wang",
            "S Xu"
          ],
          "url": "https://arxiv.org/abs/2309.16992",
          "ref_texts": "[39] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVs-net: V oting with segmentation for visual localization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6101\u20136111.",
          "ref_ids": [
            "39"
          ],
          "1": "Some early works incorporated semantic segmentation into the visual localization pipeline for filtering matching points [39], [40], improving 2D-3D matching [41], [42], and estimating camera position [43]."
        },
        "Efficient large-scale localization by global instance recognition": {
          "authors": [
            "Fei Xue",
            "Ignas Budvytis",
            "Daniel Olmeda",
            "Roberto Cipolla"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xue_Efficient_Large-Scale_Localization_by_Global_Instance_Recognition_CVPR_2022_paper.html",
          "ref_texts": "[20] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In CVPR, 2021. 8",
          "ref_ids": [
            "20"
          ],
          "1": "Besides, directly recognizing a large number of global instances in large-scale scenes is memory-consuming, but hierarchical recognition [20] has the potential to solve this problem."
        },
        "Leveraging neural radiance fields for uncertainty-aware visual localization": {
          "authors": [
            "L Chen",
            "W Chen",
            "R Wang"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10610126/",
          "ref_texts": "[2] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVs-net: V oting with segmentation for visual localization,\u201d in CVPR, 2021, pp. 6101\u20136111.",
          "ref_ids": [
            "2"
          ],
          "1": "[2] partition the 3D surfaces into 3D patches and train a segmentation network to obtain correspondences between image pixels and patch centers."
        },
        "Focustune: Tuning visual localization through focus-guided sampling": {
          "authors": [
            "Son Tung",
            "Alejandro Fontan",
            "Michael Milford",
            "Tobias Fischer"
          ],
          "url": "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_FocusTune_Tuning_Visual_Localization_Through_Focus-Guided_Sampling_WACV_2024_paper.html",
          "ref_texts": "[23] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 6101\u20136111, 2021. 1, 3",
          "ref_ids": [
            "23"
          ],
          "1": "Scene Coordinate Regression (SCR) models [3, 5, 7, 8, 13, 19, 23, 32, 48] represent a marked advancement in this regard, trading pose regression for scene coordinate regression and achieving higher accuracy.",
          "2": "SCR models can be implemented by random forests [48,51] or convolutional neural networks [3,5\u20138,19,23]."
        },
        "Reprojection Errors as Prompts for Efficient Scene Coordinate Regression": {
          "authors": [
            "TR Liu",
            "HK Yang",
            "JM Liu",
            "CW Huang"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-73404-5_17",
          "ref_texts": "24. Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: Voting with segmentation for visual localization. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.",
          "ref_ids": [
            "24"
          ],
          "1": "Moreover, unlike methods that map every 2D pixel to a 3D coordinate, some works [19,24,25] have aimed at learning to detect 3D landmarks to establish 2D-3D correspondences."
        },
        "Lazy visual localization via motion averaging": {
          "authors": [
            "S Dong",
            "S Liu",
            "H Guo",
            "B Chen",
            "M Pollefeys"
          ],
          "url": "https://arxiv.org/abs/2307.09981",
          "ref_texts": "[35] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "35"
          ],
          "1": "Scene coordinate regression with neural networks [6, 7, 9, 35, 44, 80] enables queries without depth, and the training time can be significantly reduced [19].",
          "2": "Specifically, we select Active Search (AS) [59], InLoc [71], HLoc [54, 55], and PixLoc [56] as representatives for SfM-based feature matching approaches, and select HSCNet [44], DSAC* [9], VS-Net [35], and DSM [72] to represent the scene coordinate regression series."
        },
        "LoGS: Visual Localization via Gaussian Splatting with Fewer Training Images": {
          "authors": [
            "Y Cheng",
            "J Jiao",
            "Y Wang",
            "D Kanoulas"
          ],
          "url": "https://arxiv.org/abs/2410.11505",
          "ref_texts": "[14] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6101\u20136111, 2021.",
          "ref_ids": [
            "14"
          ],
          "1": "Region classification [13] and the segmentation branch [14] are later introduced to enhance scene understanding."
        },
        "PRAM: Place Recognition Anywhere Model for Efficient Visual Localization": {
          "authors": [
            "F Xue",
            "I Budvytis",
            "R Cipolla"
          ],
          "url": "https://arxiv.org/abs/2404.07785",
          "ref_texts": "[87] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVs-net: V oting with segmentation for visual localization,\u201d in CVPR, 2021.",
          "ref_ids": [
            "87"
          ],
          "2": "Although SCRs [11]\u2013[14], [87] achieve outstanding median position (\u2248 20cm) and orientation (\u2248 0."
        },
        "Lc*: Visual-inertial loose coupling for resilient and lightweight direct visual localization": {
          "authors": [
            "S Oishi",
            "K Koide",
            "M Yokozuka"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10161443/",
          "ref_texts": "[17] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVS-Net: V oting with segmentation for visual localization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6101\u20136111.",
          "ref_ids": [
            "17"
          ],
          "1": "Recently, another approach, scene coordinate regression, has been extensively studied and has achieved state-of-the-art localization [17]."
        },
        "A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding": {
          "authors": [
            "Y Dong",
            "Y Li",
            "Z Huang",
            "W Bian",
            "J Liu",
            "H Bao"
          ],
          "url": "https://arxiv.org/abs/2411.01893",
          "ref_texts": "[26] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. Vs-net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u20136111, 2021.",
          "ref_ids": [
            "26"
          ],
          "1": "1 Traditional MVS Multi-View Stereo has been developed for many years and has many downstream or related applications such as simultaneous localization and mapping (SLAM) [25], visual localization [26], 3D reconstruction [27; 28], 3D generation [29] and scene understanding [30]."
        },
        "SGL: Structure Guidance Learning for Camera Localization": {
          "authors": [
            "X Zhang",
            "S Gao",
            "X Nan",
            "H Ning",
            "Y Yang",
            "Y Ping"
          ],
          "url": "https://arxiv.org/abs/2304.05571",
          "ref_texts": "[36] Z. Huang, H. Zhou, Y . Li, B. Yang, Y . Xu, X. Zhou, H. Bao, G. Zhang, and H. Li, \u201cVs-net: V oting with segmentation for visual localization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6101\u20136111, 2021.",
          "ref_ids": [
            "36"
          ],
          "1": "Multiple researches [35, 36] have revealed that structure-based approaches and the scene coordinates prediction branch of the end-to-end ones yield higher localization accuracy than the metrics regression branch of the end-to-end ones."
        },
        "Deep Learning Methods for Point Matching, Visual Localization and 3D Reconstruction": {
          "authors": [
            "Shuzhe Wang"
          ],
          "url": "https://aaltodoc.aalto.fi/items/becab958-d8b1-40d1-9146-0fe3248171fc",
          "ref_texts": "[72] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: Voting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6101\u20136111, 2021.",
          "ref_ids": [
            "72"
          ],
          "1": "Structure-based localization [13,15,18,52,72,87,88,147,148,150,153,170, 190,205], on the other hand, involves first finding correspondences between 2D pixels and 3D points (see Chapter 2), then solving the camera pose with minimal solvers."
        },
        "RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization": {
          "authors": [
            "YXKYL Guofeng",
            "ZXWH Li"
          ],
          "url": "https://decayale.github.io/publication/rnnpose/paper.pdf",
          "ref_texts": "[17] Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, and Hongsheng Li. VS-Net: V oting with segmentation for visual localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101\u2013",
          "ref_ids": [
            "17"
          ],
          "1": "Non-linear least squares optimization algorithms, such as Gauss-Newton [30] and Levenberg-Marquardt [31], are widely used in computer vision [17, 25, 32, 54], given their efficient and effective nature."
        }
      }
    },
    {
      "title": "detector-free structure from motion",
      "id": 24,
      "valid_pdf_number": "31/34",
      "matched_pdf_number": "26/31",
      "matched_rate": 0.8387096774193549,
      "citations": {
        "Efficient LoFTR: Semi-dense local feature matching with sparse-like speed": {
          "authors": [
            "Yifan Wang",
            "Xingyi He",
            "Sida Peng",
            "Dongli Tan",
            "Xiaowei Zhou"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_Efficient_LoFTR_Semi-Dense_Local_Feature_Matching_with_Sparse-Like_Speed_CVPR_2024_paper.html",
          "ref_texts": "[18] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In CVPR, 2024. 1",
          "ref_ids": [
            "18"
          ],
          "1": "The established matches between images have broad usages such as reconstructing the 3D world by structure from motion (SfM) [1, 18, 25, 43] or SLAM system [30, 31], and visual localization [38, 40], etc."
        },
        "Vggsfm: Visual geometry grounded deep structure from motion": {
          "authors": [
            "Jianyuan Wang",
            "Nikita Karaev",
            "Christian Rupprecht",
            "David Novotny"
          ],
          "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_VGGSfM_Visual_Geometry_Grounded_Deep_Structure_From_Motion_CVPR_2024_paper.html",
          "ref_texts": "[25] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In arxiv, 2023. 1, 2, 3, 4, 6, 7, 8",
          "ref_ids": [
            "25"
          ],
          "1": "Stateof-the-art methods [25, 39] follow the incremental SfM paradigm whose origins can be traced back to the early 2000s [23, 58].",
          "2": "Detector-free SfM [25] builds a coarse SfM model through quantized detector-free matches and then iteratively refines it with multi-view consistency constraints.",
          "3": "Recent state-of-the-art methods are PixSfM [39] and the concurrent Detector-free SfM (DFSfM) [25].",
          "4": "Traditionally, SfM frameworks first estimate pairwise image-to-image correspondences that are later chained into multi-image tracks T [25, 39, 59].",
          "5": "Following prior work [25, 39, 74], we evaluate camera pose estimation on Co3Dv2 [55] and IMC Phototourism datasets [31], and 3D triangulation on ETH3D [60].",
          "6": "For the model evaluated on IMC Phototourism and ETH3D, we follow the protocol of [25, 39, 57] and train on the MegaDepth dataset [35].",
          "7": "As in prior work [25, 40], some scenes of MegaDepth are excluded from training due to their low quality or due to overlap with the IMC test set.",
          "8": "Camera pose estimation Following [25, 31, 39, 74], we report the metric area-undercurve (AUC) to evaluate camera pose accuracy.",
          "9": "The results of Incremental SfM methods are copied from Detector-free SfM [25].",
          "10": "3D triangulation We evaluate the accuracy and completeness of 3D triangulation on the ETH3D dataset [60] using the same protocol as [17, 25, 39], which triangulates points with fixed camera poses and intrinsics.",
          "11": "Our VGGSfM achieves better accuracy and completeness than all baselines (PatchFlow [17], PixSfM [39], and DFSfM [25]), regardless of which keypoint detection or matching method they use."
        },
        "Local feature matching using deep learning: A survey": {
          "authors": [
            "S Xu",
            "S Chen",
            "R Xu",
            "C Wang",
            "P Lu",
            "L Guo"
          ],
          "url": "https://www.sciencedirect.com/science/article/pii/S1566253524001222",
          "ref_texts": "[182] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, X. Zhou, Detectorfree structure from motion, arXiv preprint arXiv:2306.15669 (2023).",
          "ref_ids": [
            "182"
          ],
          "1": "[182] introduce an innovative SfM paradigm devoid of detectors, harnessing detector-free matchers to defer the determination of keypoints.",
          "2": "Table 9 focuses on the ETH3D [239], presenting a detailed evaluation of various SfM methods as reported in the DetectorFreeSfM [182].",
          "3": "The results are derived from the DetectorFreeSfM [182].",
          "4": "28 Detector-free LoFTR [72]+DetectorFreeSfM [182] 59.",
          "5": "54 ASpanFormer [73]+DetectorFreeSfM [182] 57.",
          "6": "42 MatchFormer [162]+DetectorFreeSfM [182] 56."
        },
        "Robust Incremental Structure-from-Motion with Hybrid Features": {
          "authors": [
            "S Liu",
            "Y Gao",
            "T Zhang",
            "R Pautrat"
          ],
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-72764-1_15",
          "ref_texts": "33. He, X., Sun, J., Wang, Y., Peng, S., Huang, Q., Bao, H., Zhou, X.: Detector-free structure from motion. arXiv preprint arXiv:2306.15669 (2023)",
          "ref_ids": [
            "33"
          ],
          "1": "Improvements on using pixel-perfect features [54] and semi-dense matching [33] show great potential and could be combined with our work."
        },
        "Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos": {
          "authors": [
            "Z Li",
            "R Tucker",
            "F Cole",
            "Q Wang",
            "L Jin",
            "V Ye"
          ],
          "url": "https://arxiv.org/abs/2412.04463",
          "ref_texts": "[18] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21594\u201321603, 2024. 2",
          "ref_ids": [
            "18"
          ],
          "1": "Recently, deep visual SLAM and SfM systems have emerged that adopt deep neural networks to estimate pairwise or long-term correspondences [2, 7, 18, 19, 21, 54, 57, 59, 60, 63, 65, 73], to reconstruct radiance fields [11, 33, 41] or global 3D point clouds [28, 66]."
        },
        "Envgs: Modeling view-dependent appearance with environment gaussian": {
          "authors": [
            "T Xie",
            "X Chen",
            "Z Xu",
            "Y Xie",
            "Y Jin",
            "Y Shen"
          ],
          "url": "https://arxiv.org/abs/2412.15215",
          "ref_texts": "[10] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21594\u201321603, 2024. 5",
          "ref_ids": [
            "10"
          ],
          "1": "Optimization To enhance training stability, we initiate optimization by first training the base Gaussian Pbase, which is initialized using the sparse point cloud obtained from Structure-fromMotion (SfM) [10, 30]."
        },
        "LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural Wireframe Alignment": {
          "authors": [
            "J Zhu",
            "S Yan",
            "L Wang",
            "Y Liu"
          ],
          "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/d78ece6613953f46501b958b7bb4582f-Abstract-Conference.html",
          "ref_texts": "[33] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou. Detector-free structure from motion. arXiv preprint arXiv:2306.15669, 2023.",
          "ref_ids": [
            "33"
          ],
          "1": "However, building high-quality 3D maps using photogrammetry [12, 28, 64, 58, 33] is expensive on a global scale and requires frequent updates to account for temporal changes in visual appearance."
        },
        "Ct-nerf: Incremental optimizing neural radiance field and poses with complex trajectory": {
          "authors": [
            "Y Ran",
            "Y Li",
            "Q Ye",
            "Y Huo",
            "Z Bai",
            "J Sun"
          ],
          "url": "https://arxiv.org/abs/2404.13896",
          "ref_texts": "[16] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. 2023. Detector-Free Structure from Motion. In arxiv.",
          "ref_ids": [
            "16"
          ],
          "1": "Detector-free SfM [16] leverages the matching strategy of Loftr [30] without feature detectors, exhibiting significant advantages in low-texture regions and winning multiple competitions."
        },
        "MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion": {
          "authors": [
            "B Duisterhof",
            "L Zust",
            "P Weinzaepfel",
            "V Leroy"
          ],
          "url": "https://arxiv.org/abs/2409.19152",
          "ref_texts": "[20] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. InCVPR, 2024. 1, 2, 3, 7, 8, 14, 15",
          "ref_ids": [
            "20"
          ],
          "1": "Likewise, detector-free SfM [20] replaces the keypoint extraction and matching step of the classical pipeline with learned components.",
          "4": "Likewise, DF-SfM [20] improves for texture-less scenes thanks to relying on trainable dense pairwise matchers, but sticks to the overall COLMAP pipeline."
        },
        "RESFM: Robust Equivariant Multiview Structure from Motion": {
          "authors": [
            "F Khatib",
            "Y Kasten",
            "D Moran",
            "M Galun"
          ],
          "url": "https://arxiv.org/abs/2404.14280",
          "ref_texts": "11. He, X., Sun, J., Wang, Y., Peng, S., Huang, Q., Bao, H., Zhou, X.: Detector-free structure from motion. arXiv preprint arXiv:2306.15669 (2023)",
          "ref_ids": [
            "11"
          ],
          "1": "Existing methods attempt to improve keypoint detection and matching [8,19,20,29,34], produce point tracks [11] formulate differentiable alternatives to RANSAC [35,41,44,45], or directly infer the relative orientation and location of a pair of images [3,5,14,16,28]."
        },
        "MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training": {
          "authors": [
            "X He",
            "H Yu",
            "S Peng",
            "D Tan",
            "Z Shen",
            "H Bao"
          ],
          "url": "https://arxiv.org/abs/2501.07556",
          "ref_texts": "[25] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. CVPR, 2024. 11, 15, 17, 22",
          "ref_ids": [
            "25"
          ],
          "1": "To rectify this issue, we further refine the merged point trajectories to achieve sub-pixel accuracy using a transformer-based multi-view refinement approach [25].",
          "2": "Since the ground truth camera pose is not provided, we first leverage the current state-of-the-art SfM method [25] to recover camera poses using the visible light image sequences.",
          "3": "These trajectories are then refined for high accuracy by off-the-shelf multi-view refinement model [25].",
          "4": "To correct this, we perform (4) multi-view refinement of the entire trajectory using a transformer-based approach [25], achieving precise trajectories."
        },
        "DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection": {
          "authors": [
            "J Edstedt",
            "G B\u00f6kman",
            "M Wadenb\u00e4ck"
          ],
          "url": "https://arxiv.org/abs/2503.07347",
          "ref_texts": "[22] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21594\u201321603, 2024. 2",
          "ref_ids": [
            "22"
          ],
          "1": "construction, while performant [22], becomes computationally cumbersome and adds additional complexity."
        },
        "Light3R-SfM: Towards Feed-forward Structure-from-Motion": {
          "authors": [
            "S Elflein",
            "Q Zhou",
            "S Agostinho",
            "L Leal-Taix\u00e9"
          ],
          "url": "https://arxiv.org/abs/2501.14914",
          "ref_texts": "[18] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21594\u201321603, 2024. 2, 5",
          "ref_ids": [
            "18"
          ],
          "1": "DFSfM [18] adapts traditional keypointbased SfM for leveraging dense feature matchers [13, 14, 42, 59].",
          "2": "For optimization-based methods, we consider the classical SfM pipelines Colmap [37] (with SuperPoint [10] and SuperGlue [35]), DF-SfM [18], Glomap [30], PixelSfM [25] and VGGSfM [50], the end-to-end SfM including ACE-Zero [7] and FlowMap [40], as well as the recent state-of-the-art MASt3R-SfM [12]."
        },
        "GV-Bench: Benchmarking Local Feature Matching for Geometric Verification of Long-term Loop Closure Detection": {
          "authors": [
            "J Yu",
            "H Ye",
            "J Jiao",
            "P Tan"
          ],
          "url": "https://ieeexplore.ieee.org/abstract/document/10801481/",
          "ref_texts": "[29] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou, \u201cDetector-free structure from motion,\u201d arXiv preprint arXiv:2306.15669, 2023.",
          "ref_ids": [
            "29"
          ],
          "1": "Our results are consistent with [29] that LoFTR works especially well under texture-less situations."
        },
        "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360 Cameras": {
          "authors": [
            "D Jung",
            "J Choi",
            "Y Lee",
            "D Manocha"
          ],
          "url": "https://arxiv.org/abs/2502.12545",
          "ref_texts": "[24] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. In Proceedings of the IEEE/CVF Con9 ference on Computer Vision and Pattern Recognition, pages 21594\u201321603, 2024. 2",
          "ref_ids": [
            "24"
          ],
          "2": "DetectorFreeSfM [24] builds these detectorfree matches [53] and refines the tracks and geometry of the coarse SfM model by enforcing multi-view consistency."
        },
        "OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes from Monocular RGB": {
          "authors": [
            "Y Lin",
            "Y Zhao",
            "FJ Chu",
            "X Chen",
            "W Wang"
          ],
          "url": "https://arxiv.org/abs/2410.06694",
          "ref_texts": "[Online]. Available: http://www.blender.org [47] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou, \u201cDetector-Free structure from motion,\u201d in arxiv, 2023.",
          "ref_ids": [
            "Online",
            "47"
          ],
          "1": "To ensure adequate useful keypoints in the processed inputs, we employ a co-visibility approach inspired by [47].",
          "2": "We also compute the visibility of each keypoint by using the projected depth error and cycle projection error mentioned in [47]."
        },
        "Dense-SfM: Structure from Motion with Dense Consistent Matching": {
          "authors": [
            "JM Lee",
            "S Yoo"
          ],
          "url": "https://arxiv.org/abs/2501.14277",
          "ref_texts": "[24] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. CVPR, 2024. 1, 2, 3, 4, 5, 6, 7, 8",
          "ref_ids": [
            "24"
          ],
          "3": "To solve this inconsistency issue, [24] applied quantization to the matched keypoints, and then refined the multi-view tracks to obtain accurate camera pose and point cloud.",
          "4": "To solve this, [24] introduced quantization matching that merges nearby subpixel matches into grid nodes, improving track consistency.",
          "5": "The current state-of-the-art SfM refinement method, [24], introduces a multi-view matching module that iteratively refines the tracks obtained from the COLMAP framework.",
          "15": "Also, compared to the RoMa + DFSfM, where we apply quantization on dense matching and refine the tracks as in [24], our method achieves higher performance in terms of both accuracy and completeness, highlighting the effectiveness of our SfM framework."
        },
        "Mobile Augmented Reality Framework with Fusional Localization and Pose Estimation": {
          "authors": [
            "S Hou",
            "F Lin",
            "Y Huang",
            "Z Peng",
            "B Xiao"
          ],
          "url": "https://arxiv.org/abs/2501.03336",
          "ref_texts": "[28] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou, \u201cDetector-free structure from motion,\u201d arXiv.org, vol. abs/2306.15669, 2023.",
          "ref_ids": [
            "28"
          ],
          "1": "[28] proposed a framework without keypoint detection and achieved enhanced performance in texture-poor scenes."
        },
        "PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence": {
          "authors": [
            "Z Chen",
            "J Yang",
            "H Yang"
          ],
          "url": "https://arxiv.org/abs/2411.16877",
          "ref_texts": "[24] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free structure from motion. CVPR, 2024. 2",
          "ref_ids": [
            "24"
          ],
          "1": "Recent advancements favor prior-based reconstruction approaches that leverage generalizable learned priors [7, 15, 24, 47, 53, 59, 70, 75]."
        },
        "Bundle Adjustment in the Eager Mode": {
          "authors": [
            "Z Zhan",
            "H Xu",
            "Z Fang",
            "X Wei",
            "Y Hu",
            "C Wang"
          ],
          "url": "https://arxiv.org/abs/2409.12190",
          "ref_texts": "[2] X. He, J. Sun, Y . Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou, \u201cDetector-free structure from motion,\u201d CVPR, 2024.",
          "ref_ids": [
            "2"
          ],
          "1": "I NTRODUCTION Bundle adjustment (BA) is a fundamental technique in 3D vision, playing a crucial role in various applications such as virtual reality [1], photogrammetry [2], and simultaneous localization and mapping (SLAM) [3].",
          "2": "[2] X."
        }
      }
    }
  ]
  var barDiv = document.getElementById("bar");
  for (var paper of papers) {
    count++;
    var flag = 0;

    citations = paper.citations
    for (var citationName in citations) {
      if (citations.hasOwnProperty(citationName)) {
        var citation = citations[citationName];
        for (author of citation.authors) {
          if (author in fellows) {
            flag = 1;
            break;
          }
        }
      }
      if (flag == 1) {
        break;
      }
    }
    if (flag == 1) {
      var content = "<div class='item item-fellow' id='" + count + "'>" + paper.title + "</div>"
    }
    else {
      var content = "<div class='item' id='" + count + "'>" + paper.title + "</div>"
    }

    var objDiv = document.createElement("div");
    objDiv.innerHTML = content
    barDiv.appendChild(objDiv);
  }



  // 主页面内容js

  count = 0;
  var wrapperDiv = document.getElementById("wrapper");
  for (var paper of papers) {
    count++;
    var content = "<div class='paper' id='" + count + "'> <h1>" + paper.title + "</h1>"
      + "<ol>"
    citations = paper.citations
    for (var citationName in citations) {
      if (citations.hasOwnProperty(citationName)) {
        var citation = citations[citationName];
        var flag = 0;

        // author
        for (author of citation.authors) {
          if (author in fellows) {
            flag = 1;
          }
        }
        if (flag == 0) {
          content += "<li><div class='name'><a href='" + citation.url + "' target='_blank'> " + citationName + "</a></div>"
          content += "<div class='author'>"
        }
        else {
          content += "<li><div class='name famous'><a href='" + citation.url + "'target='_blank'> " + citationName + "</a></div>"
          content += "<div class='author'>"
        }
        for (author of citation.authors) {
          if (author in fellows) {
            content += "<span>" + author + " (" + fellows[author] + ")</span> , "
          }
          else {
            content += author + ", "
          }
        }
        content += "</div>"
        if (flag == 0) {
          content += "<div class='comment'><ul>"
        }
        else {
          content += "<div class='comment comment-fellows'><ul>"
        }

        // ref contents
        ref_ids = citation.ref_ids;
        console.log(citation)
        for (var key in citation) {
          var inputString = citation[key];
          console.log(inputString)
          if (citation.hasOwnProperty(key) && !isNaN(key)) {
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\[${number}\\]`, 'g');
              inputString = inputString.replace(regex, `<span>[${number}]</span>`);

            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\[${number},`, 'g');
              inputString = inputString.replace(regex, `[<span>${number}</span>,`);

            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number}\\]`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span>]`);

            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number},`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span>,`);

            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\s${number},`, 'g');
              inputString = inputString.replace(regex, ` <span>${number}</span>,`);
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number}\\s`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span> `);
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\s${number}\\]`, 'g');
              inputString = inputString.replace(regex, ` <span>${number}</span>]`);
            });
            content += "<li>" + inputString + "</li>";
          }
        }

        content += "</ul></div></li>"
      }
    }
    content += "</ol></div>";
    var objDiv = document.createElement("div");
    objDiv.innerHTML = content
    wrapperDiv.appendChild(objDiv);
  }


  // 点击切换
  const sidebarItems = document.querySelectorAll('.item');
  const contentItems = document.querySelectorAll('.paper');

  sidebarItems.forEach((item, index) => {
    item.addEventListener('click', () => {
      contentItems.forEach(contentItem => {
        contentItem.classList.add('hidden');
      });
      contentItems[index].classList.remove('hidden');

      sidebarItems.forEach(sidebarItem => {
        sidebarItem.classList.remove('active');
      });

      item.classList.add('active');


    });
  });

</script>

<style>
  ul li span {
    color: #f0d002;
    font-weight: 700;
  }

  #bar {
    position: fixed;
    top: 30px;
    left: 0;
    width: 22vw;
    padding-right: 1vw;
    padding-left: 2vw;
    height: 95vh;
    overflow: auto
  }

  .item {
    margin-bottom: 10px;
    border-radius: 10px;
    border: #fff 2px solid;
    padding: 5px;
    padding-left: 10px;
    transition: 0.3s;

  }

  .item:hover {

    border-color: rgb(12, 119, 186);
  }

  .active {
    border-color: rgb(12, 119, 186);
  }

  body {
    background-color: rgb(231, 231, 231);
  }

  #wrapper {

    margin-left: 26vw;
    margin-right: 2vw;
    background-color: #fff;
    padding: 40px;
  }

  .famous {
    color: rgb(12, 119, 186);
    font-weight: 800;

  }

  .item-fellow {
    color: rgb(12, 119, 186);
    font-weight: 800;
  }

  .author span {
    /* color: rgb(12, 119, 186); */
    font-weight: 800;
  }

  .comment-fellows {
    border-left: solid rgb(12, 119, 186) 2px !important;
  }

  a {
    text-decoration: none;
    color: inherit;
    transition: 0.3s;
    display: inline-block;
    font-weight: 600;
    font-size: 19px;
  }

  a:hover {
    font-size: 21px;
    font-weight: 900 !important;
  }

  li div {
    margin-bottom: 15px;
    font-size: 17px
  }

  ol>li {
    margin-bottom: 40px;
  }

  .comment {
    max-height: 300px;
    overflow: auto;
    border-left: solid grey;
  }

  .author {
    font-style: italic;
    color: rgb(81, 81, 81)
  }

  .hidden {
    display: none;
  }

  ol.no-number {
    list-style-type: none;
    padding-left: 0;
    /* 可选，去除默认的缩进 */
  }
</style>

</html>